{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Полносвязная нейронная сеть ( Fully-Connected Neural Network)\n",
    "\n",
    "2) Нормализация по мини-батчам (Batch normalization)\n",
    "\n",
    "3) Dropout\n",
    "\n",
    "4) Сверточные нейронные сети (Convolutional Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторные работы можно выполнять с использованием сервиса Google Colaboratory (https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) или на локальном компьютере. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полносвязная нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе необходимо будет реализовать полносвязную нейронную сеть, используя модульный подход. Для каждого  слоя реализации прямого и обратного проходов алгоритма обратного распространения ошибки будут иметь следующий вид:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.classifiers.fc_net import *\n",
    "\n",
    "from scripts.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from scripts.solver import Solver\n",
    "from scripts.classifiers.cnn import *\n",
    "from scripts.layers import *\n",
    "from scripts.fast_layers import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "def print_mean_std(x,axis=0):\n",
    "    print('  means: ', x.mean(axis=axis))\n",
    "    print('  stds:  ', x.std(axis=axis))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите данные из предыдущей лабораторной работы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (1077, 64)\n",
      "Размер валидационной выборки: (360, 64)\n",
      "Размер тестовой выборки: (360, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загрузим датасет\n",
    "digits = load_digits()\n",
    "\n",
    "# Создаем словарь для хранения индексов изображений по классам\n",
    "class_indices = {}\n",
    "for i in range(10):\n",
    "    class_indices[i] = []\n",
    "\n",
    "# Заполняем словарь\n",
    "for idx, label in enumerate(digits.target):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# # Выводим примеры изображений\n",
    "# num_examples = 5  # Количество примеров для каждого класса\n",
    "# for label, indices in class_indices.items():\n",
    "#     plt.figure(figsize=(12, 3))\n",
    "#     plt.suptitle(f\"Examples of class {label}\")\n",
    "\n",
    "#     for i, idx in enumerate(indices[:num_examples]):\n",
    "#         plt.subplot(1, num_examples, i + 1)\n",
    "#         plt.imshow(digits.images[idx], cmap='gray')\n",
    "#         plt.title(f\"Example {i + 1}\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "X = digits.images.reshape((len(digits.images), -1)) # Преобразуем изображения в одномерный массив\n",
    "y = digits.target\n",
    "\n",
    "# Разделение данных на обучающую, тестовую и валидационную выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Преобразуем в одномерный массив\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "X_test_flat = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train_flat.shape}\")\n",
    "print(f\"Размер валидационной выборки: {X_val_flat.shape}\")\n",
    "print(f\"Размер тестовой выборки: {X_test_flat.shape}\")\n",
    "\n",
    "data = {}\n",
    "data['X_train'] = X_train_flat\n",
    "data['y_train'] = y_train\n",
    "data['X_val'] = X_val_flat\n",
    "data['y_val'] = y_val\n",
    "data['X_test'] = X_test_flat\n",
    "data['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для полносвязного слоя реализуйте прямой проход (метод affine_forward в scripts/layers.py). Протестируйте свою реализацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.769849468192957e-10\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для полносвязного слоя реализуйте обратный проход (метод affine_backward в scripts/layers.py). Протестируйте свою реализацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  5.399100368651805e-11\n",
      "dw error:  9.904211865398145e-11\n",
      "db error:  2.4122867568119087e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте прямой проход для слоя активации ReLU (relu_forward) и протестируйте его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте обратный проход для слоя активации ReLU (relu_backward ) и протестируйте его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.2756349136310288e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В скрипте /layer_utils.py приведены реализации прямого и обратного проходов для часто используемых комбинаций слоев. Например, за полносвязным слоем часто следует слой активации. Ознакомьтесь с функциями affine_relu_forward и affine_relu_backward, запустите код ниже и убедитесь, что ошибка порядка e-10 или ниже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward and affine_relu_backward:\n",
      "dx error:  2.299579177309368e-11\n",
      "dw error:  8.162011105764925e-11\n",
      "db error:  7.826724021458994e-12\n"
     ]
    }
   ],
   "source": [
    "from scripts.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "# Relative error should be around e-10 or less\n",
    "print('Testing affine_relu_forward and affine_relu_backward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте двухслойную полносвязную сеть - класс TwoLayerNet в scripts/classifiers/fc_net.py . Проверьте свою реализацию, запустив код ниже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.83e-08\n",
      "W2 relative error: 3.12e-10\n",
      "b1 relative error: 9.83e-09\n",
      "b2 relative error: 4.33e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 2.53e-07\n",
      "W2 relative error: 2.85e-08\n",
      "b1 relative error: 1.56e-08\n",
      "b2 relative error: 7.76e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "# Errors should be around e-7 or less\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ознакомьтесь с API для обучения и тестирования моделей в scripts/solver.py . Используйте экземпляр класса Solver для обучения двухслойной полносвязной сети. Необходимо достичь минимум 50% верно классифицированных объектов на валидационном наборе. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 860) loss: 2.298009\n",
      "(Epoch 0 / 20) train acc: 0.154000; val_acc: 0.144444\n",
      "(Iteration 11 / 860) loss: 2.299667\n",
      "(Iteration 21 / 860) loss: 2.305219\n",
      "(Iteration 31 / 860) loss: 2.297263\n",
      "(Iteration 41 / 860) loss: 2.291163\n",
      "(Epoch 1 / 20) train acc: 0.306000; val_acc: 0.302778\n",
      "(Iteration 51 / 860) loss: 2.280054\n",
      "(Iteration 61 / 860) loss: 2.284139\n",
      "(Iteration 71 / 860) loss: 2.267123\n",
      "(Iteration 81 / 860) loss: 2.261771\n",
      "(Epoch 2 / 20) train acc: 0.406000; val_acc: 0.413889\n",
      "(Iteration 91 / 860) loss: 2.252632\n",
      "(Iteration 101 / 860) loss: 2.267866\n",
      "(Iteration 111 / 860) loss: 2.250431\n",
      "(Iteration 121 / 860) loss: 2.244733\n",
      "(Epoch 3 / 20) train acc: 0.524000; val_acc: 0.480556\n",
      "(Iteration 131 / 860) loss: 2.239884\n",
      "(Iteration 141 / 860) loss: 2.228506\n",
      "(Iteration 151 / 860) loss: 2.221209\n",
      "(Iteration 161 / 860) loss: 2.225644\n",
      "(Iteration 171 / 860) loss: 2.210030\n",
      "(Epoch 4 / 20) train acc: 0.538000; val_acc: 0.572222\n",
      "(Iteration 181 / 860) loss: 2.194340\n",
      "(Iteration 191 / 860) loss: 2.171627\n",
      "(Iteration 201 / 860) loss: 2.189306\n",
      "(Iteration 211 / 860) loss: 2.177770\n",
      "(Epoch 5 / 20) train acc: 0.649000; val_acc: 0.644444\n",
      "(Iteration 221 / 860) loss: 2.185206\n",
      "(Iteration 231 / 860) loss: 2.152867\n",
      "(Iteration 241 / 860) loss: 2.138809\n",
      "(Iteration 251 / 860) loss: 2.128174\n",
      "(Epoch 6 / 20) train acc: 0.682000; val_acc: 0.675000\n",
      "(Iteration 261 / 860) loss: 2.119187\n",
      "(Iteration 271 / 860) loss: 2.110672\n",
      "(Iteration 281 / 860) loss: 2.139272\n",
      "(Iteration 291 / 860) loss: 2.067159\n",
      "(Iteration 301 / 860) loss: 2.081692\n",
      "(Epoch 7 / 20) train acc: 0.665000; val_acc: 0.650000\n",
      "(Iteration 311 / 860) loss: 2.087539\n",
      "(Iteration 321 / 860) loss: 2.011454\n",
      "(Iteration 331 / 860) loss: 2.041154\n",
      "(Iteration 341 / 860) loss: 2.032918\n",
      "(Epoch 8 / 20) train acc: 0.679000; val_acc: 0.647222\n",
      "(Iteration 351 / 860) loss: 2.031203\n",
      "(Iteration 361 / 860) loss: 2.002337\n",
      "(Iteration 371 / 860) loss: 2.008576\n",
      "(Iteration 381 / 860) loss: 1.936594\n",
      "(Epoch 9 / 20) train acc: 0.659000; val_acc: 0.641667\n",
      "(Iteration 391 / 860) loss: 1.983378\n",
      "(Iteration 401 / 860) loss: 1.982783\n",
      "(Iteration 411 / 860) loss: 1.951251\n",
      "(Iteration 421 / 860) loss: 1.930872\n",
      "(Epoch 10 / 20) train acc: 0.628000; val_acc: 0.627778\n",
      "(Iteration 431 / 860) loss: 1.943698\n",
      "(Iteration 441 / 860) loss: 1.877386\n",
      "(Iteration 451 / 860) loss: 1.789888\n",
      "(Iteration 461 / 860) loss: 1.794005\n",
      "(Iteration 471 / 860) loss: 1.925258\n",
      "(Epoch 11 / 20) train acc: 0.727000; val_acc: 0.750000\n",
      "(Iteration 481 / 860) loss: 1.799003\n",
      "(Iteration 491 / 860) loss: 1.797163\n",
      "(Iteration 501 / 860) loss: 1.712444\n",
      "(Iteration 511 / 860) loss: 1.674978\n",
      "(Epoch 12 / 20) train acc: 0.725000; val_acc: 0.750000\n",
      "(Iteration 521 / 860) loss: 1.648547\n",
      "(Iteration 531 / 860) loss: 1.621177\n",
      "(Iteration 541 / 860) loss: 1.515679\n",
      "(Iteration 551 / 860) loss: 1.620465\n",
      "(Epoch 13 / 20) train acc: 0.774000; val_acc: 0.772222\n",
      "(Iteration 561 / 860) loss: 1.590011\n",
      "(Iteration 571 / 860) loss: 1.628914\n",
      "(Iteration 581 / 860) loss: 1.541515\n",
      "(Iteration 591 / 860) loss: 1.456890\n",
      "(Iteration 601 / 860) loss: 1.372096\n",
      "(Epoch 14 / 20) train acc: 0.789000; val_acc: 0.777778\n",
      "(Iteration 611 / 860) loss: 1.465770\n",
      "(Iteration 621 / 860) loss: 1.589928\n",
      "(Iteration 631 / 860) loss: 1.440049\n",
      "(Iteration 641 / 860) loss: 1.444711\n",
      "(Epoch 15 / 20) train acc: 0.808000; val_acc: 0.808333\n",
      "(Iteration 651 / 860) loss: 1.363537\n",
      "(Iteration 661 / 860) loss: 1.230943\n",
      "(Iteration 671 / 860) loss: 1.375348\n",
      "(Iteration 681 / 860) loss: 1.384221\n",
      "(Epoch 16 / 20) train acc: 0.832000; val_acc: 0.811111\n",
      "(Iteration 691 / 860) loss: 1.347358\n",
      "(Iteration 701 / 860) loss: 1.194724\n",
      "(Iteration 711 / 860) loss: 1.168274\n",
      "(Iteration 721 / 860) loss: 1.420021\n",
      "(Iteration 731 / 860) loss: 1.188360\n",
      "(Epoch 17 / 20) train acc: 0.842000; val_acc: 0.861111\n",
      "(Iteration 741 / 860) loss: 1.264103\n",
      "(Iteration 751 / 860) loss: 1.124008\n",
      "(Iteration 761 / 860) loss: 1.314629\n",
      "(Iteration 771 / 860) loss: 0.987775\n",
      "(Epoch 18 / 20) train acc: 0.846000; val_acc: 0.858333\n",
      "(Iteration 781 / 860) loss: 1.223582\n",
      "(Iteration 791 / 860) loss: 1.111390\n",
      "(Iteration 801 / 860) loss: 1.025531\n",
      "(Iteration 811 / 860) loss: 1.039951\n",
      "(Epoch 19 / 20) train acc: 0.873000; val_acc: 0.863889\n",
      "(Iteration 821 / 860) loss: 1.062212\n",
      "(Iteration 831 / 860) loss: 0.954106\n",
      "(Iteration 841 / 860) loss: 0.881551\n",
      "(Iteration 851 / 860) loss: 0.937832\n",
      "(Epoch 20 / 20) train acc: 0.876000; val_acc: 0.883333\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "model = TwoLayerNet(input_dim=64, hidden_dim=100, num_classes=10, weight_scale=1e-2, reg=0.01)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "solver = Solver(model, data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуйте полносвязную сеть с произвольным числом скрытых слоев. Ознакомьтесь с классом FullyConnectedNet в scripts/classifiers/fc_net.py . Реализуйте инициализацию, прямой и обратный проходы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  2.3004790897684924\n",
      "W1 relative error: 1.48e-07\n",
      "W2 relative error: 2.21e-05\n",
      "W3 relative error: 3.53e-07\n",
      "b1 relative error: 5.38e-09\n",
      "b2 relative error: 2.09e-09\n",
      "b3 relative error: 5.80e-11\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  5.940411485412347\n",
      "W1 relative error: 6.59e-09\n",
      "W2 relative error: 5.91e-08\n",
      "W3 relative error: 3.80e-07\n",
      "b1 relative error: 1.48e-08\n",
      "b2 relative error: 1.72e-09\n",
      "b3 relative error: 1.80e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Most of the errors should be on the order of e-7 or smaller.   \n",
    "  # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "  # for the check when reg = 0.0\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте добиться эффекта переобучения на небольшом наборе изображений (например, 50). Используйте трехслойную сеть со 100 нейронами на каждом скрытом слое. Попробуйте переобучить сеть, достигнув 100 % accuracy за 20 эпох. Для этого поэкспериментируйте с параметрами weight_scale и learning_rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 4.367810\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.220000; val_acc: 0.113889\n",
      "(Epoch 2 / 20) train acc: 0.300000; val_acc: 0.222222\n",
      "(Epoch 3 / 20) train acc: 0.500000; val_acc: 0.336111\n",
      "(Epoch 4 / 20) train acc: 0.560000; val_acc: 0.333333\n",
      "(Epoch 5 / 20) train acc: 0.780000; val_acc: 0.488889\n",
      "(Iteration 11 / 40) loss: 1.527297\n",
      "(Epoch 6 / 20) train acc: 0.740000; val_acc: 0.447222\n",
      "(Epoch 7 / 20) train acc: 0.780000; val_acc: 0.525000\n",
      "(Epoch 8 / 20) train acc: 0.780000; val_acc: 0.527778\n",
      "(Epoch 9 / 20) train acc: 0.860000; val_acc: 0.500000\n",
      "(Epoch 10 / 20) train acc: 0.940000; val_acc: 0.586111\n",
      "(Iteration 21 / 40) loss: 0.764407\n",
      "(Epoch 11 / 20) train acc: 0.920000; val_acc: 0.600000\n",
      "(Epoch 12 / 20) train acc: 0.920000; val_acc: 0.538889\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.663889\n",
      "(Epoch 14 / 20) train acc: 0.960000; val_acc: 0.636111\n",
      "(Epoch 15 / 20) train acc: 0.980000; val_acc: 0.577778\n",
      "(Iteration 31 / 40) loss: 0.368204\n",
      "(Epoch 16 / 20) train acc: 0.940000; val_acc: 0.633333\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.658333\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.625000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.683333\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.672222\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "#weight_scales = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "#learning_rates = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "weight_scales = [1e-1]\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_losses = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for ws in weight_scales:\n",
    "        model = FullyConnectedNet([100, 100], input_dim=8*8, weight_scale=ws, dtype=np.float64)\n",
    "\n",
    "        solver = Solver(model, small_data,\n",
    "                        print_every=10, num_epochs=20, batch_size=25,\n",
    "                        update_rule='sgd',\n",
    "                        optim_config={\n",
    "                          'learning_rate': lr,\n",
    "                        }\n",
    "                 )\n",
    "        solver.train()\n",
    "        if solver.train_acc_history[-1] > best_acc:\n",
    "            best_losses = solver.loss_history\n",
    "\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторите эксперимент, описанный выше, для пятислойной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 76.174690\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.125000\n",
      "(Epoch 2 / 20) train acc: 0.160000; val_acc: 0.130556\n",
      "(Epoch 3 / 20) train acc: 0.200000; val_acc: 0.177778\n",
      "(Epoch 4 / 20) train acc: 0.220000; val_acc: 0.194444\n",
      "(Epoch 5 / 20) train acc: 0.480000; val_acc: 0.225000\n",
      "(Iteration 11 / 40) loss: 0.883140\n",
      "(Epoch 6 / 20) train acc: 0.620000; val_acc: 0.294444\n",
      "(Epoch 7 / 20) train acc: 0.720000; val_acc: 0.341667\n",
      "(Epoch 8 / 20) train acc: 0.780000; val_acc: 0.341667\n",
      "(Epoch 9 / 20) train acc: 0.800000; val_acc: 0.377778\n",
      "(Epoch 10 / 20) train acc: 0.880000; val_acc: 0.400000\n",
      "(Iteration 21 / 40) loss: 0.448808\n",
      "(Epoch 11 / 20) train acc: 0.920000; val_acc: 0.405556\n",
      "(Epoch 12 / 20) train acc: 0.900000; val_acc: 0.408333\n",
      "(Epoch 13 / 20) train acc: 0.940000; val_acc: 0.436111\n",
      "(Epoch 14 / 20) train acc: 0.960000; val_acc: 0.419444\n",
      "(Epoch 15 / 20) train acc: 0.960000; val_acc: 0.441667\n",
      "(Iteration 31 / 40) loss: 0.181716\n",
      "(Epoch 16 / 20) train acc: 0.960000; val_acc: 0.438889\n",
      "(Epoch 17 / 20) train acc: 0.980000; val_acc: 0.450000\n",
      "(Epoch 18 / 20) train acc: 0.980000; val_acc: 0.441667\n",
      "(Epoch 19 / 20) train acc: 0.980000; val_acc: 0.438889\n",
      "(Epoch 20 / 20) train acc: 0.980000; val_acc: 0.450000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAK9CAYAAAAuQ13kAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABHOklEQVR4nO3de5xVdaH38e8AziCXGYSEgURFLRXxkppKFzVBwQj1ZKnlBc2jhZjipTx2KsVTadYxJVN7eiorM83TUcOTFqHi0UgNj4WiHC8YlFwyYgY0QGf28wcv5nEEYpZu2Hvg/X695hWz9tq//durlfZh3WpKpVIpAAAAdFiXSk8AAACgsxFSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAdMipp56aHXfc8U2999JLL01NTU15J9RBb2XeG8v999+fmpqa/Md//McG163G+QMgpAA6vZqamg793H///ZWeKpvI7Nmzc+mll+aFF16o9FQANlvdKj0BAN6aH/3oR+1+/+EPf5ipU6eutXz33Xd/S5/zne98J62trW/qvZ///OfzL//yL2/p87dUb2a7z549O5MmTcqhhx7qaBbARiKkADq5k046qd3vv/3tbzN16tS1lr/RK6+8kh49enT4c7baaqs3Nb8k6datW7p186+cN+OtbPdyK7rPAGzOnNoHsAU49NBDM2zYsMycOTMHH3xwevTokc997nNJkjvvvDNjxozJoEGDUldXl5133jn/9m//lpaWlnZjvPFanRdeeCE1NTX5+te/nv/zf/5Pdt5559TV1eXd7353Hn300XbvXdc1UjU1NTn77LNzxx13ZNiwYamrq8see+yRe+65Z63533///dl///3TvXv37Lzzzvn2t7/9lq67evnll3PBBRdk8ODBqaury6677pqvf/3rKZVK7dabOnVq3ve+96VPnz7p1atXdt1117bttsY3v/nN7LHHHunRo0e22Wab7L///rn55ps7NI/W1tZ8+ctfznbbbZfu3btnxIgRefbZZ9uts65rpG655Zbst99+6d27d+rr67PnnnvmmmuuSZLceOON+ehHP5ok+cAHPrDOUzuvu+667LHHHqmrq8ugQYMyYcKELF26tN1nrG+fGTduXN72trfl1VdfXev7HHHEEdl111079N0BOjt/PQiwhfjrX/+aI488MieccEJOOumkDBgwIMnq/+Pdq1evnH/++enVq1fuvffefPGLX0xzc3O+9rWvbXDcm2++OcuWLcsnP/nJ1NTU5Morr8yHP/zhPP/88xs8mvLggw/mP//zP3PWWWeld+/emTx5co499tjMmzcv/fr1S5L8z//8T0aPHp2BAwdm0qRJaWlpyWWXXZZtt932TW2HUqmUo446Kvfdd19OP/307LPPPvnlL3+Zz3zmM/nzn/+cb3zjG0mSJ598Mh/60Iey11575bLLLktdXV2effbZPPTQQ21jfec738k555yTj3zkIzn33HOzYsWK/OEPf8jDDz+cj3/84xucyxVXXJEuXbrkwgsvTFNTU6688sqceOKJefjhh9f7nqlTp+ZjH/tYRowYka9+9atJkqeeeioPPfRQzj333Bx88ME555xzMnny5Hzuc59rO6VzzX9eeumlmTRpUkaOHJnx48dnzpw5uf766/Poo4/moYceavff2br2mZ49e+aHP/xhfvnLX+ZDH/pQ27oLFy7Mvffem0suuaTAfxsAnVgJgM3KhAkTSm/8x/shhxxSSlK64YYb1lr/lVdeWWvZJz/5yVKPHj1KK1asaFs2bty40g477ND2+9y5c0tJSv369SstWbKkbfmdd95ZSlKaMmVK27JLLrlkrTklKdXW1paeffbZtmW///3vS0lK3/zmN9uWjR07ttSjR4/Sn//857ZlzzzzTKlbt25rjbkub5z3HXfcUUpS+tKXvtRuvY985COlmpqatvl84xvfKCUp/eUvf1nv2EcffXRpjz322OAc3ui+++4rJSntvvvupZUrV7Ytv+aaa0pJSrNmzVrv/M8999xSfX196bXXXlvv+LfddlspSem+++5rt3zx4sWl2tra0hFHHFFqaWlpW37ttdeWkpS+973vtS1b3z7T0tJS2m677UrHH398u+VXXXVVqaampvT88893aBsAdHZO7QPYQtTV1eW0005ba/nWW2/d9udly5blpZdeyvvf//688sorefrppzc47vHHH59tttmm7ff3v//9SZLnn39+g+8dOXJkdt5557bf99prr9TX17e9t6WlJb/+9a9zzDHHZNCgQW3r7bLLLjnyyCM3OP66/OIXv0jXrl1zzjnntFt+wQUXpFQq5e67706S9OnTJ8nqUx/Xd7OHPn365E9/+tNapzJ21GmnnZba2tq23zuy7fr06ZOXX345U6dOLfx5v/71r7Nq1apMnDgxXbr8//8LcMYZZ6S+vj7/9V//1W79de0zXbp0yYknnpif//znWbZsWdvyH//4x3nPe96TIUOGFJ4XQGckpAC2EG9/+9vb/Z/2NZ588sn80z/9UxoaGlJfX59tt9227UYVTU1NGxx3++23b/f7mqj629/+Vvi9a96/5r2LFy/O3//+9+yyyy5rrbeuZR3xxz/+MYMGDUrv3r3bLV9z6tsf//jHJKsD8b3vfW/++Z//OQMGDMgJJ5yQn/70p+2i6qKLLkqvXr1ywAEH5B3veEcmTJjQ7tS/DXkz2+6ss87KO9/5zhx55JHZbrvt8olPfGKd15Wty5rv9sbrmGpra7PTTju1vb7G+vaZU045JX//+99z++23J0nmzJmTmTNn5uSTT+7QPAA2B0IKYAvx+iNPayxdujSHHHJIfv/73+eyyy7LlClTMnXq1LZrbzpy2+2uXbuuc3npDTduKPd7N7att946DzzwQH7961/n5JNPzh/+8Iccf/zxOfzww9tuxLH77rtnzpw5ueWWW/K+970vP/vZz/K+972vw9cJvZnv379//zz++OP5+c9/3nat15FHHplx48YV/5IbsK59JkmGDh2a/fbbLzfddFOS5KabbkptbW2OO+64ss8BoFoJKYAt2P3335+//vWvufHGG3PuuefmQx/6UEaOHNnuVL1K6t+/f7p3777WneySrHNZR+ywww558cUX252WlqTtNMYddtihbVmXLl0yYsSIXHXVVZk9e3a+/OUv59577819993Xtk7Pnj1z/PHH5/vf/37mzZuXMWPG5Mtf/nJWrFjxpubXEbW1tRk7dmyuu+66PPfcc/nkJz+ZH/7wh23bZH13M1zz3ebMmdNu+apVqzJ37tx2331DTjnllNx7771ZsGBBbr755owZM6Zq9huATUFIAWzB1hwRef0RkFWrVuW6666r1JTa6dq1a0aOHJk77rgjL774YtvyZ599tu1apqI++MEPpqWlJddee2275d/4xjdSU1PTdu3VkiVL1nrvPvvskyRZuXJlktV3tXu92traDB06NKVSaZ23By+HN35mly5dstdee7WbV8+ePZNkrVuajxw5MrW1tZk8eXK7/86/+93vpqmpKWPGjOnwPD72sY+lpqYm5557bp5//vkNPrcMYHPj9ucAW7D3vOc92WabbTJu3Licc845qampyY9+9KOqOLVujUsvvTS/+tWv8t73vjfjx49vi6Bhw4bl8ccfLzze2LFj84EPfCD/+q//mhdeeCF77713fvWrX+XOO+/MxIkT225+cdlll+WBBx7ImDFjssMOO2Tx4sW57rrrst122+V973tfktXPTWpsbMx73/veDBgwIE899VSuvfbajBkzZq1rsMrln//5n7NkyZIcdthh2W677fLHP/4x3/zmN7PPPvu0Xee1zz77pGvXrvnqV7+apqam1NXV5bDDDkv//v1z8cUXZ9KkSRk9enSOOuqozJkzJ9ddd13e/e53F4qhbbfdNqNHj85tt92WPn36FIowgM2BI1IAW7B+/frlrrvuysCBA/P5z38+X//613P44YfnyiuvrPTU2uy33365++67s8022+QLX/hCvvvd7+ayyy7LiBEj0r1798LjdenSJT//+c8zceLE3HXXXZk4cWJmz56dr33ta7nqqqva1jvqqKOy/fbb53vf+14mTJiQb33rWzn44INz7733pqGhIUnyyU9+MsuXL89VV12VCRMm5I477sg555zTdu3QxnDSSSele/fuue6663LWWWflBz/4QY4//vjcfffdbXfia2xszA033JDFixfn9NNPz8c+9rHMnj07yeowvfbaazNv3rycd955+elPf5ozzzwzv/rVrzb43K83OuWUU5Ikxx13XOrq6sr7RQGqXE2pmv7aEQA66JhjjsmTTz6ZZ555ptJT2WLdeeedOeaYY/LAAw+03bodYEvhiBQAVe/vf/97u9+feeaZ/OIXv8ihhx5amQmRJPnOd76TnXbaqe1UR4AtiWukAKh6O+20U0499dS2Zx1df/31qa2tzWc/+9lKT22LdMstt+QPf/hD/uu//ivXXHPNeu8SCLA5c2ofAFXvtNNOy3333ZeFCxemrq4uw4cPz1e+8pXsu+++lZ7aFqmmpia9evXK8ccfnxtuuCHduvl7WWDLI6QAAAAKco0UAABAQUIKAACgICc1J2ltbc2LL76Y3r17u2AWAAC2YKVSKcuWLcugQYPans+3LkIqyYsvvpjBgwdXehoAAECVmD9/frbbbrv1vi6kkvTu3TvJ6o1VX19f4dkAAACV0tzcnMGDB7c1wvoIqaTtdL76+nohBQAAbPCSHzebAAAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAV1q/QEqF4traU8MndJFi9bkf69u+eAIX3TtUtNpacFAAAVJ6RYp3ueWJBJU2ZnQdOKtmUDG7rnkrFDM3rYwArODAAAKs+pfazlnicWZPxNj7WLqCRZ2LQi4296LPc8saBCMwMAgOogpGinpbWUSVNmp7SO19YsmzRldlpa17UGAABsGYQU7Twyd8laR6Jer5RkQdOKPDJ3yaabFAAAVBkhRTuLl60/ot7MegAAsDkSUrTTv3f3sq4HAACbIyFFOwcM6ZuBDd2zvpuc12T13fsOGNJ3U04LAACqipCina5danLJ2KFJslZMrfn9krFDPU8KAIAtmpBiLaOHDcz1J+2bxob2p+81NnTP9Sft6zlSAABs8TyQl3UaPWxgDh/amEfmLsniZSvSv/fq0/kciQIAACHFP9C1S02G79yv0tMAAICq49Q+AACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAVVTUhdccUVqampycSJE9uWrVixIhMmTEi/fv3Sq1evHHvssVm0aFG7982bNy9jxoxJjx490r9//3zmM5/Ja6+9tolnDwAAbEmqIqQeffTRfPvb385ee+3Vbvl5552XKVOm5Lbbbsv06dPz4osv5sMf/nDb6y0tLRkzZkxWrVqV3/zmN/nBD36QG2+8MV/84hc39VcAAAC2IBUPqeXLl+fEE0/Md77znWyzzTZty5uamvLd7343V111VQ477LDst99++f73v5/f/OY3+e1vf5sk+dWvfpXZs2fnpptuyj777JMjjzwy//Zv/5ZvfetbWbVqVaW+EgAAsJmreEhNmDAhY8aMyciRI9stnzlzZl599dV2y3fbbbdsv/32mTFjRpJkxowZ2XPPPTNgwIC2dUaNGpXm5uY8+eST6/3MlStXprm5ud0PAABAR3Wr5Iffcssteeyxx/Loo4+u9drChQtTW1ubPn36tFs+YMCALFy4sG2d10fUmtfXvLY+l19+eSZNmvQWZw8AAGypKnZEav78+Tn33HPz4x//ON27d9+kn33xxRenqamp7Wf+/Pmb9PMBAIDOrWIhNXPmzCxevDj77rtvunXrlm7dumX69OmZPHlyunXrlgEDBmTVqlVZunRpu/ctWrQojY2NSZLGxsa17uK35vc166xLXV1d6uvr2/0AAAB0VMVCasSIEZk1a1Yef/zxtp/9998/J554Ytuft9pqq0ybNq3tPXPmzMm8efMyfPjwJMnw4cMza9asLF68uG2dqVOnpr6+PkOHDt3k3wkAANgyVOwaqd69e2fYsGHtlvXs2TP9+vVrW3766afn/PPPT9++fVNfX59Pf/rTGT58eA466KAkyRFHHJGhQ4fm5JNPzpVXXpmFCxfm85//fCZMmJC6urpN/p0AAIAtQ0VvNrEh3/jGN9KlS5cce+yxWblyZUaNGpXrrruu7fWuXbvmrrvuyvjx4zN8+PD07Nkz48aNy2WXXVbBWQMAAJu7mlKpVKr0JCqtubk5DQ0NaWpqcr0UAABswTraBhV/jhQAAEBnI6QAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoKCKhtT111+fvfbaK/X19amvr8/w4cNz9913t72+YsWKTJgwIf369UuvXr1y7LHHZtGiRe3GmDdvXsaMGZMePXqkf//++cxnPpPXXnttU38VAABgC1LRkNpuu+1yxRVXZObMmfnd736Xww47LEcffXSefPLJJMl5552XKVOm5Lbbbsv06dPz4osv5sMf/nDb+1taWjJmzJisWrUqv/nNb/KDH/wgN954Y774xS9W6isBAABbgJpSqVSq9CRer2/fvvna176Wj3zkI9l2221z88035yMf+UiS5Omnn87uu++eGTNm5KCDDsrdd9+dD33oQ3nxxRczYMCAJMkNN9yQiy66KH/5y19SW1vboc9sbm5OQ0NDmpqaUl9fv9G+GwAAUN062gZVc41US0tLbrnllrz88ssZPnx4Zs6cmVdffTUjR45sW2e33XbL9ttvnxkzZiRJZsyYkT333LMtopJk1KhRaW5ubjuqtS4rV65Mc3Nzux8AAICOqnhIzZo1K7169UpdXV0+9alP5fbbb8/QoUOzcOHC1NbWpk+fPu3WHzBgQBYuXJgkWbhwYbuIWvP6mtfW5/LLL09DQ0Pbz+DBg8v7pQAAgM1axUNq1113zeOPP56HH34448ePz7hx4zJ79uyN+pkXX3xxmpqa2n7mz5+/UT8PAADYvHSr9ARqa2uzyy67JEn222+/PProo7nmmmty/PHHZ9WqVVm6dGm7o1KLFi1KY2NjkqSxsTGPPPJIu/HW3NVvzTrrUldXl7q6ujJ/EwAAYEtR8SNSb9Ta2pqVK1dmv/32y1ZbbZVp06a1vTZnzpzMmzcvw4cPT5IMHz48s2bNyuLFi9vWmTp1aurr6zN06NBNPncAAGDLUNEjUhdffHGOPPLIbL/99lm2bFluvvnm3H///fnlL3+ZhoaGnH766Tn//PPTt2/f1NfX59Of/nSGDx+egw46KElyxBFHZOjQoTn55JNz5ZVXZuHChfn85z+fCRMmOOIEAABsNBUNqcWLF+eUU07JggUL0tDQkL322iu//OUvc/jhhydJvvGNb6RLly459thjs3LlyowaNSrXXXdd2/u7du2au+66K+PHj8/w4cPTs2fPjBs3LpdddlmlvhIAALAFqLrnSFWC50gBAABJJ3yOFAAAQGchpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgoG6VngDl19JayiNzl2TxshXp37t7DhjSN1271FR6WgAAsNkQUpuZe55YkElTZmdB04q2ZQMbuueSsUMzetjACs4MAAA2H07t24zc88SCjL/psXYRlSQLm1Zk/E2P5Z4nFlRoZgAAsHkRUpuJltZSJk2ZndI6XluzbNKU2WlpXdcaAABAEUJqM/HI3CVrHYl6vVKSBU0r8sjcJZtuUgAAsJkSUpuJxcvWH1FvZj0AAGD9hNRmon/v7mVdDwAAWD8htZk4YEjfDGzonvXd5Lwmq+/ed8CQvptyWgAAsFkSUpuJrl1qcsnYoUmyVkyt+f2SsUM9TwoAAMpASG1GRg8bmOtP2jeNDe1P32ts6J7rT9rXc6QAAKBMPJB3MzN62MAcPrQxj8xdksXLVqR/79Wn8zkSBQAA5SOkNkNdu9Rk+M79Kj0NAADYbDm1DwAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgoLccUs3Nzbnjjjvy1FNPlWM+AAAAVa9wSB133HG59tprkyR///vfs//+++e4447LXnvtlZ/97GdlnyAAAEC1KRxSDzzwQN7//vcnSW6//faUSqUsXbo0kydPzpe+9KWyTxAAAKDaFA6ppqam9O3bN0lyzz335Nhjj02PHj0yZsyYPPPMM2WfIAAAQLUpHFKDBw/OjBkz8vLLL+eee+7JEUcckST529/+lu7du5d9ggAAANWmW9E3TJw4MSeeeGJ69eqVHXbYIYceemiS1af87bnnnuWeHwAAQNUpHFJnnXVWDjjggMyfPz+HH354unRZfVBrp512co0UAACwRagplUqltzJAS0tLZs2alR122CHbbLNNuea1STU3N6ehoSFNTU2pr6+v9HQAAIAK6WgbFL5GauLEifnud7+bZHVEHXLIIdl3330zePDg3H///W96wgAAAJ1F4ZD6j//4j+y9995JkilTpmTu3Ll5+umnc9555+Vf//Vfyz5BAACAalM4pF566aU0NjYmSX7xi1/kox/9aN75znfmE5/4RGbNmlX2CQIAAFSbwiE1YMCAzJ49Oy0tLbnnnnty+OGHJ0leeeWVdO3atewTBAAAqDaF79p32mmn5bjjjsvAgQNTU1OTkSNHJkkefvjh7LbbbmWfIAAAQLUpHFKXXnpphg0blvnz5+ejH/1o6urqkiRdu3bNv/zLv5R9ggAAANXmLd/+fHPg9ucAAECyEW9/niTTp0/P2LFjs8suu2SXXXbJUUcdlf/+7/9+05MFAADoTAqH1E033ZSRI0emR48eOeecc3LOOedk6623zogRI3LzzTdvjDkCAABUlcKn9u2+++4588wzc95557VbftVVV+U73/lOnnrqqbJOcFNwah8AAJBsxFP7nn/++YwdO3at5UcddVTmzp1bdDgAAIBOp3BIDR48ONOmTVtr+a9//esMHjy4LJMCAACoZoVvf37BBRfknHPOyeOPP573vOc9SZKHHnooN954Y6655pqyTxAAAKDaFA6p8ePHp7GxMf/+7/+en/70p0lWXzd166235uijjy77BAEAAKqN50jFzSYAAIDVNupzpAAAALZkHTq1b5tttklNTU2HBlyyZMlbmhAAAEC161BIXX311Rt5GgAAAJ1Hh0Jq3LhxG3seAAAAnYZrpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoKAO3bXv9f7pn/5pnc+UqqmpSffu3bPLLrvk4x//eHbdddeyTBAAAKDaFD4i1dDQkHvvvTePPfZYampqUlNTk//5n//Jvffem9deey233npr9t577zz00EMbY74AAAAVV/iIVGNjYz7+8Y/n2muvTZcuqzustbU15557bnr37p1bbrkln/rUp3LRRRflwQcfLPuEAQAAKq2mVCqVirxh2223zUMPPZR3vvOd7Zb/7//+b97znvfkpZdeyqxZs/L+978/S5cuLedcN5rm5uY0NDSkqakp9fX1lZ4OAABQIR1tg8Kn9r322mt5+umn11r+9NNPp6WlJUnSvXv3dV5HBQAAsDkofGrfySefnNNPPz2f+9zn8u53vztJ8uijj+YrX/lKTjnllCTJ9OnTs8cee5R3pgAAAFWicEh94xvfyIABA3LllVdm0aJFSZIBAwbkvPPOy0UXXZQkOeKIIzJ69OjyzhQAAKBKFL5G6vWam5uTpNNfV+QaKQAAIOl4GxQ+IvV6ogMAANgSFb7ZxKJFi3LyySdn0KBB6datW7p27druBwAAYHNX+IjUqaeemnnz5uULX/hCBg4c6O58AADAFqdwSD344IP57//+7+yzzz4bYToAAADVr/CpfYMHD85buD8FAABAp1c4pK6++ur8y7/8S1544YWNMB0AAIDqV/jUvuOPPz6vvPJKdt555/To0SNbbbVVu9eXLFlStskBAABUo8IhdfXVV2+EaQAAAHQehUNq3LhxG2MeAAAAnUaHQqq5ubnt4bvNzc3/cF0P6QUAADZ3HQqpbbbZJgsWLEj//v3Tp0+fdT47qlQqpaamJi0tLWWfJAAAQDXpUEjde++96du3b5Lkvvvu26gTAgAAqHY1JQ+FSnNzcxoaGtLU1OTURAAA2IJ1tA0K32wiSZYuXZpHHnkkixcvTmtra7vXTjnllDczJAAAQKdROKSmTJmSE088McuXL099fX2766VqamqEFAAAsNnrUvQNF1xwQT7xiU9k+fLlWbp0af72t7+1/XgYLwAAsCUoHFJ//vOfc84556RHjx4bYz4AAABVr3BIjRo1Kr/73e82xlwAAAA6hcLXSI0ZMyaf+cxnMnv27Oy5557Zaqut2r1+1FFHlW1yAAAA1ajw7c+7dFn/QazO+kBetz8HAACSjXj78zfe7hwAAGBLU/gaKQAAgC1dh45ITZ48OWeeeWa6d++eyZMn/8N1zznnnLJMDAAAoFp16BqpIUOG5He/+1369euXIUOGrH+wmpo8//zzZZ3gpuAaKQAAICnzNVJz585d558BAAC2RK6RAgAAKKjwXfuS5E9/+lN+/vOfZ968eVm1alW716666qqyTAwAAKBaFQ6padOm5aijjspOO+2Up59+OsOGDcsLL7yQUqmUfffdd2PMEQAAoKoUPrXv4osvzoUXXphZs2ale/fu+dnPfpb58+fnkEMOyUc/+tGNMUcAAICqUjiknnrqqZxyyilJkm7duuXvf/97evXqlcsuuyxf/epXyz5BAACAalM4pHr27Nl2XdTAgQPz3HPPtb320ksvlW9mAAAAVarwNVIHHXRQHnzwwey+++754Ac/mAsuuCCzZs3Kf/7nf+aggw7aGHMEAACoKoVD6qqrrsry5cuTJJMmTcry5ctz66235h3veIc79gEAAFuEQiHV0tKSP/3pT9lrr72SrD7N74YbbtgoEwMAAKhWha6R6tq1a4444oj87W9/21jzAQAAqHqFbzYxbNiwPP/882X58Msvvzzvfve707t37/Tv3z/HHHNM5syZ026dFStWZMKECenXr1969eqVY489NosWLWq3zrx58zJmzJj06NEj/fv3z2c+85m89tprZZkjAADAGxUOqS996Uu58MILc9ddd2XBggVpbm5u91PE9OnTM2HChPz2t7/N1KlT8+qrr+aII47Iyy+/3LbOeeedlylTpuS2227L9OnT8+KLL+bDH/5w2+stLS0ZM2ZMVq1ald/85jf5wQ9+kBtvvDFf/OIXi341AACADqkplUqljqx42WWX5YILLkjv3r3//5tratr+XCqVUlNTk5aWljc9mb/85S/p379/pk+fnoMPPjhNTU3Zdtttc/PNN+cjH/lIkuTpp5/O7rvvnhkzZuSggw7K3XffnQ996EN58cUXM2DAgCTJDTfckIsuuih/+ctfUltbu8HPbW5uTkNDQ5qamlJfX/+m5w8AAHRuHW2DDt9sYtKkSfnUpz6V++67rywTXJempqYkSd++fZMkM2fOzKuvvpqRI0e2rbPbbrtl++23bwupGTNmZM8992yLqCQZNWpUxo8fnyeffDLvete71vqclStXZuXKlW2/Fz2SBgAAbNk6HFJrDlwdcsghG2Uira2tmThxYt773vdm2LBhSZKFCxemtrY2ffr0abfugAEDsnDhwrZ1Xh9Ra15f89q6XH755Zk0aVKZvwEAALClKHSN1OtP5Su3CRMm5Iknnsgtt9yy0T5jjYsvvjhNTU1tP/Pnz9/onwkAAGw+Cj1H6p3vfOcGY2rJkiWFJ3H22WfnrrvuygMPPJDtttuubXljY2NWrVqVpUuXtjsqtWjRojQ2Nrat88gjj7Qbb81d/das80Z1dXWpq6srPE8AAICkYEhNmjQpDQ0NZfvwUqmUT3/607n99ttz//33Z8iQIe1e32+//bLVVltl2rRpOfbYY5Mkc+bMybx58zJ8+PAkyfDhw/PlL385ixcvTv/+/ZMkU6dOTX19fYYOHVq2uQIAAKzR4bv2denSJQsXLmyLlXI466yzcvPNN+fOO+/Mrrvu2ra8oaEhW2+9dZJk/Pjx+cUvfpEbb7wx9fX1+fSnP50k+c1vfpNk9e3P99lnnwwaNChXXnllFi5cmJNPPjn//M//nK985Ssdmoe79gEAAEnH26DDIdW1a9csWLCgrCG1vtMEv//97+fUU09NsvqBvBdccEF+8pOfZOXKlRk1alSuu+66dqft/fGPf8z48eNz//33p2fPnhk3blyuuOKKdOvWsQNuQgoAAEg2QkhtjCNS1UJIAQAAyUZ4jlRra2tZJgYAANDZFbr9OQAAAEIKAACgMCEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAqqaEg98MADGTt2bAYNGpSamprccccd7V4vlUr54he/mIEDB2brrbfOyJEj88wzz7RbZ8mSJTnxxBNTX1+fPn365PTTT8/y5cs34bcAAAC2NBUNqZdffjl77713vvWtb63z9SuvvDKTJ0/ODTfckIcffjg9e/bMqFGjsmLFirZ1TjzxxDz55JOZOnVq7rrrrjzwwAM588wzN9VXAAAAtkA1pVKpVOlJJElNTU1uv/32HHPMMUlWH40aNGhQLrjgglx44YVJkqampgwYMCA33nhjTjjhhDz11FMZOnRoHn300ey///5JknvuuScf/OAH86c//SmDBg3q0Gc3NzenoaEhTU1Nqa+v3yjfDwAAqH4dbYOqvUZq7ty5WbhwYUaOHNm2rKGhIQceeGBmzJiRJJkxY0b69OnTFlFJMnLkyHTp0iUPP/zwesdeuXJlmpub2/0AAAB0VNWG1MKFC5MkAwYMaLd8wIABba8tXLgw/fv3b/d6t27d0rdv37Z11uXyyy9PQ0ND28/gwYPLPHsAAGBzVrUhtTFdfPHFaWpqavuZP39+pacEAAB0IlUbUo2NjUmSRYsWtVu+aNGittcaGxuzePHidq+/9tprWbJkSds661JXV5f6+vp2PwAAAB1VtSE1ZMiQNDY2Ztq0aW3Lmpub8/DDD2f48OFJkuHDh2fp0qWZOXNm2zr33ntvWltbc+CBB27yOQMAAFuGbpX88OXLl+fZZ59t+33u3Ll5/PHH07dv32y//faZOHFivvSlL+Ud73hHhgwZki984QsZNGhQ2539dt9994wePTpnnHFGbrjhhrz66qs5++yzc8IJJ3T4jn0AAABFVTSkfve73+UDH/hA2+/nn39+kmTcuHG58cYb89nPfjYvv/xyzjzzzCxdujTve9/7cs8996R79+5t7/nxj3+cs88+OyNGjEiXLl1y7LHHZvLkyZv8uwAAAFuOqnmOVCV5jhQAAJBsBs+RAgAAqFZCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICCulV6Avx/La2lPDJ3SRYvW5H+vbvngCF907VLTaWnBQAAvIGQqhL3PLEgk6bMzoKmFW3LBjZ0zyVjh2b0sIEVnBkAAPBGTu2rAvc8sSDjb3qsXUQlycKmFRl/02O554kFFZoZAACwLkKqwlpaS5k0ZXZK63htzbJJU2anpXVdawAAAJUgpCrskblL1joS9XqlJAuaVuSRuUs23aQAAIB/SEhV2OJl64+oN7MeAACw8QmpCuvfu3tZ1wMAADY+IVVhBwzpm4EN3bO+m5zXZPXd+w4Y0ndTTgsAAPgHhFSFde1Sk0vGDk2StWJqze+XjB3qeVIAAFBFhFQVGD1sYK4/ad80NrQ/fa+xoXuuP2lfz5ECAIAq44G8VWL0sIE5fGhjHpm7JIuXrUj/3qtP53MkCgAAqo+QqiJdu9Rk+M79Kj0NAABgA5zaBwAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIAQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAK6lbpCbD5a2kt5ZG5S7J42Yr07909Bwzpm65daio9LQAAeNOEFBvVPU8syKQps7OgaUXbsoEN3XPJ2KEZPWxgBWcGAABvnlP72GjueWJBxt/0WLuISpKFTSsy/qbHcs8TCyo0MwAAeGuEFBtFS2spk6bMTmkdr61ZNmnK7LS0rmsNAACobkKKjeKRuUvWOhL1eqUkC5pW5JG5SzbdpAAAoEyEFBvF4mXrj6g3sx4AAFQTIcVG0b9397KuBwAA1URIsVEcMKRvBjZ0z/pucl6T1XfvO2BI3005LQAAKAshxUbRtUtNLhk7NEnWiqk1v18ydqjnSQEA0CkJKTaa0cMG5vqT9k1jQ/vT9xobuuf6k/b1HCkAADotD+Rloxo9bGAOH9qYR+YuyeJlK9K/9+rT+RyJAgCgMxNSbHRdu9Rk+M79Kj0NAAAoG6f2AQAAFCSkAAAAChJSAAAABQkpAACAgoQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAArqVukJwKbW0lrKI3OXZPGyFenfu3sOGNI3XbvUVHpaAAB0IkKKTqMcAXTPEwsyacrsLGha0bZsYEP3XDJ2aEYPG1juKQMAsJkSUnQK5Qige55YkPE3PZbSG5YvbFqR8Tc9lutP2ldMAQDQIa6RouqtCaDXR1Ty/wPonicWbHCMltZSJk2ZvVZEJWlbNmnK7LS0rmsNAABoT0hR1coVQI/MXbJWiL1xrAVNK/LI3CVveq4AAGw5hBRVrVwBtHjZ+sd4M+slqyNvxnN/zZ2P/zkznvuro1kAAFsQ10hR1coVQP17d+/QOB1dz00rAAC2bI5IUdXKFUAHDOmbgQ3ds757/NVkdQgdMKTvBj+rHNdsvZ4jWwAAnY8jUlS1NQG0sGnFOq+TqknS2IEA6tqlJpeMHZrxNz2WmqTdWGvi6pKxQzd4O/UNXbNVk9XXbB0+tLFDt2Z3ZAsAoHNyRIqqtiaAkqx1NKlIACXJ6GEDc/1J+6axof3Rq8aG7h2+9Xk5b1pR7iNbAABsOo5IUfXWBNAbj9w0vokjN6OHDczhQxvf9IN9y3XNVrmPbAEAsGkJKTqFtxpAr9e1S02G79zvTc2jXNdsFTmy1dG5trSWyrJ9AADYMCFFp/FWAqhcynXNVrlvx17Oa60EGQDAhgkpKKBcN60o5+3Y11xr9cawW3OtVUev/1ozVjXe/KJccScSAYByEVJQUDmu2SrXka1yXmtVziB7/fzeariUK+6q8aidcYxjnI0/VjWpxm0NvHk1pVJpi39oTXNzcxoaGtLU1JT6+vpKT4dO4q3+i2xNuCTrPrLVkXCZ8dxf87Hv/HaDn/WTMw76h6dFtrSW8r6v3rve67bWhN2DFx3W4e9YjnBZX9wV2UblHGfNWNUUdsYxzuY0TrnHqqZwqcZtXU3bxzjGqaa/GOhoG2w2IfWtb30rX/va17Jw4cLsvffe+eY3v5kDDjigQ+8VUlTKW/0X4p2P/znn3vL4Bte75oR9cvQ+b1/v6+UKsjXKES7lirtyRmK1hZ1xjLM5jbMxxqqWcKnWbV0t28c4xqm2Swo62gabxXOkbr311px//vm55JJL8thjj2XvvffOqFGjsnjx4kpPDf6h0cMG5sGLDstPzjgo15ywT35yxkF58KLDOvwPj3Jda1XOm19s6HTDZPXphi2t//jvcMr1zK5yjVOu72Uc4xhn449Vruf0lWOcatzW1bR9jGOczvw8zc0ipK666qqcccYZOe200zJ06NDccMMN6dGjR773ve9VemqwQWvuRnj0Pm/P8J37FTqcveZaq/W9oyar/2ZnQ9dalfPmF+UKl3LFXbnGqbawM45xNqdxyjlWtYVLtW3rats+xjHOWxmn0jp9SK1atSozZ87MyJEj25Z16dIlI0eOzIwZM9b5npUrV6a5ubndD3RGa+4imGStmCpyF8FyBVlSvnApV9xV21E74xjHOBt3rGoLl2rb1tW2fYxjnLcyTqV1+pB66aWX0tLSkgEDBrRbPmDAgCxcuHCd77n88svT0NDQ9jN48OBNMVXYKNbcRbCxoX0INDZ07/D58uUKsqR84VKuuKu2o3bGMY5xNu5Y1RYu1batq237GMc45VivUjp9SL0ZF198cZqamtp+5s+fX+kpwVvyVq+1WjPGWw2ypHzhUq64q7ajdsYxjnE27ljVFi7Vtq2rbfsYxzjlWK9SOn1Ive1tb0vXrl2zaNGidssXLVqUxsbGdb6nrq4u9fX17X6gs3sr11qtUY4gK+fRrXLFXTUdtTOOcYyzcceqtnCptm1dbdvHOMZ5K+NUWqcPqdra2uy3336ZNm1a27LW1tZMmzYtw4cPr+DMoHMqV5CVI4DWjPVW465c41RT2BnHOJvbOOUaq9rCJamubV1t28c4xnmr//uqpM3iOVK33nprxo0bl29/+9s54IADcvXVV+enP/1pnn766bWunVoXz5GCjaMaH7JXDtX2EELjGGdzGqdcY1Xjc26qaVtX2/YxjnHeyjjltsU9kPfaa69teyDvPvvsk8mTJ+fAAw/s0HuFFABsfqopXKpRtW0f4xinWv73tcWF1FshpAAAgKTjbdDpr5ECAADY1IQUAABAQUIKAACgICEFAABQkJACAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUFC3Sk+gGpRKpSRJc3NzhWcCAABU0pomWNMI6yOkkixbtixJMnjw4ArPBAAAqAbLli1LQ0PDel+vKW0otbYAra2tefHFF9O7d+/U1NRUdC7Nzc0ZPHhw5s+fn/r6+orOZXNmO286tvWmYTtvGrbzpmNbbxq286ZhO28a5drOpVIpy5Yty6BBg9Kly/qvhHJEKkmXLl2y3XbbVXoa7dTX1/sf2iZgO286tvWmYTtvGrbzpmNbbxq286ZhO28a5djO/+hI1BpuNgEAAFCQkAIAAChISFWZurq6XHLJJamrq6v0VDZrtvOmY1tvGrbzpmE7bzq29aZhO28atvOmsam3s5tNAAAAFOSIFAAAQEFCCgAAoCAhBQAAUJCQAgAAKEhIVZFvfetb2XHHHdO9e/cceOCBeeSRRyo9pc3OpZdempqamnY/u+22W6Wn1ek98MADGTt2bAYNGpSamprccccd7V4vlUr54he/mIEDB2brrbfOyJEj88wzz1Rmsp3chrb1qaeeutY+Pnr06MpMtpO6/PLL8+53vzu9e/dO//79c8wxx2TOnDnt1lmxYkUmTJiQfv36pVevXjn22GOzaNGiCs248+rItj700EPX2qc/9alPVWjGndP111+fvfbaq+0hpcOHD8/dd9/d9rr9uTw2tJ3tyxvHFVdckZqamkycOLFt2abap4VUlbj11ltz/vnn55JLLsljjz2WvffeO6NGjcrixYsrPbXNzh577JEFCxa0/Tz44IOVnlKn9/LLL2fvvffOt771rXW+fuWVV2by5Mm54YYb8vDDD6dnz54ZNWpUVqxYsYln2vltaFsnyejRo9vt4z/5yU824Qw7v+nTp2fChAn57W9/m6lTp+bVV1/NEUcckZdffrltnfPOOy9TpkzJbbfdlunTp+fFF1/Mhz/84QrOunPqyLZOkjPOOKPdPn3llVdWaMad03bbbZcrrrgiM2fOzO9+97scdthhOfroo/Pkk08msT+Xy4a2c2JfLrdHH3003/72t7PXXnu1W77J9ukSVeGAAw4oTZgwoe33lpaW0qBBg0qXX355BWe1+bnkkktKe++9d6WnsVlLUrr99tvbfm9tbS01NjaWvva1r7UtW7p0aamurq70k5/8pAIz3Hy8cVuXSqXSuHHjSkcffXRF5rO5Wrx4cSlJafr06aVSafX+u9VWW5Vuu+22tnWeeuqpUpLSjBkzKjXNzcIbt3WpVCodcsghpXPPPbdyk9pMbbPNNqX/+3//r/15I1uznUsl+3K5LVu2rPSOd7yjNHXq1HbbdlPu045IVYFVq1Zl5syZGTlyZNuyLl26ZOTIkZkxY0YFZ7Z5euaZZzJo0KDstNNOOfHEEzNv3rxKT2mzNnfu3CxcuLDd/t3Q0JADDzzQ/r2R3H///enfv3923XXXjB8/Pn/9618rPaVOrampKUnSt2/fJMnMmTPz6quvttund9ttt2y//fb26bfojdt6jR//+Md529velmHDhuXiiy/OK6+8UonpbRZaWlpyyy235OWXX87w4cPtzxvJG7fzGvbl8pkwYULGjBnTbt9NNu0/o7uVdTTelJdeeiktLS0ZMGBAu+UDBgzI008/XaFZbZ4OPPDA3Hjjjdl1112zYMGCTJo0Ke9///vzxBNPpHfv3pWe3mZp4cKFSbLO/XvNa5TP6NGj8+EPfzhDhgzJc889l8997nM58sgjM2PGjHTt2rXS0+t0WltbM3HixLz3ve/NsGHDkqzep2tra9OnT59269qn35p1besk+fjHP54ddtghgwYNyh/+8IdcdNFFmTNnTv7zP/+zgrPtfGbNmpXhw4dnxYoV6dWrV26//fYMHTo0jz/+uP25jNa3nRP7cjndcssteeyxx/Loo4+u9dqm/Ge0kGKLcuSRR7b9ea+99sqBBx6YHXbYIT/96U9z+umnV3BmUB4nnHBC25/33HPP7LXXXtl5551z//33Z8SIERWcWec0YcKEPPHEE66l3ATWt63PPPPMtj/vueeeGThwYEaMGJHnnnsuO++886aeZqe166675vHHH09TU1P+4z/+I+PGjcv06dMrPa3Nzvq289ChQ+3LZTJ//vyce+65mTp1arp3717RuTi1rwq87W1vS9euXde6m8iiRYvS2NhYoVltGfr06ZN3vvOdefbZZys9lc3Wmn3Y/l0ZO+20U972trfZx9+Es88+O3fddVfuu+++bLfddm3LGxsbs2rVqixdurTd+vbpN29923pdDjzwwCSxTxdUW1ubXXbZJfvtt18uv/zy7L333rnmmmvsz2W2vu28LvblN2fmzJlZvHhx9t1333Tr1i3dunXL9OnTM3ny5HTr1i0DBgzYZPu0kKoCtbW12W+//TJt2rS2Za2trZk2bVq782opv+XLl+e5557LwIEDKz2VzdaQIUPS2NjYbv9ubm7Oww8/bP/eBP70pz/lr3/9q328gFKplLPPPju333577r333gwZMqTd6/vtt1+22mqrdvv0nDlzMm/ePPt0QRva1uvy+OOPJ4l9+i1qbW3NypUr7c8b2ZrtvC725TdnxIgRmTVrVh5//PG2n/333z8nnnhi25831T7t1L4qcf7552fcuHHZf//9c8ABB+Tqq6/Oyy+/nNNOO63SU9usXHjhhRk7dmx22GGHvPjii7nkkkvStWvXfOxjH6v01Dq15cuXt/sbtblz5+bxxx9P3759s/3222fixIn50pe+lHe84x0ZMmRIvvCFL2TQoEE55phjKjfpTuofbeu+fftm0qRJOfbYY9PY2Jjnnnsun/3sZ7PLLrtk1KhRFZx15zJhwoTcfPPNufPOO9O7d++2c+obGhqy9dZbp6GhIaeffnrOP//89O3bN/X19fn0pz+d4cOH56CDDqrw7DuXDW3r5557LjfffHM++MEPpl+/fvnDH/6Q8847LwcffPBatztm/S6++OIceeSR2X777bNs2bLcfPPNuf/++/PLX/7S/lxG/2g725fLp3fv3u2uo0ySnj17pl+/fm3LN9k+XdZ7APKWfPOb3yxtv/32pdra2tIBBxxQ+u1vf1vpKW12jj/++NLAgQNLtbW1pbe//e2l448/vvTss89Welqd3n333VdKstbPuHHjSqXS6lugf+ELXygNGDCgVFdXVxoxYkRpzpw5lZ10J/WPtvUrr7xSOuKII0rbbrttaauttirtsMMOpTPOOKO0cOHCSk+7U1nX9k1S+v73v9+2zt///vfSWWedVdpmm21KPXr0KP3TP/1TacGCBZWbdCe1oW09b9680sEHH1zq27dvqa6urrTLLruUPvOZz5SampoqO/FO5hOf+ERphx12KNXW1pa23Xbb0ogRI0q/+tWv2l63P5fHP9rO9uWN6423lt9U+3RNqVQqlTfNAAAANm+ukQIAAChISAEAABQkpAAAAAoSUgAAAAUJKQAAgIKEFAAAQEFCCgAAoCAhBQAAUJCQAoACdtxxx1x99dWVngYAFSakAKhap556ao455pgkyaGHHpqJEyduss++8cYb06dPn7WWP/rooznzzDM32TwAqE7dKj0BANiUVq1aldra2jf9/m233baMswGgs3JECoCqd+qpp2b69Om55pprUlNTk5qamrzwwgtJkieeeCJHHnlkevXqlQEDBuTkk0/OSy+91PbeQw89NGeffXYmTpyYt73tbRk1alSS5Kqrrsqee+6Znj17ZvDgwTnrrLOyfPnyJMn999+f0047LU1NTW2fd+mllyZZ+9S+efPm5eijj06vXr1SX1+f4447LosWLWp7/dJLL80+++yTH/3oR9lxxx3T0NCQE044IcuWLdu4Gw2AjUpIAVD1rrnmmgwfPjxnnHFGFixYkAULFmTw4MFZunRpDjvssLzrXe/K7373u9xzzz1ZtGhRjjvuuHbv/8EPfpDa2to89NBDueGGG5IkXbp0yeTJk/Pkk0/mBz/4Qe6999589rOfTZK85z3vydVXX536+vq2z7vwwgvXmldra2uOPvroLFmyJNOnT8/UqVPz/PPP5/jjj2+33nPPPZc77rgjd911V+66665Mnz49V1xxxUbaWgBsCk7tA6DqNTQ0pLa2Nj169EhjY2Pb8muvvTbvete78pWvfKVt2fe+970MHjw4//u//5t3vvOdSZJ3vOMdufLKK9uN+frrrXbcccd86Utfyqc+9alcd911qa2tTUNDQ2pqatp93htNmzYts2bNyty5czN48OAkyQ9/+MPsscceefTRR/Pud787yerguvHGG9O7d+8kycknn5xp06bly1/+8lvbMABUjCNSAHRav//973PfffelV69ebT+77bZbktVHgdbYb7/91nrvr3/964wYMSJvf/vb07t375x88sn561//mldeeaXDn//UU09l8ODBbRGVJEOHDk2fPn3y1FNPtS3bcccd2yIqSQYOHJjFixcX+q4AVBdHpADotJYvX56xY8fmq1/96lqvDRw4sO3PPXv2bPfaCy+8kA996EMZP358vvzlL6dv37558MEHc/rpp2fVqlXp0aNHWee51VZbtfu9pqYmra2tZf0MADYtIQVAp1BbW5uWlpZ2y/bdd9/87Gc/y4477phu3Tr+r7SZM2emtbU1//7v/54uXVafnPHTn/50g5/3Rrvvvnvmz5+f+fPntx2Vmj17dpYuXZqhQ4d2eD4AdD5O7QOgU9hxxx3z8MMP54UXXshLL72U1tbWTJgwIUuWLMnHPvaxPProo3nuuefyy1/+Mqeddto/jKBddtklr776ar75zW/m+eefz49+9KO2m1C8/vOWL1+eadOm5aWXXlrnKX8jR47MnnvumRNPPDGPPfZYHnnkkZxyyik55JBDsv/++5d9GwBQPYQUAJ3ChRdemK5du2bo0KHZdtttM2/evAwaNCgPPfRQWlpacsQRR2TPPffMxIkT06dPn7YjTeuy995756qrrspXv/rVDBs2LD/+8Y9z+eWXt1vnPe95Tz71qU/l+OOPz7bbbrvWzSqS1afo3Xnnndlmm21y8MEHZ+TIkdlpp51y6623lv37A1BdakqlUqnSkwAAAOhMHJECAAAoSEgBAAAUJKQAAAAKElIAAAAFCSkAAICChBQAAEBBQgoAAKAgIQUAAFCQkAIAAChISAEAABQkpAAAAAr6f5n0sqhPIh3LAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples by \n",
    "# tweaking just the learning rate and initialization scale.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "#weight_scales = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "#learning_rates = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "weight_scales = [2e-1]\n",
    "learning_rates = [1e-2]\n",
    "\n",
    "best_acc = 0.0\n",
    "best_losses = []\n",
    "for lr in learning_rates:\n",
    "    for ws in weight_scales:\n",
    "        model = FullyConnectedNet([100, 100, 100, 100], input_dim=8*8, weight_scale=ws, dtype=np.float64)\n",
    "        \n",
    "        solver = Solver(model, small_data,\n",
    "                        print_every=10, num_epochs=20, batch_size=25,\n",
    "                        update_rule='sgd',\n",
    "                        optim_config={\n",
    "                          'learning_rate': lr,\n",
    "                        }\n",
    "                 )\n",
    "        solver.train()\n",
    "        if solver.train_acc_history[-1] > best_acc:\n",
    "            best_losses = solver.loss_history\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте выводы по проведенному эксперименту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 слойная сеть переобучается сильнее, чем 3-х слойная ввиду наличия большего числа нейронов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее обновление весов проходило по правилу SGD. Теперь попробуйте реализовать стохастический градиентный спуск с импульсом (SGD+momentum). http://cs231n.github.io/neural-networks-3/#sgd Реализуйте sgd_momentum в scripts/optim.py  и запустите проверку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  8.882347033505819e-09\n",
      "velocity error:  4.269287743278663e-09\n"
     ]
    }
   ],
   "source": [
    "from scripts.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните результаты обучения шестислойной сети, обученной классическим градиентным спуском и адаптивным алгоритмом с импульсом. Какой алгоритм сходится быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  sgd\n",
      "(Iteration 1 / 50) loss: 2.304912\n",
      "(Epoch 0 / 5) train acc: 0.103000; val_acc: 0.138889\n",
      "(Epoch 1 / 5) train acc: 0.109000; val_acc: 0.138889\n",
      "(Iteration 11 / 50) loss: 2.303340\n",
      "(Epoch 2 / 5) train acc: 0.111000; val_acc: 0.138889\n",
      "(Iteration 21 / 50) loss: 2.303556\n",
      "(Epoch 3 / 5) train acc: 0.110000; val_acc: 0.158333\n",
      "(Iteration 31 / 50) loss: 2.301909\n",
      "(Epoch 4 / 5) train acc: 0.109000; val_acc: 0.155556\n",
      "(Iteration 41 / 50) loss: 2.302358\n",
      "(Epoch 5 / 5) train acc: 0.118000; val_acc: 0.147222\n",
      "\n",
      "running with  sgd_momentum\n",
      "(Iteration 1 / 50) loss: 2.303347\n",
      "(Epoch 0 / 5) train acc: 0.106000; val_acc: 0.119444\n",
      "(Epoch 1 / 5) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 11 / 50) loss: 2.301465\n",
      "(Epoch 2 / 5) train acc: 0.125000; val_acc: 0.138889\n",
      "(Iteration 21 / 50) loss: 2.298929\n",
      "(Epoch 3 / 5) train acc: 0.206000; val_acc: 0.166667\n",
      "(Iteration 31 / 50) loss: 2.293427\n",
      "(Epoch 4 / 5) train acc: 0.200000; val_acc: 0.166667\n",
      "(Iteration 41 / 50) loss: 2.291818\n",
      "(Epoch 5 / 5) train acc: 0.171000; val_acc: 0.166667\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], input_dim=8*8, weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-3,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте алгоритмы RMSProp [1] and Adam [2] с коррекцией смещения  - методы rmsprop и adam . \n",
    "\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  9.524687511038133e-08\n",
      "cache error:  2.6477955807156126e-09\n"
     ]
    }
   ],
   "source": [
    "# Test RMSProp implementation\n",
    "from scripts.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  1.1395691798535431e-07\n",
      "v error:  4.208314038113071e-09\n",
      "m error:  4.214963193114416e-09\n"
     ]
    }
   ],
   "source": [
    "# Test Adam implementation\n",
    "from scripts.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "# You should see relative errors around e-7 or less\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите пару глубоких сетей с испольованием RMSProp и Adam алгоритмов обновления весов и сравните результаты обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with  rmsprop\n",
      "(Iteration 1 / 50) loss: 2.300549\n",
      "(Epoch 0 / 5) train acc: 0.100000; val_acc: 0.097222\n",
      "(Epoch 1 / 5) train acc: 0.119000; val_acc: 0.133333\n",
      "(Iteration 11 / 50) loss: 2.285633\n",
      "(Epoch 2 / 5) train acc: 0.230000; val_acc: 0.225000\n",
      "(Iteration 21 / 50) loss: 2.067332\n",
      "(Epoch 3 / 5) train acc: 0.330000; val_acc: 0.313889\n",
      "(Iteration 31 / 50) loss: 1.953811\n",
      "(Epoch 4 / 5) train acc: 0.258000; val_acc: 0.308333\n",
      "(Iteration 41 / 50) loss: 1.905620\n",
      "(Epoch 5 / 5) train acc: 0.294000; val_acc: 0.286111\n",
      "\n",
      "running with  adam\n",
      "(Iteration 1 / 50) loss: 2.305944\n",
      "(Epoch 0 / 5) train acc: 0.266000; val_acc: 0.208333\n",
      "(Epoch 1 / 5) train acc: 0.641000; val_acc: 0.613889\n",
      "(Iteration 11 / 50) loss: 1.456799\n",
      "(Epoch 2 / 5) train acc: 0.770000; val_acc: 0.697222\n",
      "(Iteration 21 / 50) loss: 0.549378\n",
      "(Epoch 3 / 5) train acc: 0.907000; val_acc: 0.858333\n",
      "(Iteration 31 / 50) loss: 0.302551\n",
      "(Epoch 4 / 5) train acc: 0.945000; val_acc: 0.886111\n",
      "(Iteration 41 / 50) loss: 0.269849\n",
      "(Epoch 5 / 5) train acc: 0.956000; val_acc: 0.916667\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['rmsprop', 'adam']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], input_dim=8*8, weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-3,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получите лучшую полносвязную сеть для классификации вашего набора данных. На наборе CIFAR-10 необходимо получить accuracy не ниже 50 % на валидационном наборе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 200) loss: 4125.436577\n",
      "(Epoch 0 / 20) train acc: 0.066000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.325000; val_acc: 0.375000\n",
      "(Iteration 11 / 200) loss: 566.205864\n",
      "(Epoch 2 / 20) train acc: 0.655000; val_acc: 0.650000\n",
      "(Iteration 21 / 200) loss: 269.084829\n",
      "(Epoch 3 / 20) train acc: 0.787000; val_acc: 0.775000\n",
      "(Iteration 31 / 200) loss: 169.558093\n",
      "(Epoch 4 / 20) train acc: 0.842000; val_acc: 0.808333\n",
      "(Iteration 41 / 200) loss: 167.432463\n",
      "(Epoch 5 / 20) train acc: 0.916000; val_acc: 0.850000\n",
      "(Iteration 51 / 200) loss: 125.747122\n",
      "(Epoch 6 / 20) train acc: 0.909000; val_acc: 0.866667\n",
      "(Iteration 61 / 200) loss: 97.333845\n",
      "(Epoch 7 / 20) train acc: 0.958000; val_acc: 0.894444\n",
      "(Iteration 71 / 200) loss: 81.698506\n",
      "(Epoch 8 / 20) train acc: 0.964000; val_acc: 0.883333\n",
      "(Iteration 81 / 200) loss: 105.012456\n",
      "(Epoch 9 / 20) train acc: 0.950000; val_acc: 0.880556\n",
      "(Iteration 91 / 200) loss: 81.224799\n",
      "(Epoch 10 / 20) train acc: 0.967000; val_acc: 0.886111\n",
      "(Iteration 101 / 200) loss: 66.255078\n",
      "(Epoch 11 / 20) train acc: 0.975000; val_acc: 0.900000\n",
      "(Iteration 111 / 200) loss: 65.674349\n",
      "(Epoch 12 / 20) train acc: 0.985000; val_acc: 0.911111\n",
      "(Iteration 121 / 200) loss: 68.739364\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.913889\n",
      "(Iteration 131 / 200) loss: 62.416069\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.913889\n",
      "(Iteration 141 / 200) loss: 63.883199\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.908333\n",
      "(Iteration 151 / 200) loss: 61.853273\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.919444\n",
      "(Iteration 161 / 200) loss: 63.109393\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 59.894331\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.927778\n",
      "(Iteration 181 / 200) loss: 59.862379\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.930556\n",
      "(Iteration 191 / 200) loss: 60.585343\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 5.619906\n",
      "(Epoch 0 / 20) train acc: 0.190000; val_acc: 0.216667\n",
      "(Epoch 1 / 20) train acc: 0.825000; val_acc: 0.794444\n",
      "(Iteration 11 / 200) loss: 1.089562\n",
      "(Epoch 2 / 20) train acc: 0.940000; val_acc: 0.919444\n",
      "(Iteration 21 / 200) loss: 0.720922\n",
      "(Epoch 3 / 20) train acc: 0.960000; val_acc: 0.955556\n",
      "(Iteration 31 / 200) loss: 0.693032\n",
      "(Epoch 4 / 20) train acc: 0.975000; val_acc: 0.969444\n",
      "(Iteration 41 / 200) loss: 0.589469\n",
      "(Epoch 5 / 20) train acc: 0.985000; val_acc: 0.977778\n",
      "(Iteration 51 / 200) loss: 0.516723\n",
      "(Epoch 6 / 20) train acc: 0.988000; val_acc: 0.983333\n",
      "(Iteration 61 / 200) loss: 0.452369\n",
      "(Epoch 7 / 20) train acc: 0.995000; val_acc: 0.986111\n",
      "(Iteration 71 / 200) loss: 0.399163\n",
      "(Epoch 8 / 20) train acc: 0.998000; val_acc: 0.986111\n",
      "(Iteration 81 / 200) loss: 0.360130\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.331675\n",
      "(Epoch 10 / 20) train acc: 0.971000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.360937\n",
      "(Epoch 11 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 111 / 200) loss: 0.326113\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.991667\n",
      "(Iteration 121 / 200) loss: 0.285283\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.290070\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.264920\n",
      "(Epoch 15 / 20) train acc: 0.986000; val_acc: 0.986111\n",
      "(Iteration 151 / 200) loss: 0.216602\n",
      "(Epoch 16 / 20) train acc: 0.965000; val_acc: 0.952778\n",
      "(Iteration 161 / 200) loss: 0.229602\n",
      "(Epoch 17 / 20) train acc: 0.978000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.243246\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.242811\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.217891\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.983333\n",
      "(Iteration 1 / 200) loss: 2.310989\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.646000; val_acc: 0.630556\n",
      "(Iteration 11 / 200) loss: 1.532081\n",
      "(Epoch 2 / 20) train acc: 0.882000; val_acc: 0.847222\n",
      "(Iteration 21 / 200) loss: 0.479783\n",
      "(Epoch 3 / 20) train acc: 0.907000; val_acc: 0.880556\n",
      "(Iteration 31 / 200) loss: 0.359392\n",
      "(Epoch 4 / 20) train acc: 0.932000; val_acc: 0.938889\n",
      "(Iteration 41 / 200) loss: 0.221654\n",
      "(Epoch 5 / 20) train acc: 0.957000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.363098\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.318474\n",
      "(Epoch 7 / 20) train acc: 0.969000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.255517\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.196412\n",
      "(Epoch 9 / 20) train acc: 0.976000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.243433\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.199576\n",
      "(Epoch 11 / 20) train acc: 0.975000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.189158\n",
      "(Epoch 12 / 20) train acc: 0.964000; val_acc: 0.941667\n",
      "(Iteration 121 / 200) loss: 0.237972\n",
      "(Epoch 13 / 20) train acc: 0.979000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.208592\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 141 / 200) loss: 0.160211\n",
      "(Epoch 15 / 20) train acc: 0.981000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.286337\n",
      "(Epoch 16 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 0.168513\n",
      "(Epoch 17 / 20) train acc: 0.981000; val_acc: 0.944444\n",
      "(Iteration 171 / 200) loss: 0.152716\n",
      "(Epoch 18 / 20) train acc: 0.970000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.198520\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.149525\n",
      "(Epoch 20 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302666\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.558000; val_acc: 0.533333\n",
      "(Iteration 11 / 200) loss: 1.002263\n",
      "(Epoch 2 / 20) train acc: 0.836000; val_acc: 0.808333\n",
      "(Iteration 21 / 200) loss: 0.743287\n",
      "(Epoch 3 / 20) train acc: 0.866000; val_acc: 0.841667\n",
      "(Iteration 31 / 200) loss: 0.608544\n",
      "(Epoch 4 / 20) train acc: 0.935000; val_acc: 0.922222\n",
      "(Iteration 41 / 200) loss: 0.400376\n",
      "(Epoch 5 / 20) train acc: 0.964000; val_acc: 0.925000\n",
      "(Iteration 51 / 200) loss: 0.317999\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.285194\n",
      "(Epoch 7 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.221711\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.190259\n",
      "(Epoch 9 / 20) train acc: 0.971000; val_acc: 0.938889\n",
      "(Iteration 91 / 200) loss: 0.178738\n",
      "(Epoch 10 / 20) train acc: 0.976000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.305914\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 111 / 200) loss: 0.176985\n",
      "(Epoch 12 / 20) train acc: 0.979000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.173580\n",
      "(Epoch 13 / 20) train acc: 0.935000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.258514\n",
      "(Epoch 14 / 20) train acc: 0.948000; val_acc: 0.927778\n",
      "(Iteration 141 / 200) loss: 0.217784\n",
      "(Epoch 15 / 20) train acc: 0.957000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.291073\n",
      "(Epoch 16 / 20) train acc: 0.982000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.240872\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.247573\n",
      "(Epoch 18 / 20) train acc: 0.983000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.193393\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.188073\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.615000; val_acc: 0.641667\n",
      "(Iteration 11 / 200) loss: 0.959116\n",
      "(Epoch 2 / 20) train acc: 0.789000; val_acc: 0.741667\n",
      "(Iteration 21 / 200) loss: 0.596986\n",
      "(Epoch 3 / 20) train acc: 0.944000; val_acc: 0.888889\n",
      "(Iteration 31 / 200) loss: 0.351571\n",
      "(Epoch 4 / 20) train acc: 0.956000; val_acc: 0.941667\n",
      "(Iteration 41 / 200) loss: 0.243565\n",
      "(Epoch 5 / 20) train acc: 0.964000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.292751\n",
      "(Epoch 6 / 20) train acc: 0.957000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.277521\n",
      "(Epoch 7 / 20) train acc: 0.981000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.246550\n",
      "(Epoch 8 / 20) train acc: 0.970000; val_acc: 0.919444\n",
      "(Iteration 81 / 200) loss: 0.262454\n",
      "(Epoch 9 / 20) train acc: 0.968000; val_acc: 0.930556\n",
      "(Iteration 91 / 200) loss: 0.187124\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.219799\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.944444\n",
      "(Iteration 111 / 200) loss: 0.189571\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.174471\n",
      "(Epoch 13 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 0.201054\n",
      "(Epoch 14 / 20) train acc: 0.936000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 0.258471\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.196646\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.220972\n",
      "(Epoch 17 / 20) train acc: 0.963000; val_acc: 0.941667\n",
      "(Iteration 171 / 200) loss: 0.260280\n",
      "(Epoch 18 / 20) train acc: 0.954000; val_acc: 0.936111\n",
      "(Iteration 181 / 200) loss: 0.326131\n",
      "(Epoch 19 / 20) train acc: 0.968000; val_acc: 0.938889\n",
      "(Iteration 191 / 200) loss: 0.189308\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.122000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.435000; val_acc: 0.430556\n",
      "(Iteration 11 / 200) loss: 1.390449\n",
      "(Epoch 2 / 20) train acc: 0.695000; val_acc: 0.758333\n",
      "(Iteration 21 / 200) loss: 0.758223\n",
      "(Epoch 3 / 20) train acc: 0.893000; val_acc: 0.880556\n",
      "(Iteration 31 / 200) loss: 0.450267\n",
      "(Epoch 4 / 20) train acc: 0.951000; val_acc: 0.913889\n",
      "(Iteration 41 / 200) loss: 0.295575\n",
      "(Epoch 5 / 20) train acc: 0.958000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 0.342292\n",
      "(Epoch 6 / 20) train acc: 0.983000; val_acc: 0.966667\n",
      "(Iteration 61 / 200) loss: 0.279584\n",
      "(Epoch 7 / 20) train acc: 0.976000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.206331\n",
      "(Epoch 8 / 20) train acc: 0.939000; val_acc: 0.922222\n",
      "(Iteration 81 / 200) loss: 0.381358\n",
      "(Epoch 9 / 20) train acc: 0.954000; val_acc: 0.936111\n",
      "(Iteration 91 / 200) loss: 0.345019\n",
      "(Epoch 10 / 20) train acc: 0.948000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 0.258071\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.190935\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.168191\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.158310\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.148347\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.174630\n",
      "(Epoch 16 / 20) train acc: 0.985000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.175018\n",
      "(Epoch 17 / 20) train acc: 0.984000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.134865\n",
      "(Epoch 18 / 20) train acc: 0.978000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.157362\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.146840\n",
      "(Epoch 20 / 20) train acc: 0.962000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 6447.177126\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.324000; val_acc: 0.302778\n",
      "(Iteration 11 / 200) loss: 1123.909358\n",
      "(Epoch 2 / 20) train acc: 0.719000; val_acc: 0.702778\n",
      "(Iteration 21 / 200) loss: 377.455469\n",
      "(Epoch 3 / 20) train acc: 0.801000; val_acc: 0.791667\n",
      "(Iteration 31 / 200) loss: 183.174119\n",
      "(Epoch 4 / 20) train acc: 0.875000; val_acc: 0.844444\n",
      "(Iteration 41 / 200) loss: 169.291841\n",
      "(Epoch 5 / 20) train acc: 0.906000; val_acc: 0.886111\n",
      "(Iteration 51 / 200) loss: 117.768030\n",
      "(Epoch 6 / 20) train acc: 0.913000; val_acc: 0.883333\n",
      "(Iteration 61 / 200) loss: 145.526555\n",
      "(Epoch 7 / 20) train acc: 0.944000; val_acc: 0.900000\n",
      "(Iteration 71 / 200) loss: 114.145557\n",
      "(Epoch 8 / 20) train acc: 0.949000; val_acc: 0.883333\n",
      "(Iteration 81 / 200) loss: 126.368425\n",
      "(Epoch 9 / 20) train acc: 0.959000; val_acc: 0.900000\n",
      "(Iteration 91 / 200) loss: 90.163262\n",
      "(Epoch 10 / 20) train acc: 0.960000; val_acc: 0.913889\n",
      "(Iteration 101 / 200) loss: 97.254705\n",
      "(Epoch 11 / 20) train acc: 0.967000; val_acc: 0.905556\n",
      "(Iteration 111 / 200) loss: 78.361541\n",
      "(Epoch 12 / 20) train acc: 0.972000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 71.619870\n",
      "(Epoch 13 / 20) train acc: 0.952000; val_acc: 0.888889\n",
      "(Iteration 131 / 200) loss: 96.097661\n",
      "(Epoch 14 / 20) train acc: 0.983000; val_acc: 0.894444\n",
      "(Iteration 141 / 200) loss: 71.594199\n",
      "(Epoch 15 / 20) train acc: 0.986000; val_acc: 0.916667\n",
      "(Iteration 151 / 200) loss: 69.407516\n",
      "(Epoch 16 / 20) train acc: 0.987000; val_acc: 0.927778\n",
      "(Iteration 161 / 200) loss: 70.993580\n",
      "(Epoch 17 / 20) train acc: 0.981000; val_acc: 0.933333\n",
      "(Iteration 171 / 200) loss: 71.792097\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.925000\n",
      "(Iteration 181 / 200) loss: 70.711924\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.916667\n",
      "(Iteration 191 / 200) loss: 80.187426\n",
      "(Epoch 20 / 20) train acc: 0.976000; val_acc: 0.916667\n",
      "(Iteration 1 / 200) loss: 6.328419\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.155556\n",
      "(Epoch 1 / 20) train acc: 0.770000; val_acc: 0.733333\n",
      "(Iteration 11 / 200) loss: 1.527606\n",
      "(Epoch 2 / 20) train acc: 0.915000; val_acc: 0.913889\n",
      "(Iteration 21 / 200) loss: 0.775348\n",
      "(Epoch 3 / 20) train acc: 0.939000; val_acc: 0.905556\n",
      "(Iteration 31 / 200) loss: 0.856199\n",
      "(Epoch 4 / 20) train acc: 0.970000; val_acc: 0.941667\n",
      "(Iteration 41 / 200) loss: 0.727746\n",
      "(Epoch 5 / 20) train acc: 0.960000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.636012\n",
      "(Epoch 6 / 20) train acc: 0.985000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.554589\n",
      "(Epoch 7 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 71 / 200) loss: 0.477267\n",
      "(Epoch 8 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.432054\n",
      "(Epoch 9 / 20) train acc: 0.984000; val_acc: 0.975000\n",
      "(Iteration 91 / 200) loss: 0.454916\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.431096\n",
      "(Epoch 11 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.357717\n",
      "(Epoch 12 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.328351\n",
      "(Epoch 13 / 20) train acc: 0.985000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.325373\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.291859\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.276626\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.251646\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.300441\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.226416\n",
      "(Epoch 19 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.225855\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.310076\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.571000; val_acc: 0.577778\n",
      "(Iteration 11 / 200) loss: 0.967164\n",
      "(Epoch 2 / 20) train acc: 0.846000; val_acc: 0.830556\n",
      "(Iteration 21 / 200) loss: 0.538481\n",
      "(Epoch 3 / 20) train acc: 0.897000; val_acc: 0.858333\n",
      "(Iteration 31 / 200) loss: 0.401417\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.936111\n",
      "(Iteration 41 / 200) loss: 0.294691\n",
      "(Epoch 5 / 20) train acc: 0.950000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.229101\n",
      "(Epoch 6 / 20) train acc: 0.962000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 0.216473\n",
      "(Epoch 7 / 20) train acc: 0.955000; val_acc: 0.905556\n",
      "(Iteration 71 / 200) loss: 0.273334\n",
      "(Epoch 8 / 20) train acc: 0.991000; val_acc: 0.983333\n",
      "(Iteration 81 / 200) loss: 0.185789\n",
      "(Epoch 9 / 20) train acc: 0.970000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.198236\n",
      "(Epoch 10 / 20) train acc: 0.978000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.164623\n",
      "(Epoch 11 / 20) train acc: 0.978000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.245970\n",
      "(Epoch 12 / 20) train acc: 0.990000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.140957\n",
      "(Epoch 13 / 20) train acc: 0.976000; val_acc: 0.955556\n",
      "(Iteration 131 / 200) loss: 0.166307\n",
      "(Epoch 14 / 20) train acc: 0.957000; val_acc: 0.936111\n",
      "(Iteration 141 / 200) loss: 0.318040\n",
      "(Epoch 15 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 151 / 200) loss: 0.181018\n",
      "(Epoch 16 / 20) train acc: 0.967000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.156468\n",
      "(Epoch 17 / 20) train acc: 0.973000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.179671\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.180430\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.133111\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.941667\n",
      "(Iteration 1 / 200) loss: 2.302667\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.666000; val_acc: 0.644444\n",
      "(Iteration 11 / 200) loss: 1.194663\n",
      "(Epoch 2 / 20) train acc: 0.791000; val_acc: 0.788889\n",
      "(Iteration 21 / 200) loss: 0.769286\n",
      "(Epoch 3 / 20) train acc: 0.909000; val_acc: 0.863889\n",
      "(Iteration 31 / 200) loss: 0.448230\n",
      "(Epoch 4 / 20) train acc: 0.922000; val_acc: 0.941667\n",
      "(Iteration 41 / 200) loss: 0.392397\n",
      "(Epoch 5 / 20) train acc: 0.954000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.325747\n",
      "(Epoch 6 / 20) train acc: 0.960000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.276641\n",
      "(Epoch 7 / 20) train acc: 0.970000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 0.221323\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.203031\n",
      "(Epoch 9 / 20) train acc: 0.974000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.170684\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.158135\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.162470\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.137943\n",
      "(Epoch 13 / 20) train acc: 0.976000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.227364\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.162618\n",
      "(Epoch 15 / 20) train acc: 0.982000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.153338\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.163437\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.184385\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.160896\n",
      "(Epoch 19 / 20) train acc: 0.980000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.160052\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.499000; val_acc: 0.505556\n",
      "(Iteration 11 / 200) loss: 1.264343\n",
      "(Epoch 2 / 20) train acc: 0.784000; val_acc: 0.775000\n",
      "(Iteration 21 / 200) loss: 0.670675\n",
      "(Epoch 3 / 20) train acc: 0.932000; val_acc: 0.908333\n",
      "(Iteration 31 / 200) loss: 0.318695\n",
      "(Epoch 4 / 20) train acc: 0.925000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.356021\n",
      "(Epoch 5 / 20) train acc: 0.973000; val_acc: 0.966667\n",
      "(Iteration 51 / 200) loss: 0.231945\n",
      "(Epoch 6 / 20) train acc: 0.968000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.213204\n",
      "(Epoch 7 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.205715\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.212388\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.181048\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.166500\n",
      "(Epoch 11 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.142555\n",
      "(Epoch 12 / 20) train acc: 0.996000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.145954\n",
      "(Epoch 13 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.131496\n",
      "(Epoch 14 / 20) train acc: 0.974000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.158332\n",
      "(Epoch 15 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.182290\n",
      "(Epoch 16 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.137564\n",
      "(Epoch 17 / 20) train acc: 0.988000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.166316\n",
      "(Epoch 18 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.127110\n",
      "(Epoch 19 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.138334\n",
      "(Epoch 20 / 20) train acc: 0.956000; val_acc: 0.947222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.351000; val_acc: 0.327778\n",
      "(Iteration 11 / 200) loss: 1.786621\n",
      "(Epoch 2 / 20) train acc: 0.751000; val_acc: 0.769444\n",
      "(Iteration 21 / 200) loss: 0.859005\n",
      "(Epoch 3 / 20) train acc: 0.823000; val_acc: 0.825000\n",
      "(Iteration 31 / 200) loss: 0.750513\n",
      "(Epoch 4 / 20) train acc: 0.934000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.421494\n",
      "(Epoch 5 / 20) train acc: 0.968000; val_acc: 0.938889\n",
      "(Iteration 51 / 200) loss: 0.254772\n",
      "(Epoch 6 / 20) train acc: 0.908000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 0.366333\n",
      "(Epoch 7 / 20) train acc: 0.987000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.186261\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 0.185053\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 0.201377\n",
      "(Epoch 10 / 20) train acc: 0.962000; val_acc: 0.941667\n",
      "(Iteration 101 / 200) loss: 0.277499\n",
      "(Epoch 11 / 20) train acc: 0.958000; val_acc: 0.936111\n",
      "(Iteration 111 / 200) loss: 0.157815\n",
      "(Epoch 12 / 20) train acc: 0.980000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.169175\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.175428\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.189803\n",
      "(Epoch 15 / 20) train acc: 0.984000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.154857\n",
      "(Epoch 16 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.181367\n",
      "(Epoch 17 / 20) train acc: 0.965000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 0.204848\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.195707\n",
      "(Epoch 19 / 20) train acc: 0.961000; val_acc: 0.938889\n",
      "(Iteration 191 / 200) loss: 0.154220\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 7228.281929\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.108333\n",
      "(Epoch 1 / 20) train acc: 0.270000; val_acc: 0.286111\n",
      "(Iteration 11 / 200) loss: 1376.473130\n",
      "(Epoch 2 / 20) train acc: 0.653000; val_acc: 0.644444\n",
      "(Iteration 21 / 200) loss: 428.740308\n",
      "(Epoch 3 / 20) train acc: 0.792000; val_acc: 0.758333\n",
      "(Iteration 31 / 200) loss: 200.411538\n",
      "(Epoch 4 / 20) train acc: 0.889000; val_acc: 0.850000\n",
      "(Iteration 41 / 200) loss: 151.458652\n",
      "(Epoch 5 / 20) train acc: 0.891000; val_acc: 0.847222\n",
      "(Iteration 51 / 200) loss: 151.176926\n",
      "(Epoch 6 / 20) train acc: 0.930000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 119.100747\n",
      "(Epoch 7 / 20) train acc: 0.931000; val_acc: 0.888889\n",
      "(Iteration 71 / 200) loss: 88.760944\n",
      "(Epoch 8 / 20) train acc: 0.943000; val_acc: 0.886111\n",
      "(Iteration 81 / 200) loss: 78.015941\n",
      "(Epoch 9 / 20) train acc: 0.955000; val_acc: 0.888889\n",
      "(Iteration 91 / 200) loss: 86.999321\n",
      "(Epoch 10 / 20) train acc: 0.958000; val_acc: 0.894444\n",
      "(Iteration 101 / 200) loss: 91.112961\n",
      "(Epoch 11 / 20) train acc: 0.952000; val_acc: 0.902778\n",
      "(Iteration 111 / 200) loss: 85.264133\n",
      "(Epoch 12 / 20) train acc: 0.951000; val_acc: 0.916667\n",
      "(Iteration 121 / 200) loss: 73.635143\n",
      "(Epoch 13 / 20) train acc: 0.972000; val_acc: 0.900000\n",
      "(Iteration 131 / 200) loss: 78.027294\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.919444\n",
      "(Iteration 141 / 200) loss: 65.261428\n",
      "(Epoch 15 / 20) train acc: 0.980000; val_acc: 0.916667\n",
      "(Iteration 151 / 200) loss: 70.949917\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.922222\n",
      "(Iteration 161 / 200) loss: 63.738359\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 63.264421\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.925000\n",
      "(Iteration 181 / 200) loss: 62.819702\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 63.512029\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.927778\n",
      "(Iteration 1 / 200) loss: 5.113058\n",
      "(Epoch 0 / 20) train acc: 0.258000; val_acc: 0.238889\n",
      "(Epoch 1 / 20) train acc: 0.879000; val_acc: 0.852778\n",
      "(Iteration 11 / 200) loss: 1.058532\n",
      "(Epoch 2 / 20) train acc: 0.927000; val_acc: 0.933333\n",
      "(Iteration 21 / 200) loss: 0.812131\n",
      "(Epoch 3 / 20) train acc: 0.948000; val_acc: 0.938889\n",
      "(Iteration 31 / 200) loss: 0.754909\n",
      "(Epoch 4 / 20) train acc: 0.974000; val_acc: 0.969444\n",
      "(Iteration 41 / 200) loss: 0.600524\n",
      "(Epoch 5 / 20) train acc: 0.986000; val_acc: 0.972222\n",
      "(Iteration 51 / 200) loss: 0.521229\n",
      "(Epoch 6 / 20) train acc: 0.990000; val_acc: 0.980556\n",
      "(Iteration 61 / 200) loss: 0.462548\n",
      "(Epoch 7 / 20) train acc: 0.995000; val_acc: 0.980556\n",
      "(Iteration 71 / 200) loss: 0.418305\n",
      "(Epoch 8 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 81 / 200) loss: 0.394656\n",
      "(Epoch 9 / 20) train acc: 0.995000; val_acc: 0.986111\n",
      "(Iteration 91 / 200) loss: 0.393196\n",
      "(Epoch 10 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.311465\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.983333\n",
      "(Iteration 111 / 200) loss: 0.296646\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.311789\n",
      "(Epoch 13 / 20) train acc: 0.985000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.274608\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.240135\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.273440\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.240946\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.209110\n",
      "(Epoch 18 / 20) train acc: 0.980000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.236376\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.191307\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.310647\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.744000; val_acc: 0.683333\n",
      "(Iteration 11 / 200) loss: 0.864904\n",
      "(Epoch 2 / 20) train acc: 0.846000; val_acc: 0.880556\n",
      "(Iteration 21 / 200) loss: 0.591506\n",
      "(Epoch 3 / 20) train acc: 0.882000; val_acc: 0.855556\n",
      "(Iteration 31 / 200) loss: 0.732626\n",
      "(Epoch 4 / 20) train acc: 0.933000; val_acc: 0.930556\n",
      "(Iteration 41 / 200) loss: 0.393258\n",
      "(Epoch 5 / 20) train acc: 0.960000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.339394\n",
      "(Epoch 6 / 20) train acc: 0.966000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.283434\n",
      "(Epoch 7 / 20) train acc: 0.976000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 0.271821\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.944444\n",
      "(Iteration 81 / 200) loss: 0.211315\n",
      "(Epoch 9 / 20) train acc: 0.976000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.250094\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.153811\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.213681\n",
      "(Epoch 12 / 20) train acc: 0.975000; val_acc: 0.938889\n",
      "(Iteration 121 / 200) loss: 0.179276\n",
      "(Epoch 13 / 20) train acc: 0.985000; val_acc: 0.955556\n",
      "(Iteration 131 / 200) loss: 0.163214\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.144406\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.143680\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.182579\n",
      "(Epoch 17 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.141719\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.123359\n",
      "(Epoch 19 / 20) train acc: 0.982000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.123770\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302667\n",
      "(Epoch 0 / 20) train acc: 0.078000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.596000; val_acc: 0.638889\n",
      "(Iteration 11 / 200) loss: 1.296892\n",
      "(Epoch 2 / 20) train acc: 0.829000; val_acc: 0.786111\n",
      "(Iteration 21 / 200) loss: 0.614865\n",
      "(Epoch 3 / 20) train acc: 0.896000; val_acc: 0.900000\n",
      "(Iteration 31 / 200) loss: 0.308682\n",
      "(Epoch 4 / 20) train acc: 0.934000; val_acc: 0.938889\n",
      "(Iteration 41 / 200) loss: 0.322594\n",
      "(Epoch 5 / 20) train acc: 0.948000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.264546\n",
      "(Epoch 6 / 20) train acc: 0.946000; val_acc: 0.922222\n",
      "(Iteration 61 / 200) loss: 0.248056\n",
      "(Epoch 7 / 20) train acc: 0.982000; val_acc: 0.972222\n",
      "(Iteration 71 / 200) loss: 0.172639\n",
      "(Epoch 8 / 20) train acc: 0.981000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.207019\n",
      "(Epoch 9 / 20) train acc: 0.983000; val_acc: 0.950000\n",
      "(Iteration 91 / 200) loss: 0.188387\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.148147\n",
      "(Epoch 11 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.160612\n",
      "(Epoch 12 / 20) train acc: 0.984000; val_acc: 0.958333\n",
      "(Iteration 121 / 200) loss: 0.181574\n",
      "(Epoch 13 / 20) train acc: 0.977000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.164176\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.154069\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.114861\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.134064\n",
      "(Epoch 17 / 20) train acc: 0.983000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.177168\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.121775\n",
      "(Epoch 19 / 20) train acc: 0.957000; val_acc: 0.936111\n",
      "(Iteration 191 / 200) loss: 0.150914\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.584000; val_acc: 0.552778\n",
      "(Iteration 11 / 200) loss: 0.901087\n",
      "(Epoch 2 / 20) train acc: 0.790000; val_acc: 0.786111\n",
      "(Iteration 21 / 200) loss: 0.642425\n",
      "(Epoch 3 / 20) train acc: 0.890000; val_acc: 0.891667\n",
      "(Iteration 31 / 200) loss: 0.405998\n",
      "(Epoch 4 / 20) train acc: 0.941000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.381067\n",
      "(Epoch 5 / 20) train acc: 0.952000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.202775\n",
      "(Epoch 6 / 20) train acc: 0.934000; val_acc: 0.883333\n",
      "(Iteration 61 / 200) loss: 0.375051\n",
      "(Epoch 7 / 20) train acc: 0.951000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.329748\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.289327\n",
      "(Epoch 9 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 91 / 200) loss: 0.213430\n",
      "(Epoch 10 / 20) train acc: 0.965000; val_acc: 0.925000\n",
      "(Iteration 101 / 200) loss: 0.242016\n",
      "(Epoch 11 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.187176\n",
      "(Epoch 12 / 20) train acc: 0.968000; val_acc: 0.936111\n",
      "(Iteration 121 / 200) loss: 0.289047\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.174473\n",
      "(Epoch 14 / 20) train acc: 0.982000; val_acc: 0.925000\n",
      "(Iteration 141 / 200) loss: 0.194054\n",
      "(Epoch 15 / 20) train acc: 0.982000; val_acc: 0.955556\n",
      "(Iteration 151 / 200) loss: 0.153714\n",
      "(Epoch 16 / 20) train acc: 0.984000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.147220\n",
      "(Epoch 17 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.133009\n",
      "(Epoch 18 / 20) train acc: 0.984000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.295712\n",
      "(Epoch 19 / 20) train acc: 0.960000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.198021\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.163889\n",
      "(Epoch 1 / 20) train acc: 0.423000; val_acc: 0.430556\n",
      "(Iteration 11 / 200) loss: 1.712150\n",
      "(Epoch 2 / 20) train acc: 0.755000; val_acc: 0.800000\n",
      "(Iteration 21 / 200) loss: 0.935454\n",
      "(Epoch 3 / 20) train acc: 0.871000; val_acc: 0.855556\n",
      "(Iteration 31 / 200) loss: 0.327476\n",
      "(Epoch 4 / 20) train acc: 0.931000; val_acc: 0.919444\n",
      "(Iteration 41 / 200) loss: 0.370420\n",
      "(Epoch 5 / 20) train acc: 0.961000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.236962\n",
      "(Epoch 6 / 20) train acc: 0.956000; val_acc: 0.955556\n",
      "(Iteration 61 / 200) loss: 0.342191\n",
      "(Epoch 7 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.210427\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.952778\n",
      "(Iteration 81 / 200) loss: 0.293356\n",
      "(Epoch 9 / 20) train acc: 0.991000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.195521\n",
      "(Epoch 10 / 20) train acc: 0.972000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.231297\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.193019\n",
      "(Epoch 12 / 20) train acc: 0.987000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.179503\n",
      "(Epoch 13 / 20) train acc: 0.975000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.135033\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.194173\n",
      "(Epoch 15 / 20) train acc: 0.967000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.138030\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.167669\n",
      "(Epoch 17 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.155557\n",
      "(Epoch 18 / 20) train acc: 0.968000; val_acc: 0.938889\n",
      "(Iteration 181 / 200) loss: 0.170678\n",
      "(Epoch 19 / 20) train acc: 0.955000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.219893\n",
      "(Epoch 20 / 20) train acc: 0.976000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 6112.314789\n",
      "(Epoch 0 / 20) train acc: 0.145000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.288000; val_acc: 0.258333\n",
      "(Iteration 11 / 200) loss: 1187.295430\n",
      "(Epoch 2 / 20) train acc: 0.635000; val_acc: 0.566667\n",
      "(Iteration 21 / 200) loss: 399.523226\n",
      "(Epoch 3 / 20) train acc: 0.748000; val_acc: 0.711111\n",
      "(Iteration 31 / 200) loss: 218.211583\n",
      "(Epoch 4 / 20) train acc: 0.842000; val_acc: 0.794444\n",
      "(Iteration 41 / 200) loss: 85.800404\n",
      "(Epoch 5 / 20) train acc: 0.874000; val_acc: 0.855556\n",
      "(Iteration 51 / 200) loss: 90.007311\n",
      "(Epoch 6 / 20) train acc: 0.919000; val_acc: 0.872222\n",
      "(Iteration 61 / 200) loss: 58.897300\n",
      "(Epoch 7 / 20) train acc: 0.943000; val_acc: 0.872222\n",
      "(Iteration 71 / 200) loss: 30.784372\n",
      "(Epoch 8 / 20) train acc: 0.950000; val_acc: 0.897222\n",
      "(Iteration 81 / 200) loss: 35.234088\n",
      "(Epoch 9 / 20) train acc: 0.952000; val_acc: 0.905556\n",
      "(Iteration 91 / 200) loss: 18.134100\n",
      "(Epoch 10 / 20) train acc: 0.962000; val_acc: 0.919444\n",
      "(Iteration 101 / 200) loss: 32.522125\n",
      "(Epoch 11 / 20) train acc: 0.966000; val_acc: 0.897222\n",
      "(Iteration 111 / 200) loss: 15.659395\n",
      "(Epoch 12 / 20) train acc: 0.951000; val_acc: 0.913889\n",
      "(Iteration 121 / 200) loss: 9.465681\n",
      "(Epoch 13 / 20) train acc: 0.954000; val_acc: 0.905556\n",
      "(Iteration 131 / 200) loss: 23.826531\n",
      "(Epoch 14 / 20) train acc: 0.976000; val_acc: 0.933333\n",
      "(Iteration 141 / 200) loss: 17.284490\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.911111\n",
      "(Iteration 151 / 200) loss: 6.988394\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.927778\n",
      "(Iteration 161 / 200) loss: 18.143937\n",
      "(Epoch 17 / 20) train acc: 0.979000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 10.484808\n",
      "(Epoch 18 / 20) train acc: 0.984000; val_acc: 0.919444\n",
      "(Iteration 181 / 200) loss: 8.724751\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 9.421437\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.927778\n",
      "(Iteration 1 / 200) loss: 5.543163\n",
      "(Epoch 0 / 20) train acc: 0.229000; val_acc: 0.227778\n",
      "(Epoch 1 / 20) train acc: 0.750000; val_acc: 0.733333\n",
      "(Iteration 11 / 200) loss: 0.902760\n",
      "(Epoch 2 / 20) train acc: 0.932000; val_acc: 0.897222\n",
      "(Iteration 21 / 200) loss: 0.401575\n",
      "(Epoch 3 / 20) train acc: 0.952000; val_acc: 0.936111\n",
      "(Iteration 31 / 200) loss: 0.239306\n",
      "(Epoch 4 / 20) train acc: 0.980000; val_acc: 0.947222\n",
      "(Iteration 41 / 200) loss: 0.170088\n",
      "(Epoch 5 / 20) train acc: 0.983000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.128308\n",
      "(Epoch 6 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 61 / 200) loss: 0.124892\n",
      "(Epoch 7 / 20) train acc: 0.988000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.094313\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 0.085638\n",
      "(Epoch 9 / 20) train acc: 0.995000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.098726\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.098959\n",
      "(Epoch 11 / 20) train acc: 0.998000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.082762\n",
      "(Epoch 12 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.081258\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.109431\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.082480\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.079643\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.070083\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.072474\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.068982\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.065505\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.304357\n",
      "(Epoch 0 / 20) train acc: 0.148000; val_acc: 0.155556\n",
      "(Epoch 1 / 20) train acc: 0.634000; val_acc: 0.686111\n",
      "(Iteration 11 / 200) loss: 1.062007\n",
      "(Epoch 2 / 20) train acc: 0.867000; val_acc: 0.855556\n",
      "(Iteration 21 / 200) loss: 0.487047\n",
      "(Epoch 3 / 20) train acc: 0.926000; val_acc: 0.902778\n",
      "(Iteration 31 / 200) loss: 0.263913\n",
      "(Epoch 4 / 20) train acc: 0.954000; val_acc: 0.919444\n",
      "(Iteration 41 / 200) loss: 0.214403\n",
      "(Epoch 5 / 20) train acc: 0.953000; val_acc: 0.902778\n",
      "(Iteration 51 / 200) loss: 0.353264\n",
      "(Epoch 6 / 20) train acc: 0.970000; val_acc: 0.911111\n",
      "(Iteration 61 / 200) loss: 0.128680\n",
      "(Epoch 7 / 20) train acc: 0.977000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 0.070506\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.076469\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.146187\n",
      "(Epoch 10 / 20) train acc: 0.979000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.090388\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.944444\n",
      "(Iteration 111 / 200) loss: 0.057511\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.058642\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.046439\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.039473\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.037563\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.042240\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.057823\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.051147\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.036901\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.599000; val_acc: 0.544444\n",
      "(Iteration 11 / 200) loss: 1.058368\n",
      "(Epoch 2 / 20) train acc: 0.796000; val_acc: 0.769444\n",
      "(Iteration 21 / 200) loss: 0.860865\n",
      "(Epoch 3 / 20) train acc: 0.926000; val_acc: 0.880556\n",
      "(Iteration 31 / 200) loss: 0.342379\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 0.208557\n",
      "(Epoch 5 / 20) train acc: 0.909000; val_acc: 0.894444\n",
      "(Iteration 51 / 200) loss: 0.391339\n",
      "(Epoch 6 / 20) train acc: 0.948000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.150749\n",
      "(Epoch 7 / 20) train acc: 0.945000; val_acc: 0.905556\n",
      "(Iteration 71 / 200) loss: 0.079216\n",
      "(Epoch 8 / 20) train acc: 0.976000; val_acc: 0.938889\n",
      "(Iteration 81 / 200) loss: 0.201933\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.063026\n",
      "(Epoch 10 / 20) train acc: 0.978000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.085814\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.049354\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.061075\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.062708\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.074354\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.042682\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.063128\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.039650\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.033182\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.054084\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.502000; val_acc: 0.533333\n",
      "(Iteration 11 / 200) loss: 1.144811\n",
      "(Epoch 2 / 20) train acc: 0.846000; val_acc: 0.791667\n",
      "(Iteration 21 / 200) loss: 0.650180\n",
      "(Epoch 3 / 20) train acc: 0.901000; val_acc: 0.877778\n",
      "(Iteration 31 / 200) loss: 0.330417\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.241219\n",
      "(Epoch 5 / 20) train acc: 0.952000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 0.209116\n",
      "(Epoch 6 / 20) train acc: 0.968000; val_acc: 0.919444\n",
      "(Iteration 61 / 200) loss: 0.110494\n",
      "(Epoch 7 / 20) train acc: 0.968000; val_acc: 0.927778\n",
      "(Iteration 71 / 200) loss: 0.052549\n",
      "(Epoch 8 / 20) train acc: 0.976000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 0.117816\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 91 / 200) loss: 0.064268\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.042714\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.952778\n",
      "(Iteration 111 / 200) loss: 0.068537\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.108115\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.037412\n",
      "(Epoch 14 / 20) train acc: 0.954000; val_acc: 0.930556\n",
      "(Iteration 141 / 200) loss: 0.095906\n",
      "(Epoch 15 / 20) train acc: 0.959000; val_acc: 0.933333\n",
      "(Iteration 151 / 200) loss: 0.087758\n",
      "(Epoch 16 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.073566\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.041187\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.053972\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.936111\n",
      "(Iteration 191 / 200) loss: 0.060034\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.205000; val_acc: 0.194444\n",
      "(Epoch 1 / 20) train acc: 0.438000; val_acc: 0.441667\n",
      "(Iteration 11 / 200) loss: 1.610086\n",
      "(Epoch 2 / 20) train acc: 0.644000; val_acc: 0.644444\n",
      "(Iteration 21 / 200) loss: 0.958138\n",
      "(Epoch 3 / 20) train acc: 0.834000; val_acc: 0.811111\n",
      "(Iteration 31 / 200) loss: 0.477251\n",
      "(Epoch 4 / 20) train acc: 0.882000; val_acc: 0.886111\n",
      "(Iteration 41 / 200) loss: 0.445550\n",
      "(Epoch 5 / 20) train acc: 0.916000; val_acc: 0.902778\n",
      "(Iteration 51 / 200) loss: 0.253701\n",
      "(Epoch 6 / 20) train acc: 0.967000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.244938\n",
      "(Epoch 7 / 20) train acc: 0.969000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.140174\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.083360\n",
      "(Epoch 9 / 20) train acc: 0.968000; val_acc: 0.952778\n",
      "(Iteration 91 / 200) loss: 0.078741\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.055451\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.076666\n",
      "(Epoch 12 / 20) train acc: 0.997000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.055221\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 131 / 200) loss: 0.042305\n",
      "(Epoch 14 / 20) train acc: 0.983000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.078876\n",
      "(Epoch 15 / 20) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.086683\n",
      "(Epoch 16 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 161 / 200) loss: 0.122141\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.944444\n",
      "(Iteration 171 / 200) loss: 0.077161\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 181 / 200) loss: 0.229532\n",
      "(Epoch 19 / 20) train acc: 0.972000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.067336\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 4468.141051\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.369000; val_acc: 0.366667\n",
      "(Iteration 11 / 200) loss: 898.187361\n",
      "(Epoch 2 / 20) train acc: 0.708000; val_acc: 0.677778\n",
      "(Iteration 21 / 200) loss: 282.367116\n",
      "(Epoch 3 / 20) train acc: 0.854000; val_acc: 0.822222\n",
      "(Iteration 31 / 200) loss: 117.202949\n",
      "(Epoch 4 / 20) train acc: 0.890000; val_acc: 0.875000\n",
      "(Iteration 41 / 200) loss: 77.429048\n",
      "(Epoch 5 / 20) train acc: 0.932000; val_acc: 0.888889\n",
      "(Iteration 51 / 200) loss: 32.248960\n",
      "(Epoch 6 / 20) train acc: 0.912000; val_acc: 0.891667\n",
      "(Iteration 61 / 200) loss: 47.529972\n",
      "(Epoch 7 / 20) train acc: 0.937000; val_acc: 0.925000\n",
      "(Iteration 71 / 200) loss: 35.344686\n",
      "(Epoch 8 / 20) train acc: 0.949000; val_acc: 0.913889\n",
      "(Iteration 81 / 200) loss: 25.197612\n",
      "(Epoch 9 / 20) train acc: 0.953000; val_acc: 0.922222\n",
      "(Iteration 91 / 200) loss: 12.925358\n",
      "(Epoch 10 / 20) train acc: 0.952000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 9.784707\n",
      "(Epoch 11 / 20) train acc: 0.960000; val_acc: 0.925000\n",
      "(Iteration 111 / 200) loss: 19.172946\n",
      "(Epoch 12 / 20) train acc: 0.954000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 9.108236\n",
      "(Epoch 13 / 20) train acc: 0.974000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 7.185481\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.938889\n",
      "(Iteration 141 / 200) loss: 8.976103\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.938889\n",
      "(Iteration 151 / 200) loss: 15.754612\n",
      "(Epoch 16 / 20) train acc: 0.978000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 9.558921\n",
      "(Epoch 17 / 20) train acc: 0.987000; val_acc: 0.936111\n",
      "(Iteration 171 / 200) loss: 8.582875\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.938889\n",
      "(Iteration 181 / 200) loss: 8.300752\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.941667\n",
      "(Iteration 191 / 200) loss: 7.058284\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 4.076680\n",
      "(Epoch 0 / 20) train acc: 0.198000; val_acc: 0.205556\n",
      "(Epoch 1 / 20) train acc: 0.855000; val_acc: 0.802778\n",
      "(Iteration 11 / 200) loss: 0.744591\n",
      "(Epoch 2 / 20) train acc: 0.928000; val_acc: 0.900000\n",
      "(Iteration 21 / 200) loss: 0.349853\n",
      "(Epoch 3 / 20) train acc: 0.973000; val_acc: 0.919444\n",
      "(Iteration 31 / 200) loss: 0.238412\n",
      "(Epoch 4 / 20) train acc: 0.980000; val_acc: 0.977778\n",
      "(Iteration 41 / 200) loss: 0.107567\n",
      "(Epoch 5 / 20) train acc: 0.979000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 0.126714\n",
      "(Epoch 6 / 20) train acc: 0.992000; val_acc: 0.986111\n",
      "(Iteration 61 / 200) loss: 0.124879\n",
      "(Epoch 7 / 20) train acc: 0.979000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.131925\n",
      "(Epoch 8 / 20) train acc: 0.995000; val_acc: 0.986111\n",
      "(Iteration 81 / 200) loss: 0.099364\n",
      "(Epoch 9 / 20) train acc: 0.998000; val_acc: 0.991667\n",
      "(Iteration 91 / 200) loss: 0.086443\n",
      "(Epoch 10 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.079606\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.988889\n",
      "(Iteration 111 / 200) loss: 0.078088\n",
      "(Epoch 12 / 20) train acc: 0.999000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.078613\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 131 / 200) loss: 0.080305\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 141 / 200) loss: 0.069891\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.991667\n",
      "(Iteration 151 / 200) loss: 0.066874\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.066725\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.991667\n",
      "(Iteration 171 / 200) loss: 0.070194\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.991667\n",
      "(Iteration 181 / 200) loss: 0.071528\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.061052\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302668\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.608000; val_acc: 0.594444\n",
      "(Iteration 11 / 200) loss: 1.012667\n",
      "(Epoch 2 / 20) train acc: 0.870000; val_acc: 0.808333\n",
      "(Iteration 21 / 200) loss: 0.529842\n",
      "(Epoch 3 / 20) train acc: 0.921000; val_acc: 0.908333\n",
      "(Iteration 31 / 200) loss: 0.348259\n",
      "(Epoch 4 / 20) train acc: 0.955000; val_acc: 0.913889\n",
      "(Iteration 41 / 200) loss: 0.161409\n",
      "(Epoch 5 / 20) train acc: 0.901000; val_acc: 0.905556\n",
      "(Iteration 51 / 200) loss: 0.132721\n",
      "(Epoch 6 / 20) train acc: 0.952000; val_acc: 0.908333\n",
      "(Iteration 61 / 200) loss: 0.254110\n",
      "(Epoch 7 / 20) train acc: 0.977000; val_acc: 0.966667\n",
      "(Iteration 71 / 200) loss: 0.176282\n",
      "(Epoch 8 / 20) train acc: 0.978000; val_acc: 0.955556\n",
      "(Iteration 81 / 200) loss: 0.094364\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.938889\n",
      "(Iteration 91 / 200) loss: 0.089079\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.072924\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.067544\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.055749\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 131 / 200) loss: 0.047836\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.035204\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.040523\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.041805\n",
      "(Epoch 17 / 20) train acc: 0.959000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.090886\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.046478\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.088506\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.639000; val_acc: 0.622222\n",
      "(Iteration 11 / 200) loss: 1.104255\n",
      "(Epoch 2 / 20) train acc: 0.805000; val_acc: 0.775000\n",
      "(Iteration 21 / 200) loss: 0.519579\n",
      "(Epoch 3 / 20) train acc: 0.933000; val_acc: 0.905556\n",
      "(Iteration 31 / 200) loss: 0.201159\n",
      "(Epoch 4 / 20) train acc: 0.929000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.207306\n",
      "(Epoch 5 / 20) train acc: 0.977000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.062876\n",
      "(Epoch 6 / 20) train acc: 0.987000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.076671\n",
      "(Epoch 7 / 20) train acc: 0.969000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.094008\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.056254\n",
      "(Epoch 9 / 20) train acc: 0.995000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.111278\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.086346\n",
      "(Epoch 11 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.049503\n",
      "(Epoch 12 / 20) train acc: 0.973000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.099161\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.083585\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.069220\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.084842\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.081486\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.118431\n",
      "(Epoch 18 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.068845\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.042364\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.081000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.498000; val_acc: 0.458333\n",
      "(Iteration 11 / 200) loss: 1.339250\n",
      "(Epoch 2 / 20) train acc: 0.793000; val_acc: 0.780556\n",
      "(Iteration 21 / 200) loss: 0.740042\n",
      "(Epoch 3 / 20) train acc: 0.917000; val_acc: 0.900000\n",
      "(Iteration 31 / 200) loss: 0.331187\n",
      "(Epoch 4 / 20) train acc: 0.941000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.141546\n",
      "(Epoch 5 / 20) train acc: 0.957000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.103627\n",
      "(Epoch 6 / 20) train acc: 0.976000; val_acc: 0.969444\n",
      "(Iteration 61 / 200) loss: 0.099855\n",
      "(Epoch 7 / 20) train acc: 0.981000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.111122\n",
      "(Epoch 8 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.174251\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.042926\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.089785\n",
      "(Epoch 11 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.179024\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.063461\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.050546\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.043165\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.037598\n",
      "(Epoch 16 / 20) train acc: 0.972000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 0.069523\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.046746\n",
      "(Epoch 18 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.048617\n",
      "(Epoch 19 / 20) train acc: 0.985000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.117088\n",
      "(Epoch 20 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.146000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.381000; val_acc: 0.366667\n",
      "(Iteration 11 / 200) loss: 1.449838\n",
      "(Epoch 2 / 20) train acc: 0.831000; val_acc: 0.825000\n",
      "(Iteration 21 / 200) loss: 0.558329\n",
      "(Epoch 3 / 20) train acc: 0.892000; val_acc: 0.880556\n",
      "(Iteration 31 / 200) loss: 0.286755\n",
      "(Epoch 4 / 20) train acc: 0.941000; val_acc: 0.916667\n",
      "(Iteration 41 / 200) loss: 0.127832\n",
      "(Epoch 5 / 20) train acc: 0.945000; val_acc: 0.913889\n",
      "(Iteration 51 / 200) loss: 0.247334\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.922222\n",
      "(Iteration 61 / 200) loss: 0.222860\n",
      "(Epoch 7 / 20) train acc: 0.959000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.225424\n",
      "(Epoch 8 / 20) train acc: 0.930000; val_acc: 0.922222\n",
      "(Iteration 81 / 200) loss: 0.326971\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.952778\n",
      "(Iteration 91 / 200) loss: 0.125348\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.049759\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.922222\n",
      "(Iteration 111 / 200) loss: 0.119905\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.050365\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.062156\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.053691\n",
      "(Epoch 15 / 20) train acc: 0.982000; val_acc: 0.944444\n",
      "(Iteration 151 / 200) loss: 0.204869\n",
      "(Epoch 16 / 20) train acc: 0.972000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.072840\n",
      "(Epoch 17 / 20) train acc: 0.942000; val_acc: 0.911111\n",
      "(Iteration 171 / 200) loss: 0.180004\n",
      "(Epoch 18 / 20) train acc: 0.977000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.065895\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.057297\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2951.055695\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.448000; val_acc: 0.444444\n",
      "(Iteration 11 / 200) loss: 563.430278\n",
      "(Epoch 2 / 20) train acc: 0.766000; val_acc: 0.752778\n",
      "(Iteration 21 / 200) loss: 225.434422\n",
      "(Epoch 3 / 20) train acc: 0.852000; val_acc: 0.800000\n",
      "(Iteration 31 / 200) loss: 137.776298\n",
      "(Epoch 4 / 20) train acc: 0.895000; val_acc: 0.858333\n",
      "(Iteration 41 / 200) loss: 51.110981\n",
      "(Epoch 5 / 20) train acc: 0.902000; val_acc: 0.883333\n",
      "(Iteration 51 / 200) loss: 70.329317\n",
      "(Epoch 6 / 20) train acc: 0.911000; val_acc: 0.908333\n",
      "(Iteration 61 / 200) loss: 47.076720\n",
      "(Epoch 7 / 20) train acc: 0.947000; val_acc: 0.902778\n",
      "(Iteration 71 / 200) loss: 43.094191\n",
      "(Epoch 8 / 20) train acc: 0.957000; val_acc: 0.908333\n",
      "(Iteration 81 / 200) loss: 45.168641\n",
      "(Epoch 9 / 20) train acc: 0.965000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 15.016272\n",
      "(Epoch 10 / 20) train acc: 0.953000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 12.439018\n",
      "(Epoch 11 / 20) train acc: 0.961000; val_acc: 0.927778\n",
      "(Iteration 111 / 200) loss: 15.480086\n",
      "(Epoch 12 / 20) train acc: 0.972000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 10.097744\n",
      "(Epoch 13 / 20) train acc: 0.975000; val_acc: 0.911111\n",
      "(Iteration 131 / 200) loss: 7.218352\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.908333\n",
      "(Iteration 141 / 200) loss: 7.068927\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 14.753840\n",
      "(Epoch 16 / 20) train acc: 0.983000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 10.665234\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.908333\n",
      "(Iteration 171 / 200) loss: 21.002083\n",
      "(Epoch 18 / 20) train acc: 0.983000; val_acc: 0.913889\n",
      "(Iteration 181 / 200) loss: 8.952693\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.916667\n",
      "(Iteration 191 / 200) loss: 7.175785\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 4.452905\n",
      "(Epoch 0 / 20) train acc: 0.254000; val_acc: 0.252778\n",
      "(Epoch 1 / 20) train acc: 0.825000; val_acc: 0.775000\n",
      "(Iteration 11 / 200) loss: 0.464033\n",
      "(Epoch 2 / 20) train acc: 0.934000; val_acc: 0.900000\n",
      "(Iteration 21 / 200) loss: 0.297017\n",
      "(Epoch 3 / 20) train acc: 0.963000; val_acc: 0.947222\n",
      "(Iteration 31 / 200) loss: 0.157907\n",
      "(Epoch 4 / 20) train acc: 0.967000; val_acc: 0.944444\n",
      "(Iteration 41 / 200) loss: 0.173547\n",
      "(Epoch 5 / 20) train acc: 0.981000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 0.210773\n",
      "(Epoch 6 / 20) train acc: 0.974000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.150415\n",
      "(Epoch 7 / 20) train acc: 0.989000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.098345\n",
      "(Epoch 8 / 20) train acc: 0.993000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.096253\n",
      "(Epoch 9 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 91 / 200) loss: 0.096442\n",
      "(Epoch 10 / 20) train acc: 0.991000; val_acc: 0.988889\n",
      "(Iteration 101 / 200) loss: 0.084075\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.107840\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.087891\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.087598\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.079423\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.988889\n",
      "(Iteration 151 / 200) loss: 0.069049\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.070788\n",
      "(Epoch 17 / 20) train acc: 0.985000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.133327\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.086615\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.104991\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.303457\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.628000; val_acc: 0.647222\n",
      "(Iteration 11 / 200) loss: 1.085341\n",
      "(Epoch 2 / 20) train acc: 0.831000; val_acc: 0.780556\n",
      "(Iteration 21 / 200) loss: 0.603303\n",
      "(Epoch 3 / 20) train acc: 0.912000; val_acc: 0.877778\n",
      "(Iteration 31 / 200) loss: 0.268891\n",
      "(Epoch 4 / 20) train acc: 0.959000; val_acc: 0.927778\n",
      "(Iteration 41 / 200) loss: 0.135497\n",
      "(Epoch 5 / 20) train acc: 0.976000; val_acc: 0.952778\n",
      "(Iteration 51 / 200) loss: 0.102632\n",
      "(Epoch 6 / 20) train acc: 0.974000; val_acc: 0.969444\n",
      "(Iteration 61 / 200) loss: 0.073513\n",
      "(Epoch 7 / 20) train acc: 0.965000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.067142\n",
      "(Epoch 8 / 20) train acc: 0.944000; val_acc: 0.944444\n",
      "(Iteration 81 / 200) loss: 0.140654\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.070646\n",
      "(Epoch 10 / 20) train acc: 0.995000; val_acc: 0.988889\n",
      "(Iteration 101 / 200) loss: 0.053377\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.983333\n",
      "(Iteration 111 / 200) loss: 0.046251\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.050421\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.046319\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.101961\n",
      "(Epoch 15 / 20) train acc: 0.967000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.200658\n",
      "(Epoch 16 / 20) train acc: 0.963000; val_acc: 0.933333\n",
      "(Iteration 161 / 200) loss: 0.040524\n",
      "(Epoch 17 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.092970\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.062913\n",
      "(Epoch 19 / 20) train acc: 0.989000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.103814\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.679000; val_acc: 0.683333\n",
      "(Iteration 11 / 200) loss: 1.054260\n",
      "(Epoch 2 / 20) train acc: 0.788000; val_acc: 0.777778\n",
      "(Iteration 21 / 200) loss: 0.518410\n",
      "(Epoch 3 / 20) train acc: 0.879000; val_acc: 0.827778\n",
      "(Iteration 31 / 200) loss: 0.642386\n",
      "(Epoch 4 / 20) train acc: 0.946000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.174076\n",
      "(Epoch 5 / 20) train acc: 0.955000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.086438\n",
      "(Epoch 6 / 20) train acc: 0.972000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.070615\n",
      "(Epoch 7 / 20) train acc: 0.955000; val_acc: 0.919444\n",
      "(Iteration 71 / 200) loss: 0.193787\n",
      "(Epoch 8 / 20) train acc: 0.975000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 0.118464\n",
      "(Epoch 9 / 20) train acc: 0.984000; val_acc: 0.950000\n",
      "(Iteration 91 / 200) loss: 0.054381\n",
      "(Epoch 10 / 20) train acc: 0.978000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.067775\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.056148\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.952778\n",
      "(Iteration 121 / 200) loss: 0.072947\n",
      "(Epoch 13 / 20) train acc: 0.988000; val_acc: 0.925000\n",
      "(Iteration 131 / 200) loss: 0.066029\n",
      "(Epoch 14 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.112761\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.113364\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.049339\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.044344\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.048733\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.969444\n",
      "(Iteration 191 / 200) loss: 0.094696\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.638000; val_acc: 0.594444\n",
      "(Iteration 11 / 200) loss: 1.067703\n",
      "(Epoch 2 / 20) train acc: 0.827000; val_acc: 0.800000\n",
      "(Iteration 21 / 200) loss: 0.526749\n",
      "(Epoch 3 / 20) train acc: 0.900000; val_acc: 0.886111\n",
      "(Iteration 31 / 200) loss: 0.366238\n",
      "(Epoch 4 / 20) train acc: 0.944000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.164962\n",
      "(Epoch 5 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 0.254758\n",
      "(Epoch 6 / 20) train acc: 0.960000; val_acc: 0.922222\n",
      "(Iteration 61 / 200) loss: 0.077794\n",
      "(Epoch 7 / 20) train acc: 0.979000; val_acc: 0.930556\n",
      "(Iteration 71 / 200) loss: 0.054916\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.919444\n",
      "(Iteration 81 / 200) loss: 0.248391\n",
      "(Epoch 9 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.099917\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.129731\n",
      "(Epoch 11 / 20) train acc: 0.970000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.111761\n",
      "(Epoch 12 / 20) train acc: 0.974000; val_acc: 0.938889\n",
      "(Iteration 121 / 200) loss: 0.048310\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 0.098890\n",
      "(Epoch 14 / 20) train acc: 0.975000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.062483\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.047196\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.041619\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.051224\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.044852\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.044582\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.376000; val_acc: 0.422222\n",
      "(Iteration 11 / 200) loss: 1.494257\n",
      "(Epoch 2 / 20) train acc: 0.735000; val_acc: 0.697222\n",
      "(Iteration 21 / 200) loss: 0.878185\n",
      "(Epoch 3 / 20) train acc: 0.878000; val_acc: 0.850000\n",
      "(Iteration 31 / 200) loss: 0.428922\n",
      "(Epoch 4 / 20) train acc: 0.919000; val_acc: 0.894444\n",
      "(Iteration 41 / 200) loss: 0.254737\n",
      "(Epoch 5 / 20) train acc: 0.939000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 0.132062\n",
      "(Epoch 6 / 20) train acc: 0.978000; val_acc: 0.963889\n",
      "(Iteration 61 / 200) loss: 0.176817\n",
      "(Epoch 7 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.075469\n",
      "(Epoch 8 / 20) train acc: 0.977000; val_acc: 0.952778\n",
      "(Iteration 81 / 200) loss: 0.090088\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.066914\n",
      "(Epoch 10 / 20) train acc: 0.965000; val_acc: 0.927778\n",
      "(Iteration 101 / 200) loss: 0.078796\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.955556\n",
      "(Iteration 111 / 200) loss: 0.047263\n",
      "(Epoch 12 / 20) train acc: 0.985000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.070800\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.952778\n",
      "(Iteration 131 / 200) loss: 0.084178\n",
      "(Epoch 14 / 20) train acc: 0.983000; val_acc: 0.927778\n",
      "(Iteration 141 / 200) loss: 0.074236\n",
      "(Epoch 15 / 20) train acc: 0.970000; val_acc: 0.925000\n",
      "(Iteration 151 / 200) loss: 0.115522\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.060122\n",
      "(Epoch 17 / 20) train acc: 0.987000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.067510\n",
      "(Epoch 18 / 20) train acc: 0.985000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.041552\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.041570\n",
      "(Epoch 20 / 20) train acc: 0.982000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 3565.468857\n",
      "(Epoch 0 / 20) train acc: 0.137000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.497000; val_acc: 0.436111\n",
      "(Iteration 11 / 200) loss: 705.563440\n",
      "(Epoch 2 / 20) train acc: 0.686000; val_acc: 0.672222\n",
      "(Iteration 21 / 200) loss: 264.776930\n",
      "(Epoch 3 / 20) train acc: 0.823000; val_acc: 0.794444\n",
      "(Iteration 31 / 200) loss: 158.428306\n",
      "(Epoch 4 / 20) train acc: 0.893000; val_acc: 0.850000\n",
      "(Iteration 41 / 200) loss: 75.672539\n",
      "(Epoch 5 / 20) train acc: 0.908000; val_acc: 0.877778\n",
      "(Iteration 51 / 200) loss: 67.805591\n",
      "(Epoch 6 / 20) train acc: 0.933000; val_acc: 0.880556\n",
      "(Iteration 61 / 200) loss: 28.991479\n",
      "(Epoch 7 / 20) train acc: 0.931000; val_acc: 0.886111\n",
      "(Iteration 71 / 200) loss: 6.434095\n",
      "(Epoch 8 / 20) train acc: 0.950000; val_acc: 0.886111\n",
      "(Iteration 81 / 200) loss: 26.306955\n",
      "(Epoch 9 / 20) train acc: 0.939000; val_acc: 0.880556\n",
      "(Iteration 91 / 200) loss: 7.573480\n",
      "(Epoch 10 / 20) train acc: 0.960000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 2.786601\n",
      "(Epoch 11 / 20) train acc: 0.977000; val_acc: 0.905556\n",
      "(Iteration 111 / 200) loss: 5.213966\n",
      "(Epoch 12 / 20) train acc: 0.960000; val_acc: 0.913889\n",
      "(Iteration 121 / 200) loss: 0.703057\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.900000\n",
      "(Iteration 131 / 200) loss: 2.160867\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.902778\n",
      "(Iteration 141 / 200) loss: 3.059947\n",
      "(Epoch 15 / 20) train acc: 0.985000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 1.433749\n",
      "(Epoch 16 / 20) train acc: 0.976000; val_acc: 0.905556\n",
      "(Iteration 161 / 200) loss: 5.340711\n",
      "(Epoch 17 / 20) train acc: 0.990000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 16.343344\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.922222\n",
      "(Iteration 181 / 200) loss: 0.691913\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.911111\n",
      "(Iteration 191 / 200) loss: 0.690714\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 4.252322\n",
      "(Epoch 0 / 20) train acc: 0.208000; val_acc: 0.208333\n",
      "(Epoch 1 / 20) train acc: 0.743000; val_acc: 0.713889\n",
      "(Iteration 11 / 200) loss: 0.887203\n",
      "(Epoch 2 / 20) train acc: 0.883000; val_acc: 0.863889\n",
      "(Iteration 21 / 200) loss: 0.311312\n",
      "(Epoch 3 / 20) train acc: 0.949000; val_acc: 0.938889\n",
      "(Iteration 31 / 200) loss: 0.317977\n",
      "(Epoch 4 / 20) train acc: 0.948000; val_acc: 0.958333\n",
      "(Iteration 41 / 200) loss: 0.167890\n",
      "(Epoch 5 / 20) train acc: 0.968000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.062123\n",
      "(Epoch 6 / 20) train acc: 0.982000; val_acc: 0.961111\n",
      "(Iteration 61 / 200) loss: 0.054337\n",
      "(Epoch 7 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 71 / 200) loss: 0.072862\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.029515\n",
      "(Epoch 9 / 20) train acc: 0.984000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.078419\n",
      "(Epoch 10 / 20) train acc: 0.995000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.079313\n",
      "(Epoch 11 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.023644\n",
      "(Epoch 12 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.016301\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.014321\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.025678\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.026479\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.021015\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.015032\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 181 / 200) loss: 0.011959\n",
      "(Epoch 19 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.010611\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.303221\n",
      "(Epoch 0 / 20) train acc: 0.180000; val_acc: 0.180556\n",
      "(Epoch 1 / 20) train acc: 0.624000; val_acc: 0.577778\n",
      "(Iteration 11 / 200) loss: 1.163301\n",
      "(Epoch 2 / 20) train acc: 0.859000; val_acc: 0.852778\n",
      "(Iteration 21 / 200) loss: 0.348318\n",
      "(Epoch 3 / 20) train acc: 0.905000; val_acc: 0.908333\n",
      "(Iteration 31 / 200) loss: 0.299256\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.952778\n",
      "(Iteration 41 / 200) loss: 0.215294\n",
      "(Epoch 5 / 20) train acc: 0.969000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.100328\n",
      "(Epoch 6 / 20) train acc: 0.951000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 0.084775\n",
      "(Epoch 7 / 20) train acc: 0.984000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.091574\n",
      "(Epoch 8 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.068945\n",
      "(Epoch 9 / 20) train acc: 0.976000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.144283\n",
      "(Epoch 10 / 20) train acc: 0.970000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.015098\n",
      "(Epoch 11 / 20) train acc: 0.969000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.062568\n",
      "(Epoch 12 / 20) train acc: 0.955000; val_acc: 0.941667\n",
      "(Iteration 121 / 200) loss: 0.100396\n",
      "(Epoch 13 / 20) train acc: 0.952000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 0.237625\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.066127\n",
      "(Epoch 15 / 20) train acc: 0.985000; val_acc: 0.952778\n",
      "(Iteration 151 / 200) loss: 0.100318\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.066526\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.017010\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.035873\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 191 / 200) loss: 0.006702\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.382000; val_acc: 0.386111\n",
      "(Iteration 11 / 200) loss: 1.587704\n",
      "(Epoch 2 / 20) train acc: 0.653000; val_acc: 0.622222\n",
      "(Iteration 21 / 200) loss: 0.878011\n",
      "(Epoch 3 / 20) train acc: 0.858000; val_acc: 0.830556\n",
      "(Iteration 31 / 200) loss: 0.427841\n",
      "(Epoch 4 / 20) train acc: 0.909000; val_acc: 0.902778\n",
      "(Iteration 41 / 200) loss: 0.231312\n",
      "(Epoch 5 / 20) train acc: 0.939000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.146939\n",
      "(Epoch 6 / 20) train acc: 0.945000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.151568\n",
      "(Epoch 7 / 20) train acc: 0.925000; val_acc: 0.938889\n",
      "(Iteration 71 / 200) loss: 0.243171\n",
      "(Epoch 8 / 20) train acc: 0.961000; val_acc: 0.933333\n",
      "(Iteration 81 / 200) loss: 0.129926\n",
      "(Epoch 9 / 20) train acc: 0.961000; val_acc: 0.950000\n",
      "(Iteration 91 / 200) loss: 0.103755\n",
      "(Epoch 10 / 20) train acc: 0.990000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.078877\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.938889\n",
      "(Iteration 111 / 200) loss: 0.047017\n",
      "(Epoch 12 / 20) train acc: 0.981000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.059772\n",
      "(Epoch 13 / 20) train acc: 0.975000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.135379\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.955556\n",
      "(Iteration 141 / 200) loss: 0.061588\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.062368\n",
      "(Epoch 16 / 20) train acc: 0.976000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.046844\n",
      "(Epoch 17 / 20) train acc: 0.981000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.061218\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.112118\n",
      "(Epoch 19 / 20) train acc: 0.964000; val_acc: 0.947222\n",
      "(Iteration 191 / 200) loss: 0.048674\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.133000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.632000; val_acc: 0.602778\n",
      "(Iteration 11 / 200) loss: 1.183068\n",
      "(Epoch 2 / 20) train acc: 0.775000; val_acc: 0.786111\n",
      "(Iteration 21 / 200) loss: 0.692097\n",
      "(Epoch 3 / 20) train acc: 0.911000; val_acc: 0.877778\n",
      "(Iteration 31 / 200) loss: 0.504618\n",
      "(Epoch 4 / 20) train acc: 0.948000; val_acc: 0.930556\n",
      "(Iteration 41 / 200) loss: 0.131826\n",
      "(Epoch 5 / 20) train acc: 0.964000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.148211\n",
      "(Epoch 6 / 20) train acc: 0.968000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.069605\n",
      "(Epoch 7 / 20) train acc: 0.984000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.056248\n",
      "(Epoch 8 / 20) train acc: 0.976000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 0.053878\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.083785\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.057659\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.028697\n",
      "(Epoch 12 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.012518\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.013502\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.007791\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.034040\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.018628\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.013970\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.006014\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.012446\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.406000; val_acc: 0.416667\n",
      "(Iteration 11 / 200) loss: 1.675548\n",
      "(Epoch 2 / 20) train acc: 0.609000; val_acc: 0.588889\n",
      "(Iteration 21 / 200) loss: 0.806494\n",
      "(Epoch 3 / 20) train acc: 0.811000; val_acc: 0.808333\n",
      "(Iteration 31 / 200) loss: 0.476537\n",
      "(Epoch 4 / 20) train acc: 0.862000; val_acc: 0.847222\n",
      "(Iteration 41 / 200) loss: 0.392119\n",
      "(Epoch 5 / 20) train acc: 0.916000; val_acc: 0.905556\n",
      "(Iteration 51 / 200) loss: 0.322198\n",
      "(Epoch 6 / 20) train acc: 0.966000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.058697\n",
      "(Epoch 7 / 20) train acc: 0.970000; val_acc: 0.938889\n",
      "(Iteration 71 / 200) loss: 0.066424\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 81 / 200) loss: 0.072293\n",
      "(Epoch 9 / 20) train acc: 0.975000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.084383\n",
      "(Epoch 10 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.102115\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.027618\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.044832\n",
      "(Epoch 13 / 20) train acc: 0.997000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.029682\n",
      "(Epoch 14 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.015902\n",
      "(Epoch 15 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.041881\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.025475\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.013079\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.007580\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.005748\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 4558.357476\n",
      "(Epoch 0 / 20) train acc: 0.172000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.455000; val_acc: 0.433333\n",
      "(Iteration 11 / 200) loss: 719.112770\n",
      "(Epoch 2 / 20) train acc: 0.712000; val_acc: 0.680556\n",
      "(Iteration 21 / 200) loss: 190.655924\n",
      "(Epoch 3 / 20) train acc: 0.828000; val_acc: 0.830556\n",
      "(Iteration 31 / 200) loss: 146.131694\n",
      "(Epoch 4 / 20) train acc: 0.875000; val_acc: 0.863889\n",
      "(Iteration 41 / 200) loss: 91.787699\n",
      "(Epoch 5 / 20) train acc: 0.919000; val_acc: 0.877778\n",
      "(Iteration 51 / 200) loss: 41.082918\n",
      "(Epoch 6 / 20) train acc: 0.935000; val_acc: 0.880556\n",
      "(Iteration 61 / 200) loss: 44.241994\n",
      "(Epoch 7 / 20) train acc: 0.943000; val_acc: 0.897222\n",
      "(Iteration 71 / 200) loss: 48.336723\n",
      "(Epoch 8 / 20) train acc: 0.952000; val_acc: 0.911111\n",
      "(Iteration 81 / 200) loss: 6.516772\n",
      "(Epoch 9 / 20) train acc: 0.943000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 13.008831\n",
      "(Epoch 10 / 20) train acc: 0.961000; val_acc: 0.908333\n",
      "(Iteration 101 / 200) loss: 0.764076\n",
      "(Epoch 11 / 20) train acc: 0.975000; val_acc: 0.911111\n",
      "(Iteration 111 / 200) loss: 13.815925\n",
      "(Epoch 12 / 20) train acc: 0.975000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 0.920566\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.902778\n",
      "(Iteration 131 / 200) loss: 0.855444\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.900000\n",
      "(Iteration 141 / 200) loss: 1.310486\n",
      "(Epoch 15 / 20) train acc: 0.976000; val_acc: 0.900000\n",
      "(Iteration 151 / 200) loss: 1.993636\n",
      "(Epoch 16 / 20) train acc: 0.988000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 0.754482\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 1.050115\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.916667\n",
      "(Iteration 181 / 200) loss: 0.752831\n",
      "(Epoch 19 / 20) train acc: 0.998000; val_acc: 0.913889\n",
      "(Iteration 191 / 200) loss: 0.752160\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.913889\n",
      "(Iteration 1 / 200) loss: 5.454528\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.793000; val_acc: 0.747222\n",
      "(Iteration 11 / 200) loss: 0.897438\n",
      "(Epoch 2 / 20) train acc: 0.904000; val_acc: 0.888889\n",
      "(Iteration 21 / 200) loss: 0.202116\n",
      "(Epoch 3 / 20) train acc: 0.931000; val_acc: 0.919444\n",
      "(Iteration 31 / 200) loss: 0.149711\n",
      "(Epoch 4 / 20) train acc: 0.958000; val_acc: 0.958333\n",
      "(Iteration 41 / 200) loss: 0.039742\n",
      "(Epoch 5 / 20) train acc: 0.984000; val_acc: 0.966667\n",
      "(Iteration 51 / 200) loss: 0.151019\n",
      "(Epoch 6 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 61 / 200) loss: 0.048367\n",
      "(Epoch 7 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 71 / 200) loss: 0.021211\n",
      "(Epoch 8 / 20) train acc: 0.991000; val_acc: 0.986111\n",
      "(Iteration 81 / 200) loss: 0.044154\n",
      "(Epoch 9 / 20) train acc: 0.993000; val_acc: 0.988889\n",
      "(Iteration 91 / 200) loss: 0.029687\n",
      "(Epoch 10 / 20) train acc: 0.997000; val_acc: 0.991667\n",
      "(Iteration 101 / 200) loss: 0.016682\n",
      "(Epoch 11 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 111 / 200) loss: 0.015790\n",
      "(Epoch 12 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.037175\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 131 / 200) loss: 0.024322\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.013548\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.014047\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.034200\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.027709\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.013876\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.022652\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302866\n",
      "(Epoch 0 / 20) train acc: 0.320000; val_acc: 0.288889\n",
      "(Epoch 1 / 20) train acc: 0.652000; val_acc: 0.663889\n",
      "(Iteration 11 / 200) loss: 1.107054\n",
      "(Epoch 2 / 20) train acc: 0.902000; val_acc: 0.866667\n",
      "(Iteration 21 / 200) loss: 0.208047\n",
      "(Epoch 3 / 20) train acc: 0.943000; val_acc: 0.938889\n",
      "(Iteration 31 / 200) loss: 0.186274\n",
      "(Epoch 4 / 20) train acc: 0.953000; val_acc: 0.922222\n",
      "(Iteration 41 / 200) loss: 0.227577\n",
      "(Epoch 5 / 20) train acc: 0.974000; val_acc: 0.938889\n",
      "(Iteration 51 / 200) loss: 0.090516\n",
      "(Epoch 6 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.031903\n",
      "(Epoch 7 / 20) train acc: 0.982000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.042117\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.944444\n",
      "(Iteration 81 / 200) loss: 0.046232\n",
      "(Epoch 9 / 20) train acc: 0.982000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.081143\n",
      "(Epoch 10 / 20) train acc: 0.979000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.010502\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.023138\n",
      "(Epoch 12 / 20) train acc: 0.996000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.011573\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.007158\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.018129\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.053076\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.012673\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.016834\n",
      "(Epoch 18 / 20) train acc: 0.975000; val_acc: 0.936111\n",
      "(Iteration 181 / 200) loss: 0.055897\n",
      "(Epoch 19 / 20) train acc: 0.982000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.012170\n",
      "(Epoch 20 / 20) train acc: 0.982000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.577000; val_acc: 0.619444\n",
      "(Iteration 11 / 200) loss: 1.219463\n",
      "(Epoch 2 / 20) train acc: 0.767000; val_acc: 0.755556\n",
      "(Iteration 21 / 200) loss: 0.571892\n",
      "(Epoch 3 / 20) train acc: 0.913000; val_acc: 0.911111\n",
      "(Iteration 31 / 200) loss: 0.208415\n",
      "(Epoch 4 / 20) train acc: 0.949000; val_acc: 0.913889\n",
      "(Iteration 41 / 200) loss: 0.150668\n",
      "(Epoch 5 / 20) train acc: 0.965000; val_acc: 0.927778\n",
      "(Iteration 51 / 200) loss: 0.105549\n",
      "(Epoch 6 / 20) train acc: 0.980000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.269949\n",
      "(Epoch 7 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.050712\n",
      "(Epoch 8 / 20) train acc: 0.980000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.027746\n",
      "(Epoch 9 / 20) train acc: 0.988000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.030273\n",
      "(Epoch 10 / 20) train acc: 0.976000; val_acc: 0.938889\n",
      "(Iteration 101 / 200) loss: 0.051033\n",
      "(Epoch 11 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.031757\n",
      "(Epoch 12 / 20) train acc: 0.971000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.105218\n",
      "(Epoch 13 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.072513\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.008174\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.952778\n",
      "(Iteration 151 / 200) loss: 0.072725\n",
      "(Epoch 16 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.015839\n",
      "(Epoch 17 / 20) train acc: 0.980000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.070389\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.010369\n",
      "(Epoch 19 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.018004\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.475000; val_acc: 0.447222\n",
      "(Iteration 11 / 200) loss: 1.486854\n",
      "(Epoch 2 / 20) train acc: 0.756000; val_acc: 0.752778\n",
      "(Iteration 21 / 200) loss: 0.765841\n",
      "(Epoch 3 / 20) train acc: 0.820000; val_acc: 0.811111\n",
      "(Iteration 31 / 200) loss: 0.424880\n",
      "(Epoch 4 / 20) train acc: 0.937000; val_acc: 0.891667\n",
      "(Iteration 41 / 200) loss: 0.280757\n",
      "(Epoch 5 / 20) train acc: 0.943000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.177273\n",
      "(Epoch 6 / 20) train acc: 0.944000; val_acc: 0.905556\n",
      "(Iteration 61 / 200) loss: 0.169693\n",
      "(Epoch 7 / 20) train acc: 0.966000; val_acc: 0.930556\n",
      "(Iteration 71 / 200) loss: 0.134498\n",
      "(Epoch 8 / 20) train acc: 0.981000; val_acc: 0.933333\n",
      "(Iteration 81 / 200) loss: 0.071203\n",
      "(Epoch 9 / 20) train acc: 0.988000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.034181\n",
      "(Epoch 10 / 20) train acc: 0.952000; val_acc: 0.936111\n",
      "(Iteration 101 / 200) loss: 0.047059\n",
      "(Epoch 11 / 20) train acc: 0.967000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.014866\n",
      "(Epoch 12 / 20) train acc: 0.979000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.081379\n",
      "(Epoch 13 / 20) train acc: 0.984000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 0.079915\n",
      "(Epoch 14 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.094060\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.030794\n",
      "(Epoch 16 / 20) train acc: 0.977000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.059933\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.060377\n",
      "(Epoch 18 / 20) train acc: 0.986000; val_acc: 0.927778\n",
      "(Iteration 181 / 200) loss: 0.071298\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.010284\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.518000; val_acc: 0.486111\n",
      "(Iteration 11 / 200) loss: 1.400353\n",
      "(Epoch 2 / 20) train acc: 0.774000; val_acc: 0.752778\n",
      "(Iteration 21 / 200) loss: 0.678621\n",
      "(Epoch 3 / 20) train acc: 0.875000; val_acc: 0.841667\n",
      "(Iteration 31 / 200) loss: 0.458037\n",
      "(Epoch 4 / 20) train acc: 0.870000; val_acc: 0.841667\n",
      "(Iteration 41 / 200) loss: 0.270169\n",
      "(Epoch 5 / 20) train acc: 0.937000; val_acc: 0.908333\n",
      "(Iteration 51 / 200) loss: 0.232071\n",
      "(Epoch 6 / 20) train acc: 0.953000; val_acc: 0.913889\n",
      "(Iteration 61 / 200) loss: 0.247405\n",
      "(Epoch 7 / 20) train acc: 0.950000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.220026\n",
      "(Epoch 8 / 20) train acc: 0.960000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.116935\n",
      "(Epoch 9 / 20) train acc: 0.958000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.174931\n",
      "(Epoch 10 / 20) train acc: 0.971000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.081147\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.015610\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.023363\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.024165\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.020583\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.018179\n",
      "(Epoch 16 / 20) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.028426\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.022387\n",
      "(Epoch 18 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.016091\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.069717\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 3359.708485\n",
      "(Epoch 0 / 20) train acc: 0.212000; val_acc: 0.202778\n",
      "(Epoch 1 / 20) train acc: 0.413000; val_acc: 0.411111\n",
      "(Iteration 11 / 200) loss: 853.701035\n",
      "(Epoch 2 / 20) train acc: 0.726000; val_acc: 0.711111\n",
      "(Iteration 21 / 200) loss: 235.353390\n",
      "(Epoch 3 / 20) train acc: 0.866000; val_acc: 0.813889\n",
      "(Iteration 31 / 200) loss: 122.407733\n",
      "(Epoch 4 / 20) train acc: 0.893000; val_acc: 0.875000\n",
      "(Iteration 41 / 200) loss: 35.816035\n",
      "(Epoch 5 / 20) train acc: 0.946000; val_acc: 0.900000\n",
      "(Iteration 51 / 200) loss: 9.642598\n",
      "(Epoch 6 / 20) train acc: 0.936000; val_acc: 0.908333\n",
      "(Iteration 61 / 200) loss: 45.934253\n",
      "(Epoch 7 / 20) train acc: 0.957000; val_acc: 0.897222\n",
      "(Iteration 71 / 200) loss: 17.728116\n",
      "(Epoch 8 / 20) train acc: 0.964000; val_acc: 0.936111\n",
      "(Iteration 81 / 200) loss: 4.722393\n",
      "(Epoch 9 / 20) train acc: 0.974000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 11.518392\n",
      "(Epoch 10 / 20) train acc: 0.981000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 0.733247\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.930556\n",
      "(Iteration 111 / 200) loss: 9.275835\n",
      "(Epoch 12 / 20) train acc: 0.978000; val_acc: 0.925000\n",
      "(Iteration 121 / 200) loss: 3.022927\n",
      "(Epoch 13 / 20) train acc: 0.985000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.726598\n",
      "(Epoch 14 / 20) train acc: 0.979000; val_acc: 0.919444\n",
      "(Iteration 141 / 200) loss: 5.051157\n",
      "(Epoch 15 / 20) train acc: 0.983000; val_acc: 0.922222\n",
      "(Iteration 151 / 200) loss: 4.359040\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.922222\n",
      "(Iteration 161 / 200) loss: 2.842302\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.930556\n",
      "(Iteration 171 / 200) loss: 1.581259\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.936111\n",
      "(Iteration 181 / 200) loss: 1.308313\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.938889\n",
      "(Iteration 191 / 200) loss: 4.665096\n",
      "(Epoch 20 / 20) train acc: 0.983000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 5.943386\n",
      "(Epoch 0 / 20) train acc: 0.237000; val_acc: 0.272222\n",
      "(Epoch 1 / 20) train acc: 0.656000; val_acc: 0.686111\n",
      "(Iteration 11 / 200) loss: 1.030652\n",
      "(Epoch 2 / 20) train acc: 0.896000; val_acc: 0.888889\n",
      "(Iteration 21 / 200) loss: 0.494160\n",
      "(Epoch 3 / 20) train acc: 0.948000; val_acc: 0.933333\n",
      "(Iteration 31 / 200) loss: 0.157030\n",
      "(Epoch 4 / 20) train acc: 0.954000; val_acc: 0.947222\n",
      "(Iteration 41 / 200) loss: 0.173326\n",
      "(Epoch 5 / 20) train acc: 0.973000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.071881\n",
      "(Epoch 6 / 20) train acc: 0.977000; val_acc: 0.961111\n",
      "(Iteration 61 / 200) loss: 0.112258\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.070881\n",
      "(Epoch 8 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 81 / 200) loss: 0.024906\n",
      "(Epoch 9 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 0.030281\n",
      "(Epoch 10 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.017625\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.977778\n",
      "(Iteration 111 / 200) loss: 0.031376\n",
      "(Epoch 12 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.048901\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.983333\n",
      "(Iteration 131 / 200) loss: 0.012009\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.034964\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.013440\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.015415\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.015564\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.014762\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.010604\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.302654\n",
      "(Epoch 0 / 20) train acc: 0.188000; val_acc: 0.219444\n",
      "(Epoch 1 / 20) train acc: 0.794000; val_acc: 0.738889\n",
      "(Iteration 11 / 200) loss: 0.621593\n",
      "(Epoch 2 / 20) train acc: 0.795000; val_acc: 0.800000\n",
      "(Iteration 21 / 200) loss: 0.609868\n",
      "(Epoch 3 / 20) train acc: 0.915000; val_acc: 0.880556\n",
      "(Iteration 31 / 200) loss: 0.200466\n",
      "(Epoch 4 / 20) train acc: 0.945000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.257017\n",
      "(Epoch 5 / 20) train acc: 0.952000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.075319\n",
      "(Epoch 6 / 20) train acc: 0.935000; val_acc: 0.925000\n",
      "(Iteration 61 / 200) loss: 0.175165\n",
      "(Epoch 7 / 20) train acc: 0.957000; val_acc: 0.911111\n",
      "(Iteration 71 / 200) loss: 0.212538\n",
      "(Epoch 8 / 20) train acc: 0.980000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.042842\n",
      "(Epoch 9 / 20) train acc: 0.983000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.029403\n",
      "(Epoch 10 / 20) train acc: 0.968000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.160096\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.051062\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.011185\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.006908\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.031286\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.009707\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.030799\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.005827\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.007871\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.038702\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.477000; val_acc: 0.436111\n",
      "(Iteration 11 / 200) loss: 1.389245\n",
      "(Epoch 2 / 20) train acc: 0.643000; val_acc: 0.683333\n",
      "(Iteration 21 / 200) loss: 0.897954\n",
      "(Epoch 3 / 20) train acc: 0.864000; val_acc: 0.822222\n",
      "(Iteration 31 / 200) loss: 0.305683\n",
      "(Epoch 4 / 20) train acc: 0.893000; val_acc: 0.883333\n",
      "(Iteration 41 / 200) loss: 0.330174\n",
      "(Epoch 5 / 20) train acc: 0.921000; val_acc: 0.877778\n",
      "(Iteration 51 / 200) loss: 0.273300\n",
      "(Epoch 6 / 20) train acc: 0.952000; val_acc: 0.911111\n",
      "(Iteration 61 / 200) loss: 0.259865\n",
      "(Epoch 7 / 20) train acc: 0.962000; val_acc: 0.900000\n",
      "(Iteration 71 / 200) loss: 0.140121\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.947222\n",
      "(Iteration 81 / 200) loss: 0.065838\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.986111\n",
      "(Iteration 91 / 200) loss: 0.033016\n",
      "(Epoch 10 / 20) train acc: 0.973000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.027793\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.069115\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.018103\n",
      "(Epoch 13 / 20) train acc: 0.985000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.048638\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.032250\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.007015\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.030807\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.007288\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.006810\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.012191\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.375000; val_acc: 0.405556\n",
      "(Iteration 11 / 200) loss: 1.700496\n",
      "(Epoch 2 / 20) train acc: 0.777000; val_acc: 0.788889\n",
      "(Iteration 21 / 200) loss: 0.534149\n",
      "(Epoch 3 / 20) train acc: 0.885000; val_acc: 0.875000\n",
      "(Iteration 31 / 200) loss: 0.303330\n",
      "(Epoch 4 / 20) train acc: 0.934000; val_acc: 0.927778\n",
      "(Iteration 41 / 200) loss: 0.160802\n",
      "(Epoch 5 / 20) train acc: 0.948000; val_acc: 0.955556\n",
      "(Iteration 51 / 200) loss: 0.062281\n",
      "(Epoch 6 / 20) train acc: 0.961000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.101100\n",
      "(Epoch 7 / 20) train acc: 0.956000; val_acc: 0.922222\n",
      "(Iteration 71 / 200) loss: 0.191429\n",
      "(Epoch 8 / 20) train acc: 0.976000; val_acc: 0.947222\n",
      "(Iteration 81 / 200) loss: 0.067018\n",
      "(Epoch 9 / 20) train acc: 0.982000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.036222\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.069144\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.075172\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.021276\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.021264\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.024812\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.012936\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.008224\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.009551\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.006839\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.007265\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.385000; val_acc: 0.319444\n",
      "(Iteration 11 / 200) loss: 1.631790\n",
      "(Epoch 2 / 20) train acc: 0.527000; val_acc: 0.561111\n",
      "(Iteration 21 / 200) loss: 1.193477\n",
      "(Epoch 3 / 20) train acc: 0.845000; val_acc: 0.838889\n",
      "(Iteration 31 / 200) loss: 0.405376\n",
      "(Epoch 4 / 20) train acc: 0.864000; val_acc: 0.816667\n",
      "(Iteration 41 / 200) loss: 0.296901\n",
      "(Epoch 5 / 20) train acc: 0.911000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.116684\n",
      "(Epoch 6 / 20) train acc: 0.928000; val_acc: 0.886111\n",
      "(Iteration 61 / 200) loss: 0.258272\n",
      "(Epoch 7 / 20) train acc: 0.953000; val_acc: 0.916667\n",
      "(Iteration 71 / 200) loss: 0.101010\n",
      "(Epoch 8 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 81 / 200) loss: 0.076733\n",
      "(Epoch 9 / 20) train acc: 0.970000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.073296\n",
      "(Epoch 10 / 20) train acc: 0.969000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.181020\n",
      "(Epoch 11 / 20) train acc: 0.938000; val_acc: 0.900000\n",
      "(Iteration 111 / 200) loss: 0.191850\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.113732\n",
      "(Epoch 13 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.039414\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.023146\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.018408\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.024379\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.020707\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.010598\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.046709\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.947222\n",
      "(Iteration 1 / 200) loss: 5067.731119\n",
      "(Epoch 0 / 20) train acc: 0.042000; val_acc: 0.055556\n",
      "(Epoch 1 / 20) train acc: 0.285000; val_acc: 0.313889\n",
      "(Iteration 11 / 200) loss: 1229.715035\n",
      "(Epoch 2 / 20) train acc: 0.715000; val_acc: 0.675000\n",
      "(Iteration 21 / 200) loss: 329.908022\n",
      "(Epoch 3 / 20) train acc: 0.822000; val_acc: 0.791667\n",
      "(Iteration 31 / 200) loss: 72.350263\n",
      "(Epoch 4 / 20) train acc: 0.866000; val_acc: 0.819444\n",
      "(Iteration 41 / 200) loss: 78.327897\n",
      "(Epoch 5 / 20) train acc: 0.903000; val_acc: 0.852778\n",
      "(Iteration 51 / 200) loss: 56.913596\n",
      "(Epoch 6 / 20) train acc: 0.905000; val_acc: 0.855556\n",
      "(Iteration 61 / 200) loss: 64.444763\n",
      "(Epoch 7 / 20) train acc: 0.945000; val_acc: 0.883333\n",
      "(Iteration 71 / 200) loss: 74.071107\n",
      "(Epoch 8 / 20) train acc: 0.946000; val_acc: 0.883333\n",
      "(Iteration 81 / 200) loss: 2.117850\n",
      "(Epoch 9 / 20) train acc: 0.954000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 13.711300\n",
      "(Epoch 10 / 20) train acc: 0.967000; val_acc: 0.900000\n",
      "(Iteration 101 / 200) loss: 7.810620\n",
      "(Epoch 11 / 20) train acc: 0.962000; val_acc: 0.886111\n",
      "(Iteration 111 / 200) loss: 9.900382\n",
      "(Epoch 12 / 20) train acc: 0.975000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 2.028632\n",
      "(Epoch 13 / 20) train acc: 0.990000; val_acc: 0.897222\n",
      "(Iteration 131 / 200) loss: 6.721284\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.902778\n",
      "(Iteration 141 / 200) loss: 10.149837\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.900000\n",
      "(Iteration 151 / 200) loss: 0.074615\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.897222\n",
      "(Iteration 161 / 200) loss: 0.074510\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.785280\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.897222\n",
      "(Iteration 181 / 200) loss: 1.949342\n",
      "(Epoch 19 / 20) train acc: 0.984000; val_acc: 0.894444\n",
      "(Iteration 191 / 200) loss: 1.427658\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.916667\n",
      "(Iteration 1 / 200) loss: 6.574360\n",
      "(Epoch 0 / 20) train acc: 0.196000; val_acc: 0.208333\n",
      "(Epoch 1 / 20) train acc: 0.819000; val_acc: 0.791667\n",
      "(Iteration 11 / 200) loss: 0.583562\n",
      "(Epoch 2 / 20) train acc: 0.905000; val_acc: 0.888889\n",
      "(Iteration 21 / 200) loss: 0.227890\n",
      "(Epoch 3 / 20) train acc: 0.943000; val_acc: 0.888889\n",
      "(Iteration 31 / 200) loss: 0.111553\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.922222\n",
      "(Iteration 41 / 200) loss: 0.081226\n",
      "(Epoch 5 / 20) train acc: 0.993000; val_acc: 0.952778\n",
      "(Iteration 51 / 200) loss: 0.032562\n",
      "(Epoch 6 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 61 / 200) loss: 0.018807\n",
      "(Epoch 7 / 20) train acc: 0.994000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.005371\n",
      "(Epoch 8 / 20) train acc: 0.998000; val_acc: 0.955556\n",
      "(Iteration 81 / 200) loss: 0.026708\n",
      "(Epoch 9 / 20) train acc: 0.995000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.037607\n",
      "(Epoch 10 / 20) train acc: 0.998000; val_acc: 0.966667\n",
      "(Iteration 101 / 200) loss: 0.039470\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.004034\n",
      "(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.013872\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.006286\n",
      "(Epoch 14 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.008537\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.003206\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.003432\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.002396\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 181 / 200) loss: 0.001418\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.001836\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.302504\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.632000; val_acc: 0.622222\n",
      "(Iteration 11 / 200) loss: 0.957420\n",
      "(Epoch 2 / 20) train acc: 0.851000; val_acc: 0.863889\n",
      "(Iteration 21 / 200) loss: 0.300411\n",
      "(Epoch 3 / 20) train acc: 0.887000; val_acc: 0.869444\n",
      "(Iteration 31 / 200) loss: 0.458731\n",
      "(Epoch 4 / 20) train acc: 0.944000; val_acc: 0.938889\n",
      "(Iteration 41 / 200) loss: 0.101933\n",
      "(Epoch 5 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 51 / 200) loss: 0.039353\n",
      "(Epoch 6 / 20) train acc: 0.975000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.119492\n",
      "(Epoch 7 / 20) train acc: 0.975000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.035715\n",
      "(Epoch 8 / 20) train acc: 0.979000; val_acc: 0.947222\n",
      "(Iteration 81 / 200) loss: 0.073532\n",
      "(Epoch 9 / 20) train acc: 0.983000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.071190\n",
      "(Epoch 10 / 20) train acc: 0.982000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.045256\n",
      "(Epoch 11 / 20) train acc: 0.979000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.124061\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.043418\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.020254\n",
      "(Epoch 14 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.066839\n",
      "(Epoch 15 / 20) train acc: 0.982000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.035069\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.007333\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.022996\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.023808\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.022788\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.698000; val_acc: 0.691667\n",
      "(Iteration 11 / 200) loss: 1.060146\n",
      "(Epoch 2 / 20) train acc: 0.771000; val_acc: 0.800000\n",
      "(Iteration 21 / 200) loss: 1.015281\n",
      "(Epoch 3 / 20) train acc: 0.894000; val_acc: 0.869444\n",
      "(Iteration 31 / 200) loss: 0.398715\n",
      "(Epoch 4 / 20) train acc: 0.932000; val_acc: 0.900000\n",
      "(Iteration 41 / 200) loss: 0.302146\n",
      "(Epoch 5 / 20) train acc: 0.970000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.060658\n",
      "(Epoch 6 / 20) train acc: 0.951000; val_acc: 0.927778\n",
      "(Iteration 61 / 200) loss: 0.156650\n",
      "(Epoch 7 / 20) train acc: 0.975000; val_acc: 0.927778\n",
      "(Iteration 71 / 200) loss: 0.231697\n",
      "(Epoch 8 / 20) train acc: 0.966000; val_acc: 0.927778\n",
      "(Iteration 81 / 200) loss: 0.123473\n",
      "(Epoch 9 / 20) train acc: 0.996000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.036309\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.049752\n",
      "(Epoch 11 / 20) train acc: 0.975000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.070527\n",
      "(Epoch 12 / 20) train acc: 0.998000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.017643\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.022539\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.118531\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.021280\n",
      "(Epoch 16 / 20) train acc: 0.978000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.066590\n",
      "(Epoch 17 / 20) train acc: 0.956000; val_acc: 0.936111\n",
      "(Iteration 171 / 200) loss: 0.017084\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.007199\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.022133\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.594000; val_acc: 0.602778\n",
      "(Iteration 11 / 200) loss: 1.362988\n",
      "(Epoch 2 / 20) train acc: 0.772000; val_acc: 0.694444\n",
      "(Iteration 21 / 200) loss: 0.750321\n",
      "(Epoch 3 / 20) train acc: 0.849000; val_acc: 0.833333\n",
      "(Iteration 31 / 200) loss: 0.424683\n",
      "(Epoch 4 / 20) train acc: 0.939000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.180045\n",
      "(Epoch 5 / 20) train acc: 0.944000; val_acc: 0.902778\n",
      "(Iteration 51 / 200) loss: 0.213858\n",
      "(Epoch 6 / 20) train acc: 0.972000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.040344\n",
      "(Epoch 7 / 20) train acc: 0.963000; val_acc: 0.922222\n",
      "(Iteration 71 / 200) loss: 0.113241\n",
      "(Epoch 8 / 20) train acc: 0.972000; val_acc: 0.955556\n",
      "(Iteration 81 / 200) loss: 0.095886\n",
      "(Epoch 9 / 20) train acc: 0.985000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.074236\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.019647\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.078569\n",
      "(Epoch 12 / 20) train acc: 0.996000; val_acc: 0.958333\n",
      "(Iteration 121 / 200) loss: 0.012348\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.057317\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.049332\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.016355\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.005571\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.947222\n",
      "(Iteration 171 / 200) loss: 0.035818\n",
      "(Epoch 18 / 20) train acc: 0.975000; val_acc: 0.938889\n",
      "(Iteration 181 / 200) loss: 0.061110\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.035552\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.081000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.275468\n",
      "(Epoch 2 / 20) train acc: 0.302000; val_acc: 0.277778\n",
      "(Iteration 21 / 200) loss: 1.787421\n",
      "(Epoch 3 / 20) train acc: 0.551000; val_acc: 0.508333\n",
      "(Iteration 31 / 200) loss: 1.428445\n",
      "(Epoch 4 / 20) train acc: 0.680000; val_acc: 0.669444\n",
      "(Iteration 41 / 200) loss: 1.091255\n",
      "(Epoch 5 / 20) train acc: 0.778000; val_acc: 0.738889\n",
      "(Iteration 51 / 200) loss: 0.727310\n",
      "(Epoch 6 / 20) train acc: 0.870000; val_acc: 0.855556\n",
      "(Iteration 61 / 200) loss: 0.411767\n",
      "(Epoch 7 / 20) train acc: 0.897000; val_acc: 0.902778\n",
      "(Iteration 71 / 200) loss: 0.206291\n",
      "(Epoch 8 / 20) train acc: 0.909000; val_acc: 0.894444\n",
      "(Iteration 81 / 200) loss: 0.275940\n",
      "(Epoch 9 / 20) train acc: 0.914000; val_acc: 0.908333\n",
      "(Iteration 91 / 200) loss: 0.289225\n",
      "(Epoch 10 / 20) train acc: 0.908000; val_acc: 0.869444\n",
      "(Iteration 101 / 200) loss: 0.356352\n",
      "(Epoch 11 / 20) train acc: 0.942000; val_acc: 0.905556\n",
      "(Iteration 111 / 200) loss: 0.135467\n",
      "(Epoch 12 / 20) train acc: 0.941000; val_acc: 0.902778\n",
      "(Iteration 121 / 200) loss: 0.163918\n",
      "(Epoch 13 / 20) train acc: 0.945000; val_acc: 0.919444\n",
      "(Iteration 131 / 200) loss: 0.329928\n",
      "(Epoch 14 / 20) train acc: 0.962000; val_acc: 0.930556\n",
      "(Iteration 141 / 200) loss: 0.175742\n",
      "(Epoch 15 / 20) train acc: 0.954000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.107725\n",
      "(Epoch 16 / 20) train acc: 0.950000; val_acc: 0.936111\n",
      "(Iteration 161 / 200) loss: 0.099955\n",
      "(Epoch 17 / 20) train acc: 0.958000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.221712\n",
      "(Epoch 18 / 20) train acc: 0.966000; val_acc: 0.950000\n",
      "(Iteration 181 / 200) loss: 0.102009\n",
      "(Epoch 19 / 20) train acc: 0.941000; val_acc: 0.922222\n",
      "(Iteration 191 / 200) loss: 0.156893\n",
      "(Epoch 20 / 20) train acc: 0.971000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 5534.863426\n",
      "(Epoch 0 / 20) train acc: 0.084000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.438000; val_acc: 0.391667\n",
      "(Iteration 11 / 200) loss: 1323.829576\n",
      "(Epoch 2 / 20) train acc: 0.755000; val_acc: 0.733333\n",
      "(Iteration 21 / 200) loss: 193.116968\n",
      "(Epoch 3 / 20) train acc: 0.824000; val_acc: 0.813889\n",
      "(Iteration 31 / 200) loss: 128.949553\n",
      "(Epoch 4 / 20) train acc: 0.871000; val_acc: 0.855556\n",
      "(Iteration 41 / 200) loss: 57.205055\n",
      "(Epoch 5 / 20) train acc: 0.926000; val_acc: 0.897222\n",
      "(Iteration 51 / 200) loss: 16.164607\n",
      "(Epoch 6 / 20) train acc: 0.933000; val_acc: 0.902778\n",
      "(Iteration 61 / 200) loss: 17.053282\n",
      "(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.913889\n",
      "(Iteration 71 / 200) loss: 18.200100\n",
      "(Epoch 8 / 20) train acc: 0.958000; val_acc: 0.927778\n",
      "(Iteration 81 / 200) loss: 0.425855\n",
      "(Epoch 9 / 20) train acc: 0.963000; val_acc: 0.933333\n",
      "(Iteration 91 / 200) loss: 0.076418\n",
      "(Epoch 10 / 20) train acc: 0.971000; val_acc: 0.925000\n",
      "(Iteration 101 / 200) loss: 22.851224\n",
      "(Epoch 11 / 20) train acc: 0.978000; val_acc: 0.922222\n",
      "(Iteration 111 / 200) loss: 10.675296\n",
      "(Epoch 12 / 20) train acc: 0.984000; val_acc: 0.930556\n",
      "(Iteration 121 / 200) loss: 3.886227\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.925000\n",
      "(Iteration 131 / 200) loss: 1.884641\n",
      "(Epoch 14 / 20) train acc: 0.978000; val_acc: 0.936111\n",
      "(Iteration 141 / 200) loss: 26.414810\n",
      "(Epoch 15 / 20) train acc: 0.980000; val_acc: 0.927778\n",
      "(Iteration 151 / 200) loss: 1.334694\n",
      "(Epoch 16 / 20) train acc: 0.986000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 5.876286\n",
      "(Epoch 17 / 20) train acc: 0.959000; val_acc: 0.933333\n",
      "(Iteration 171 / 200) loss: 22.230619\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.933333\n",
      "(Iteration 181 / 200) loss: 6.017734\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.930556\n",
      "(Iteration 191 / 200) loss: 0.075278\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.930556\n",
      "(Iteration 1 / 200) loss: 3.995909\n",
      "(Epoch 0 / 20) train acc: 0.204000; val_acc: 0.205556\n",
      "(Epoch 1 / 20) train acc: 0.840000; val_acc: 0.875000\n",
      "(Iteration 11 / 200) loss: 0.413250\n",
      "(Epoch 2 / 20) train acc: 0.918000; val_acc: 0.933333\n",
      "(Iteration 21 / 200) loss: 0.160031\n",
      "(Epoch 3 / 20) train acc: 0.940000; val_acc: 0.944444\n",
      "(Iteration 31 / 200) loss: 0.267865\n",
      "(Epoch 4 / 20) train acc: 0.979000; val_acc: 0.952778\n",
      "(Iteration 41 / 200) loss: 0.098739\n",
      "(Epoch 5 / 20) train acc: 0.984000; val_acc: 0.972222\n",
      "(Iteration 51 / 200) loss: 0.066874\n",
      "(Epoch 6 / 20) train acc: 0.990000; val_acc: 0.975000\n",
      "(Iteration 61 / 200) loss: 0.017256\n",
      "(Epoch 7 / 20) train acc: 0.984000; val_acc: 0.977778\n",
      "(Iteration 71 / 200) loss: 0.078220\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.022220\n",
      "(Epoch 9 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 91 / 200) loss: 0.009483\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.020253\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.080973\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.019145\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 131 / 200) loss: 0.015787\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.003291\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.006020\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.004189\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 171 / 200) loss: 0.002757\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.991667\n",
      "(Iteration 181 / 200) loss: 0.002752\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.002688\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.303089\n",
      "(Epoch 0 / 20) train acc: 0.122000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.644000; val_acc: 0.627778\n",
      "(Iteration 11 / 200) loss: 1.032776\n",
      "(Epoch 2 / 20) train acc: 0.876000; val_acc: 0.858333\n",
      "(Iteration 21 / 200) loss: 0.396190\n",
      "(Epoch 3 / 20) train acc: 0.932000; val_acc: 0.905556\n",
      "(Iteration 31 / 200) loss: 0.168637\n",
      "(Epoch 4 / 20) train acc: 0.957000; val_acc: 0.938889\n",
      "(Iteration 41 / 200) loss: 0.132780\n",
      "(Epoch 5 / 20) train acc: 0.968000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.102823\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.955556\n",
      "(Iteration 61 / 200) loss: 0.081199\n",
      "(Epoch 7 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.136277\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.944444\n",
      "(Iteration 81 / 200) loss: 0.065853\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.153836\n",
      "(Epoch 10 / 20) train acc: 0.950000; val_acc: 0.933333\n",
      "(Iteration 101 / 200) loss: 0.062534\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.930556\n",
      "(Iteration 111 / 200) loss: 0.027000\n",
      "(Epoch 12 / 20) train acc: 0.996000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.015646\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.052828\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.022346\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.008050\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.961111\n",
      "(Iteration 161 / 200) loss: 0.017676\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.003918\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.035218\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.003504\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.494000; val_acc: 0.555556\n",
      "(Iteration 11 / 200) loss: 1.145145\n",
      "(Epoch 2 / 20) train acc: 0.725000; val_acc: 0.688889\n",
      "(Iteration 21 / 200) loss: 0.662585\n",
      "(Epoch 3 / 20) train acc: 0.901000; val_acc: 0.861111\n",
      "(Iteration 31 / 200) loss: 0.539372\n",
      "(Epoch 4 / 20) train acc: 0.929000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.171588\n",
      "(Epoch 5 / 20) train acc: 0.956000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.209340\n",
      "(Epoch 6 / 20) train acc: 0.963000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.099651\n",
      "(Epoch 7 / 20) train acc: 0.967000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.105883\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.047992\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 0.104243\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.068223\n",
      "(Epoch 11 / 20) train acc: 0.981000; val_acc: 0.936111\n",
      "(Iteration 111 / 200) loss: 0.047372\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 121 / 200) loss: 0.021278\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 131 / 200) loss: 0.002747\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.031212\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.941667\n",
      "(Iteration 151 / 200) loss: 0.084360\n",
      "(Epoch 16 / 20) train acc: 0.979000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.210872\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.013288\n",
      "(Epoch 18 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.010250\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.061928\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.509000; val_acc: 0.497222\n",
      "(Iteration 11 / 200) loss: 1.433931\n",
      "(Epoch 2 / 20) train acc: 0.814000; val_acc: 0.808333\n",
      "(Iteration 21 / 200) loss: 0.503843\n",
      "(Epoch 3 / 20) train acc: 0.878000; val_acc: 0.858333\n",
      "(Iteration 31 / 200) loss: 0.661394\n",
      "(Epoch 4 / 20) train acc: 0.913000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.327085\n",
      "(Epoch 5 / 20) train acc: 0.910000; val_acc: 0.925000\n",
      "(Iteration 51 / 200) loss: 0.102363\n",
      "(Epoch 6 / 20) train acc: 0.955000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.070239\n",
      "(Epoch 7 / 20) train acc: 0.973000; val_acc: 0.927778\n",
      "(Iteration 71 / 200) loss: 0.096763\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.049228\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.952778\n",
      "(Iteration 91 / 200) loss: 0.065763\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.024432\n",
      "(Epoch 11 / 20) train acc: 0.989000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.042417\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.014253\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.029322\n",
      "(Epoch 14 / 20) train acc: 0.965000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.073782\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.010009\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.017410\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.008535\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.003789\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.029273\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.181000; val_acc: 0.188889\n",
      "(Iteration 11 / 200) loss: 1.997611\n",
      "(Epoch 2 / 20) train acc: 0.643000; val_acc: 0.644444\n",
      "(Iteration 21 / 200) loss: 1.208418\n",
      "(Epoch 3 / 20) train acc: 0.791000; val_acc: 0.758333\n",
      "(Iteration 31 / 200) loss: 0.639158\n",
      "(Epoch 4 / 20) train acc: 0.852000; val_acc: 0.872222\n",
      "(Iteration 41 / 200) loss: 0.335201\n",
      "(Epoch 5 / 20) train acc: 0.942000; val_acc: 0.911111\n",
      "(Iteration 51 / 200) loss: 0.205760\n",
      "(Epoch 6 / 20) train acc: 0.945000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.136248\n",
      "(Epoch 7 / 20) train acc: 0.924000; val_acc: 0.902778\n",
      "(Iteration 71 / 200) loss: 0.167896\n",
      "(Epoch 8 / 20) train acc: 0.943000; val_acc: 0.905556\n",
      "(Iteration 81 / 200) loss: 0.238688\n",
      "(Epoch 9 / 20) train acc: 0.972000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.043090\n",
      "(Epoch 10 / 20) train acc: 0.972000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.060087\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.955556\n",
      "(Iteration 111 / 200) loss: 0.089745\n",
      "(Epoch 12 / 20) train acc: 0.971000; val_acc: 0.913889\n",
      "(Iteration 121 / 200) loss: 0.141824\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.011399\n",
      "(Epoch 14 / 20) train acc: 0.972000; val_acc: 0.927778\n",
      "(Iteration 141 / 200) loss: 0.070332\n",
      "(Epoch 15 / 20) train acc: 0.986000; val_acc: 0.944444\n",
      "(Iteration 151 / 200) loss: 0.082791\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.009899\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.006526\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.007532\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.001410\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 4289.674672\n",
      "(Epoch 0 / 20) train acc: 0.036000; val_acc: 0.022222\n",
      "(Epoch 1 / 20) train acc: 0.426000; val_acc: 0.341667\n",
      "(Iteration 11 / 200) loss: 731.804567\n",
      "(Epoch 2 / 20) train acc: 0.699000; val_acc: 0.627778\n",
      "(Iteration 21 / 200) loss: 249.160677\n",
      "(Epoch 3 / 20) train acc: 0.761000; val_acc: 0.738889\n",
      "(Iteration 31 / 200) loss: 154.395577\n",
      "(Epoch 4 / 20) train acc: 0.829000; val_acc: 0.805556\n",
      "(Iteration 41 / 200) loss: 44.466312\n",
      "(Epoch 5 / 20) train acc: 0.888000; val_acc: 0.836111\n",
      "(Iteration 51 / 200) loss: 32.259259\n",
      "(Epoch 6 / 20) train acc: 0.931000; val_acc: 0.855556\n",
      "(Iteration 61 / 200) loss: 29.799654\n",
      "(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.858333\n",
      "(Iteration 71 / 200) loss: 28.805082\n",
      "(Epoch 8 / 20) train acc: 0.920000; val_acc: 0.850000\n",
      "(Iteration 81 / 200) loss: 13.238788\n",
      "(Epoch 9 / 20) train acc: 0.952000; val_acc: 0.869444\n",
      "(Iteration 91 / 200) loss: 12.454076\n",
      "(Epoch 10 / 20) train acc: 0.969000; val_acc: 0.863889\n",
      "(Iteration 101 / 200) loss: 4.450546\n",
      "(Epoch 11 / 20) train acc: 0.971000; val_acc: 0.875000\n",
      "(Iteration 111 / 200) loss: 8.690695\n",
      "(Epoch 12 / 20) train acc: 0.976000; val_acc: 0.886111\n",
      "(Iteration 121 / 200) loss: 1.518067\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.886111\n",
      "(Iteration 131 / 200) loss: 2.616019\n",
      "(Epoch 14 / 20) train acc: 0.982000; val_acc: 0.886111\n",
      "(Iteration 141 / 200) loss: 2.419971\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.877778\n",
      "(Iteration 151 / 200) loss: 0.068201\n",
      "(Epoch 16 / 20) train acc: 0.984000; val_acc: 0.897222\n",
      "(Iteration 161 / 200) loss: 17.597744\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.900000\n",
      "(Iteration 171 / 200) loss: 0.067712\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.900000\n",
      "(Iteration 181 / 200) loss: 1.642732\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.911111\n",
      "(Iteration 191 / 200) loss: 0.067507\n",
      "(Epoch 20 / 20) train acc: 0.991000; val_acc: 0.902778\n",
      "(Iteration 1 / 200) loss: 7.376461\n",
      "(Epoch 0 / 20) train acc: 0.159000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.799000; val_acc: 0.791667\n",
      "(Iteration 11 / 200) loss: 0.696315\n",
      "(Epoch 2 / 20) train acc: 0.923000; val_acc: 0.905556\n",
      "(Iteration 21 / 200) loss: 0.279637\n",
      "(Epoch 3 / 20) train acc: 0.936000; val_acc: 0.927778\n",
      "(Iteration 31 / 200) loss: 0.169514\n",
      "(Epoch 4 / 20) train acc: 0.961000; val_acc: 0.947222\n",
      "(Iteration 41 / 200) loss: 0.201512\n",
      "(Epoch 5 / 20) train acc: 0.972000; val_acc: 0.938889\n",
      "(Iteration 51 / 200) loss: 0.052763\n",
      "(Epoch 6 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 61 / 200) loss: 0.113310\n",
      "(Epoch 7 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 71 / 200) loss: 0.045234\n",
      "(Epoch 8 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.064975\n",
      "(Epoch 9 / 20) train acc: 0.975000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.077444\n",
      "(Epoch 10 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.017429\n",
      "(Epoch 11 / 20) train acc: 0.981000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.064659\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.052731\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.012503\n",
      "(Epoch 14 / 20) train acc: 0.982000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.024268\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.074546\n",
      "(Epoch 16 / 20) train acc: 0.978000; val_acc: 0.961111\n",
      "(Iteration 161 / 200) loss: 0.091026\n",
      "(Epoch 17 / 20) train acc: 0.990000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.021727\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.010828\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.030872\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302332\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.679000; val_acc: 0.658333\n",
      "(Iteration 11 / 200) loss: 0.931073\n",
      "(Epoch 2 / 20) train acc: 0.855000; val_acc: 0.802778\n",
      "(Iteration 21 / 200) loss: 0.564440\n",
      "(Epoch 3 / 20) train acc: 0.920000; val_acc: 0.869444\n",
      "(Iteration 31 / 200) loss: 0.363242\n",
      "(Epoch 4 / 20) train acc: 0.937000; val_acc: 0.941667\n",
      "(Iteration 41 / 200) loss: 0.162346\n",
      "(Epoch 5 / 20) train acc: 0.950000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.140251\n",
      "(Epoch 6 / 20) train acc: 0.973000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.027029\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.038737\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.024370\n",
      "(Epoch 9 / 20) train acc: 0.977000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.081571\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.046614\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.010426\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.044676\n",
      "(Epoch 13 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.010586\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.014660\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.016214\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.047461\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.006068\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.013753\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.021792\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.642000; val_acc: 0.625000\n",
      "(Iteration 11 / 200) loss: 1.130570\n",
      "(Epoch 2 / 20) train acc: 0.822000; val_acc: 0.808333\n",
      "(Iteration 21 / 200) loss: 0.650353\n",
      "(Epoch 3 / 20) train acc: 0.842000; val_acc: 0.830556\n",
      "(Iteration 31 / 200) loss: 0.550690\n",
      "(Epoch 4 / 20) train acc: 0.923000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 0.160865\n",
      "(Epoch 5 / 20) train acc: 0.918000; val_acc: 0.886111\n",
      "(Iteration 51 / 200) loss: 0.189900\n",
      "(Epoch 6 / 20) train acc: 0.947000; val_acc: 0.916667\n",
      "(Iteration 61 / 200) loss: 0.273179\n",
      "(Epoch 7 / 20) train acc: 0.959000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.105330\n",
      "(Epoch 8 / 20) train acc: 0.977000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 0.163258\n",
      "(Epoch 9 / 20) train acc: 0.977000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.097576\n",
      "(Epoch 10 / 20) train acc: 0.973000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.106504\n",
      "(Epoch 11 / 20) train acc: 0.972000; val_acc: 0.944444\n",
      "(Iteration 111 / 200) loss: 0.035966\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 121 / 200) loss: 0.067781\n",
      "(Epoch 13 / 20) train acc: 0.977000; val_acc: 0.938889\n",
      "(Iteration 131 / 200) loss: 0.110782\n",
      "(Epoch 14 / 20) train acc: 0.976000; val_acc: 0.927778\n",
      "(Iteration 141 / 200) loss: 0.051123\n",
      "(Epoch 15 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.016437\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.013923\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.002739\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.016457\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 191 / 200) loss: 0.015600\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.542000; val_acc: 0.516667\n",
      "(Iteration 11 / 200) loss: 1.177684\n",
      "(Epoch 2 / 20) train acc: 0.672000; val_acc: 0.630556\n",
      "(Iteration 21 / 200) loss: 0.851898\n",
      "(Epoch 3 / 20) train acc: 0.862000; val_acc: 0.844444\n",
      "(Iteration 31 / 200) loss: 0.379321\n",
      "(Epoch 4 / 20) train acc: 0.903000; val_acc: 0.850000\n",
      "(Iteration 41 / 200) loss: 0.274624\n",
      "(Epoch 5 / 20) train acc: 0.914000; val_acc: 0.894444\n",
      "(Iteration 51 / 200) loss: 0.452336\n",
      "(Epoch 6 / 20) train acc: 0.942000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.240198\n",
      "(Epoch 7 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.215101\n",
      "(Epoch 8 / 20) train acc: 0.944000; val_acc: 0.886111\n",
      "(Iteration 81 / 200) loss: 0.115892\n",
      "(Epoch 9 / 20) train acc: 0.969000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.040792\n",
      "(Epoch 10 / 20) train acc: 0.974000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.057968\n",
      "(Epoch 11 / 20) train acc: 0.978000; val_acc: 0.955556\n",
      "(Iteration 111 / 200) loss: 0.037829\n",
      "(Epoch 12 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 121 / 200) loss: 0.020852\n",
      "(Epoch 13 / 20) train acc: 0.982000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.071028\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.046781\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.034986\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.017096\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.001350\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.015844\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.024269\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.278000; val_acc: 0.313889\n",
      "(Iteration 11 / 200) loss: 1.998290\n",
      "(Epoch 2 / 20) train acc: 0.546000; val_acc: 0.497222\n",
      "(Iteration 21 / 200) loss: 1.124943\n",
      "(Epoch 3 / 20) train acc: 0.688000; val_acc: 0.697222\n",
      "(Iteration 31 / 200) loss: 0.800092\n",
      "(Epoch 4 / 20) train acc: 0.788000; val_acc: 0.763889\n",
      "(Iteration 41 / 200) loss: 0.621684\n",
      "(Epoch 5 / 20) train acc: 0.849000; val_acc: 0.858333\n",
      "(Iteration 51 / 200) loss: 0.409791\n",
      "(Epoch 6 / 20) train acc: 0.902000; val_acc: 0.883333\n",
      "(Iteration 61 / 200) loss: 0.312237\n",
      "(Epoch 7 / 20) train acc: 0.925000; val_acc: 0.922222\n",
      "(Iteration 71 / 200) loss: 0.167989\n",
      "(Epoch 8 / 20) train acc: 0.949000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.079241\n",
      "(Epoch 9 / 20) train acc: 0.961000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.020027\n",
      "(Epoch 10 / 20) train acc: 0.979000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.239857\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.102831\n",
      "(Epoch 12 / 20) train acc: 0.958000; val_acc: 0.944444\n",
      "(Iteration 121 / 200) loss: 0.047115\n",
      "(Epoch 13 / 20) train acc: 0.969000; val_acc: 0.938889\n",
      "(Iteration 131 / 200) loss: 0.065768\n",
      "(Epoch 14 / 20) train acc: 0.977000; val_acc: 0.919444\n",
      "(Iteration 141 / 200) loss: 0.140256\n",
      "(Epoch 15 / 20) train acc: 0.986000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.046772\n",
      "(Epoch 16 / 20) train acc: 0.986000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.010544\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.031334\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.028860\n",
      "(Epoch 19 / 20) train acc: 0.985000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.022996\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.936111\n",
      "(Iteration 1 / 200) loss: 3512.379048\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.145000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2250.798716\n",
      "(Epoch 2 / 20) train acc: 0.214000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 1442.638748\n",
      "(Epoch 3 / 20) train acc: 0.284000; val_acc: 0.244444\n",
      "(Iteration 31 / 200) loss: 1048.647783\n",
      "(Epoch 4 / 20) train acc: 0.320000; val_acc: 0.291667\n",
      "(Iteration 41 / 200) loss: 879.465554\n",
      "(Epoch 5 / 20) train acc: 0.372000; val_acc: 0.316667\n",
      "(Iteration 51 / 200) loss: 764.446584\n",
      "(Epoch 6 / 20) train acc: 0.402000; val_acc: 0.361111\n",
      "(Iteration 61 / 200) loss: 672.142607\n",
      "(Epoch 7 / 20) train acc: 0.450000; val_acc: 0.411111\n",
      "(Iteration 71 / 200) loss: 735.148037\n",
      "(Epoch 8 / 20) train acc: 0.474000; val_acc: 0.477778\n",
      "(Iteration 81 / 200) loss: 565.119116\n",
      "(Epoch 9 / 20) train acc: 0.530000; val_acc: 0.500000\n",
      "(Iteration 91 / 200) loss: 524.617690\n",
      "(Epoch 10 / 20) train acc: 0.605000; val_acc: 0.522222\n",
      "(Iteration 101 / 200) loss: 399.986948\n",
      "(Epoch 11 / 20) train acc: 0.602000; val_acc: 0.544444\n",
      "(Iteration 111 / 200) loss: 524.289211\n",
      "(Epoch 12 / 20) train acc: 0.601000; val_acc: 0.572222\n",
      "(Iteration 121 / 200) loss: 321.928037\n",
      "(Epoch 13 / 20) train acc: 0.651000; val_acc: 0.611111\n",
      "(Iteration 131 / 200) loss: 370.762559\n",
      "(Epoch 14 / 20) train acc: 0.681000; val_acc: 0.644444\n",
      "(Iteration 141 / 200) loss: 295.668794\n",
      "(Epoch 15 / 20) train acc: 0.724000; val_acc: 0.677778\n",
      "(Iteration 151 / 200) loss: 300.307449\n",
      "(Epoch 16 / 20) train acc: 0.755000; val_acc: 0.694444\n",
      "(Iteration 161 / 200) loss: 239.840430\n",
      "(Epoch 17 / 20) train acc: 0.736000; val_acc: 0.691667\n",
      "(Iteration 171 / 200) loss: 167.378960\n",
      "(Epoch 18 / 20) train acc: 0.743000; val_acc: 0.713889\n",
      "(Iteration 181 / 200) loss: 282.513000\n",
      "(Epoch 19 / 20) train acc: 0.771000; val_acc: 0.727778\n",
      "(Iteration 191 / 200) loss: 201.585769\n",
      "(Epoch 20 / 20) train acc: 0.795000; val_acc: 0.750000\n",
      "(Iteration 1 / 200) loss: 4.368531\n",
      "(Epoch 0 / 20) train acc: 0.150000; val_acc: 0.152778\n",
      "(Epoch 1 / 20) train acc: 0.656000; val_acc: 0.638889\n",
      "(Iteration 11 / 200) loss: 1.918562\n",
      "(Epoch 2 / 20) train acc: 0.869000; val_acc: 0.844444\n",
      "(Iteration 21 / 200) loss: 1.248545\n",
      "(Epoch 3 / 20) train acc: 0.917000; val_acc: 0.886111\n",
      "(Iteration 31 / 200) loss: 1.086034\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.859965\n",
      "(Epoch 5 / 20) train acc: 0.965000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.862211\n",
      "(Epoch 6 / 20) train acc: 0.957000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.791182\n",
      "(Epoch 7 / 20) train acc: 0.973000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.799592\n",
      "(Epoch 8 / 20) train acc: 0.976000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.671484\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.676704\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.661402\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.645666\n",
      "(Epoch 12 / 20) train acc: 0.987000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.640837\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.606563\n",
      "(Epoch 14 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.605006\n",
      "(Epoch 15 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.593263\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.551089\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.532787\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.523512\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.516036\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.311055\n",
      "(Epoch 0 / 20) train acc: 0.199000; val_acc: 0.144444\n",
      "(Epoch 1 / 20) train acc: 0.245000; val_acc: 0.236111\n",
      "(Iteration 11 / 200) loss: 2.246550\n",
      "(Epoch 2 / 20) train acc: 0.546000; val_acc: 0.555556\n",
      "(Iteration 21 / 200) loss: 1.952285\n",
      "(Epoch 3 / 20) train acc: 0.653000; val_acc: 0.663889\n",
      "(Iteration 31 / 200) loss: 1.239419\n",
      "(Epoch 4 / 20) train acc: 0.834000; val_acc: 0.797222\n",
      "(Iteration 41 / 200) loss: 0.838536\n",
      "(Epoch 5 / 20) train acc: 0.853000; val_acc: 0.838889\n",
      "(Iteration 51 / 200) loss: 0.515834\n",
      "(Epoch 6 / 20) train acc: 0.870000; val_acc: 0.850000\n",
      "(Iteration 61 / 200) loss: 0.362434\n",
      "(Epoch 7 / 20) train acc: 0.894000; val_acc: 0.880556\n",
      "(Iteration 71 / 200) loss: 0.345414\n",
      "(Epoch 8 / 20) train acc: 0.931000; val_acc: 0.908333\n",
      "(Iteration 81 / 200) loss: 0.391796\n",
      "(Epoch 9 / 20) train acc: 0.931000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 0.420818\n",
      "(Epoch 10 / 20) train acc: 0.939000; val_acc: 0.927778\n",
      "(Iteration 101 / 200) loss: 0.354856\n",
      "(Epoch 11 / 20) train acc: 0.942000; val_acc: 0.927778\n",
      "(Iteration 111 / 200) loss: 0.210481\n",
      "(Epoch 12 / 20) train acc: 0.962000; val_acc: 0.930556\n",
      "(Iteration 121 / 200) loss: 0.240922\n",
      "(Epoch 13 / 20) train acc: 0.948000; val_acc: 0.925000\n",
      "(Iteration 131 / 200) loss: 0.290073\n",
      "(Epoch 14 / 20) train acc: 0.964000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.197639\n",
      "(Epoch 15 / 20) train acc: 0.948000; val_acc: 0.933333\n",
      "(Iteration 151 / 200) loss: 0.195013\n",
      "(Epoch 16 / 20) train acc: 0.951000; val_acc: 0.950000\n",
      "(Iteration 161 / 200) loss: 0.170106\n",
      "(Epoch 17 / 20) train acc: 0.962000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.250455\n",
      "(Epoch 18 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.235489\n",
      "(Epoch 19 / 20) train acc: 0.968000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.175320\n",
      "(Epoch 20 / 20) train acc: 0.974000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 2.302670\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.218000; val_acc: 0.211111\n",
      "(Iteration 11 / 200) loss: 2.298130\n",
      "(Epoch 2 / 20) train acc: 0.188000; val_acc: 0.188889\n",
      "(Iteration 21 / 200) loss: 2.154627\n",
      "(Epoch 3 / 20) train acc: 0.194000; val_acc: 0.208333\n",
      "(Iteration 31 / 200) loss: 1.804851\n",
      "(Epoch 4 / 20) train acc: 0.370000; val_acc: 0.347222\n",
      "(Iteration 41 / 200) loss: 1.361678\n",
      "(Epoch 5 / 20) train acc: 0.583000; val_acc: 0.566667\n",
      "(Iteration 51 / 200) loss: 1.259396\n",
      "(Epoch 6 / 20) train acc: 0.609000; val_acc: 0.611111\n",
      "(Iteration 61 / 200) loss: 0.984529\n",
      "(Epoch 7 / 20) train acc: 0.638000; val_acc: 0.669444\n",
      "(Iteration 71 / 200) loss: 0.960020\n",
      "(Epoch 8 / 20) train acc: 0.648000; val_acc: 0.641667\n",
      "(Iteration 81 / 200) loss: 0.896533\n",
      "(Epoch 9 / 20) train acc: 0.726000; val_acc: 0.719444\n",
      "(Iteration 91 / 200) loss: 0.951478\n",
      "(Epoch 10 / 20) train acc: 0.749000; val_acc: 0.738889\n",
      "(Iteration 101 / 200) loss: 0.675214\n",
      "(Epoch 11 / 20) train acc: 0.733000; val_acc: 0.733333\n",
      "(Iteration 111 / 200) loss: 0.836125\n",
      "(Epoch 12 / 20) train acc: 0.785000; val_acc: 0.755556\n",
      "(Iteration 121 / 200) loss: 0.668378\n",
      "(Epoch 13 / 20) train acc: 0.793000; val_acc: 0.761111\n",
      "(Iteration 131 / 200) loss: 0.507597\n",
      "(Epoch 14 / 20) train acc: 0.811000; val_acc: 0.783333\n",
      "(Iteration 141 / 200) loss: 0.673514\n",
      "(Epoch 15 / 20) train acc: 0.832000; val_acc: 0.813889\n",
      "(Iteration 151 / 200) loss: 0.610886\n",
      "(Epoch 16 / 20) train acc: 0.887000; val_acc: 0.819444\n",
      "(Iteration 161 / 200) loss: 0.586730\n",
      "(Epoch 17 / 20) train acc: 0.883000; val_acc: 0.830556\n",
      "(Iteration 171 / 200) loss: 0.556717\n",
      "(Epoch 18 / 20) train acc: 0.916000; val_acc: 0.861111\n",
      "(Iteration 181 / 200) loss: 0.440300\n",
      "(Epoch 19 / 20) train acc: 0.928000; val_acc: 0.883333\n",
      "(Iteration 191 / 200) loss: 0.455455\n",
      "(Epoch 20 / 20) train acc: 0.925000; val_acc: 0.872222\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 2.288822\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 2.124611\n",
      "(Epoch 3 / 20) train acc: 0.194000; val_acc: 0.211111\n",
      "(Iteration 31 / 200) loss: 2.196855\n",
      "(Epoch 4 / 20) train acc: 0.187000; val_acc: 0.208333\n",
      "(Iteration 41 / 200) loss: 1.910348\n",
      "(Epoch 5 / 20) train acc: 0.253000; val_acc: 0.277778\n",
      "(Iteration 51 / 200) loss: 1.744267\n",
      "(Epoch 6 / 20) train acc: 0.335000; val_acc: 0.341667\n",
      "(Iteration 61 / 200) loss: 1.598271\n",
      "(Epoch 7 / 20) train acc: 0.333000; val_acc: 0.319444\n",
      "(Iteration 71 / 200) loss: 1.460211\n",
      "(Epoch 8 / 20) train acc: 0.397000; val_acc: 0.391667\n",
      "(Iteration 81 / 200) loss: 1.378627\n",
      "(Epoch 9 / 20) train acc: 0.402000; val_acc: 0.380556\n",
      "(Iteration 91 / 200) loss: 1.427531\n",
      "(Epoch 10 / 20) train acc: 0.438000; val_acc: 0.397222\n",
      "(Iteration 101 / 200) loss: 1.374830\n",
      "(Epoch 11 / 20) train acc: 0.421000; val_acc: 0.411111\n",
      "(Iteration 111 / 200) loss: 1.478620\n",
      "(Epoch 12 / 20) train acc: 0.430000; val_acc: 0.430556\n",
      "(Iteration 121 / 200) loss: 1.270558\n",
      "(Epoch 13 / 20) train acc: 0.499000; val_acc: 0.466667\n",
      "(Iteration 131 / 200) loss: 1.247800\n",
      "(Epoch 14 / 20) train acc: 0.593000; val_acc: 0.566667\n",
      "(Iteration 141 / 200) loss: 1.092489\n",
      "(Epoch 15 / 20) train acc: 0.585000; val_acc: 0.552778\n",
      "(Iteration 151 / 200) loss: 0.954331\n",
      "(Epoch 16 / 20) train acc: 0.686000; val_acc: 0.677778\n",
      "(Iteration 161 / 200) loss: 0.783179\n",
      "(Epoch 17 / 20) train acc: 0.802000; val_acc: 0.747222\n",
      "(Iteration 171 / 200) loss: 0.777195\n",
      "(Epoch 18 / 20) train acc: 0.796000; val_acc: 0.766667\n",
      "(Iteration 181 / 200) loss: 0.491574\n",
      "(Epoch 19 / 20) train acc: 0.853000; val_acc: 0.791667\n",
      "(Iteration 191 / 200) loss: 0.539642\n",
      "(Epoch 20 / 20) train acc: 0.828000; val_acc: 0.816667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.200000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2.301426\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.108333\n",
      "(Iteration 21 / 200) loss: 2.202400\n",
      "(Epoch 3 / 20) train acc: 0.201000; val_acc: 0.194444\n",
      "(Iteration 31 / 200) loss: 1.962820\n",
      "(Epoch 4 / 20) train acc: 0.209000; val_acc: 0.219444\n",
      "(Iteration 41 / 200) loss: 1.790993\n",
      "(Epoch 5 / 20) train acc: 0.306000; val_acc: 0.266667\n",
      "(Iteration 51 / 200) loss: 1.712926\n",
      "(Epoch 6 / 20) train acc: 0.307000; val_acc: 0.294444\n",
      "(Iteration 61 / 200) loss: 1.654341\n",
      "(Epoch 7 / 20) train acc: 0.289000; val_acc: 0.241667\n",
      "(Iteration 71 / 200) loss: 1.544528\n",
      "(Epoch 8 / 20) train acc: 0.319000; val_acc: 0.313889\n",
      "(Iteration 81 / 200) loss: 1.532547\n",
      "(Epoch 9 / 20) train acc: 0.352000; val_acc: 0.350000\n",
      "(Iteration 91 / 200) loss: 1.541877\n",
      "(Epoch 10 / 20) train acc: 0.366000; val_acc: 0.380556\n",
      "(Iteration 101 / 200) loss: 1.496826\n",
      "(Epoch 11 / 20) train acc: 0.406000; val_acc: 0.386111\n",
      "(Iteration 111 / 200) loss: 1.478703\n",
      "(Epoch 12 / 20) train acc: 0.404000; val_acc: 0.405556\n",
      "(Iteration 121 / 200) loss: 1.430994\n",
      "(Epoch 13 / 20) train acc: 0.446000; val_acc: 0.388889\n",
      "(Iteration 131 / 200) loss: 1.453785\n",
      "(Epoch 14 / 20) train acc: 0.465000; val_acc: 0.447222\n",
      "(Iteration 141 / 200) loss: 1.360708\n",
      "(Epoch 15 / 20) train acc: 0.560000; val_acc: 0.572222\n",
      "(Iteration 151 / 200) loss: 1.070298\n",
      "(Epoch 16 / 20) train acc: 0.585000; val_acc: 0.577778\n",
      "(Iteration 161 / 200) loss: 1.179441\n",
      "(Epoch 17 / 20) train acc: 0.663000; val_acc: 0.644444\n",
      "(Iteration 171 / 200) loss: 0.891132\n",
      "(Epoch 18 / 20) train acc: 0.718000; val_acc: 0.683333\n",
      "(Iteration 181 / 200) loss: 1.067325\n",
      "(Epoch 19 / 20) train acc: 0.723000; val_acc: 0.716667\n",
      "(Iteration 191 / 200) loss: 0.783190\n",
      "(Epoch 20 / 20) train acc: 0.767000; val_acc: 0.750000\n",
      "(Iteration 1 / 200) loss: 4759.026936\n",
      "(Epoch 0 / 20) train acc: 0.139000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.148000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 2994.087600\n",
      "(Epoch 2 / 20) train acc: 0.141000; val_acc: 0.155556\n",
      "(Iteration 21 / 200) loss: 2527.838411\n",
      "(Epoch 3 / 20) train acc: 0.216000; val_acc: 0.177778\n",
      "(Iteration 31 / 200) loss: 1975.838416\n",
      "(Epoch 4 / 20) train acc: 0.235000; val_acc: 0.219444\n",
      "(Iteration 41 / 200) loss: 1591.665986\n",
      "(Epoch 5 / 20) train acc: 0.251000; val_acc: 0.250000\n",
      "(Iteration 51 / 200) loss: 1143.719026\n",
      "(Epoch 6 / 20) train acc: 0.322000; val_acc: 0.283333\n",
      "(Iteration 61 / 200) loss: 1025.762900\n",
      "(Epoch 7 / 20) train acc: 0.391000; val_acc: 0.327778\n",
      "(Iteration 71 / 200) loss: 909.419453\n",
      "(Epoch 8 / 20) train acc: 0.454000; val_acc: 0.391667\n",
      "(Iteration 81 / 200) loss: 754.278823\n",
      "(Epoch 9 / 20) train acc: 0.471000; val_acc: 0.427778\n",
      "(Iteration 91 / 200) loss: 775.773696\n",
      "(Epoch 10 / 20) train acc: 0.495000; val_acc: 0.455556\n",
      "(Iteration 101 / 200) loss: 538.822676\n",
      "(Epoch 11 / 20) train acc: 0.555000; val_acc: 0.502778\n",
      "(Iteration 111 / 200) loss: 411.215125\n",
      "(Epoch 12 / 20) train acc: 0.615000; val_acc: 0.538889\n",
      "(Iteration 121 / 200) loss: 371.753613\n",
      "(Epoch 13 / 20) train acc: 0.646000; val_acc: 0.580556\n",
      "(Iteration 131 / 200) loss: 452.520842\n",
      "(Epoch 14 / 20) train acc: 0.650000; val_acc: 0.602778\n",
      "(Iteration 141 / 200) loss: 281.102866\n",
      "(Epoch 15 / 20) train acc: 0.682000; val_acc: 0.647222\n",
      "(Iteration 151 / 200) loss: 386.834473\n",
      "(Epoch 16 / 20) train acc: 0.715000; val_acc: 0.661111\n",
      "(Iteration 161 / 200) loss: 284.450273\n",
      "(Epoch 17 / 20) train acc: 0.736000; val_acc: 0.694444\n",
      "(Iteration 171 / 200) loss: 249.200371\n",
      "(Epoch 18 / 20) train acc: 0.747000; val_acc: 0.722222\n",
      "(Iteration 181 / 200) loss: 294.762090\n",
      "(Epoch 19 / 20) train acc: 0.801000; val_acc: 0.750000\n",
      "(Iteration 191 / 200) loss: 216.714849\n",
      "(Epoch 20 / 20) train acc: 0.806000; val_acc: 0.755556\n",
      "(Iteration 1 / 200) loss: 4.919177\n",
      "(Epoch 0 / 20) train acc: 0.136000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.571000; val_acc: 0.533333\n",
      "(Iteration 11 / 200) loss: 2.025052\n",
      "(Epoch 2 / 20) train acc: 0.839000; val_acc: 0.802778\n",
      "(Iteration 21 / 200) loss: 1.311862\n",
      "(Epoch 3 / 20) train acc: 0.914000; val_acc: 0.869444\n",
      "(Iteration 31 / 200) loss: 1.092337\n",
      "(Epoch 4 / 20) train acc: 0.931000; val_acc: 0.905556\n",
      "(Iteration 41 / 200) loss: 0.898980\n",
      "(Epoch 5 / 20) train acc: 0.953000; val_acc: 0.913889\n",
      "(Iteration 51 / 200) loss: 0.878432\n",
      "(Epoch 6 / 20) train acc: 0.954000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.801839\n",
      "(Epoch 7 / 20) train acc: 0.977000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.798606\n",
      "(Epoch 8 / 20) train acc: 0.979000; val_acc: 0.947222\n",
      "(Iteration 81 / 200) loss: 0.690313\n",
      "(Epoch 9 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 91 / 200) loss: 0.660501\n",
      "(Epoch 10 / 20) train acc: 0.981000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.702371\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.660113\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.631817\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 131 / 200) loss: 0.592106\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.576499\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.542731\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.522991\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.564814\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.512301\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.496202\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.310608\n",
      "(Epoch 0 / 20) train acc: 0.195000; val_acc: 0.186111\n",
      "(Epoch 1 / 20) train acc: 0.284000; val_acc: 0.316667\n",
      "(Iteration 11 / 200) loss: 2.237104\n",
      "(Epoch 2 / 20) train acc: 0.515000; val_acc: 0.516667\n",
      "(Iteration 21 / 200) loss: 1.915573\n",
      "(Epoch 3 / 20) train acc: 0.798000; val_acc: 0.755556\n",
      "(Iteration 31 / 200) loss: 1.229576\n",
      "(Epoch 4 / 20) train acc: 0.806000; val_acc: 0.802778\n",
      "(Iteration 41 / 200) loss: 0.709968\n",
      "(Epoch 5 / 20) train acc: 0.841000; val_acc: 0.833333\n",
      "(Iteration 51 / 200) loss: 0.559552\n",
      "(Epoch 6 / 20) train acc: 0.887000; val_acc: 0.866667\n",
      "(Iteration 61 / 200) loss: 0.486130\n",
      "(Epoch 7 / 20) train acc: 0.895000; val_acc: 0.877778\n",
      "(Iteration 71 / 200) loss: 0.427084\n",
      "(Epoch 8 / 20) train acc: 0.911000; val_acc: 0.891667\n",
      "(Iteration 81 / 200) loss: 0.334954\n",
      "(Epoch 9 / 20) train acc: 0.922000; val_acc: 0.930556\n",
      "(Iteration 91 / 200) loss: 0.288537\n",
      "(Epoch 10 / 20) train acc: 0.945000; val_acc: 0.925000\n",
      "(Iteration 101 / 200) loss: 0.386288\n",
      "(Epoch 11 / 20) train acc: 0.952000; val_acc: 0.930556\n",
      "(Iteration 111 / 200) loss: 0.262865\n",
      "(Epoch 12 / 20) train acc: 0.948000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.286796\n",
      "(Epoch 13 / 20) train acc: 0.965000; val_acc: 0.947222\n",
      "(Iteration 131 / 200) loss: 0.376864\n",
      "(Epoch 14 / 20) train acc: 0.963000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.188237\n",
      "(Epoch 15 / 20) train acc: 0.971000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.194408\n",
      "(Epoch 16 / 20) train acc: 0.965000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.214977\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.171438\n",
      "(Epoch 18 / 20) train acc: 0.980000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.179370\n",
      "(Epoch 19 / 20) train acc: 0.973000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.160872\n",
      "(Epoch 20 / 20) train acc: 0.977000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.302667\n",
      "(Epoch 0 / 20) train acc: 0.174000; val_acc: 0.163889\n",
      "(Epoch 1 / 20) train acc: 0.176000; val_acc: 0.186111\n",
      "(Iteration 11 / 200) loss: 2.295326\n",
      "(Epoch 2 / 20) train acc: 0.260000; val_acc: 0.275000\n",
      "(Iteration 21 / 200) loss: 2.077637\n",
      "(Epoch 3 / 20) train acc: 0.247000; val_acc: 0.288889\n",
      "(Iteration 31 / 200) loss: 1.826183\n",
      "(Epoch 4 / 20) train acc: 0.475000; val_acc: 0.469444\n",
      "(Iteration 41 / 200) loss: 1.405354\n",
      "(Epoch 5 / 20) train acc: 0.545000; val_acc: 0.547222\n",
      "(Iteration 51 / 200) loss: 1.064536\n",
      "(Epoch 6 / 20) train acc: 0.576000; val_acc: 0.577778\n",
      "(Iteration 61 / 200) loss: 1.154799\n",
      "(Epoch 7 / 20) train acc: 0.618000; val_acc: 0.580556\n",
      "(Iteration 71 / 200) loss: 1.071437\n",
      "(Epoch 8 / 20) train acc: 0.638000; val_acc: 0.627778\n",
      "(Iteration 81 / 200) loss: 1.014913\n",
      "(Epoch 9 / 20) train acc: 0.693000; val_acc: 0.661111\n",
      "(Iteration 91 / 200) loss: 0.932780\n",
      "(Epoch 10 / 20) train acc: 0.695000; val_acc: 0.680556\n",
      "(Iteration 101 / 200) loss: 0.920552\n",
      "(Epoch 11 / 20) train acc: 0.707000; val_acc: 0.700000\n",
      "(Iteration 111 / 200) loss: 0.708186\n",
      "(Epoch 12 / 20) train acc: 0.761000; val_acc: 0.719444\n",
      "(Iteration 121 / 200) loss: 0.729788\n",
      "(Epoch 13 / 20) train acc: 0.823000; val_acc: 0.755556\n",
      "(Iteration 131 / 200) loss: 0.569927\n",
      "(Epoch 14 / 20) train acc: 0.804000; val_acc: 0.761111\n",
      "(Iteration 141 / 200) loss: 0.517140\n",
      "(Epoch 15 / 20) train acc: 0.850000; val_acc: 0.805556\n",
      "(Iteration 151 / 200) loss: 0.554398\n",
      "(Epoch 16 / 20) train acc: 0.867000; val_acc: 0.825000\n",
      "(Iteration 161 / 200) loss: 0.448163\n",
      "(Epoch 17 / 20) train acc: 0.884000; val_acc: 0.852778\n",
      "(Iteration 171 / 200) loss: 0.453458\n",
      "(Epoch 18 / 20) train acc: 0.879000; val_acc: 0.844444\n",
      "(Iteration 181 / 200) loss: 0.467323\n",
      "(Epoch 19 / 20) train acc: 0.874000; val_acc: 0.844444\n",
      "(Iteration 191 / 200) loss: 0.563694\n",
      "(Epoch 20 / 20) train acc: 0.918000; val_acc: 0.897222\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.296475\n",
      "(Epoch 2 / 20) train acc: 0.127000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.151581\n",
      "(Epoch 3 / 20) train acc: 0.198000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 2.115157\n",
      "(Epoch 4 / 20) train acc: 0.207000; val_acc: 0.225000\n",
      "(Iteration 41 / 200) loss: 1.923616\n",
      "(Epoch 5 / 20) train acc: 0.252000; val_acc: 0.255556\n",
      "(Iteration 51 / 200) loss: 1.747766\n",
      "(Epoch 6 / 20) train acc: 0.306000; val_acc: 0.283333\n",
      "(Iteration 61 / 200) loss: 1.603058\n",
      "(Epoch 7 / 20) train acc: 0.345000; val_acc: 0.386111\n",
      "(Iteration 71 / 200) loss: 1.557621\n",
      "(Epoch 8 / 20) train acc: 0.484000; val_acc: 0.452778\n",
      "(Iteration 81 / 200) loss: 1.475891\n",
      "(Epoch 9 / 20) train acc: 0.507000; val_acc: 0.555556\n",
      "(Iteration 91 / 200) loss: 1.204791\n",
      "(Epoch 10 / 20) train acc: 0.579000; val_acc: 0.536111\n",
      "(Iteration 101 / 200) loss: 1.146015\n",
      "(Epoch 11 / 20) train acc: 0.673000; val_acc: 0.619444\n",
      "(Iteration 111 / 200) loss: 0.992071\n",
      "(Epoch 12 / 20) train acc: 0.661000; val_acc: 0.666667\n",
      "(Iteration 121 / 200) loss: 0.885700\n",
      "(Epoch 13 / 20) train acc: 0.722000; val_acc: 0.716667\n",
      "(Iteration 131 / 200) loss: 0.816572\n",
      "(Epoch 14 / 20) train acc: 0.752000; val_acc: 0.727778\n",
      "(Iteration 141 / 200) loss: 0.721592\n",
      "(Epoch 15 / 20) train acc: 0.802000; val_acc: 0.783333\n",
      "(Iteration 151 / 200) loss: 0.652953\n",
      "(Epoch 16 / 20) train acc: 0.850000; val_acc: 0.794444\n",
      "(Iteration 161 / 200) loss: 0.478386\n",
      "(Epoch 17 / 20) train acc: 0.860000; val_acc: 0.830556\n",
      "(Iteration 171 / 200) loss: 0.646777\n",
      "(Epoch 18 / 20) train acc: 0.880000; val_acc: 0.838889\n",
      "(Iteration 181 / 200) loss: 0.532498\n",
      "(Epoch 19 / 20) train acc: 0.887000; val_acc: 0.861111\n",
      "(Iteration 191 / 200) loss: 0.406073\n",
      "(Epoch 20 / 20) train acc: 0.888000; val_acc: 0.875000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.181000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.102778\n",
      "(Iteration 11 / 200) loss: 2.300605\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.243697\n",
      "(Epoch 3 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.096600\n",
      "(Epoch 4 / 20) train acc: 0.196000; val_acc: 0.202778\n",
      "(Iteration 41 / 200) loss: 2.019809\n",
      "(Epoch 5 / 20) train acc: 0.210000; val_acc: 0.211111\n",
      "(Iteration 51 / 200) loss: 1.918086\n",
      "(Epoch 6 / 20) train acc: 0.337000; val_acc: 0.269444\n",
      "(Iteration 61 / 200) loss: 1.791664\n",
      "(Epoch 7 / 20) train acc: 0.230000; val_acc: 0.188889\n",
      "(Iteration 71 / 200) loss: 1.684752\n",
      "(Epoch 8 / 20) train acc: 0.324000; val_acc: 0.350000\n",
      "(Iteration 81 / 200) loss: 1.600183\n",
      "(Epoch 9 / 20) train acc: 0.395000; val_acc: 0.427778\n",
      "(Iteration 91 / 200) loss: 1.539812\n",
      "(Epoch 10 / 20) train acc: 0.451000; val_acc: 0.438889\n",
      "(Iteration 101 / 200) loss: 1.455195\n",
      "(Epoch 11 / 20) train acc: 0.488000; val_acc: 0.436111\n",
      "(Iteration 111 / 200) loss: 1.345593\n",
      "(Epoch 12 / 20) train acc: 0.630000; val_acc: 0.569444\n",
      "(Iteration 121 / 200) loss: 1.167277\n",
      "(Epoch 13 / 20) train acc: 0.674000; val_acc: 0.636111\n",
      "(Iteration 131 / 200) loss: 0.926187\n",
      "(Epoch 14 / 20) train acc: 0.746000; val_acc: 0.719444\n",
      "(Iteration 141 / 200) loss: 0.787472\n",
      "(Epoch 15 / 20) train acc: 0.720000; val_acc: 0.697222\n",
      "(Iteration 151 / 200) loss: 0.800573\n",
      "(Epoch 16 / 20) train acc: 0.811000; val_acc: 0.788889\n",
      "(Iteration 161 / 200) loss: 0.652825\n",
      "(Epoch 17 / 20) train acc: 0.804000; val_acc: 0.800000\n",
      "(Iteration 171 / 200) loss: 0.697288\n",
      "(Epoch 18 / 20) train acc: 0.853000; val_acc: 0.825000\n",
      "(Iteration 181 / 200) loss: 0.448214\n",
      "(Epoch 19 / 20) train acc: 0.841000; val_acc: 0.805556\n",
      "(Iteration 191 / 200) loss: 0.516308\n",
      "(Epoch 20 / 20) train acc: 0.876000; val_acc: 0.836111\n",
      "(Iteration 1 / 200) loss: 4672.969102\n",
      "(Epoch 0 / 20) train acc: 0.048000; val_acc: 0.038889\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 3142.001230\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 2312.643364\n",
      "(Epoch 3 / 20) train acc: 0.136000; val_acc: 0.161111\n",
      "(Iteration 31 / 200) loss: 2211.988550\n",
      "(Epoch 4 / 20) train acc: 0.203000; val_acc: 0.205556\n",
      "(Iteration 41 / 200) loss: 1696.745171\n",
      "(Epoch 5 / 20) train acc: 0.261000; val_acc: 0.241667\n",
      "(Iteration 51 / 200) loss: 1357.341770\n",
      "(Epoch 6 / 20) train acc: 0.294000; val_acc: 0.275000\n",
      "(Iteration 61 / 200) loss: 1123.781401\n",
      "(Epoch 7 / 20) train acc: 0.346000; val_acc: 0.327778\n",
      "(Iteration 71 / 200) loss: 642.142930\n",
      "(Epoch 8 / 20) train acc: 0.409000; val_acc: 0.394444\n",
      "(Iteration 81 / 200) loss: 869.300852\n",
      "(Epoch 9 / 20) train acc: 0.464000; val_acc: 0.425000\n",
      "(Iteration 91 / 200) loss: 656.756812\n",
      "(Epoch 10 / 20) train acc: 0.547000; val_acc: 0.480556\n",
      "(Iteration 101 / 200) loss: 480.405896\n",
      "(Epoch 11 / 20) train acc: 0.606000; val_acc: 0.533333\n",
      "(Iteration 111 / 200) loss: 446.528784\n",
      "(Epoch 12 / 20) train acc: 0.637000; val_acc: 0.577778\n",
      "(Iteration 121 / 200) loss: 374.534854\n",
      "(Epoch 13 / 20) train acc: 0.666000; val_acc: 0.608333\n",
      "(Iteration 131 / 200) loss: 411.672200\n",
      "(Epoch 14 / 20) train acc: 0.706000; val_acc: 0.638889\n",
      "(Iteration 141 / 200) loss: 360.816782\n",
      "(Epoch 15 / 20) train acc: 0.732000; val_acc: 0.675000\n",
      "(Iteration 151 / 200) loss: 390.929922\n",
      "(Epoch 16 / 20) train acc: 0.725000; val_acc: 0.691667\n",
      "(Iteration 161 / 200) loss: 215.749766\n",
      "(Epoch 17 / 20) train acc: 0.767000; val_acc: 0.700000\n",
      "(Iteration 171 / 200) loss: 303.406167\n",
      "(Epoch 18 / 20) train acc: 0.755000; val_acc: 0.691667\n",
      "(Iteration 181 / 200) loss: 227.821450\n",
      "(Epoch 19 / 20) train acc: 0.794000; val_acc: 0.713889\n",
      "(Iteration 191 / 200) loss: 238.927117\n",
      "(Epoch 20 / 20) train acc: 0.819000; val_acc: 0.716667\n",
      "(Iteration 1 / 200) loss: 5.795772\n",
      "(Epoch 0 / 20) train acc: 0.079000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.451000; val_acc: 0.380556\n",
      "(Iteration 11 / 200) loss: 2.451376\n",
      "(Epoch 2 / 20) train acc: 0.771000; val_acc: 0.758333\n",
      "(Iteration 21 / 200) loss: 1.721143\n",
      "(Epoch 3 / 20) train acc: 0.891000; val_acc: 0.850000\n",
      "(Iteration 31 / 200) loss: 1.355646\n",
      "(Epoch 4 / 20) train acc: 0.904000; val_acc: 0.891667\n",
      "(Iteration 41 / 200) loss: 1.087474\n",
      "(Epoch 5 / 20) train acc: 0.941000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.914140\n",
      "(Epoch 6 / 20) train acc: 0.954000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.829613\n",
      "(Epoch 7 / 20) train acc: 0.936000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.752797\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.733326\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.671709\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.752410\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 111 / 200) loss: 0.639599\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.625520\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.988889\n",
      "(Iteration 131 / 200) loss: 0.632254\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.582190\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.595347\n",
      "(Epoch 16 / 20) train acc: 0.988000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.528912\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.538873\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.541824\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.502786\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.991667\n",
      "(Iteration 1 / 200) loss: 2.311054\n",
      "(Epoch 0 / 20) train acc: 0.331000; val_acc: 0.280556\n",
      "(Epoch 1 / 20) train acc: 0.450000; val_acc: 0.438889\n",
      "(Iteration 11 / 200) loss: 2.234230\n",
      "(Epoch 2 / 20) train acc: 0.550000; val_acc: 0.602778\n",
      "(Iteration 21 / 200) loss: 1.868253\n",
      "(Epoch 3 / 20) train acc: 0.758000; val_acc: 0.783333\n",
      "(Iteration 31 / 200) loss: 1.221947\n",
      "(Epoch 4 / 20) train acc: 0.821000; val_acc: 0.813889\n",
      "(Iteration 41 / 200) loss: 0.766536\n",
      "(Epoch 5 / 20) train acc: 0.844000; val_acc: 0.813889\n",
      "(Iteration 51 / 200) loss: 0.699734\n",
      "(Epoch 6 / 20) train acc: 0.884000; val_acc: 0.866667\n",
      "(Iteration 61 / 200) loss: 0.490902\n",
      "(Epoch 7 / 20) train acc: 0.905000; val_acc: 0.883333\n",
      "(Iteration 71 / 200) loss: 0.642192\n",
      "(Epoch 8 / 20) train acc: 0.925000; val_acc: 0.894444\n",
      "(Iteration 81 / 200) loss: 0.388168\n",
      "(Epoch 9 / 20) train acc: 0.927000; val_acc: 0.883333\n",
      "(Iteration 91 / 200) loss: 0.366451\n",
      "(Epoch 10 / 20) train acc: 0.947000; val_acc: 0.888889\n",
      "(Iteration 101 / 200) loss: 0.295092\n",
      "(Epoch 11 / 20) train acc: 0.947000; val_acc: 0.936111\n",
      "(Iteration 111 / 200) loss: 0.215551\n",
      "(Epoch 12 / 20) train acc: 0.943000; val_acc: 0.944444\n",
      "(Iteration 121 / 200) loss: 0.240565\n",
      "(Epoch 13 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 0.298075\n",
      "(Epoch 14 / 20) train acc: 0.968000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.212618\n",
      "(Epoch 15 / 20) train acc: 0.963000; val_acc: 0.941667\n",
      "(Iteration 151 / 200) loss: 0.208479\n",
      "(Epoch 16 / 20) train acc: 0.967000; val_acc: 0.933333\n",
      "(Iteration 161 / 200) loss: 0.254770\n",
      "(Epoch 17 / 20) train acc: 0.978000; val_acc: 0.941667\n",
      "(Iteration 171 / 200) loss: 0.226523\n",
      "(Epoch 18 / 20) train acc: 0.969000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.214349\n",
      "(Epoch 19 / 20) train acc: 0.957000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.190313\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302666\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.287174\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2.114293\n",
      "(Epoch 3 / 20) train acc: 0.247000; val_acc: 0.308333\n",
      "(Iteration 31 / 200) loss: 2.032749\n",
      "(Epoch 4 / 20) train acc: 0.271000; val_acc: 0.294444\n",
      "(Iteration 41 / 200) loss: 1.933953\n",
      "(Epoch 5 / 20) train acc: 0.490000; val_acc: 0.488889\n",
      "(Iteration 51 / 200) loss: 1.624131\n",
      "(Epoch 6 / 20) train acc: 0.598000; val_acc: 0.586111\n",
      "(Iteration 61 / 200) loss: 1.160259\n",
      "(Epoch 7 / 20) train acc: 0.719000; val_acc: 0.688889\n",
      "(Iteration 71 / 200) loss: 0.931630\n",
      "(Epoch 8 / 20) train acc: 0.751000; val_acc: 0.750000\n",
      "(Iteration 81 / 200) loss: 0.801235\n",
      "(Epoch 9 / 20) train acc: 0.800000; val_acc: 0.783333\n",
      "(Iteration 91 / 200) loss: 0.861131\n",
      "(Epoch 10 / 20) train acc: 0.861000; val_acc: 0.808333\n",
      "(Iteration 101 / 200) loss: 0.600472\n",
      "(Epoch 11 / 20) train acc: 0.817000; val_acc: 0.822222\n",
      "(Iteration 111 / 200) loss: 0.417046\n",
      "(Epoch 12 / 20) train acc: 0.888000; val_acc: 0.830556\n",
      "(Iteration 121 / 200) loss: 0.436062\n",
      "(Epoch 13 / 20) train acc: 0.875000; val_acc: 0.875000\n",
      "(Iteration 131 / 200) loss: 0.401688\n",
      "(Epoch 14 / 20) train acc: 0.891000; val_acc: 0.869444\n",
      "(Iteration 141 / 200) loss: 0.372621\n",
      "(Epoch 15 / 20) train acc: 0.889000; val_acc: 0.869444\n",
      "(Iteration 151 / 200) loss: 0.400903\n",
      "(Epoch 16 / 20) train acc: 0.895000; val_acc: 0.880556\n",
      "(Iteration 161 / 200) loss: 0.417530\n",
      "(Epoch 17 / 20) train acc: 0.912000; val_acc: 0.886111\n",
      "(Iteration 171 / 200) loss: 0.368903\n",
      "(Epoch 18 / 20) train acc: 0.901000; val_acc: 0.872222\n",
      "(Iteration 181 / 200) loss: 0.358549\n",
      "(Epoch 19 / 20) train acc: 0.905000; val_acc: 0.897222\n",
      "(Iteration 191 / 200) loss: 0.256653\n",
      "(Epoch 20 / 20) train acc: 0.920000; val_acc: 0.888889\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.297337\n",
      "(Epoch 2 / 20) train acc: 0.137000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 2.235062\n",
      "(Epoch 3 / 20) train acc: 0.146000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 2.139265\n",
      "(Epoch 4 / 20) train acc: 0.190000; val_acc: 0.144444\n",
      "(Iteration 41 / 200) loss: 2.018754\n",
      "(Epoch 5 / 20) train acc: 0.232000; val_acc: 0.205556\n",
      "(Iteration 51 / 200) loss: 1.952383\n",
      "(Epoch 6 / 20) train acc: 0.348000; val_acc: 0.291667\n",
      "(Iteration 61 / 200) loss: 1.820809\n",
      "(Epoch 7 / 20) train acc: 0.399000; val_acc: 0.425000\n",
      "(Iteration 71 / 200) loss: 1.478958\n",
      "(Epoch 8 / 20) train acc: 0.563000; val_acc: 0.527778\n",
      "(Iteration 81 / 200) loss: 1.441239\n",
      "(Epoch 9 / 20) train acc: 0.580000; val_acc: 0.652778\n",
      "(Iteration 91 / 200) loss: 1.229631\n",
      "(Epoch 10 / 20) train acc: 0.668000; val_acc: 0.658333\n",
      "(Iteration 101 / 200) loss: 0.909752\n",
      "(Epoch 11 / 20) train acc: 0.677000; val_acc: 0.711111\n",
      "(Iteration 111 / 200) loss: 0.923265\n",
      "(Epoch 12 / 20) train acc: 0.717000; val_acc: 0.744444\n",
      "(Iteration 121 / 200) loss: 0.756306\n",
      "(Epoch 13 / 20) train acc: 0.744000; val_acc: 0.772222\n",
      "(Iteration 131 / 200) loss: 0.821179\n",
      "(Epoch 14 / 20) train acc: 0.765000; val_acc: 0.769444\n",
      "(Iteration 141 / 200) loss: 0.814369\n",
      "(Epoch 15 / 20) train acc: 0.800000; val_acc: 0.791667\n",
      "(Iteration 151 / 200) loss: 0.621220\n",
      "(Epoch 16 / 20) train acc: 0.787000; val_acc: 0.797222\n",
      "(Iteration 161 / 200) loss: 0.764672\n",
      "(Epoch 17 / 20) train acc: 0.825000; val_acc: 0.808333\n",
      "(Iteration 171 / 200) loss: 0.611339\n",
      "(Epoch 18 / 20) train acc: 0.846000; val_acc: 0.827778\n",
      "(Iteration 181 / 200) loss: 0.581048\n",
      "(Epoch 19 / 20) train acc: 0.816000; val_acc: 0.813889\n",
      "(Iteration 191 / 200) loss: 0.387458\n",
      "(Epoch 20 / 20) train acc: 0.881000; val_acc: 0.830556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.301382\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.213522\n",
      "(Epoch 3 / 20) train acc: 0.188000; val_acc: 0.180556\n",
      "(Iteration 31 / 200) loss: 2.101854\n",
      "(Epoch 4 / 20) train acc: 0.182000; val_acc: 0.194444\n",
      "(Iteration 41 / 200) loss: 1.980754\n",
      "(Epoch 5 / 20) train acc: 0.225000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 1.857452\n",
      "(Epoch 6 / 20) train acc: 0.246000; val_acc: 0.241667\n",
      "(Iteration 61 / 200) loss: 1.786024\n",
      "(Epoch 7 / 20) train acc: 0.312000; val_acc: 0.300000\n",
      "(Iteration 71 / 200) loss: 1.781452\n",
      "(Epoch 8 / 20) train acc: 0.335000; val_acc: 0.347222\n",
      "(Iteration 81 / 200) loss: 1.627389\n",
      "(Epoch 9 / 20) train acc: 0.344000; val_acc: 0.313889\n",
      "(Iteration 91 / 200) loss: 1.551646\n",
      "(Epoch 10 / 20) train acc: 0.453000; val_acc: 0.480556\n",
      "(Iteration 101 / 200) loss: 1.466554\n",
      "(Epoch 11 / 20) train acc: 0.529000; val_acc: 0.525000\n",
      "(Iteration 111 / 200) loss: 1.300125\n",
      "(Epoch 12 / 20) train acc: 0.608000; val_acc: 0.594444\n",
      "(Iteration 121 / 200) loss: 1.088884\n",
      "(Epoch 13 / 20) train acc: 0.647000; val_acc: 0.611111\n",
      "(Iteration 131 / 200) loss: 0.934905\n",
      "(Epoch 14 / 20) train acc: 0.703000; val_acc: 0.683333\n",
      "(Iteration 141 / 200) loss: 0.682756\n",
      "(Epoch 15 / 20) train acc: 0.755000; val_acc: 0.744444\n",
      "(Iteration 151 / 200) loss: 0.825971\n",
      "(Epoch 16 / 20) train acc: 0.788000; val_acc: 0.794444\n",
      "(Iteration 161 / 200) loss: 0.693425\n",
      "(Epoch 17 / 20) train acc: 0.809000; val_acc: 0.794444\n",
      "(Iteration 171 / 200) loss: 0.686612\n",
      "(Epoch 18 / 20) train acc: 0.841000; val_acc: 0.819444\n",
      "(Iteration 181 / 200) loss: 0.647243\n",
      "(Epoch 19 / 20) train acc: 0.837000; val_acc: 0.822222\n",
      "(Iteration 191 / 200) loss: 0.581926\n",
      "(Epoch 20 / 20) train acc: 0.846000; val_acc: 0.833333\n",
      "(Iteration 1 / 200) loss: 3455.068011\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.136111\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 2615.122750\n",
      "(Epoch 2 / 20) train acc: 0.146000; val_acc: 0.136111\n",
      "(Iteration 21 / 200) loss: 1849.126834\n",
      "(Epoch 3 / 20) train acc: 0.167000; val_acc: 0.150000\n",
      "(Iteration 31 / 200) loss: 1631.483932\n",
      "(Epoch 4 / 20) train acc: 0.202000; val_acc: 0.183333\n",
      "(Iteration 41 / 200) loss: 1223.842556\n",
      "(Epoch 5 / 20) train acc: 0.290000; val_acc: 0.269444\n",
      "(Iteration 51 / 200) loss: 1009.908558\n",
      "(Epoch 6 / 20) train acc: 0.343000; val_acc: 0.288889\n",
      "(Iteration 61 / 200) loss: 627.584734\n",
      "(Epoch 7 / 20) train acc: 0.396000; val_acc: 0.341667\n",
      "(Iteration 71 / 200) loss: 612.423604\n",
      "(Epoch 8 / 20) train acc: 0.451000; val_acc: 0.388889\n",
      "(Iteration 81 / 200) loss: 523.058294\n",
      "(Epoch 9 / 20) train acc: 0.472000; val_acc: 0.433333\n",
      "(Iteration 91 / 200) loss: 519.449873\n",
      "(Epoch 10 / 20) train acc: 0.517000; val_acc: 0.491667\n",
      "(Iteration 101 / 200) loss: 387.063287\n",
      "(Epoch 11 / 20) train acc: 0.599000; val_acc: 0.511111\n",
      "(Iteration 111 / 200) loss: 306.557028\n",
      "(Epoch 12 / 20) train acc: 0.627000; val_acc: 0.530556\n",
      "(Iteration 121 / 200) loss: 284.659543\n",
      "(Epoch 13 / 20) train acc: 0.662000; val_acc: 0.541667\n",
      "(Iteration 131 / 200) loss: 218.815972\n",
      "(Epoch 14 / 20) train acc: 0.656000; val_acc: 0.572222\n",
      "(Iteration 141 / 200) loss: 233.402133\n",
      "(Epoch 15 / 20) train acc: 0.705000; val_acc: 0.600000\n",
      "(Iteration 151 / 200) loss: 139.886629\n",
      "(Epoch 16 / 20) train acc: 0.734000; val_acc: 0.616667\n",
      "(Iteration 161 / 200) loss: 218.316862\n",
      "(Epoch 17 / 20) train acc: 0.744000; val_acc: 0.633333\n",
      "(Iteration 171 / 200) loss: 231.336513\n",
      "(Epoch 18 / 20) train acc: 0.742000; val_acc: 0.644444\n",
      "(Iteration 181 / 200) loss: 239.949254\n",
      "(Epoch 19 / 20) train acc: 0.770000; val_acc: 0.663889\n",
      "(Iteration 191 / 200) loss: 135.038858\n",
      "(Epoch 20 / 20) train acc: 0.787000; val_acc: 0.691667\n",
      "(Iteration 1 / 200) loss: 4.791244\n",
      "(Epoch 0 / 20) train acc: 0.152000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.557000; val_acc: 0.572222\n",
      "(Iteration 11 / 200) loss: 1.451556\n",
      "(Epoch 2 / 20) train acc: 0.823000; val_acc: 0.816667\n",
      "(Iteration 21 / 200) loss: 0.732698\n",
      "(Epoch 3 / 20) train acc: 0.909000; val_acc: 0.883333\n",
      "(Iteration 31 / 200) loss: 0.483935\n",
      "(Epoch 4 / 20) train acc: 0.939000; val_acc: 0.922222\n",
      "(Iteration 41 / 200) loss: 0.295768\n",
      "(Epoch 5 / 20) train acc: 0.942000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 0.242204\n",
      "(Epoch 6 / 20) train acc: 0.969000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.186594\n",
      "(Epoch 7 / 20) train acc: 0.965000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.146305\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.237779\n",
      "(Epoch 9 / 20) train acc: 0.971000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.157997\n",
      "(Epoch 10 / 20) train acc: 0.982000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.146188\n",
      "(Epoch 11 / 20) train acc: 0.985000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.129131\n",
      "(Epoch 12 / 20) train acc: 0.990000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.153535\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.101864\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.118530\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.090886\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.103449\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.988889\n",
      "(Iteration 171 / 200) loss: 0.089418\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.099285\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.093292\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.991667\n",
      "(Iteration 1 / 200) loss: 2.304218\n",
      "(Epoch 0 / 20) train acc: 0.143000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.413000; val_acc: 0.394444\n",
      "(Iteration 11 / 200) loss: 2.204627\n",
      "(Epoch 2 / 20) train acc: 0.666000; val_acc: 0.658333\n",
      "(Iteration 21 / 200) loss: 1.813495\n",
      "(Epoch 3 / 20) train acc: 0.753000; val_acc: 0.741667\n",
      "(Iteration 31 / 200) loss: 1.137830\n",
      "(Epoch 4 / 20) train acc: 0.808000; val_acc: 0.794444\n",
      "(Iteration 41 / 200) loss: 0.829380\n",
      "(Epoch 5 / 20) train acc: 0.820000; val_acc: 0.855556\n",
      "(Iteration 51 / 200) loss: 0.466689\n",
      "(Epoch 6 / 20) train acc: 0.877000; val_acc: 0.855556\n",
      "(Iteration 61 / 200) loss: 0.428856\n",
      "(Epoch 7 / 20) train acc: 0.898000; val_acc: 0.886111\n",
      "(Iteration 71 / 200) loss: 0.431983\n",
      "(Epoch 8 / 20) train acc: 0.909000; val_acc: 0.880556\n",
      "(Iteration 81 / 200) loss: 0.303036\n",
      "(Epoch 9 / 20) train acc: 0.933000; val_acc: 0.891667\n",
      "(Iteration 91 / 200) loss: 0.197678\n",
      "(Epoch 10 / 20) train acc: 0.909000; val_acc: 0.894444\n",
      "(Iteration 101 / 200) loss: 0.304432\n",
      "(Epoch 11 / 20) train acc: 0.934000; val_acc: 0.927778\n",
      "(Iteration 111 / 200) loss: 0.228206\n",
      "(Epoch 12 / 20) train acc: 0.931000; val_acc: 0.933333\n",
      "(Iteration 121 / 200) loss: 0.248719\n",
      "(Epoch 13 / 20) train acc: 0.951000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.142132\n",
      "(Epoch 14 / 20) train acc: 0.950000; val_acc: 0.938889\n",
      "(Iteration 141 / 200) loss: 0.132263\n",
      "(Epoch 15 / 20) train acc: 0.968000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.194718\n",
      "(Epoch 16 / 20) train acc: 0.955000; val_acc: 0.936111\n",
      "(Iteration 161 / 200) loss: 0.154921\n",
      "(Epoch 17 / 20) train acc: 0.969000; val_acc: 0.947222\n",
      "(Iteration 171 / 200) loss: 0.148937\n",
      "(Epoch 18 / 20) train acc: 0.971000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.088648\n",
      "(Epoch 19 / 20) train acc: 0.969000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.133122\n",
      "(Epoch 20 / 20) train acc: 0.968000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.094444\n",
      "(Iteration 11 / 200) loss: 2.285719\n",
      "(Epoch 2 / 20) train acc: 0.140000; val_acc: 0.144444\n",
      "(Iteration 21 / 200) loss: 2.029854\n",
      "(Epoch 3 / 20) train acc: 0.395000; val_acc: 0.397222\n",
      "(Iteration 31 / 200) loss: 1.809024\n",
      "(Epoch 4 / 20) train acc: 0.481000; val_acc: 0.486111\n",
      "(Iteration 41 / 200) loss: 1.388244\n",
      "(Epoch 5 / 20) train acc: 0.620000; val_acc: 0.600000\n",
      "(Iteration 51 / 200) loss: 1.119619\n",
      "(Epoch 6 / 20) train acc: 0.667000; val_acc: 0.638889\n",
      "(Iteration 61 / 200) loss: 1.106613\n",
      "(Epoch 7 / 20) train acc: 0.662000; val_acc: 0.644444\n",
      "(Iteration 71 / 200) loss: 0.921609\n",
      "(Epoch 8 / 20) train acc: 0.709000; val_acc: 0.713889\n",
      "(Iteration 81 / 200) loss: 0.838950\n",
      "(Epoch 9 / 20) train acc: 0.734000; val_acc: 0.713889\n",
      "(Iteration 91 / 200) loss: 0.700747\n",
      "(Epoch 10 / 20) train acc: 0.824000; val_acc: 0.777778\n",
      "(Iteration 101 / 200) loss: 0.719262\n",
      "(Epoch 11 / 20) train acc: 0.783000; val_acc: 0.780556\n",
      "(Iteration 111 / 200) loss: 0.519204\n",
      "(Epoch 12 / 20) train acc: 0.822000; val_acc: 0.808333\n",
      "(Iteration 121 / 200) loss: 0.446373\n",
      "(Epoch 13 / 20) train acc: 0.854000; val_acc: 0.833333\n",
      "(Iteration 131 / 200) loss: 0.460297\n",
      "(Epoch 14 / 20) train acc: 0.868000; val_acc: 0.836111\n",
      "(Iteration 141 / 200) loss: 0.546772\n",
      "(Epoch 15 / 20) train acc: 0.884000; val_acc: 0.855556\n",
      "(Iteration 151 / 200) loss: 0.395339\n",
      "(Epoch 16 / 20) train acc: 0.902000; val_acc: 0.875000\n",
      "(Iteration 161 / 200) loss: 0.332944\n",
      "(Epoch 17 / 20) train acc: 0.905000; val_acc: 0.872222\n",
      "(Iteration 171 / 200) loss: 0.233803\n",
      "(Epoch 18 / 20) train acc: 0.880000; val_acc: 0.869444\n",
      "(Iteration 181 / 200) loss: 0.398688\n",
      "(Epoch 19 / 20) train acc: 0.913000; val_acc: 0.880556\n",
      "(Iteration 191 / 200) loss: 0.229844\n",
      "(Epoch 20 / 20) train acc: 0.914000; val_acc: 0.886111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.115000; val_acc: 0.094444\n",
      "(Iteration 11 / 200) loss: 2.289666\n",
      "(Epoch 2 / 20) train acc: 0.157000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 2.161971\n",
      "(Epoch 3 / 20) train acc: 0.189000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 1.914115\n",
      "(Epoch 4 / 20) train acc: 0.343000; val_acc: 0.286111\n",
      "(Iteration 41 / 200) loss: 1.745813\n",
      "(Epoch 5 / 20) train acc: 0.466000; val_acc: 0.461111\n",
      "(Iteration 51 / 200) loss: 1.449514\n",
      "(Epoch 6 / 20) train acc: 0.570000; val_acc: 0.558333\n",
      "(Iteration 61 / 200) loss: 1.010931\n",
      "(Epoch 7 / 20) train acc: 0.622000; val_acc: 0.627778\n",
      "(Iteration 71 / 200) loss: 0.994694\n",
      "(Epoch 8 / 20) train acc: 0.690000; val_acc: 0.708333\n",
      "(Iteration 81 / 200) loss: 0.785573\n",
      "(Epoch 9 / 20) train acc: 0.711000; val_acc: 0.711111\n",
      "(Iteration 91 / 200) loss: 0.816922\n",
      "(Epoch 10 / 20) train acc: 0.698000; val_acc: 0.744444\n",
      "(Iteration 101 / 200) loss: 1.005126\n",
      "(Epoch 11 / 20) train acc: 0.744000; val_acc: 0.750000\n",
      "(Iteration 111 / 200) loss: 0.796092\n",
      "(Epoch 12 / 20) train acc: 0.751000; val_acc: 0.747222\n",
      "(Iteration 121 / 200) loss: 0.734120\n",
      "(Epoch 13 / 20) train acc: 0.746000; val_acc: 0.744444\n",
      "(Iteration 131 / 200) loss: 0.610565\n",
      "(Epoch 14 / 20) train acc: 0.795000; val_acc: 0.783333\n",
      "(Iteration 141 / 200) loss: 0.691911\n",
      "(Epoch 15 / 20) train acc: 0.823000; val_acc: 0.786111\n",
      "(Iteration 151 / 200) loss: 0.479008\n",
      "(Epoch 16 / 20) train acc: 0.880000; val_acc: 0.841667\n",
      "(Iteration 161 / 200) loss: 0.386688\n",
      "(Epoch 17 / 20) train acc: 0.880000; val_acc: 0.863889\n",
      "(Iteration 171 / 200) loss: 0.295426\n",
      "(Epoch 18 / 20) train acc: 0.893000; val_acc: 0.869444\n",
      "(Iteration 181 / 200) loss: 0.338303\n",
      "(Epoch 19 / 20) train acc: 0.921000; val_acc: 0.880556\n",
      "(Iteration 191 / 200) loss: 0.345168\n",
      "(Epoch 20 / 20) train acc: 0.922000; val_acc: 0.880556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.282227\n",
      "(Epoch 2 / 20) train acc: 0.140000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 2.069163\n",
      "(Epoch 3 / 20) train acc: 0.175000; val_acc: 0.191667\n",
      "(Iteration 31 / 200) loss: 1.963167\n",
      "(Epoch 4 / 20) train acc: 0.212000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 2.008994\n",
      "(Epoch 5 / 20) train acc: 0.218000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 1.941863\n",
      "(Epoch 6 / 20) train acc: 0.197000; val_acc: 0.194444\n",
      "(Iteration 61 / 200) loss: 1.984318\n",
      "(Epoch 7 / 20) train acc: 0.177000; val_acc: 0.211111\n",
      "(Iteration 71 / 200) loss: 1.889408\n",
      "(Epoch 8 / 20) train acc: 0.199000; val_acc: 0.197222\n",
      "(Iteration 81 / 200) loss: 1.934782\n",
      "(Epoch 9 / 20) train acc: 0.292000; val_acc: 0.280556\n",
      "(Iteration 91 / 200) loss: 1.903954\n",
      "(Epoch 10 / 20) train acc: 0.269000; val_acc: 0.252778\n",
      "(Iteration 101 / 200) loss: 1.811969\n",
      "(Epoch 11 / 20) train acc: 0.277000; val_acc: 0.261111\n",
      "(Iteration 111 / 200) loss: 1.710016\n",
      "(Epoch 12 / 20) train acc: 0.357000; val_acc: 0.355556\n",
      "(Iteration 121 / 200) loss: 1.679021\n",
      "(Epoch 13 / 20) train acc: 0.432000; val_acc: 0.427778\n",
      "(Iteration 131 / 200) loss: 1.532105\n",
      "(Epoch 14 / 20) train acc: 0.445000; val_acc: 0.433333\n",
      "(Iteration 141 / 200) loss: 1.629333\n",
      "(Epoch 15 / 20) train acc: 0.506000; val_acc: 0.452778\n",
      "(Iteration 151 / 200) loss: 1.561974\n",
      "(Epoch 16 / 20) train acc: 0.488000; val_acc: 0.475000\n",
      "(Iteration 161 / 200) loss: 1.432099\n",
      "(Epoch 17 / 20) train acc: 0.590000; val_acc: 0.547222\n",
      "(Iteration 171 / 200) loss: 1.146773\n",
      "(Epoch 18 / 20) train acc: 0.701000; val_acc: 0.666667\n",
      "(Iteration 181 / 200) loss: 1.022736\n",
      "(Epoch 19 / 20) train acc: 0.728000; val_acc: 0.730556\n",
      "(Iteration 191 / 200) loss: 0.757460\n",
      "(Epoch 20 / 20) train acc: 0.766000; val_acc: 0.769444\n",
      "(Iteration 1 / 200) loss: 4946.684629\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.079000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 3750.185742\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2981.987057\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 2235.922456\n",
      "(Epoch 4 / 20) train acc: 0.178000; val_acc: 0.158333\n",
      "(Iteration 41 / 200) loss: 1464.747646\n",
      "(Epoch 5 / 20) train acc: 0.197000; val_acc: 0.205556\n",
      "(Iteration 51 / 200) loss: 1178.073522\n",
      "(Epoch 6 / 20) train acc: 0.255000; val_acc: 0.236111\n",
      "(Iteration 61 / 200) loss: 1282.553764\n",
      "(Epoch 7 / 20) train acc: 0.298000; val_acc: 0.277778\n",
      "(Iteration 71 / 200) loss: 1001.963643\n",
      "(Epoch 8 / 20) train acc: 0.336000; val_acc: 0.300000\n",
      "(Iteration 81 / 200) loss: 776.940567\n",
      "(Epoch 9 / 20) train acc: 0.394000; val_acc: 0.333333\n",
      "(Iteration 91 / 200) loss: 615.220409\n",
      "(Epoch 10 / 20) train acc: 0.452000; val_acc: 0.361111\n",
      "(Iteration 101 / 200) loss: 581.404834\n",
      "(Epoch 11 / 20) train acc: 0.498000; val_acc: 0.416667\n",
      "(Iteration 111 / 200) loss: 454.805522\n",
      "(Epoch 12 / 20) train acc: 0.497000; val_acc: 0.441667\n",
      "(Iteration 121 / 200) loss: 380.113054\n",
      "(Epoch 13 / 20) train acc: 0.555000; val_acc: 0.463889\n",
      "(Iteration 131 / 200) loss: 451.742577\n",
      "(Epoch 14 / 20) train acc: 0.601000; val_acc: 0.516667\n",
      "(Iteration 141 / 200) loss: 328.021892\n",
      "(Epoch 15 / 20) train acc: 0.601000; val_acc: 0.547222\n",
      "(Iteration 151 / 200) loss: 399.579271\n",
      "(Epoch 16 / 20) train acc: 0.640000; val_acc: 0.572222\n",
      "(Iteration 161 / 200) loss: 300.637650\n",
      "(Epoch 17 / 20) train acc: 0.659000; val_acc: 0.591667\n",
      "(Iteration 171 / 200) loss: 389.498075\n",
      "(Epoch 18 / 20) train acc: 0.680000; val_acc: 0.605556\n",
      "(Iteration 181 / 200) loss: 354.494581\n",
      "(Epoch 19 / 20) train acc: 0.711000; val_acc: 0.633333\n",
      "(Iteration 191 / 200) loss: 128.412435\n",
      "(Epoch 20 / 20) train acc: 0.681000; val_acc: 0.655556\n",
      "(Iteration 1 / 200) loss: 4.418780\n",
      "(Epoch 0 / 20) train acc: 0.064000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.581000; val_acc: 0.497222\n",
      "(Iteration 11 / 200) loss: 1.641202\n",
      "(Epoch 2 / 20) train acc: 0.843000; val_acc: 0.805556\n",
      "(Iteration 21 / 200) loss: 0.796384\n",
      "(Epoch 3 / 20) train acc: 0.909000; val_acc: 0.869444\n",
      "(Iteration 31 / 200) loss: 0.500625\n",
      "(Epoch 4 / 20) train acc: 0.944000; val_acc: 0.905556\n",
      "(Iteration 41 / 200) loss: 0.227909\n",
      "(Epoch 5 / 20) train acc: 0.948000; val_acc: 0.913889\n",
      "(Iteration 51 / 200) loss: 0.220668\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.230614\n",
      "(Epoch 7 / 20) train acc: 0.957000; val_acc: 0.938889\n",
      "(Iteration 71 / 200) loss: 0.170392\n",
      "(Epoch 8 / 20) train acc: 0.978000; val_acc: 0.950000\n",
      "(Iteration 81 / 200) loss: 0.164332\n",
      "(Epoch 9 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.130299\n",
      "(Epoch 10 / 20) train acc: 0.978000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.111229\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.112150\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.096041\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.176446\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.103150\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.108148\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.104782\n",
      "(Epoch 17 / 20) train acc: 0.990000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.102292\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.111999\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.093535\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.303660\n",
      "(Epoch 0 / 20) train acc: 0.193000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.355000; val_acc: 0.352778\n",
      "(Iteration 11 / 200) loss: 2.199490\n",
      "(Epoch 2 / 20) train acc: 0.525000; val_acc: 0.511111\n",
      "(Iteration 21 / 200) loss: 1.795235\n",
      "(Epoch 3 / 20) train acc: 0.727000; val_acc: 0.766667\n",
      "(Iteration 31 / 200) loss: 1.166197\n",
      "(Epoch 4 / 20) train acc: 0.813000; val_acc: 0.797222\n",
      "(Iteration 41 / 200) loss: 0.561089\n",
      "(Epoch 5 / 20) train acc: 0.838000; val_acc: 0.830556\n",
      "(Iteration 51 / 200) loss: 0.367695\n",
      "(Epoch 6 / 20) train acc: 0.866000; val_acc: 0.855556\n",
      "(Iteration 61 / 200) loss: 0.386845\n",
      "(Epoch 7 / 20) train acc: 0.875000; val_acc: 0.872222\n",
      "(Iteration 71 / 200) loss: 0.318389\n",
      "(Epoch 8 / 20) train acc: 0.916000; val_acc: 0.888889\n",
      "(Iteration 81 / 200) loss: 0.331385\n",
      "(Epoch 9 / 20) train acc: 0.924000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 0.278273\n",
      "(Epoch 10 / 20) train acc: 0.925000; val_acc: 0.894444\n",
      "(Iteration 101 / 200) loss: 0.220919\n",
      "(Epoch 11 / 20) train acc: 0.941000; val_acc: 0.922222\n",
      "(Iteration 111 / 200) loss: 0.175557\n",
      "(Epoch 12 / 20) train acc: 0.949000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 0.201949\n",
      "(Epoch 13 / 20) train acc: 0.950000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.181244\n",
      "(Epoch 14 / 20) train acc: 0.961000; val_acc: 0.936111\n",
      "(Iteration 141 / 200) loss: 0.198579\n",
      "(Epoch 15 / 20) train acc: 0.964000; val_acc: 0.933333\n",
      "(Iteration 151 / 200) loss: 0.079410\n",
      "(Epoch 16 / 20) train acc: 0.963000; val_acc: 0.952778\n",
      "(Iteration 161 / 200) loss: 0.180270\n",
      "(Epoch 17 / 20) train acc: 0.962000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.121359\n",
      "(Epoch 18 / 20) train acc: 0.965000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.117909\n",
      "(Epoch 19 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 191 / 200) loss: 0.086653\n",
      "(Epoch 20 / 20) train acc: 0.963000; val_acc: 0.936111\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.223000; val_acc: 0.194444\n",
      "(Iteration 11 / 200) loss: 2.286195\n",
      "(Epoch 2 / 20) train acc: 0.212000; val_acc: 0.216667\n",
      "(Iteration 21 / 200) loss: 2.014189\n",
      "(Epoch 3 / 20) train acc: 0.260000; val_acc: 0.252778\n",
      "(Iteration 31 / 200) loss: 1.756168\n",
      "(Epoch 4 / 20) train acc: 0.262000; val_acc: 0.283333\n",
      "(Iteration 41 / 200) loss: 1.498938\n",
      "(Epoch 5 / 20) train acc: 0.489000; val_acc: 0.483333\n",
      "(Iteration 51 / 200) loss: 1.430320\n",
      "(Epoch 6 / 20) train acc: 0.626000; val_acc: 0.647222\n",
      "(Iteration 61 / 200) loss: 1.178692\n",
      "(Epoch 7 / 20) train acc: 0.659000; val_acc: 0.675000\n",
      "(Iteration 71 / 200) loss: 0.788594\n",
      "(Epoch 8 / 20) train acc: 0.685000; val_acc: 0.716667\n",
      "(Iteration 81 / 200) loss: 0.987451\n",
      "(Epoch 9 / 20) train acc: 0.710000; val_acc: 0.708333\n",
      "(Iteration 91 / 200) loss: 0.715846\n",
      "(Epoch 10 / 20) train acc: 0.779000; val_acc: 0.797222\n",
      "(Iteration 101 / 200) loss: 0.555681\n",
      "(Epoch 11 / 20) train acc: 0.812000; val_acc: 0.794444\n",
      "(Iteration 111 / 200) loss: 0.677649\n",
      "(Epoch 12 / 20) train acc: 0.841000; val_acc: 0.794444\n",
      "(Iteration 121 / 200) loss: 0.705702\n",
      "(Epoch 13 / 20) train acc: 0.860000; val_acc: 0.838889\n",
      "(Iteration 131 / 200) loss: 0.364090\n",
      "(Epoch 14 / 20) train acc: 0.870000; val_acc: 0.852778\n",
      "(Iteration 141 / 200) loss: 0.552429\n",
      "(Epoch 15 / 20) train acc: 0.919000; val_acc: 0.872222\n",
      "(Iteration 151 / 200) loss: 0.295969\n",
      "(Epoch 16 / 20) train acc: 0.898000; val_acc: 0.858333\n",
      "(Iteration 161 / 200) loss: 0.290797\n",
      "(Epoch 17 / 20) train acc: 0.933000; val_acc: 0.869444\n",
      "(Iteration 171 / 200) loss: 0.400920\n",
      "(Epoch 18 / 20) train acc: 0.917000; val_acc: 0.883333\n",
      "(Iteration 181 / 200) loss: 0.370216\n",
      "(Epoch 19 / 20) train acc: 0.920000; val_acc: 0.891667\n",
      "(Iteration 191 / 200) loss: 0.259057\n",
      "(Epoch 20 / 20) train acc: 0.923000; val_acc: 0.894444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.327000; val_acc: 0.316667\n",
      "(Iteration 11 / 200) loss: 2.297255\n",
      "(Epoch 2 / 20) train acc: 0.361000; val_acc: 0.347222\n",
      "(Iteration 21 / 200) loss: 2.194302\n",
      "(Epoch 3 / 20) train acc: 0.452000; val_acc: 0.436111\n",
      "(Iteration 31 / 200) loss: 1.749431\n",
      "(Epoch 4 / 20) train acc: 0.552000; val_acc: 0.536111\n",
      "(Iteration 41 / 200) loss: 1.205774\n",
      "(Epoch 5 / 20) train acc: 0.576000; val_acc: 0.588889\n",
      "(Iteration 51 / 200) loss: 1.217702\n",
      "(Epoch 6 / 20) train acc: 0.619000; val_acc: 0.669444\n",
      "(Iteration 61 / 200) loss: 0.896586\n",
      "(Epoch 7 / 20) train acc: 0.680000; val_acc: 0.694444\n",
      "(Iteration 71 / 200) loss: 0.916667\n",
      "(Epoch 8 / 20) train acc: 0.711000; val_acc: 0.705556\n",
      "(Iteration 81 / 200) loss: 0.940614\n",
      "(Epoch 9 / 20) train acc: 0.738000; val_acc: 0.744444\n",
      "(Iteration 91 / 200) loss: 0.671760\n",
      "(Epoch 10 / 20) train acc: 0.780000; val_acc: 0.758333\n",
      "(Iteration 101 / 200) loss: 0.787641\n",
      "(Epoch 11 / 20) train acc: 0.834000; val_acc: 0.825000\n",
      "(Iteration 111 / 200) loss: 0.507550\n",
      "(Epoch 12 / 20) train acc: 0.825000; val_acc: 0.833333\n",
      "(Iteration 121 / 200) loss: 0.490583\n",
      "(Epoch 13 / 20) train acc: 0.840000; val_acc: 0.816667\n",
      "(Iteration 131 / 200) loss: 0.506075\n",
      "(Epoch 14 / 20) train acc: 0.910000; val_acc: 0.875000\n",
      "(Iteration 141 / 200) loss: 0.425119\n",
      "(Epoch 15 / 20) train acc: 0.900000; val_acc: 0.850000\n",
      "(Iteration 151 / 200) loss: 0.431996\n",
      "(Epoch 16 / 20) train acc: 0.911000; val_acc: 0.875000\n",
      "(Iteration 161 / 200) loss: 0.302690\n",
      "(Epoch 17 / 20) train acc: 0.920000; val_acc: 0.886111\n",
      "(Iteration 171 / 200) loss: 0.186325\n",
      "(Epoch 18 / 20) train acc: 0.915000; val_acc: 0.891667\n",
      "(Iteration 181 / 200) loss: 0.308527\n",
      "(Epoch 19 / 20) train acc: 0.934000; val_acc: 0.894444\n",
      "(Iteration 191 / 200) loss: 0.275649\n",
      "(Epoch 20 / 20) train acc: 0.915000; val_acc: 0.891667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 2.300879\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.237787\n",
      "(Epoch 3 / 20) train acc: 0.125000; val_acc: 0.094444\n",
      "(Iteration 31 / 200) loss: 2.084914\n",
      "(Epoch 4 / 20) train acc: 0.327000; val_acc: 0.319444\n",
      "(Iteration 41 / 200) loss: 1.765549\n",
      "(Epoch 5 / 20) train acc: 0.338000; val_acc: 0.366667\n",
      "(Iteration 51 / 200) loss: 1.728504\n",
      "(Epoch 6 / 20) train acc: 0.482000; val_acc: 0.530556\n",
      "(Iteration 61 / 200) loss: 1.553593\n",
      "(Epoch 7 / 20) train acc: 0.573000; val_acc: 0.588889\n",
      "(Iteration 71 / 200) loss: 1.135149\n",
      "(Epoch 8 / 20) train acc: 0.667000; val_acc: 0.616667\n",
      "(Iteration 81 / 200) loss: 0.989735\n",
      "(Epoch 9 / 20) train acc: 0.721000; val_acc: 0.686111\n",
      "(Iteration 91 / 200) loss: 0.908383\n",
      "(Epoch 10 / 20) train acc: 0.693000; val_acc: 0.691667\n",
      "(Iteration 101 / 200) loss: 0.852668\n",
      "(Epoch 11 / 20) train acc: 0.744000; val_acc: 0.686111\n",
      "(Iteration 111 / 200) loss: 0.800487\n",
      "(Epoch 12 / 20) train acc: 0.741000; val_acc: 0.705556\n",
      "(Iteration 121 / 200) loss: 0.613172\n",
      "(Epoch 13 / 20) train acc: 0.789000; val_acc: 0.750000\n",
      "(Iteration 131 / 200) loss: 0.682202\n",
      "(Epoch 14 / 20) train acc: 0.788000; val_acc: 0.744444\n",
      "(Iteration 141 / 200) loss: 0.381047\n",
      "(Epoch 15 / 20) train acc: 0.787000; val_acc: 0.769444\n",
      "(Iteration 151 / 200) loss: 0.557522\n",
      "(Epoch 16 / 20) train acc: 0.810000; val_acc: 0.794444\n",
      "(Iteration 161 / 200) loss: 0.454948\n",
      "(Epoch 17 / 20) train acc: 0.813000; val_acc: 0.819444\n",
      "(Iteration 171 / 200) loss: 0.363238\n",
      "(Epoch 18 / 20) train acc: 0.817000; val_acc: 0.805556\n",
      "(Iteration 181 / 200) loss: 0.489559\n",
      "(Epoch 19 / 20) train acc: 0.840000; val_acc: 0.825000\n",
      "(Iteration 191 / 200) loss: 0.320311\n",
      "(Epoch 20 / 20) train acc: 0.821000; val_acc: 0.811111\n",
      "(Iteration 1 / 200) loss: 3177.830381\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.128000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2322.493956\n",
      "(Epoch 2 / 20) train acc: 0.152000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 1662.270446\n",
      "(Epoch 3 / 20) train acc: 0.174000; val_acc: 0.213889\n",
      "(Iteration 31 / 200) loss: 1291.654661\n",
      "(Epoch 4 / 20) train acc: 0.215000; val_acc: 0.244444\n",
      "(Iteration 41 / 200) loss: 1135.753104\n",
      "(Epoch 5 / 20) train acc: 0.308000; val_acc: 0.291667\n",
      "(Iteration 51 / 200) loss: 842.663562\n",
      "(Epoch 6 / 20) train acc: 0.350000; val_acc: 0.358333\n",
      "(Iteration 61 / 200) loss: 810.831201\n",
      "(Epoch 7 / 20) train acc: 0.398000; val_acc: 0.416667\n",
      "(Iteration 71 / 200) loss: 681.360952\n",
      "(Epoch 8 / 20) train acc: 0.473000; val_acc: 0.452778\n",
      "(Iteration 81 / 200) loss: 383.998503\n",
      "(Epoch 9 / 20) train acc: 0.528000; val_acc: 0.491667\n",
      "(Iteration 91 / 200) loss: 403.754992\n",
      "(Epoch 10 / 20) train acc: 0.526000; val_acc: 0.527778\n",
      "(Iteration 101 / 200) loss: 347.954356\n",
      "(Epoch 11 / 20) train acc: 0.607000; val_acc: 0.572222\n",
      "(Iteration 111 / 200) loss: 331.073482\n",
      "(Epoch 12 / 20) train acc: 0.592000; val_acc: 0.602778\n",
      "(Iteration 121 / 200) loss: 245.295193\n",
      "(Epoch 13 / 20) train acc: 0.637000; val_acc: 0.619444\n",
      "(Iteration 131 / 200) loss: 243.024752\n",
      "(Epoch 14 / 20) train acc: 0.716000; val_acc: 0.647222\n",
      "(Iteration 141 / 200) loss: 107.244801\n",
      "(Epoch 15 / 20) train acc: 0.744000; val_acc: 0.675000\n",
      "(Iteration 151 / 200) loss: 174.397202\n",
      "(Epoch 16 / 20) train acc: 0.728000; val_acc: 0.680556\n",
      "(Iteration 161 / 200) loss: 128.700375\n",
      "(Epoch 17 / 20) train acc: 0.757000; val_acc: 0.694444\n",
      "(Iteration 171 / 200) loss: 216.237239\n",
      "(Epoch 18 / 20) train acc: 0.766000; val_acc: 0.711111\n",
      "(Iteration 181 / 200) loss: 179.738194\n",
      "(Epoch 19 / 20) train acc: 0.793000; val_acc: 0.713889\n",
      "(Iteration 191 / 200) loss: 110.503676\n",
      "(Epoch 20 / 20) train acc: 0.763000; val_acc: 0.725000\n",
      "(Iteration 1 / 200) loss: 4.577521\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.546000; val_acc: 0.538889\n",
      "(Iteration 11 / 200) loss: 1.512170\n",
      "(Epoch 2 / 20) train acc: 0.825000; val_acc: 0.791667\n",
      "(Iteration 21 / 200) loss: 0.615042\n",
      "(Epoch 3 / 20) train acc: 0.891000; val_acc: 0.869444\n",
      "(Iteration 31 / 200) loss: 0.383160\n",
      "(Epoch 4 / 20) train acc: 0.922000; val_acc: 0.894444\n",
      "(Iteration 41 / 200) loss: 0.290496\n",
      "(Epoch 5 / 20) train acc: 0.940000; val_acc: 0.911111\n",
      "(Iteration 51 / 200) loss: 0.362281\n",
      "(Epoch 6 / 20) train acc: 0.958000; val_acc: 0.933333\n",
      "(Iteration 61 / 200) loss: 0.226349\n",
      "(Epoch 7 / 20) train acc: 0.960000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.157279\n",
      "(Epoch 8 / 20) train acc: 0.975000; val_acc: 0.952778\n",
      "(Iteration 81 / 200) loss: 0.235072\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.215325\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.130308\n",
      "(Epoch 11 / 20) train acc: 0.979000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.190680\n",
      "(Epoch 12 / 20) train acc: 0.985000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.194198\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.109598\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.123142\n",
      "(Epoch 15 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.133317\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.094468\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.088209\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.099684\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.134269\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.303930\n",
      "(Epoch 0 / 20) train acc: 0.237000; val_acc: 0.194444\n",
      "(Epoch 1 / 20) train acc: 0.611000; val_acc: 0.552778\n",
      "(Iteration 11 / 200) loss: 2.231088\n",
      "(Epoch 2 / 20) train acc: 0.569000; val_acc: 0.605556\n",
      "(Iteration 21 / 200) loss: 1.805207\n",
      "(Epoch 3 / 20) train acc: 0.716000; val_acc: 0.716667\n",
      "(Iteration 31 / 200) loss: 1.164173\n",
      "(Epoch 4 / 20) train acc: 0.831000; val_acc: 0.780556\n",
      "(Iteration 41 / 200) loss: 0.636964\n",
      "(Epoch 5 / 20) train acc: 0.849000; val_acc: 0.833333\n",
      "(Iteration 51 / 200) loss: 0.543634\n",
      "(Epoch 6 / 20) train acc: 0.854000; val_acc: 0.830556\n",
      "(Iteration 61 / 200) loss: 0.387424\n",
      "(Epoch 7 / 20) train acc: 0.880000; val_acc: 0.872222\n",
      "(Iteration 71 / 200) loss: 0.328613\n",
      "(Epoch 8 / 20) train acc: 0.910000; val_acc: 0.897222\n",
      "(Iteration 81 / 200) loss: 0.435165\n",
      "(Epoch 9 / 20) train acc: 0.910000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 0.301682\n",
      "(Epoch 10 / 20) train acc: 0.923000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 0.358656\n",
      "(Epoch 11 / 20) train acc: 0.936000; val_acc: 0.905556\n",
      "(Iteration 111 / 200) loss: 0.241475\n",
      "(Epoch 12 / 20) train acc: 0.946000; val_acc: 0.897222\n",
      "(Iteration 121 / 200) loss: 0.183572\n",
      "(Epoch 13 / 20) train acc: 0.954000; val_acc: 0.944444\n",
      "(Iteration 131 / 200) loss: 0.130895\n",
      "(Epoch 14 / 20) train acc: 0.954000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 0.142013\n",
      "(Epoch 15 / 20) train acc: 0.958000; val_acc: 0.941667\n",
      "(Iteration 151 / 200) loss: 0.163754\n",
      "(Epoch 16 / 20) train acc: 0.958000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 0.243671\n",
      "(Epoch 17 / 20) train acc: 0.946000; val_acc: 0.925000\n",
      "(Iteration 171 / 200) loss: 0.180096\n",
      "(Epoch 18 / 20) train acc: 0.956000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.169724\n",
      "(Epoch 19 / 20) train acc: 0.957000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.122555\n",
      "(Epoch 20 / 20) train acc: 0.974000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.281291\n",
      "(Epoch 2 / 20) train acc: 0.217000; val_acc: 0.216667\n",
      "(Iteration 21 / 200) loss: 2.128958\n",
      "(Epoch 3 / 20) train acc: 0.242000; val_acc: 0.261111\n",
      "(Iteration 31 / 200) loss: 1.922197\n",
      "(Epoch 4 / 20) train acc: 0.387000; val_acc: 0.394444\n",
      "(Iteration 41 / 200) loss: 1.602676\n",
      "(Epoch 5 / 20) train acc: 0.376000; val_acc: 0.347222\n",
      "(Iteration 51 / 200) loss: 1.408069\n",
      "(Epoch 6 / 20) train acc: 0.417000; val_acc: 0.388889\n",
      "(Iteration 61 / 200) loss: 1.418987\n",
      "(Epoch 7 / 20) train acc: 0.515000; val_acc: 0.530556\n",
      "(Iteration 71 / 200) loss: 1.280668\n",
      "(Epoch 8 / 20) train acc: 0.655000; val_acc: 0.666667\n",
      "(Iteration 81 / 200) loss: 0.951962\n",
      "(Epoch 9 / 20) train acc: 0.718000; val_acc: 0.683333\n",
      "(Iteration 91 / 200) loss: 0.800061\n",
      "(Epoch 10 / 20) train acc: 0.794000; val_acc: 0.766667\n",
      "(Iteration 101 / 200) loss: 0.655583\n",
      "(Epoch 11 / 20) train acc: 0.810000; val_acc: 0.794444\n",
      "(Iteration 111 / 200) loss: 0.551774\n",
      "(Epoch 12 / 20) train acc: 0.834000; val_acc: 0.833333\n",
      "(Iteration 121 / 200) loss: 0.602261\n",
      "(Epoch 13 / 20) train acc: 0.861000; val_acc: 0.841667\n",
      "(Iteration 131 / 200) loss: 0.388378\n",
      "(Epoch 14 / 20) train acc: 0.895000; val_acc: 0.875000\n",
      "(Iteration 141 / 200) loss: 0.364996\n",
      "(Epoch 15 / 20) train acc: 0.880000; val_acc: 0.863889\n",
      "(Iteration 151 / 200) loss: 0.354151\n",
      "(Epoch 16 / 20) train acc: 0.909000; val_acc: 0.891667\n",
      "(Iteration 161 / 200) loss: 0.351424\n",
      "(Epoch 17 / 20) train acc: 0.904000; val_acc: 0.880556\n",
      "(Iteration 171 / 200) loss: 0.179110\n",
      "(Epoch 18 / 20) train acc: 0.935000; val_acc: 0.897222\n",
      "(Iteration 181 / 200) loss: 0.314670\n",
      "(Epoch 19 / 20) train acc: 0.926000; val_acc: 0.902778\n",
      "(Iteration 191 / 200) loss: 0.346491\n",
      "(Epoch 20 / 20) train acc: 0.940000; val_acc: 0.894444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.286477\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 2.087033\n",
      "(Epoch 3 / 20) train acc: 0.266000; val_acc: 0.222222\n",
      "(Iteration 31 / 200) loss: 1.962325\n",
      "(Epoch 4 / 20) train acc: 0.326000; val_acc: 0.377778\n",
      "(Iteration 41 / 200) loss: 1.777623\n",
      "(Epoch 5 / 20) train acc: 0.408000; val_acc: 0.450000\n",
      "(Iteration 51 / 200) loss: 1.550259\n",
      "(Epoch 6 / 20) train acc: 0.552000; val_acc: 0.611111\n",
      "(Iteration 61 / 200) loss: 1.345735\n",
      "(Epoch 7 / 20) train acc: 0.631000; val_acc: 0.644444\n",
      "(Iteration 71 / 200) loss: 0.866538\n",
      "(Epoch 8 / 20) train acc: 0.626000; val_acc: 0.630556\n",
      "(Iteration 81 / 200) loss: 0.894809\n",
      "(Epoch 9 / 20) train acc: 0.648000; val_acc: 0.700000\n",
      "(Iteration 91 / 200) loss: 0.789558\n",
      "(Epoch 10 / 20) train acc: 0.694000; val_acc: 0.708333\n",
      "(Iteration 101 / 200) loss: 0.859088\n",
      "(Epoch 11 / 20) train acc: 0.711000; val_acc: 0.702778\n",
      "(Iteration 111 / 200) loss: 0.740645\n",
      "(Epoch 12 / 20) train acc: 0.722000; val_acc: 0.736111\n",
      "(Iteration 121 / 200) loss: 0.739886\n",
      "(Epoch 13 / 20) train acc: 0.801000; val_acc: 0.758333\n",
      "(Iteration 131 / 200) loss: 0.764005\n",
      "(Epoch 14 / 20) train acc: 0.757000; val_acc: 0.769444\n",
      "(Iteration 141 / 200) loss: 0.573926\n",
      "(Epoch 15 / 20) train acc: 0.814000; val_acc: 0.819444\n",
      "(Iteration 151 / 200) loss: 0.530229\n",
      "(Epoch 16 / 20) train acc: 0.796000; val_acc: 0.766667\n",
      "(Iteration 161 / 200) loss: 0.441211\n",
      "(Epoch 17 / 20) train acc: 0.873000; val_acc: 0.841667\n",
      "(Iteration 171 / 200) loss: 0.453714\n",
      "(Epoch 18 / 20) train acc: 0.860000; val_acc: 0.841667\n",
      "(Iteration 181 / 200) loss: 0.448570\n",
      "(Epoch 19 / 20) train acc: 0.889000; val_acc: 0.841667\n",
      "(Iteration 191 / 200) loss: 0.368983\n",
      "(Epoch 20 / 20) train acc: 0.901000; val_acc: 0.875000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.295807\n",
      "(Epoch 2 / 20) train acc: 0.175000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 2.158082\n",
      "(Epoch 3 / 20) train acc: 0.189000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 2.025237\n",
      "(Epoch 4 / 20) train acc: 0.192000; val_acc: 0.208333\n",
      "(Iteration 41 / 200) loss: 2.077799\n",
      "(Epoch 5 / 20) train acc: 0.206000; val_acc: 0.177778\n",
      "(Iteration 51 / 200) loss: 1.992246\n",
      "(Epoch 6 / 20) train acc: 0.229000; val_acc: 0.230556\n",
      "(Iteration 61 / 200) loss: 2.018668\n",
      "(Epoch 7 / 20) train acc: 0.223000; val_acc: 0.227778\n",
      "(Iteration 71 / 200) loss: 1.959638\n",
      "(Epoch 8 / 20) train acc: 0.272000; val_acc: 0.244444\n",
      "(Iteration 81 / 200) loss: 1.965757\n",
      "(Epoch 9 / 20) train acc: 0.277000; val_acc: 0.322222\n",
      "(Iteration 91 / 200) loss: 1.868172\n",
      "(Epoch 10 / 20) train acc: 0.305000; val_acc: 0.325000\n",
      "(Iteration 101 / 200) loss: 1.919500\n",
      "(Epoch 11 / 20) train acc: 0.312000; val_acc: 0.355556\n",
      "(Iteration 111 / 200) loss: 1.816576\n",
      "(Epoch 12 / 20) train acc: 0.405000; val_acc: 0.416667\n",
      "(Iteration 121 / 200) loss: 1.608902\n",
      "(Epoch 13 / 20) train acc: 0.414000; val_acc: 0.452778\n",
      "(Iteration 131 / 200) loss: 1.669672\n",
      "(Epoch 14 / 20) train acc: 0.492000; val_acc: 0.525000\n",
      "(Iteration 141 / 200) loss: 1.597449\n",
      "(Epoch 15 / 20) train acc: 0.533000; val_acc: 0.519444\n",
      "(Iteration 151 / 200) loss: 1.206860\n",
      "(Epoch 16 / 20) train acc: 0.533000; val_acc: 0.552778\n",
      "(Iteration 161 / 200) loss: 1.199002\n",
      "(Epoch 17 / 20) train acc: 0.592000; val_acc: 0.605556\n",
      "(Iteration 171 / 200) loss: 1.001910\n",
      "(Epoch 18 / 20) train acc: 0.605000; val_acc: 0.636111\n",
      "(Iteration 181 / 200) loss: 0.825808\n",
      "(Epoch 19 / 20) train acc: 0.612000; val_acc: 0.666667\n",
      "(Iteration 191 / 200) loss: 0.830034\n",
      "(Epoch 20 / 20) train acc: 0.758000; val_acc: 0.741667\n",
      "(Iteration 1 / 200) loss: 3456.337084\n",
      "(Epoch 0 / 20) train acc: 0.129000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.168000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2266.022978\n",
      "(Epoch 2 / 20) train acc: 0.240000; val_acc: 0.202778\n",
      "(Iteration 21 / 200) loss: 1623.508162\n",
      "(Epoch 3 / 20) train acc: 0.237000; val_acc: 0.233333\n",
      "(Iteration 31 / 200) loss: 1407.481085\n",
      "(Epoch 4 / 20) train acc: 0.309000; val_acc: 0.291667\n",
      "(Iteration 41 / 200) loss: 1192.354444\n",
      "(Epoch 5 / 20) train acc: 0.355000; val_acc: 0.333333\n",
      "(Iteration 51 / 200) loss: 859.387948\n",
      "(Epoch 6 / 20) train acc: 0.377000; val_acc: 0.388889\n",
      "(Iteration 61 / 200) loss: 645.701366\n",
      "(Epoch 7 / 20) train acc: 0.437000; val_acc: 0.433333\n",
      "(Iteration 71 / 200) loss: 652.078175\n",
      "(Epoch 8 / 20) train acc: 0.485000; val_acc: 0.480556\n",
      "(Iteration 81 / 200) loss: 420.880929\n",
      "(Epoch 9 / 20) train acc: 0.556000; val_acc: 0.519444\n",
      "(Iteration 91 / 200) loss: 405.575168\n",
      "(Epoch 10 / 20) train acc: 0.578000; val_acc: 0.566667\n",
      "(Iteration 101 / 200) loss: 351.082698\n",
      "(Epoch 11 / 20) train acc: 0.607000; val_acc: 0.594444\n",
      "(Iteration 111 / 200) loss: 217.219954\n",
      "(Epoch 12 / 20) train acc: 0.640000; val_acc: 0.613889\n",
      "(Iteration 121 / 200) loss: 258.454269\n",
      "(Epoch 13 / 20) train acc: 0.684000; val_acc: 0.652778\n",
      "(Iteration 131 / 200) loss: 282.042867\n",
      "(Epoch 14 / 20) train acc: 0.713000; val_acc: 0.658333\n",
      "(Iteration 141 / 200) loss: 142.537407\n",
      "(Epoch 15 / 20) train acc: 0.747000; val_acc: 0.697222\n",
      "(Iteration 151 / 200) loss: 202.476141\n",
      "(Epoch 16 / 20) train acc: 0.743000; val_acc: 0.705556\n",
      "(Iteration 161 / 200) loss: 138.435363\n",
      "(Epoch 17 / 20) train acc: 0.768000; val_acc: 0.730556\n",
      "(Iteration 171 / 200) loss: 158.556115\n",
      "(Epoch 18 / 20) train acc: 0.781000; val_acc: 0.744444\n",
      "(Iteration 181 / 200) loss: 149.425467\n",
      "(Epoch 19 / 20) train acc: 0.785000; val_acc: 0.763889\n",
      "(Iteration 191 / 200) loss: 91.495449\n",
      "(Epoch 20 / 20) train acc: 0.845000; val_acc: 0.777778\n",
      "(Iteration 1 / 200) loss: 5.175551\n",
      "(Epoch 0 / 20) train acc: 0.059000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.349000; val_acc: 0.361111\n",
      "(Iteration 11 / 200) loss: 2.016423\n",
      "(Epoch 2 / 20) train acc: 0.758000; val_acc: 0.763889\n",
      "(Iteration 21 / 200) loss: 0.792029\n",
      "(Epoch 3 / 20) train acc: 0.881000; val_acc: 0.841667\n",
      "(Iteration 31 / 200) loss: 0.403681\n",
      "(Epoch 4 / 20) train acc: 0.929000; val_acc: 0.886111\n",
      "(Iteration 41 / 200) loss: 0.297256\n",
      "(Epoch 5 / 20) train acc: 0.940000; val_acc: 0.894444\n",
      "(Iteration 51 / 200) loss: 0.194068\n",
      "(Epoch 6 / 20) train acc: 0.975000; val_acc: 0.922222\n",
      "(Iteration 61 / 200) loss: 0.189021\n",
      "(Epoch 7 / 20) train acc: 0.966000; val_acc: 0.925000\n",
      "(Iteration 71 / 200) loss: 0.114771\n",
      "(Epoch 8 / 20) train acc: 0.976000; val_acc: 0.933333\n",
      "(Iteration 81 / 200) loss: 0.096834\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.094223\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.158171\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.118784\n",
      "(Epoch 12 / 20) train acc: 0.984000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.072761\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.046181\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.048526\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.044508\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.961111\n",
      "(Iteration 161 / 200) loss: 0.028435\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.049967\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.053971\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.029613\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302970\n",
      "(Epoch 0 / 20) train acc: 0.321000; val_acc: 0.350000\n",
      "(Epoch 1 / 20) train acc: 0.487000; val_acc: 0.463889\n",
      "(Iteration 11 / 200) loss: 2.216737\n",
      "(Epoch 2 / 20) train acc: 0.633000; val_acc: 0.661111\n",
      "(Iteration 21 / 200) loss: 1.820707\n",
      "(Epoch 3 / 20) train acc: 0.678000; val_acc: 0.683333\n",
      "(Iteration 31 / 200) loss: 1.120283\n",
      "(Epoch 4 / 20) train acc: 0.812000; val_acc: 0.791667\n",
      "(Iteration 41 / 200) loss: 0.712530\n",
      "(Epoch 5 / 20) train acc: 0.842000; val_acc: 0.833333\n",
      "(Iteration 51 / 200) loss: 0.523190\n",
      "(Epoch 6 / 20) train acc: 0.901000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 0.400323\n",
      "(Epoch 7 / 20) train acc: 0.882000; val_acc: 0.877778\n",
      "(Iteration 71 / 200) loss: 0.322251\n",
      "(Epoch 8 / 20) train acc: 0.887000; val_acc: 0.869444\n",
      "(Iteration 81 / 200) loss: 0.458235\n",
      "(Epoch 9 / 20) train acc: 0.923000; val_acc: 0.902778\n",
      "(Iteration 91 / 200) loss: 0.251864\n",
      "(Epoch 10 / 20) train acc: 0.937000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 0.200992\n",
      "(Epoch 11 / 20) train acc: 0.920000; val_acc: 0.900000\n",
      "(Iteration 111 / 200) loss: 0.315737\n",
      "(Epoch 12 / 20) train acc: 0.935000; val_acc: 0.925000\n",
      "(Iteration 121 / 200) loss: 0.272120\n",
      "(Epoch 13 / 20) train acc: 0.938000; val_acc: 0.922222\n",
      "(Iteration 131 / 200) loss: 0.203088\n",
      "(Epoch 14 / 20) train acc: 0.958000; val_acc: 0.936111\n",
      "(Iteration 141 / 200) loss: 0.154129\n",
      "(Epoch 15 / 20) train acc: 0.968000; val_acc: 0.938889\n",
      "(Iteration 151 / 200) loss: 0.160339\n",
      "(Epoch 16 / 20) train acc: 0.961000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 0.140712\n",
      "(Epoch 17 / 20) train acc: 0.955000; val_acc: 0.930556\n",
      "(Iteration 171 / 200) loss: 0.143629\n",
      "(Epoch 18 / 20) train acc: 0.964000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 0.154646\n",
      "(Epoch 19 / 20) train acc: 0.966000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.087319\n",
      "(Epoch 20 / 20) train acc: 0.953000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.162000; val_acc: 0.177778\n",
      "(Iteration 11 / 200) loss: 2.279722\n",
      "(Epoch 2 / 20) train acc: 0.241000; val_acc: 0.230556\n",
      "(Iteration 21 / 200) loss: 2.052861\n",
      "(Epoch 3 / 20) train acc: 0.355000; val_acc: 0.344444\n",
      "(Iteration 31 / 200) loss: 1.681014\n",
      "(Epoch 4 / 20) train acc: 0.501000; val_acc: 0.541667\n",
      "(Iteration 41 / 200) loss: 1.445240\n",
      "(Epoch 5 / 20) train acc: 0.583000; val_acc: 0.569444\n",
      "(Iteration 51 / 200) loss: 1.174656\n",
      "(Epoch 6 / 20) train acc: 0.711000; val_acc: 0.711111\n",
      "(Iteration 61 / 200) loss: 0.843579\n",
      "(Epoch 7 / 20) train acc: 0.781000; val_acc: 0.780556\n",
      "(Iteration 71 / 200) loss: 0.662318\n",
      "(Epoch 8 / 20) train acc: 0.803000; val_acc: 0.797222\n",
      "(Iteration 81 / 200) loss: 0.474375\n",
      "(Epoch 9 / 20) train acc: 0.813000; val_acc: 0.811111\n",
      "(Iteration 91 / 200) loss: 0.638571\n",
      "(Epoch 10 / 20) train acc: 0.851000; val_acc: 0.841667\n",
      "(Iteration 101 / 200) loss: 0.455721\n",
      "(Epoch 11 / 20) train acc: 0.863000; val_acc: 0.830556\n",
      "(Iteration 111 / 200) loss: 0.446780\n",
      "(Epoch 12 / 20) train acc: 0.852000; val_acc: 0.847222\n",
      "(Iteration 121 / 200) loss: 0.546393\n",
      "(Epoch 13 / 20) train acc: 0.897000; val_acc: 0.841667\n",
      "(Iteration 131 / 200) loss: 0.257635\n",
      "(Epoch 14 / 20) train acc: 0.886000; val_acc: 0.852778\n",
      "(Iteration 141 / 200) loss: 0.320521\n",
      "(Epoch 15 / 20) train acc: 0.896000; val_acc: 0.863889\n",
      "(Iteration 151 / 200) loss: 0.405983\n",
      "(Epoch 16 / 20) train acc: 0.896000; val_acc: 0.877778\n",
      "(Iteration 161 / 200) loss: 0.323556\n",
      "(Epoch 17 / 20) train acc: 0.905000; val_acc: 0.875000\n",
      "(Iteration 171 / 200) loss: 0.261644\n",
      "(Epoch 18 / 20) train acc: 0.907000; val_acc: 0.866667\n",
      "(Iteration 181 / 200) loss: 0.322375\n",
      "(Epoch 19 / 20) train acc: 0.933000; val_acc: 0.891667\n",
      "(Iteration 191 / 200) loss: 0.320548\n",
      "(Epoch 20 / 20) train acc: 0.911000; val_acc: 0.883333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.281119\n",
      "(Epoch 2 / 20) train acc: 0.124000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 2.166418\n",
      "(Epoch 3 / 20) train acc: 0.187000; val_acc: 0.222222\n",
      "(Iteration 31 / 200) loss: 2.013637\n",
      "(Epoch 4 / 20) train acc: 0.209000; val_acc: 0.188889\n",
      "(Iteration 41 / 200) loss: 1.930238\n",
      "(Epoch 5 / 20) train acc: 0.260000; val_acc: 0.286111\n",
      "(Iteration 51 / 200) loss: 1.889257\n",
      "(Epoch 6 / 20) train acc: 0.397000; val_acc: 0.397222\n",
      "(Iteration 61 / 200) loss: 1.651009\n",
      "(Epoch 7 / 20) train acc: 0.536000; val_acc: 0.494444\n",
      "(Iteration 71 / 200) loss: 1.354208\n",
      "(Epoch 8 / 20) train acc: 0.592000; val_acc: 0.558333\n",
      "(Iteration 81 / 200) loss: 1.126628\n",
      "(Epoch 9 / 20) train acc: 0.630000; val_acc: 0.611111\n",
      "(Iteration 91 / 200) loss: 0.939240\n",
      "(Epoch 10 / 20) train acc: 0.651000; val_acc: 0.627778\n",
      "(Iteration 101 / 200) loss: 0.838798\n",
      "(Epoch 11 / 20) train acc: 0.707000; val_acc: 0.669444\n",
      "(Iteration 111 / 200) loss: 0.919639\n",
      "(Epoch 12 / 20) train acc: 0.729000; val_acc: 0.686111\n",
      "(Iteration 121 / 200) loss: 0.743027\n",
      "(Epoch 13 / 20) train acc: 0.720000; val_acc: 0.680556\n",
      "(Iteration 131 / 200) loss: 0.778325\n",
      "(Epoch 14 / 20) train acc: 0.751000; val_acc: 0.705556\n",
      "(Iteration 141 / 200) loss: 0.689208\n",
      "(Epoch 15 / 20) train acc: 0.757000; val_acc: 0.688889\n",
      "(Iteration 151 / 200) loss: 0.867231\n",
      "(Epoch 16 / 20) train acc: 0.792000; val_acc: 0.730556\n",
      "(Iteration 161 / 200) loss: 0.567960\n",
      "(Epoch 17 / 20) train acc: 0.806000; val_acc: 0.763889\n",
      "(Iteration 171 / 200) loss: 0.555970\n",
      "(Epoch 18 / 20) train acc: 0.814000; val_acc: 0.777778\n",
      "(Iteration 181 / 200) loss: 0.428014\n",
      "(Epoch 19 / 20) train acc: 0.849000; val_acc: 0.794444\n",
      "(Iteration 191 / 200) loss: 0.515381\n",
      "(Epoch 20 / 20) train acc: 0.850000; val_acc: 0.813889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.299544\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.191668\n",
      "(Epoch 3 / 20) train acc: 0.211000; val_acc: 0.227778\n",
      "(Iteration 31 / 200) loss: 2.034416\n",
      "(Epoch 4 / 20) train acc: 0.202000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 1.847279\n",
      "(Epoch 5 / 20) train acc: 0.221000; val_acc: 0.238889\n",
      "(Iteration 51 / 200) loss: 1.790201\n",
      "(Epoch 6 / 20) train acc: 0.234000; val_acc: 0.211111\n",
      "(Iteration 61 / 200) loss: 1.664287\n",
      "(Epoch 7 / 20) train acc: 0.311000; val_acc: 0.325000\n",
      "(Iteration 71 / 200) loss: 1.568148\n",
      "(Epoch 8 / 20) train acc: 0.268000; val_acc: 0.291667\n",
      "(Iteration 81 / 200) loss: 1.547729\n",
      "(Epoch 9 / 20) train acc: 0.374000; val_acc: 0.397222\n",
      "(Iteration 91 / 200) loss: 1.544469\n",
      "(Epoch 10 / 20) train acc: 0.332000; val_acc: 0.386111\n",
      "(Iteration 101 / 200) loss: 1.475994\n",
      "(Epoch 11 / 20) train acc: 0.436000; val_acc: 0.416667\n",
      "(Iteration 111 / 200) loss: 1.333766\n",
      "(Epoch 12 / 20) train acc: 0.522000; val_acc: 0.572222\n",
      "(Iteration 121 / 200) loss: 1.284056\n",
      "(Epoch 13 / 20) train acc: 0.632000; val_acc: 0.647222\n",
      "(Iteration 131 / 200) loss: 1.212206\n",
      "(Epoch 14 / 20) train acc: 0.669000; val_acc: 0.641667\n",
      "(Iteration 141 / 200) loss: 0.952680\n",
      "(Epoch 15 / 20) train acc: 0.668000; val_acc: 0.672222\n",
      "(Iteration 151 / 200) loss: 0.954645\n",
      "(Epoch 16 / 20) train acc: 0.700000; val_acc: 0.736111\n",
      "(Iteration 161 / 200) loss: 0.633378\n",
      "(Epoch 17 / 20) train acc: 0.722000; val_acc: 0.741667\n",
      "(Iteration 171 / 200) loss: 0.676854\n",
      "(Epoch 18 / 20) train acc: 0.736000; val_acc: 0.738889\n",
      "(Iteration 181 / 200) loss: 0.826261\n",
      "(Epoch 19 / 20) train acc: 0.738000; val_acc: 0.738889\n",
      "(Iteration 191 / 200) loss: 0.733721\n",
      "(Epoch 20 / 20) train acc: 0.739000; val_acc: 0.766667\n",
      "(Iteration 1 / 200) loss: 4390.642547\n",
      "(Epoch 0 / 20) train acc: 0.149000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.122000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 3132.354699\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2326.560481\n",
      "(Epoch 3 / 20) train acc: 0.148000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 1998.039653\n",
      "(Epoch 4 / 20) train acc: 0.187000; val_acc: 0.147222\n",
      "(Iteration 41 / 200) loss: 1540.889506\n",
      "(Epoch 5 / 20) train acc: 0.210000; val_acc: 0.194444\n",
      "(Iteration 51 / 200) loss: 1432.595669\n",
      "(Epoch 6 / 20) train acc: 0.274000; val_acc: 0.222222\n",
      "(Iteration 61 / 200) loss: 1346.142985\n",
      "(Epoch 7 / 20) train acc: 0.342000; val_acc: 0.258333\n",
      "(Iteration 71 / 200) loss: 892.553169\n",
      "(Epoch 8 / 20) train acc: 0.398000; val_acc: 0.325000\n",
      "(Iteration 81 / 200) loss: 720.461927\n",
      "(Epoch 9 / 20) train acc: 0.399000; val_acc: 0.372222\n",
      "(Iteration 91 / 200) loss: 627.222861\n",
      "(Epoch 10 / 20) train acc: 0.468000; val_acc: 0.433333\n",
      "(Iteration 101 / 200) loss: 587.224314\n",
      "(Epoch 11 / 20) train acc: 0.515000; val_acc: 0.463889\n",
      "(Iteration 111 / 200) loss: 532.695050\n",
      "(Epoch 12 / 20) train acc: 0.556000; val_acc: 0.522222\n",
      "(Iteration 121 / 200) loss: 364.405376\n",
      "(Epoch 13 / 20) train acc: 0.623000; val_acc: 0.555556\n",
      "(Iteration 131 / 200) loss: 316.951501\n",
      "(Epoch 14 / 20) train acc: 0.624000; val_acc: 0.583333\n",
      "(Iteration 141 / 200) loss: 285.382757\n",
      "(Epoch 15 / 20) train acc: 0.687000; val_acc: 0.602778\n",
      "(Iteration 151 / 200) loss: 346.946929\n",
      "(Epoch 16 / 20) train acc: 0.693000; val_acc: 0.630556\n",
      "(Iteration 161 / 200) loss: 243.477840\n",
      "(Epoch 17 / 20) train acc: 0.732000; val_acc: 0.652778\n",
      "(Iteration 171 / 200) loss: 264.815052\n",
      "(Epoch 18 / 20) train acc: 0.736000; val_acc: 0.672222\n",
      "(Iteration 181 / 200) loss: 218.699163\n",
      "(Epoch 19 / 20) train acc: 0.759000; val_acc: 0.688889\n",
      "(Iteration 191 / 200) loss: 223.200108\n",
      "(Epoch 20 / 20) train acc: 0.774000; val_acc: 0.697222\n",
      "(Iteration 1 / 200) loss: 5.663668\n",
      "(Epoch 0 / 20) train acc: 0.071000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.405000; val_acc: 0.377778\n",
      "(Iteration 11 / 200) loss: 1.942080\n",
      "(Epoch 2 / 20) train acc: 0.781000; val_acc: 0.747222\n",
      "(Iteration 21 / 200) loss: 0.817645\n",
      "(Epoch 3 / 20) train acc: 0.877000; val_acc: 0.841667\n",
      "(Iteration 31 / 200) loss: 0.526974\n",
      "(Epoch 4 / 20) train acc: 0.933000; val_acc: 0.902778\n",
      "(Iteration 41 / 200) loss: 0.285876\n",
      "(Epoch 5 / 20) train acc: 0.939000; val_acc: 0.919444\n",
      "(Iteration 51 / 200) loss: 0.149685\n",
      "(Epoch 6 / 20) train acc: 0.954000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.216410\n",
      "(Epoch 7 / 20) train acc: 0.962000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 0.172536\n",
      "(Epoch 8 / 20) train acc: 0.979000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.132915\n",
      "(Epoch 9 / 20) train acc: 0.974000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.067063\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.106958\n",
      "(Epoch 11 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.076192\n",
      "(Epoch 12 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.032820\n",
      "(Epoch 13 / 20) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.114732\n",
      "(Epoch 14 / 20) train acc: 0.989000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.046004\n",
      "(Epoch 15 / 20) train acc: 0.986000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.079756\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.049384\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.036844\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.024369\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.021075\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.625000; val_acc: 0.638889\n",
      "(Iteration 11 / 200) loss: 2.215904\n",
      "(Epoch 2 / 20) train acc: 0.645000; val_acc: 0.591667\n",
      "(Iteration 21 / 200) loss: 1.807951\n",
      "(Epoch 3 / 20) train acc: 0.733000; val_acc: 0.744444\n",
      "(Iteration 31 / 200) loss: 1.227393\n",
      "(Epoch 4 / 20) train acc: 0.844000; val_acc: 0.811111\n",
      "(Iteration 41 / 200) loss: 0.667355\n",
      "(Epoch 5 / 20) train acc: 0.839000; val_acc: 0.819444\n",
      "(Iteration 51 / 200) loss: 0.559406\n",
      "(Epoch 6 / 20) train acc: 0.870000; val_acc: 0.866667\n",
      "(Iteration 61 / 200) loss: 0.483269\n",
      "(Epoch 7 / 20) train acc: 0.893000; val_acc: 0.875000\n",
      "(Iteration 71 / 200) loss: 0.397992\n",
      "(Epoch 8 / 20) train acc: 0.900000; val_acc: 0.888889\n",
      "(Iteration 81 / 200) loss: 0.398430\n",
      "(Epoch 9 / 20) train acc: 0.940000; val_acc: 0.902778\n",
      "(Iteration 91 / 200) loss: 0.220330\n",
      "(Epoch 10 / 20) train acc: 0.928000; val_acc: 0.911111\n",
      "(Iteration 101 / 200) loss: 0.175404\n",
      "(Epoch 11 / 20) train acc: 0.955000; val_acc: 0.922222\n",
      "(Iteration 111 / 200) loss: 0.146609\n",
      "(Epoch 12 / 20) train acc: 0.951000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 0.163710\n",
      "(Epoch 13 / 20) train acc: 0.969000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 0.247461\n",
      "(Epoch 14 / 20) train acc: 0.941000; val_acc: 0.938889\n",
      "(Iteration 141 / 200) loss: 0.183604\n",
      "(Epoch 15 / 20) train acc: 0.960000; val_acc: 0.936111\n",
      "(Iteration 151 / 200) loss: 0.106980\n",
      "(Epoch 16 / 20) train acc: 0.969000; val_acc: 0.950000\n",
      "(Iteration 161 / 200) loss: 0.154216\n",
      "(Epoch 17 / 20) train acc: 0.955000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.092131\n",
      "(Epoch 18 / 20) train acc: 0.969000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.109633\n",
      "(Epoch 19 / 20) train acc: 0.976000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.143325\n",
      "(Epoch 20 / 20) train acc: 0.959000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.210000; val_acc: 0.180556\n",
      "(Iteration 11 / 200) loss: 2.281357\n",
      "(Epoch 2 / 20) train acc: 0.248000; val_acc: 0.247222\n",
      "(Iteration 21 / 200) loss: 2.095366\n",
      "(Epoch 3 / 20) train acc: 0.319000; val_acc: 0.294444\n",
      "(Iteration 31 / 200) loss: 1.579158\n",
      "(Epoch 4 / 20) train acc: 0.611000; val_acc: 0.602778\n",
      "(Iteration 41 / 200) loss: 1.160859\n",
      "(Epoch 5 / 20) train acc: 0.762000; val_acc: 0.752778\n",
      "(Iteration 51 / 200) loss: 0.838581\n",
      "(Epoch 6 / 20) train acc: 0.744000; val_acc: 0.769444\n",
      "(Iteration 61 / 200) loss: 0.714224\n",
      "(Epoch 7 / 20) train acc: 0.810000; val_acc: 0.811111\n",
      "(Iteration 71 / 200) loss: 0.568403\n",
      "(Epoch 8 / 20) train acc: 0.810000; val_acc: 0.816667\n",
      "(Iteration 81 / 200) loss: 0.538410\n",
      "(Epoch 9 / 20) train acc: 0.820000; val_acc: 0.850000\n",
      "(Iteration 91 / 200) loss: 0.455958\n",
      "(Epoch 10 / 20) train acc: 0.860000; val_acc: 0.850000\n",
      "(Iteration 101 / 200) loss: 0.267024\n",
      "(Epoch 11 / 20) train acc: 0.881000; val_acc: 0.863889\n",
      "(Iteration 111 / 200) loss: 0.415211\n",
      "(Epoch 12 / 20) train acc: 0.868000; val_acc: 0.855556\n",
      "(Iteration 121 / 200) loss: 0.347476\n",
      "(Epoch 13 / 20) train acc: 0.906000; val_acc: 0.869444\n",
      "(Iteration 131 / 200) loss: 0.437136\n",
      "(Epoch 14 / 20) train acc: 0.923000; val_acc: 0.861111\n",
      "(Iteration 141 / 200) loss: 0.291463\n",
      "(Epoch 15 / 20) train acc: 0.921000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 0.324724\n",
      "(Epoch 16 / 20) train acc: 0.925000; val_acc: 0.886111\n",
      "(Iteration 161 / 200) loss: 0.220783\n",
      "(Epoch 17 / 20) train acc: 0.916000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.281372\n",
      "(Epoch 18 / 20) train acc: 0.936000; val_acc: 0.900000\n",
      "(Iteration 181 / 200) loss: 0.183614\n",
      "(Epoch 19 / 20) train acc: 0.927000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.249241\n",
      "(Epoch 20 / 20) train acc: 0.944000; val_acc: 0.911111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.282287\n",
      "(Epoch 2 / 20) train acc: 0.191000; val_acc: 0.211111\n",
      "(Iteration 21 / 200) loss: 2.047921\n",
      "(Epoch 3 / 20) train acc: 0.203000; val_acc: 0.216667\n",
      "(Iteration 31 / 200) loss: 1.852848\n",
      "(Epoch 4 / 20) train acc: 0.202000; val_acc: 0.222222\n",
      "(Iteration 41 / 200) loss: 1.957952\n",
      "(Epoch 5 / 20) train acc: 0.181000; val_acc: 0.216667\n",
      "(Iteration 51 / 200) loss: 1.749814\n",
      "(Epoch 6 / 20) train acc: 0.265000; val_acc: 0.275000\n",
      "(Iteration 61 / 200) loss: 1.699312\n",
      "(Epoch 7 / 20) train acc: 0.342000; val_acc: 0.319444\n",
      "(Iteration 71 / 200) loss: 1.624475\n",
      "(Epoch 8 / 20) train acc: 0.450000; val_acc: 0.402778\n",
      "(Iteration 81 / 200) loss: 1.513361\n",
      "(Epoch 9 / 20) train acc: 0.454000; val_acc: 0.422222\n",
      "(Iteration 91 / 200) loss: 1.321374\n",
      "(Epoch 10 / 20) train acc: 0.485000; val_acc: 0.452778\n",
      "(Iteration 101 / 200) loss: 1.189351\n",
      "(Epoch 11 / 20) train acc: 0.531000; val_acc: 0.486111\n",
      "(Iteration 111 / 200) loss: 1.216895\n",
      "(Epoch 12 / 20) train acc: 0.542000; val_acc: 0.502778\n",
      "(Iteration 121 / 200) loss: 1.133538\n",
      "(Epoch 13 / 20) train acc: 0.587000; val_acc: 0.544444\n",
      "(Iteration 131 / 200) loss: 1.209810\n",
      "(Epoch 14 / 20) train acc: 0.603000; val_acc: 0.533333\n",
      "(Iteration 141 / 200) loss: 0.772582\n",
      "(Epoch 15 / 20) train acc: 0.664000; val_acc: 0.636111\n",
      "(Iteration 151 / 200) loss: 0.819391\n",
      "(Epoch 16 / 20) train acc: 0.696000; val_acc: 0.669444\n",
      "(Iteration 161 / 200) loss: 0.749712\n",
      "(Epoch 17 / 20) train acc: 0.688000; val_acc: 0.675000\n",
      "(Iteration 171 / 200) loss: 0.863602\n",
      "(Epoch 18 / 20) train acc: 0.752000; val_acc: 0.725000\n",
      "(Iteration 181 / 200) loss: 0.715725\n",
      "(Epoch 19 / 20) train acc: 0.782000; val_acc: 0.747222\n",
      "(Iteration 191 / 200) loss: 0.532876\n",
      "(Epoch 20 / 20) train acc: 0.791000; val_acc: 0.766667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.295631\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.143253\n",
      "(Epoch 3 / 20) train acc: 0.212000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 2.045154\n",
      "(Epoch 4 / 20) train acc: 0.203000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 2.105948\n",
      "(Epoch 5 / 20) train acc: 0.219000; val_acc: 0.205556\n",
      "(Iteration 51 / 200) loss: 2.011876\n",
      "(Epoch 6 / 20) train acc: 0.223000; val_acc: 0.225000\n",
      "(Iteration 61 / 200) loss: 2.126486\n",
      "(Epoch 7 / 20) train acc: 0.254000; val_acc: 0.283333\n",
      "(Iteration 71 / 200) loss: 1.951627\n",
      "(Epoch 8 / 20) train acc: 0.251000; val_acc: 0.252778\n",
      "(Iteration 81 / 200) loss: 1.974463\n",
      "(Epoch 9 / 20) train acc: 0.214000; val_acc: 0.236111\n",
      "(Iteration 91 / 200) loss: 1.933417\n",
      "(Epoch 10 / 20) train acc: 0.298000; val_acc: 0.311111\n",
      "(Iteration 101 / 200) loss: 1.837077\n",
      "(Epoch 11 / 20) train acc: 0.349000; val_acc: 0.341667\n",
      "(Iteration 111 / 200) loss: 1.700941\n",
      "(Epoch 12 / 20) train acc: 0.453000; val_acc: 0.472222\n",
      "(Iteration 121 / 200) loss: 1.644517\n",
      "(Epoch 13 / 20) train acc: 0.425000; val_acc: 0.458333\n",
      "(Iteration 131 / 200) loss: 1.444450\n",
      "(Epoch 14 / 20) train acc: 0.416000; val_acc: 0.455556\n",
      "(Iteration 141 / 200) loss: 1.526981\n",
      "(Epoch 15 / 20) train acc: 0.519000; val_acc: 0.522222\n",
      "(Iteration 151 / 200) loss: 1.433942\n",
      "(Epoch 16 / 20) train acc: 0.569000; val_acc: 0.605556\n",
      "(Iteration 161 / 200) loss: 1.185184\n",
      "(Epoch 17 / 20) train acc: 0.693000; val_acc: 0.669444\n",
      "(Iteration 171 / 200) loss: 0.967925\n",
      "(Epoch 18 / 20) train acc: 0.707000; val_acc: 0.708333\n",
      "(Iteration 181 / 200) loss: 0.992686\n",
      "(Epoch 19 / 20) train acc: 0.769000; val_acc: 0.758333\n",
      "(Iteration 191 / 200) loss: 0.772541\n",
      "(Epoch 20 / 20) train acc: 0.812000; val_acc: 0.833333\n",
      "(Iteration 1 / 200) loss: 4060.802933\n",
      "(Epoch 0 / 20) train acc: 0.039000; val_acc: 0.063889\n",
      "(Epoch 1 / 20) train acc: 0.052000; val_acc: 0.061111\n",
      "(Iteration 11 / 200) loss: 3601.328803\n",
      "(Epoch 2 / 20) train acc: 0.127000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2127.805644\n",
      "(Epoch 3 / 20) train acc: 0.202000; val_acc: 0.180556\n",
      "(Iteration 31 / 200) loss: 1951.289999\n",
      "(Epoch 4 / 20) train acc: 0.207000; val_acc: 0.191667\n",
      "(Iteration 41 / 200) loss: 1423.238902\n",
      "(Epoch 5 / 20) train acc: 0.249000; val_acc: 0.247222\n",
      "(Iteration 51 / 200) loss: 1289.317575\n",
      "(Epoch 6 / 20) train acc: 0.343000; val_acc: 0.302778\n",
      "(Iteration 61 / 200) loss: 1022.159836\n",
      "(Epoch 7 / 20) train acc: 0.413000; val_acc: 0.355556\n",
      "(Iteration 71 / 200) loss: 1074.371825\n",
      "(Epoch 8 / 20) train acc: 0.429000; val_acc: 0.427778\n",
      "(Iteration 81 / 200) loss: 712.477480\n",
      "(Epoch 9 / 20) train acc: 0.494000; val_acc: 0.483333\n",
      "(Iteration 91 / 200) loss: 721.113948\n",
      "(Epoch 10 / 20) train acc: 0.542000; val_acc: 0.525000\n",
      "(Iteration 101 / 200) loss: 372.302032\n",
      "(Epoch 11 / 20) train acc: 0.577000; val_acc: 0.547222\n",
      "(Iteration 111 / 200) loss: 380.304197\n",
      "(Epoch 12 / 20) train acc: 0.615000; val_acc: 0.580556\n",
      "(Iteration 121 / 200) loss: 544.171902\n",
      "(Epoch 13 / 20) train acc: 0.651000; val_acc: 0.613889\n",
      "(Iteration 131 / 200) loss: 451.735193\n",
      "(Epoch 14 / 20) train acc: 0.642000; val_acc: 0.616667\n",
      "(Iteration 141 / 200) loss: 282.399321\n",
      "(Epoch 15 / 20) train acc: 0.672000; val_acc: 0.638889\n",
      "(Iteration 151 / 200) loss: 297.263735\n",
      "(Epoch 16 / 20) train acc: 0.708000; val_acc: 0.652778\n",
      "(Iteration 161 / 200) loss: 332.638581\n",
      "(Epoch 17 / 20) train acc: 0.712000; val_acc: 0.680556\n",
      "(Iteration 171 / 200) loss: 274.580876\n",
      "(Epoch 18 / 20) train acc: 0.734000; val_acc: 0.700000\n",
      "(Iteration 181 / 200) loss: 205.015688\n",
      "(Epoch 19 / 20) train acc: 0.753000; val_acc: 0.708333\n",
      "(Iteration 191 / 200) loss: 169.772018\n",
      "(Epoch 20 / 20) train acc: 0.760000; val_acc: 0.716667\n",
      "(Iteration 1 / 200) loss: 5.115255\n",
      "(Epoch 0 / 20) train acc: 0.041000; val_acc: 0.050000\n",
      "(Epoch 1 / 20) train acc: 0.371000; val_acc: 0.347222\n",
      "(Iteration 11 / 200) loss: 1.794828\n",
      "(Epoch 2 / 20) train acc: 0.779000; val_acc: 0.758333\n",
      "(Iteration 21 / 200) loss: 0.814895\n",
      "(Epoch 3 / 20) train acc: 0.875000; val_acc: 0.819444\n",
      "(Iteration 31 / 200) loss: 0.459638\n",
      "(Epoch 4 / 20) train acc: 0.918000; val_acc: 0.861111\n",
      "(Iteration 41 / 200) loss: 0.292780\n",
      "(Epoch 5 / 20) train acc: 0.934000; val_acc: 0.911111\n",
      "(Iteration 51 / 200) loss: 0.234455\n",
      "(Epoch 6 / 20) train acc: 0.961000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.130198\n",
      "(Epoch 7 / 20) train acc: 0.952000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.145445\n",
      "(Epoch 8 / 20) train acc: 0.981000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.087545\n",
      "(Epoch 9 / 20) train acc: 0.979000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 0.128384\n",
      "(Epoch 10 / 20) train acc: 0.982000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.063865\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.121914\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.980556\n",
      "(Iteration 121 / 200) loss: 0.049135\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.048311\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.062494\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.983333\n",
      "(Iteration 151 / 200) loss: 0.056418\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.033143\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.074836\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.039084\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.029589\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.302651\n",
      "(Epoch 0 / 20) train acc: 0.208000; val_acc: 0.238889\n",
      "(Epoch 1 / 20) train acc: 0.436000; val_acc: 0.386111\n",
      "(Iteration 11 / 200) loss: 2.206960\n",
      "(Epoch 2 / 20) train acc: 0.578000; val_acc: 0.577778\n",
      "(Iteration 21 / 200) loss: 1.750036\n",
      "(Epoch 3 / 20) train acc: 0.735000; val_acc: 0.722222\n",
      "(Iteration 31 / 200) loss: 1.154984\n",
      "(Epoch 4 / 20) train acc: 0.819000; val_acc: 0.744444\n",
      "(Iteration 41 / 200) loss: 0.714569\n",
      "(Epoch 5 / 20) train acc: 0.834000; val_acc: 0.830556\n",
      "(Iteration 51 / 200) loss: 0.427906\n",
      "(Epoch 6 / 20) train acc: 0.876000; val_acc: 0.880556\n",
      "(Iteration 61 / 200) loss: 0.446030\n",
      "(Epoch 7 / 20) train acc: 0.920000; val_acc: 0.875000\n",
      "(Iteration 71 / 200) loss: 0.274541\n",
      "(Epoch 8 / 20) train acc: 0.903000; val_acc: 0.900000\n",
      "(Iteration 81 / 200) loss: 0.274800\n",
      "(Epoch 9 / 20) train acc: 0.925000; val_acc: 0.922222\n",
      "(Iteration 91 / 200) loss: 0.273886\n",
      "(Epoch 10 / 20) train acc: 0.941000; val_acc: 0.902778\n",
      "(Iteration 101 / 200) loss: 0.135364\n",
      "(Epoch 11 / 20) train acc: 0.941000; val_acc: 0.913889\n",
      "(Iteration 111 / 200) loss: 0.302674\n",
      "(Epoch 12 / 20) train acc: 0.945000; val_acc: 0.927778\n",
      "(Iteration 121 / 200) loss: 0.296568\n",
      "(Epoch 13 / 20) train acc: 0.940000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.205858\n",
      "(Epoch 14 / 20) train acc: 0.954000; val_acc: 0.930556\n",
      "(Iteration 141 / 200) loss: 0.100675\n",
      "(Epoch 15 / 20) train acc: 0.968000; val_acc: 0.938889\n",
      "(Iteration 151 / 200) loss: 0.105852\n",
      "(Epoch 16 / 20) train acc: 0.967000; val_acc: 0.950000\n",
      "(Iteration 161 / 200) loss: 0.136317\n",
      "(Epoch 17 / 20) train acc: 0.959000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.094019\n",
      "(Epoch 18 / 20) train acc: 0.972000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.126453\n",
      "(Epoch 19 / 20) train acc: 0.971000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.118768\n",
      "(Epoch 20 / 20) train acc: 0.953000; val_acc: 0.927778\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.164000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.192000; val_acc: 0.172222\n",
      "(Iteration 11 / 200) loss: 2.274413\n",
      "(Epoch 2 / 20) train acc: 0.219000; val_acc: 0.211111\n",
      "(Iteration 21 / 200) loss: 2.046206\n",
      "(Epoch 3 / 20) train acc: 0.346000; val_acc: 0.327778\n",
      "(Iteration 31 / 200) loss: 1.744799\n",
      "(Epoch 4 / 20) train acc: 0.468000; val_acc: 0.463889\n",
      "(Iteration 41 / 200) loss: 1.431021\n",
      "(Epoch 5 / 20) train acc: 0.557000; val_acc: 0.533333\n",
      "(Iteration 51 / 200) loss: 1.129315\n",
      "(Epoch 6 / 20) train acc: 0.573000; val_acc: 0.569444\n",
      "(Iteration 61 / 200) loss: 1.130644\n",
      "(Epoch 7 / 20) train acc: 0.704000; val_acc: 0.711111\n",
      "(Iteration 71 / 200) loss: 0.955002\n",
      "(Epoch 8 / 20) train acc: 0.748000; val_acc: 0.755556\n",
      "(Iteration 81 / 200) loss: 0.849752\n",
      "(Epoch 9 / 20) train acc: 0.754000; val_acc: 0.780556\n",
      "(Iteration 91 / 200) loss: 0.637966\n",
      "(Epoch 10 / 20) train acc: 0.796000; val_acc: 0.800000\n",
      "(Iteration 101 / 200) loss: 0.555217\n",
      "(Epoch 11 / 20) train acc: 0.844000; val_acc: 0.836111\n",
      "(Iteration 111 / 200) loss: 0.567646\n",
      "(Epoch 12 / 20) train acc: 0.858000; val_acc: 0.836111\n",
      "(Iteration 121 / 200) loss: 0.432752\n",
      "(Epoch 13 / 20) train acc: 0.877000; val_acc: 0.861111\n",
      "(Iteration 131 / 200) loss: 0.312714\n",
      "(Epoch 14 / 20) train acc: 0.904000; val_acc: 0.886111\n",
      "(Iteration 141 / 200) loss: 0.261956\n",
      "(Epoch 15 / 20) train acc: 0.908000; val_acc: 0.886111\n",
      "(Iteration 151 / 200) loss: 0.284748\n",
      "(Epoch 16 / 20) train acc: 0.909000; val_acc: 0.908333\n",
      "(Iteration 161 / 200) loss: 0.266850\n",
      "(Epoch 17 / 20) train acc: 0.941000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.329093\n",
      "(Epoch 18 / 20) train acc: 0.939000; val_acc: 0.913889\n",
      "(Iteration 181 / 200) loss: 0.241347\n",
      "(Epoch 19 / 20) train acc: 0.929000; val_acc: 0.897222\n",
      "(Iteration 191 / 200) loss: 0.141646\n",
      "(Epoch 20 / 20) train acc: 0.935000; val_acc: 0.911111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.121000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.290497\n",
      "(Epoch 2 / 20) train acc: 0.168000; val_acc: 0.158333\n",
      "(Iteration 21 / 200) loss: 2.121261\n",
      "(Epoch 3 / 20) train acc: 0.197000; val_acc: 0.230556\n",
      "(Iteration 31 / 200) loss: 1.991807\n",
      "(Epoch 4 / 20) train acc: 0.241000; val_acc: 0.227778\n",
      "(Iteration 41 / 200) loss: 1.656777\n",
      "(Epoch 5 / 20) train acc: 0.315000; val_acc: 0.319444\n",
      "(Iteration 51 / 200) loss: 1.602636\n",
      "(Epoch 6 / 20) train acc: 0.313000; val_acc: 0.325000\n",
      "(Iteration 61 / 200) loss: 1.470399\n",
      "(Epoch 7 / 20) train acc: 0.400000; val_acc: 0.366667\n",
      "(Iteration 71 / 200) loss: 1.536666\n",
      "(Epoch 8 / 20) train acc: 0.416000; val_acc: 0.427778\n",
      "(Iteration 81 / 200) loss: 1.419759\n",
      "(Epoch 9 / 20) train acc: 0.434000; val_acc: 0.377778\n",
      "(Iteration 91 / 200) loss: 1.366510\n",
      "(Epoch 10 / 20) train acc: 0.456000; val_acc: 0.458333\n",
      "(Iteration 101 / 200) loss: 1.306393\n",
      "(Epoch 11 / 20) train acc: 0.532000; val_acc: 0.561111\n",
      "(Iteration 111 / 200) loss: 1.438988\n",
      "(Epoch 12 / 20) train acc: 0.587000; val_acc: 0.616667\n",
      "(Iteration 121 / 200) loss: 0.956967\n",
      "(Epoch 13 / 20) train acc: 0.668000; val_acc: 0.650000\n",
      "(Iteration 131 / 200) loss: 0.914082\n",
      "(Epoch 14 / 20) train acc: 0.653000; val_acc: 0.658333\n",
      "(Iteration 141 / 200) loss: 0.814710\n",
      "(Epoch 15 / 20) train acc: 0.708000; val_acc: 0.719444\n",
      "(Iteration 151 / 200) loss: 0.831200\n",
      "(Epoch 16 / 20) train acc: 0.695000; val_acc: 0.733333\n",
      "(Iteration 161 / 200) loss: 0.582381\n",
      "(Epoch 17 / 20) train acc: 0.739000; val_acc: 0.738889\n",
      "(Iteration 171 / 200) loss: 0.718555\n",
      "(Epoch 18 / 20) train acc: 0.748000; val_acc: 0.725000\n",
      "(Iteration 181 / 200) loss: 0.697276\n",
      "(Epoch 19 / 20) train acc: 0.747000; val_acc: 0.741667\n",
      "(Iteration 191 / 200) loss: 0.698365\n",
      "(Epoch 20 / 20) train acc: 0.771000; val_acc: 0.736111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.297717\n",
      "(Epoch 2 / 20) train acc: 0.202000; val_acc: 0.152778\n",
      "(Iteration 21 / 200) loss: 2.100366\n",
      "(Epoch 3 / 20) train acc: 0.176000; val_acc: 0.186111\n",
      "(Iteration 31 / 200) loss: 2.171557\n",
      "(Epoch 4 / 20) train acc: 0.201000; val_acc: 0.222222\n",
      "(Iteration 41 / 200) loss: 1.889681\n",
      "(Epoch 5 / 20) train acc: 0.217000; val_acc: 0.205556\n",
      "(Iteration 51 / 200) loss: 1.826496\n",
      "(Epoch 6 / 20) train acc: 0.309000; val_acc: 0.344444\n",
      "(Iteration 61 / 200) loss: 1.589017\n",
      "(Epoch 7 / 20) train acc: 0.485000; val_acc: 0.502778\n",
      "(Iteration 71 / 200) loss: 1.405889\n",
      "(Epoch 8 / 20) train acc: 0.604000; val_acc: 0.577778\n",
      "(Iteration 81 / 200) loss: 1.191658\n",
      "(Epoch 9 / 20) train acc: 0.647000; val_acc: 0.630556\n",
      "(Iteration 91 / 200) loss: 1.063783\n",
      "(Epoch 10 / 20) train acc: 0.657000; val_acc: 0.680556\n",
      "(Iteration 101 / 200) loss: 0.928370\n",
      "(Epoch 11 / 20) train acc: 0.697000; val_acc: 0.680556\n",
      "(Iteration 111 / 200) loss: 0.823540\n",
      "(Epoch 12 / 20) train acc: 0.704000; val_acc: 0.713889\n",
      "(Iteration 121 / 200) loss: 0.667108\n",
      "(Epoch 13 / 20) train acc: 0.711000; val_acc: 0.688889\n",
      "(Iteration 131 / 200) loss: 0.595851\n",
      "(Epoch 14 / 20) train acc: 0.730000; val_acc: 0.725000\n",
      "(Iteration 141 / 200) loss: 0.646001\n",
      "(Epoch 15 / 20) train acc: 0.752000; val_acc: 0.722222\n",
      "(Iteration 151 / 200) loss: 0.628163\n",
      "(Epoch 16 / 20) train acc: 0.798000; val_acc: 0.763889\n",
      "(Iteration 161 / 200) loss: 0.662306\n",
      "(Epoch 17 / 20) train acc: 0.814000; val_acc: 0.755556\n",
      "(Iteration 171 / 200) loss: 0.485275\n",
      "(Epoch 18 / 20) train acc: 0.849000; val_acc: 0.805556\n",
      "(Iteration 181 / 200) loss: 0.576265\n",
      "(Epoch 19 / 20) train acc: 0.842000; val_acc: 0.830556\n",
      "(Iteration 191 / 200) loss: 0.394400\n",
      "(Epoch 20 / 20) train acc: 0.847000; val_acc: 0.833333\n",
      "(Iteration 1 / 200) loss: 4627.910307\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.079000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 3938.432497\n",
      "(Epoch 2 / 20) train acc: 0.077000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 3105.653488\n",
      "(Epoch 3 / 20) train acc: 0.086000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 2934.211990\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.138889\n",
      "(Iteration 41 / 200) loss: 2408.294095\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.175000\n",
      "(Iteration 51 / 200) loss: 1807.703710\n",
      "(Epoch 6 / 20) train acc: 0.150000; val_acc: 0.222222\n",
      "(Iteration 61 / 200) loss: 1391.067555\n",
      "(Epoch 7 / 20) train acc: 0.199000; val_acc: 0.283333\n",
      "(Iteration 71 / 200) loss: 1299.936332\n",
      "(Epoch 8 / 20) train acc: 0.277000; val_acc: 0.325000\n",
      "(Iteration 81 / 200) loss: 1035.627539\n",
      "(Epoch 9 / 20) train acc: 0.369000; val_acc: 0.386111\n",
      "(Iteration 91 / 200) loss: 754.050315\n",
      "(Epoch 10 / 20) train acc: 0.396000; val_acc: 0.422222\n",
      "(Iteration 101 / 200) loss: 641.953018\n",
      "(Epoch 11 / 20) train acc: 0.491000; val_acc: 0.475000\n",
      "(Iteration 111 / 200) loss: 564.550764\n",
      "(Epoch 12 / 20) train acc: 0.483000; val_acc: 0.525000\n",
      "(Iteration 121 / 200) loss: 487.613130\n",
      "(Epoch 13 / 20) train acc: 0.538000; val_acc: 0.552778\n",
      "(Iteration 131 / 200) loss: 507.057806\n",
      "(Epoch 14 / 20) train acc: 0.585000; val_acc: 0.583333\n",
      "(Iteration 141 / 200) loss: 379.828579\n",
      "(Epoch 15 / 20) train acc: 0.638000; val_acc: 0.613889\n",
      "(Iteration 151 / 200) loss: 335.101308\n",
      "(Epoch 16 / 20) train acc: 0.661000; val_acc: 0.636111\n",
      "(Iteration 161 / 200) loss: 342.814196\n",
      "(Epoch 17 / 20) train acc: 0.666000; val_acc: 0.661111\n",
      "(Iteration 171 / 200) loss: 196.291988\n",
      "(Epoch 18 / 20) train acc: 0.710000; val_acc: 0.658333\n",
      "(Iteration 181 / 200) loss: 162.183406\n",
      "(Epoch 19 / 20) train acc: 0.726000; val_acc: 0.666667\n",
      "(Iteration 191 / 200) loss: 193.716002\n",
      "(Epoch 20 / 20) train acc: 0.739000; val_acc: 0.691667\n",
      "(Iteration 1 / 200) loss: 5.215967\n",
      "(Epoch 0 / 20) train acc: 0.211000; val_acc: 0.222222\n",
      "(Epoch 1 / 20) train acc: 0.431000; val_acc: 0.447222\n",
      "(Iteration 11 / 200) loss: 1.635202\n",
      "(Epoch 2 / 20) train acc: 0.790000; val_acc: 0.766667\n",
      "(Iteration 21 / 200) loss: 0.795455\n",
      "(Epoch 3 / 20) train acc: 0.862000; val_acc: 0.875000\n",
      "(Iteration 31 / 200) loss: 0.425275\n",
      "(Epoch 4 / 20) train acc: 0.916000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 0.240173\n",
      "(Epoch 5 / 20) train acc: 0.950000; val_acc: 0.927778\n",
      "(Iteration 51 / 200) loss: 0.199330\n",
      "(Epoch 6 / 20) train acc: 0.951000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.231608\n",
      "(Epoch 7 / 20) train acc: 0.957000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.187756\n",
      "(Epoch 8 / 20) train acc: 0.978000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.071893\n",
      "(Epoch 9 / 20) train acc: 0.973000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.104512\n",
      "(Epoch 10 / 20) train acc: 0.976000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.074405\n",
      "(Epoch 11 / 20) train acc: 0.978000; val_acc: 0.977778\n",
      "(Iteration 111 / 200) loss: 0.054410\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.983333\n",
      "(Iteration 121 / 200) loss: 0.071660\n",
      "(Epoch 13 / 20) train acc: 0.980000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.079440\n",
      "(Epoch 14 / 20) train acc: 0.986000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.058329\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.054225\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.986111\n",
      "(Iteration 161 / 200) loss: 0.031680\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 171 / 200) loss: 0.047214\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.039336\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.034645\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.991667\n",
      "(Iteration 1 / 200) loss: 2.302369\n",
      "(Epoch 0 / 20) train acc: 0.179000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.353000; val_acc: 0.283333\n",
      "(Iteration 11 / 200) loss: 2.198651\n",
      "(Epoch 2 / 20) train acc: 0.471000; val_acc: 0.430556\n",
      "(Iteration 21 / 200) loss: 1.797697\n",
      "(Epoch 3 / 20) train acc: 0.718000; val_acc: 0.725000\n",
      "(Iteration 31 / 200) loss: 1.136358\n",
      "(Epoch 4 / 20) train acc: 0.798000; val_acc: 0.783333\n",
      "(Iteration 41 / 200) loss: 0.875270\n",
      "(Epoch 5 / 20) train acc: 0.839000; val_acc: 0.816667\n",
      "(Iteration 51 / 200) loss: 0.463141\n",
      "(Epoch 6 / 20) train acc: 0.868000; val_acc: 0.869444\n",
      "(Iteration 61 / 200) loss: 0.543342\n",
      "(Epoch 7 / 20) train acc: 0.909000; val_acc: 0.886111\n",
      "(Iteration 71 / 200) loss: 0.354099\n",
      "(Epoch 8 / 20) train acc: 0.911000; val_acc: 0.894444\n",
      "(Iteration 81 / 200) loss: 0.333819\n",
      "(Epoch 9 / 20) train acc: 0.928000; val_acc: 0.908333\n",
      "(Iteration 91 / 200) loss: 0.222751\n",
      "(Epoch 10 / 20) train acc: 0.953000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.205611\n",
      "(Epoch 11 / 20) train acc: 0.936000; val_acc: 0.897222\n",
      "(Iteration 111 / 200) loss: 0.252216\n",
      "(Epoch 12 / 20) train acc: 0.940000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 0.135086\n",
      "(Epoch 13 / 20) train acc: 0.950000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 0.203819\n",
      "(Epoch 14 / 20) train acc: 0.958000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.193460\n",
      "(Epoch 15 / 20) train acc: 0.960000; val_acc: 0.944444\n",
      "(Iteration 151 / 200) loss: 0.172847\n",
      "(Epoch 16 / 20) train acc: 0.976000; val_acc: 0.952778\n",
      "(Iteration 161 / 200) loss: 0.137406\n",
      "(Epoch 17 / 20) train acc: 0.965000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.106877\n",
      "(Epoch 18 / 20) train acc: 0.963000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.095846\n",
      "(Epoch 19 / 20) train acc: 0.982000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.111869\n",
      "(Epoch 20 / 20) train acc: 0.963000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.119000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 2.284442\n",
      "(Epoch 2 / 20) train acc: 0.147000; val_acc: 0.155556\n",
      "(Iteration 21 / 200) loss: 2.131618\n",
      "(Epoch 3 / 20) train acc: 0.260000; val_acc: 0.241667\n",
      "(Iteration 31 / 200) loss: 1.953122\n",
      "(Epoch 4 / 20) train acc: 0.325000; val_acc: 0.283333\n",
      "(Iteration 41 / 200) loss: 1.790745\n",
      "(Epoch 5 / 20) train acc: 0.402000; val_acc: 0.377778\n",
      "(Iteration 51 / 200) loss: 1.543625\n",
      "(Epoch 6 / 20) train acc: 0.563000; val_acc: 0.522222\n",
      "(Iteration 61 / 200) loss: 1.352730\n",
      "(Epoch 7 / 20) train acc: 0.626000; val_acc: 0.605556\n",
      "(Iteration 71 / 200) loss: 1.072510\n",
      "(Epoch 8 / 20) train acc: 0.666000; val_acc: 0.630556\n",
      "(Iteration 81 / 200) loss: 1.041950\n",
      "(Epoch 9 / 20) train acc: 0.693000; val_acc: 0.630556\n",
      "(Iteration 91 / 200) loss: 0.898783\n",
      "(Epoch 10 / 20) train acc: 0.750000; val_acc: 0.705556\n",
      "(Iteration 101 / 200) loss: 0.742518\n",
      "(Epoch 11 / 20) train acc: 0.752000; val_acc: 0.716667\n",
      "(Iteration 111 / 200) loss: 0.566541\n",
      "(Epoch 12 / 20) train acc: 0.813000; val_acc: 0.811111\n",
      "(Iteration 121 / 200) loss: 0.551802\n",
      "(Epoch 13 / 20) train acc: 0.865000; val_acc: 0.825000\n",
      "(Iteration 131 / 200) loss: 0.489205\n",
      "(Epoch 14 / 20) train acc: 0.858000; val_acc: 0.861111\n",
      "(Iteration 141 / 200) loss: 0.510038\n",
      "(Epoch 15 / 20) train acc: 0.868000; val_acc: 0.852778\n",
      "(Iteration 151 / 200) loss: 0.483868\n",
      "(Epoch 16 / 20) train acc: 0.884000; val_acc: 0.863889\n",
      "(Iteration 161 / 200) loss: 0.292993\n",
      "(Epoch 17 / 20) train acc: 0.881000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.362437\n",
      "(Epoch 18 / 20) train acc: 0.892000; val_acc: 0.877778\n",
      "(Iteration 181 / 200) loss: 0.323910\n",
      "(Epoch 19 / 20) train acc: 0.895000; val_acc: 0.880556\n",
      "(Iteration 191 / 200) loss: 0.444884\n",
      "(Epoch 20 / 20) train acc: 0.928000; val_acc: 0.891667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.287324\n",
      "(Epoch 2 / 20) train acc: 0.210000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 2.164700\n",
      "(Epoch 3 / 20) train acc: 0.192000; val_acc: 0.208333\n",
      "(Iteration 31 / 200) loss: 1.974266\n",
      "(Epoch 4 / 20) train acc: 0.239000; val_acc: 0.233333\n",
      "(Iteration 41 / 200) loss: 1.706215\n",
      "(Epoch 5 / 20) train acc: 0.288000; val_acc: 0.300000\n",
      "(Iteration 51 / 200) loss: 1.593225\n",
      "(Epoch 6 / 20) train acc: 0.452000; val_acc: 0.405556\n",
      "(Iteration 61 / 200) loss: 1.576296\n",
      "(Epoch 7 / 20) train acc: 0.401000; val_acc: 0.416667\n",
      "(Iteration 71 / 200) loss: 1.455606\n",
      "(Epoch 8 / 20) train acc: 0.531000; val_acc: 0.488889\n",
      "(Iteration 81 / 200) loss: 1.297712\n",
      "(Epoch 9 / 20) train acc: 0.558000; val_acc: 0.536111\n",
      "(Iteration 91 / 200) loss: 1.067416\n",
      "(Epoch 10 / 20) train acc: 0.590000; val_acc: 0.583333\n",
      "(Iteration 101 / 200) loss: 1.147733\n",
      "(Epoch 11 / 20) train acc: 0.678000; val_acc: 0.652778\n",
      "(Iteration 111 / 200) loss: 1.019698\n",
      "(Epoch 12 / 20) train acc: 0.690000; val_acc: 0.705556\n",
      "(Iteration 121 / 200) loss: 0.870334\n",
      "(Epoch 13 / 20) train acc: 0.765000; val_acc: 0.744444\n",
      "(Iteration 131 / 200) loss: 0.599828\n",
      "(Epoch 14 / 20) train acc: 0.819000; val_acc: 0.777778\n",
      "(Iteration 141 / 200) loss: 0.597487\n",
      "(Epoch 15 / 20) train acc: 0.820000; val_acc: 0.808333\n",
      "(Iteration 151 / 200) loss: 0.532473\n",
      "(Epoch 16 / 20) train acc: 0.866000; val_acc: 0.813889\n",
      "(Iteration 161 / 200) loss: 0.538158\n",
      "(Epoch 17 / 20) train acc: 0.854000; val_acc: 0.830556\n",
      "(Iteration 171 / 200) loss: 0.470260\n",
      "(Epoch 18 / 20) train acc: 0.848000; val_acc: 0.838889\n",
      "(Iteration 181 / 200) loss: 0.511927\n",
      "(Epoch 19 / 20) train acc: 0.883000; val_acc: 0.847222\n",
      "(Iteration 191 / 200) loss: 0.385659\n",
      "(Epoch 20 / 20) train acc: 0.873000; val_acc: 0.841667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.292196\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.196600\n",
      "(Epoch 3 / 20) train acc: 0.201000; val_acc: 0.186111\n",
      "(Iteration 31 / 200) loss: 2.098147\n",
      "(Epoch 4 / 20) train acc: 0.204000; val_acc: 0.194444\n",
      "(Iteration 41 / 200) loss: 2.009549\n",
      "(Epoch 5 / 20) train acc: 0.185000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 2.057522\n",
      "(Epoch 6 / 20) train acc: 0.187000; val_acc: 0.194444\n",
      "(Iteration 61 / 200) loss: 1.970335\n",
      "(Epoch 7 / 20) train acc: 0.218000; val_acc: 0.205556\n",
      "(Iteration 71 / 200) loss: 1.927052\n",
      "(Epoch 8 / 20) train acc: 0.224000; val_acc: 0.213889\n",
      "(Iteration 81 / 200) loss: 1.787663\n",
      "(Epoch 9 / 20) train acc: 0.285000; val_acc: 0.244444\n",
      "(Iteration 91 / 200) loss: 1.714605\n",
      "(Epoch 10 / 20) train acc: 0.317000; val_acc: 0.325000\n",
      "(Iteration 101 / 200) loss: 1.786342\n",
      "(Epoch 11 / 20) train acc: 0.297000; val_acc: 0.269444\n",
      "(Iteration 111 / 200) loss: 1.600356\n",
      "(Epoch 12 / 20) train acc: 0.288000; val_acc: 0.250000\n",
      "(Iteration 121 / 200) loss: 1.533758\n",
      "(Epoch 13 / 20) train acc: 0.315000; val_acc: 0.341667\n",
      "(Iteration 131 / 200) loss: 1.664007\n",
      "(Epoch 14 / 20) train acc: 0.283000; val_acc: 0.266667\n",
      "(Iteration 141 / 200) loss: 1.608061\n",
      "(Epoch 15 / 20) train acc: 0.322000; val_acc: 0.291667\n",
      "(Iteration 151 / 200) loss: 1.554658\n",
      "(Epoch 16 / 20) train acc: 0.344000; val_acc: 0.344444\n",
      "(Iteration 161 / 200) loss: 1.448836\n",
      "(Epoch 17 / 20) train acc: 0.388000; val_acc: 0.369444\n",
      "(Iteration 171 / 200) loss: 1.425716\n",
      "(Epoch 18 / 20) train acc: 0.424000; val_acc: 0.400000\n",
      "(Iteration 181 / 200) loss: 1.414539\n",
      "(Epoch 19 / 20) train acc: 0.469000; val_acc: 0.433333\n",
      "(Iteration 191 / 200) loss: 1.405672\n",
      "(Epoch 20 / 20) train acc: 0.460000; val_acc: 0.538889\n",
      "(Iteration 1 / 200) loss: 2090.552598\n",
      "(Epoch 0 / 20) train acc: 0.149000; val_acc: 0.161111\n",
      "(Epoch 1 / 20) train acc: 0.215000; val_acc: 0.194444\n",
      "(Iteration 11 / 200) loss: 1567.908629\n",
      "(Epoch 2 / 20) train acc: 0.260000; val_acc: 0.225000\n",
      "(Iteration 21 / 200) loss: 1297.984710\n",
      "(Epoch 3 / 20) train acc: 0.283000; val_acc: 0.255556\n",
      "(Iteration 31 / 200) loss: 1120.549321\n",
      "(Epoch 4 / 20) train acc: 0.306000; val_acc: 0.288889\n",
      "(Iteration 41 / 200) loss: 1073.531365\n",
      "(Epoch 5 / 20) train acc: 0.361000; val_acc: 0.333333\n",
      "(Iteration 51 / 200) loss: 742.342087\n",
      "(Epoch 6 / 20) train acc: 0.397000; val_acc: 0.347222\n",
      "(Iteration 61 / 200) loss: 421.596915\n",
      "(Epoch 7 / 20) train acc: 0.431000; val_acc: 0.380556\n",
      "(Iteration 71 / 200) loss: 524.380653\n",
      "(Epoch 8 / 20) train acc: 0.458000; val_acc: 0.405556\n",
      "(Iteration 81 / 200) loss: 391.804944\n",
      "(Epoch 9 / 20) train acc: 0.486000; val_acc: 0.455556\n",
      "(Iteration 91 / 200) loss: 345.984512\n",
      "(Epoch 10 / 20) train acc: 0.534000; val_acc: 0.488889\n",
      "(Iteration 101 / 200) loss: 278.410566\n",
      "(Epoch 11 / 20) train acc: 0.567000; val_acc: 0.525000\n",
      "(Iteration 111 / 200) loss: 264.595959\n",
      "(Epoch 12 / 20) train acc: 0.670000; val_acc: 0.561111\n",
      "(Iteration 121 / 200) loss: 280.869442\n",
      "(Epoch 13 / 20) train acc: 0.675000; val_acc: 0.594444\n",
      "(Iteration 131 / 200) loss: 235.020074\n",
      "(Epoch 14 / 20) train acc: 0.723000; val_acc: 0.625000\n",
      "(Iteration 141 / 200) loss: 215.309243\n",
      "(Epoch 15 / 20) train acc: 0.742000; val_acc: 0.638889\n",
      "(Iteration 151 / 200) loss: 99.264890\n",
      "(Epoch 16 / 20) train acc: 0.780000; val_acc: 0.658333\n",
      "(Iteration 161 / 200) loss: 168.043742\n",
      "(Epoch 17 / 20) train acc: 0.758000; val_acc: 0.677778\n",
      "(Iteration 171 / 200) loss: 131.092959\n",
      "(Epoch 18 / 20) train acc: 0.781000; val_acc: 0.694444\n",
      "(Iteration 181 / 200) loss: 104.485127\n",
      "(Epoch 19 / 20) train acc: 0.809000; val_acc: 0.694444\n",
      "(Iteration 191 / 200) loss: 126.240012\n",
      "(Epoch 20 / 20) train acc: 0.812000; val_acc: 0.705556\n",
      "(Iteration 1 / 200) loss: 4.726644\n",
      "(Epoch 0 / 20) train acc: 0.123000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.494000; val_acc: 0.472222\n",
      "(Iteration 11 / 200) loss: 1.417472\n",
      "(Epoch 2 / 20) train acc: 0.818000; val_acc: 0.763889\n",
      "(Iteration 21 / 200) loss: 0.732073\n",
      "(Epoch 3 / 20) train acc: 0.919000; val_acc: 0.877778\n",
      "(Iteration 31 / 200) loss: 0.362816\n",
      "(Epoch 4 / 20) train acc: 0.941000; val_acc: 0.891667\n",
      "(Iteration 41 / 200) loss: 0.237501\n",
      "(Epoch 5 / 20) train acc: 0.968000; val_acc: 0.911111\n",
      "(Iteration 51 / 200) loss: 0.239991\n",
      "(Epoch 6 / 20) train acc: 0.978000; val_acc: 0.927778\n",
      "(Iteration 61 / 200) loss: 0.130531\n",
      "(Epoch 7 / 20) train acc: 0.986000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.125159\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 81 / 200) loss: 0.100688\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.085931\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.062560\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.127098\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.041949\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.051192\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.019758\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.039020\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.038963\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.017858\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.027263\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.019209\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302813\n",
      "(Epoch 0 / 20) train acc: 0.225000; val_acc: 0.233333\n",
      "(Epoch 1 / 20) train acc: 0.432000; val_acc: 0.408333\n",
      "(Iteration 11 / 200) loss: 2.205746\n",
      "(Epoch 2 / 20) train acc: 0.655000; val_acc: 0.655556\n",
      "(Iteration 21 / 200) loss: 1.701578\n",
      "(Epoch 3 / 20) train acc: 0.658000; val_acc: 0.694444\n",
      "(Iteration 31 / 200) loss: 1.243464\n",
      "(Epoch 4 / 20) train acc: 0.792000; val_acc: 0.777778\n",
      "(Iteration 41 / 200) loss: 0.815810\n",
      "(Epoch 5 / 20) train acc: 0.821000; val_acc: 0.822222\n",
      "(Iteration 51 / 200) loss: 0.378688\n",
      "(Epoch 6 / 20) train acc: 0.862000; val_acc: 0.833333\n",
      "(Iteration 61 / 200) loss: 0.445378\n",
      "(Epoch 7 / 20) train acc: 0.889000; val_acc: 0.852778\n",
      "(Iteration 71 / 200) loss: 0.260056\n",
      "(Epoch 8 / 20) train acc: 0.893000; val_acc: 0.897222\n",
      "(Iteration 81 / 200) loss: 0.360770\n",
      "(Epoch 9 / 20) train acc: 0.913000; val_acc: 0.905556\n",
      "(Iteration 91 / 200) loss: 0.336721\n",
      "(Epoch 10 / 20) train acc: 0.904000; val_acc: 0.900000\n",
      "(Iteration 101 / 200) loss: 0.316027\n",
      "(Epoch 11 / 20) train acc: 0.932000; val_acc: 0.905556\n",
      "(Iteration 111 / 200) loss: 0.218534\n",
      "(Epoch 12 / 20) train acc: 0.954000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 0.173161\n",
      "(Epoch 13 / 20) train acc: 0.948000; val_acc: 0.922222\n",
      "(Iteration 131 / 200) loss: 0.205074\n",
      "(Epoch 14 / 20) train acc: 0.936000; val_acc: 0.913889\n",
      "(Iteration 141 / 200) loss: 0.215090\n",
      "(Epoch 15 / 20) train acc: 0.958000; val_acc: 0.936111\n",
      "(Iteration 151 / 200) loss: 0.170798\n",
      "(Epoch 16 / 20) train acc: 0.955000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.124729\n",
      "(Epoch 17 / 20) train acc: 0.947000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.183823\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.114136\n",
      "(Epoch 19 / 20) train acc: 0.960000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.159438\n",
      "(Epoch 20 / 20) train acc: 0.964000; val_acc: 0.927778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.230000; val_acc: 0.202778\n",
      "(Iteration 11 / 200) loss: 2.269251\n",
      "(Epoch 2 / 20) train acc: 0.195000; val_acc: 0.222222\n",
      "(Iteration 21 / 200) loss: 2.117939\n",
      "(Epoch 3 / 20) train acc: 0.269000; val_acc: 0.275000\n",
      "(Iteration 31 / 200) loss: 1.768651\n",
      "(Epoch 4 / 20) train acc: 0.565000; val_acc: 0.575000\n",
      "(Iteration 41 / 200) loss: 1.258975\n",
      "(Epoch 5 / 20) train acc: 0.698000; val_acc: 0.713889\n",
      "(Iteration 51 / 200) loss: 0.884280\n",
      "(Epoch 6 / 20) train acc: 0.743000; val_acc: 0.752778\n",
      "(Iteration 61 / 200) loss: 0.685158\n",
      "(Epoch 7 / 20) train acc: 0.816000; val_acc: 0.805556\n",
      "(Iteration 71 / 200) loss: 0.647025\n",
      "(Epoch 8 / 20) train acc: 0.815000; val_acc: 0.791667\n",
      "(Iteration 81 / 200) loss: 0.620661\n",
      "(Epoch 9 / 20) train acc: 0.811000; val_acc: 0.822222\n",
      "(Iteration 91 / 200) loss: 0.463220\n",
      "(Epoch 10 / 20) train acc: 0.838000; val_acc: 0.861111\n",
      "(Iteration 101 / 200) loss: 0.452568\n",
      "(Epoch 11 / 20) train acc: 0.834000; val_acc: 0.838889\n",
      "(Iteration 111 / 200) loss: 0.604503\n",
      "(Epoch 12 / 20) train acc: 0.864000; val_acc: 0.833333\n",
      "(Iteration 121 / 200) loss: 0.396719\n",
      "(Epoch 13 / 20) train acc: 0.881000; val_acc: 0.877778\n",
      "(Iteration 131 / 200) loss: 0.388166\n",
      "(Epoch 14 / 20) train acc: 0.851000; val_acc: 0.872222\n",
      "(Iteration 141 / 200) loss: 0.427531\n",
      "(Epoch 15 / 20) train acc: 0.861000; val_acc: 0.883333\n",
      "(Iteration 151 / 200) loss: 0.356742\n",
      "(Epoch 16 / 20) train acc: 0.888000; val_acc: 0.872222\n",
      "(Iteration 161 / 200) loss: 0.340927\n",
      "(Epoch 17 / 20) train acc: 0.891000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.300844\n",
      "(Epoch 18 / 20) train acc: 0.892000; val_acc: 0.886111\n",
      "(Iteration 181 / 200) loss: 0.235151\n",
      "(Epoch 19 / 20) train acc: 0.888000; val_acc: 0.913889\n",
      "(Iteration 191 / 200) loss: 0.242430\n",
      "(Epoch 20 / 20) train acc: 0.892000; val_acc: 0.888889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.293255\n",
      "(Epoch 2 / 20) train acc: 0.196000; val_acc: 0.202778\n",
      "(Iteration 21 / 200) loss: 2.202658\n",
      "(Epoch 3 / 20) train acc: 0.201000; val_acc: 0.230556\n",
      "(Iteration 31 / 200) loss: 2.074465\n",
      "(Epoch 4 / 20) train acc: 0.267000; val_acc: 0.288889\n",
      "(Iteration 41 / 200) loss: 1.884995\n",
      "(Epoch 5 / 20) train acc: 0.411000; val_acc: 0.397222\n",
      "(Iteration 51 / 200) loss: 1.722838\n",
      "(Epoch 6 / 20) train acc: 0.458000; val_acc: 0.505556\n",
      "(Iteration 61 / 200) loss: 1.436489\n",
      "(Epoch 7 / 20) train acc: 0.589000; val_acc: 0.569444\n",
      "(Iteration 71 / 200) loss: 1.195410\n",
      "(Epoch 8 / 20) train acc: 0.654000; val_acc: 0.641667\n",
      "(Iteration 81 / 200) loss: 0.910296\n",
      "(Epoch 9 / 20) train acc: 0.637000; val_acc: 0.597222\n",
      "(Iteration 91 / 200) loss: 0.922244\n",
      "(Epoch 10 / 20) train acc: 0.679000; val_acc: 0.677778\n",
      "(Iteration 101 / 200) loss: 0.911267\n",
      "(Epoch 11 / 20) train acc: 0.728000; val_acc: 0.755556\n",
      "(Iteration 111 / 200) loss: 0.727258\n",
      "(Epoch 12 / 20) train acc: 0.762000; val_acc: 0.750000\n",
      "(Iteration 121 / 200) loss: 0.736841\n",
      "(Epoch 13 / 20) train acc: 0.829000; val_acc: 0.752778\n",
      "(Iteration 131 / 200) loss: 0.634498\n",
      "(Epoch 14 / 20) train acc: 0.807000; val_acc: 0.788889\n",
      "(Iteration 141 / 200) loss: 0.622281\n",
      "(Epoch 15 / 20) train acc: 0.820000; val_acc: 0.794444\n",
      "(Iteration 151 / 200) loss: 0.507600\n",
      "(Epoch 16 / 20) train acc: 0.823000; val_acc: 0.794444\n",
      "(Iteration 161 / 200) loss: 0.550720\n",
      "(Epoch 17 / 20) train acc: 0.842000; val_acc: 0.788889\n",
      "(Iteration 171 / 200) loss: 0.440476\n",
      "(Epoch 18 / 20) train acc: 0.875000; val_acc: 0.825000\n",
      "(Iteration 181 / 200) loss: 0.459575\n",
      "(Epoch 19 / 20) train acc: 0.864000; val_acc: 0.852778\n",
      "(Iteration 191 / 200) loss: 0.370246\n",
      "(Epoch 20 / 20) train acc: 0.868000; val_acc: 0.836111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.297812\n",
      "(Epoch 2 / 20) train acc: 0.156000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 2.222744\n",
      "(Epoch 3 / 20) train acc: 0.212000; val_acc: 0.208333\n",
      "(Iteration 31 / 200) loss: 2.064192\n",
      "(Epoch 4 / 20) train acc: 0.190000; val_acc: 0.208333\n",
      "(Iteration 41 / 200) loss: 1.962318\n",
      "(Epoch 5 / 20) train acc: 0.239000; val_acc: 0.233333\n",
      "(Iteration 51 / 200) loss: 1.762879\n",
      "(Epoch 6 / 20) train acc: 0.228000; val_acc: 0.213889\n",
      "(Iteration 61 / 200) loss: 1.790020\n",
      "(Epoch 7 / 20) train acc: 0.309000; val_acc: 0.305556\n",
      "(Iteration 71 / 200) loss: 1.713992\n",
      "(Epoch 8 / 20) train acc: 0.284000; val_acc: 0.283333\n",
      "(Iteration 81 / 200) loss: 1.562152\n",
      "(Epoch 9 / 20) train acc: 0.325000; val_acc: 0.305556\n",
      "(Iteration 91 / 200) loss: 1.588554\n",
      "(Epoch 10 / 20) train acc: 0.339000; val_acc: 0.322222\n",
      "(Iteration 101 / 200) loss: 1.562861\n",
      "(Epoch 11 / 20) train acc: 0.332000; val_acc: 0.325000\n",
      "(Iteration 111 / 200) loss: 1.481263\n",
      "(Epoch 12 / 20) train acc: 0.319000; val_acc: 0.325000\n",
      "(Iteration 121 / 200) loss: 1.446462\n",
      "(Epoch 13 / 20) train acc: 0.343000; val_acc: 0.341667\n",
      "(Iteration 131 / 200) loss: 1.494181\n",
      "(Epoch 14 / 20) train acc: 0.356000; val_acc: 0.327778\n",
      "(Iteration 141 / 200) loss: 1.608426\n",
      "(Epoch 15 / 20) train acc: 0.376000; val_acc: 0.363889\n",
      "(Iteration 151 / 200) loss: 1.461892\n",
      "(Epoch 16 / 20) train acc: 0.358000; val_acc: 0.358333\n",
      "(Iteration 161 / 200) loss: 1.296742\n",
      "(Epoch 17 / 20) train acc: 0.342000; val_acc: 0.330556\n",
      "(Iteration 171 / 200) loss: 1.506591\n",
      "(Epoch 18 / 20) train acc: 0.404000; val_acc: 0.366667\n",
      "(Iteration 181 / 200) loss: 1.259043\n",
      "(Epoch 19 / 20) train acc: 0.397000; val_acc: 0.386111\n",
      "(Iteration 191 / 200) loss: 1.383908\n",
      "(Epoch 20 / 20) train acc: 0.440000; val_acc: 0.386111\n",
      "(Iteration 1 / 200) loss: 5318.999583\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.108333\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 4491.922783\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 3310.399480\n",
      "(Epoch 3 / 20) train acc: 0.123000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 3016.497444\n",
      "(Epoch 4 / 20) train acc: 0.140000; val_acc: 0.136111\n",
      "(Iteration 41 / 200) loss: 2215.543075\n",
      "(Epoch 5 / 20) train acc: 0.154000; val_acc: 0.136111\n",
      "(Iteration 51 / 200) loss: 1944.585436\n",
      "(Epoch 6 / 20) train acc: 0.202000; val_acc: 0.163889\n",
      "(Iteration 61 / 200) loss: 1516.308428\n",
      "(Epoch 7 / 20) train acc: 0.238000; val_acc: 0.216667\n",
      "(Iteration 71 / 200) loss: 1096.897987\n",
      "(Epoch 8 / 20) train acc: 0.341000; val_acc: 0.283333\n",
      "(Iteration 81 / 200) loss: 870.025910\n",
      "(Epoch 9 / 20) train acc: 0.411000; val_acc: 0.344444\n",
      "(Iteration 91 / 200) loss: 568.714775\n",
      "(Epoch 10 / 20) train acc: 0.500000; val_acc: 0.413889\n",
      "(Iteration 101 / 200) loss: 544.285717\n",
      "(Epoch 11 / 20) train acc: 0.548000; val_acc: 0.480556\n",
      "(Iteration 111 / 200) loss: 346.278812\n",
      "(Epoch 12 / 20) train acc: 0.584000; val_acc: 0.541667\n",
      "(Iteration 121 / 200) loss: 318.821323\n",
      "(Epoch 13 / 20) train acc: 0.637000; val_acc: 0.583333\n",
      "(Iteration 131 / 200) loss: 248.319677\n",
      "(Epoch 14 / 20) train acc: 0.666000; val_acc: 0.608333\n",
      "(Iteration 141 / 200) loss: 217.478227\n",
      "(Epoch 15 / 20) train acc: 0.721000; val_acc: 0.630556\n",
      "(Iteration 151 / 200) loss: 243.369026\n",
      "(Epoch 16 / 20) train acc: 0.731000; val_acc: 0.655556\n",
      "(Iteration 161 / 200) loss: 192.000963\n",
      "(Epoch 17 / 20) train acc: 0.746000; val_acc: 0.675000\n",
      "(Iteration 171 / 200) loss: 139.223498\n",
      "(Epoch 18 / 20) train acc: 0.782000; val_acc: 0.680556\n",
      "(Iteration 181 / 200) loss: 217.384219\n",
      "(Epoch 19 / 20) train acc: 0.788000; val_acc: 0.700000\n",
      "(Iteration 191 / 200) loss: 300.435342\n",
      "(Epoch 20 / 20) train acc: 0.816000; val_acc: 0.716667\n",
      "(Iteration 1 / 200) loss: 6.196859\n",
      "(Epoch 0 / 20) train acc: 0.077000; val_acc: 0.063889\n",
      "(Epoch 1 / 20) train acc: 0.424000; val_acc: 0.366667\n",
      "(Iteration 11 / 200) loss: 1.787663\n",
      "(Epoch 2 / 20) train acc: 0.777000; val_acc: 0.741667\n",
      "(Iteration 21 / 200) loss: 0.677349\n",
      "(Epoch 3 / 20) train acc: 0.866000; val_acc: 0.813889\n",
      "(Iteration 31 / 200) loss: 0.516788\n",
      "(Epoch 4 / 20) train acc: 0.922000; val_acc: 0.877778\n",
      "(Iteration 41 / 200) loss: 0.341905\n",
      "(Epoch 5 / 20) train acc: 0.944000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.236238\n",
      "(Epoch 6 / 20) train acc: 0.960000; val_acc: 0.902778\n",
      "(Iteration 61 / 200) loss: 0.277664\n",
      "(Epoch 7 / 20) train acc: 0.970000; val_acc: 0.933333\n",
      "(Iteration 71 / 200) loss: 0.169382\n",
      "(Epoch 8 / 20) train acc: 0.970000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.085006\n",
      "(Epoch 9 / 20) train acc: 0.977000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.110325\n",
      "(Epoch 10 / 20) train acc: 0.977000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.101800\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.117520\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.058004\n",
      "(Epoch 13 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.043154\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.068304\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.029636\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.091828\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.038914\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.050752\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 191 / 200) loss: 0.043556\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302122\n",
      "(Epoch 0 / 20) train acc: 0.142000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.397000; val_acc: 0.322222\n",
      "(Iteration 11 / 200) loss: 2.196668\n",
      "(Epoch 2 / 20) train acc: 0.602000; val_acc: 0.544444\n",
      "(Iteration 21 / 200) loss: 1.752664\n",
      "(Epoch 3 / 20) train acc: 0.810000; val_acc: 0.786111\n",
      "(Iteration 31 / 200) loss: 1.059618\n",
      "(Epoch 4 / 20) train acc: 0.832000; val_acc: 0.808333\n",
      "(Iteration 41 / 200) loss: 0.620017\n",
      "(Epoch 5 / 20) train acc: 0.864000; val_acc: 0.858333\n",
      "(Iteration 51 / 200) loss: 0.595646\n",
      "(Epoch 6 / 20) train acc: 0.883000; val_acc: 0.888889\n",
      "(Iteration 61 / 200) loss: 0.412065\n",
      "(Epoch 7 / 20) train acc: 0.904000; val_acc: 0.908333\n",
      "(Iteration 71 / 200) loss: 0.176797\n",
      "(Epoch 8 / 20) train acc: 0.935000; val_acc: 0.908333\n",
      "(Iteration 81 / 200) loss: 0.355671\n",
      "(Epoch 9 / 20) train acc: 0.932000; val_acc: 0.905556\n",
      "(Iteration 91 / 200) loss: 0.286932\n",
      "(Epoch 10 / 20) train acc: 0.950000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.234146\n",
      "(Epoch 11 / 20) train acc: 0.942000; val_acc: 0.936111\n",
      "(Iteration 111 / 200) loss: 0.229042\n",
      "(Epoch 12 / 20) train acc: 0.956000; val_acc: 0.925000\n",
      "(Iteration 121 / 200) loss: 0.142053\n",
      "(Epoch 13 / 20) train acc: 0.961000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 0.155639\n",
      "(Epoch 14 / 20) train acc: 0.968000; val_acc: 0.936111\n",
      "(Iteration 141 / 200) loss: 0.077050\n",
      "(Epoch 15 / 20) train acc: 0.971000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.086426\n",
      "(Epoch 16 / 20) train acc: 0.954000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 0.082041\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.154244\n",
      "(Epoch 18 / 20) train acc: 0.963000; val_acc: 0.944444\n",
      "(Iteration 181 / 200) loss: 0.090439\n",
      "(Epoch 19 / 20) train acc: 0.967000; val_acc: 0.936111\n",
      "(Iteration 191 / 200) loss: 0.109329\n",
      "(Epoch 20 / 20) train acc: 0.978000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302584\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.200000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2.274964\n",
      "(Epoch 2 / 20) train acc: 0.198000; val_acc: 0.213889\n",
      "(Iteration 21 / 200) loss: 2.095780\n",
      "(Epoch 3 / 20) train acc: 0.236000; val_acc: 0.250000\n",
      "(Iteration 31 / 200) loss: 1.769855\n",
      "(Epoch 4 / 20) train acc: 0.403000; val_acc: 0.394444\n",
      "(Iteration 41 / 200) loss: 1.511140\n",
      "(Epoch 5 / 20) train acc: 0.553000; val_acc: 0.527778\n",
      "(Iteration 51 / 200) loss: 1.163051\n",
      "(Epoch 6 / 20) train acc: 0.596000; val_acc: 0.563889\n",
      "(Iteration 61 / 200) loss: 1.294000\n",
      "(Epoch 7 / 20) train acc: 0.605000; val_acc: 0.616667\n",
      "(Iteration 71 / 200) loss: 0.938988\n",
      "(Epoch 8 / 20) train acc: 0.674000; val_acc: 0.675000\n",
      "(Iteration 81 / 200) loss: 0.965693\n",
      "(Epoch 9 / 20) train acc: 0.698000; val_acc: 0.675000\n",
      "(Iteration 91 / 200) loss: 0.838643\n",
      "(Epoch 10 / 20) train acc: 0.725000; val_acc: 0.719444\n",
      "(Iteration 101 / 200) loss: 0.732050\n",
      "(Epoch 11 / 20) train acc: 0.763000; val_acc: 0.747222\n",
      "(Iteration 111 / 200) loss: 0.623536\n",
      "(Epoch 12 / 20) train acc: 0.768000; val_acc: 0.783333\n",
      "(Iteration 121 / 200) loss: 0.614777\n",
      "(Epoch 13 / 20) train acc: 0.779000; val_acc: 0.800000\n",
      "(Iteration 131 / 200) loss: 0.580519\n",
      "(Epoch 14 / 20) train acc: 0.841000; val_acc: 0.827778\n",
      "(Iteration 141 / 200) loss: 0.521161\n",
      "(Epoch 15 / 20) train acc: 0.833000; val_acc: 0.822222\n",
      "(Iteration 151 / 200) loss: 0.442019\n",
      "(Epoch 16 / 20) train acc: 0.894000; val_acc: 0.852778\n",
      "(Iteration 161 / 200) loss: 0.441561\n",
      "(Epoch 17 / 20) train acc: 0.887000; val_acc: 0.855556\n",
      "(Iteration 171 / 200) loss: 0.433694\n",
      "(Epoch 18 / 20) train acc: 0.872000; val_acc: 0.861111\n",
      "(Iteration 181 / 200) loss: 0.297136\n",
      "(Epoch 19 / 20) train acc: 0.903000; val_acc: 0.858333\n",
      "(Iteration 191 / 200) loss: 0.242626\n",
      "(Epoch 20 / 20) train acc: 0.924000; val_acc: 0.875000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.288434\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.182620\n",
      "(Epoch 3 / 20) train acc: 0.295000; val_acc: 0.300000\n",
      "(Iteration 31 / 200) loss: 2.111458\n",
      "(Epoch 4 / 20) train acc: 0.246000; val_acc: 0.258333\n",
      "(Iteration 41 / 200) loss: 1.974526\n",
      "(Epoch 5 / 20) train acc: 0.295000; val_acc: 0.305556\n",
      "(Iteration 51 / 200) loss: 1.865142\n",
      "(Epoch 6 / 20) train acc: 0.453000; val_acc: 0.508333\n",
      "(Iteration 61 / 200) loss: 1.469654\n",
      "(Epoch 7 / 20) train acc: 0.564000; val_acc: 0.575000\n",
      "(Iteration 71 / 200) loss: 1.105780\n",
      "(Epoch 8 / 20) train acc: 0.620000; val_acc: 0.647222\n",
      "(Iteration 81 / 200) loss: 0.981083\n",
      "(Epoch 9 / 20) train acc: 0.645000; val_acc: 0.663889\n",
      "(Iteration 91 / 200) loss: 0.808350\n",
      "(Epoch 10 / 20) train acc: 0.690000; val_acc: 0.691667\n",
      "(Iteration 101 / 200) loss: 0.737597\n",
      "(Epoch 11 / 20) train acc: 0.723000; val_acc: 0.722222\n",
      "(Iteration 111 / 200) loss: 0.650154\n",
      "(Epoch 12 / 20) train acc: 0.728000; val_acc: 0.708333\n",
      "(Iteration 121 / 200) loss: 0.689197\n",
      "(Epoch 13 / 20) train acc: 0.786000; val_acc: 0.755556\n",
      "(Iteration 131 / 200) loss: 0.712434\n",
      "(Epoch 14 / 20) train acc: 0.808000; val_acc: 0.800000\n",
      "(Iteration 141 / 200) loss: 0.530346\n",
      "(Epoch 15 / 20) train acc: 0.816000; val_acc: 0.777778\n",
      "(Iteration 151 / 200) loss: 0.554655\n",
      "(Epoch 16 / 20) train acc: 0.784000; val_acc: 0.761111\n",
      "(Iteration 161 / 200) loss: 0.824455\n",
      "(Epoch 17 / 20) train acc: 0.825000; val_acc: 0.794444\n",
      "(Iteration 171 / 200) loss: 0.399784\n",
      "(Epoch 18 / 20) train acc: 0.833000; val_acc: 0.833333\n",
      "(Iteration 181 / 200) loss: 0.664897\n",
      "(Epoch 19 / 20) train acc: 0.871000; val_acc: 0.836111\n",
      "(Iteration 191 / 200) loss: 0.362425\n",
      "(Epoch 20 / 20) train acc: 0.819000; val_acc: 0.791667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.075000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.288687\n",
      "(Epoch 2 / 20) train acc: 0.172000; val_acc: 0.163889\n",
      "(Iteration 21 / 200) loss: 2.092647\n",
      "(Epoch 3 / 20) train acc: 0.222000; val_acc: 0.211111\n",
      "(Iteration 31 / 200) loss: 1.928022\n",
      "(Epoch 4 / 20) train acc: 0.225000; val_acc: 0.208333\n",
      "(Iteration 41 / 200) loss: 1.883461\n",
      "(Epoch 5 / 20) train acc: 0.209000; val_acc: 0.208333\n",
      "(Iteration 51 / 200) loss: 1.845668\n",
      "(Epoch 6 / 20) train acc: 0.217000; val_acc: 0.230556\n",
      "(Iteration 61 / 200) loss: 1.802379\n",
      "(Epoch 7 / 20) train acc: 0.225000; val_acc: 0.258333\n",
      "(Iteration 71 / 200) loss: 1.813662\n",
      "(Epoch 8 / 20) train acc: 0.247000; val_acc: 0.227778\n",
      "(Iteration 81 / 200) loss: 1.764231\n",
      "(Epoch 9 / 20) train acc: 0.222000; val_acc: 0.175000\n",
      "(Iteration 91 / 200) loss: 1.702794\n",
      "(Epoch 10 / 20) train acc: 0.200000; val_acc: 0.202778\n",
      "(Iteration 101 / 200) loss: 1.656481\n",
      "(Epoch 11 / 20) train acc: 0.299000; val_acc: 0.325000\n",
      "(Iteration 111 / 200) loss: 1.613899\n",
      "(Epoch 12 / 20) train acc: 0.293000; val_acc: 0.313889\n",
      "(Iteration 121 / 200) loss: 1.502818\n",
      "(Epoch 13 / 20) train acc: 0.368000; val_acc: 0.358333\n",
      "(Iteration 131 / 200) loss: 1.459189\n",
      "(Epoch 14 / 20) train acc: 0.375000; val_acc: 0.352778\n",
      "(Iteration 141 / 200) loss: 1.409671\n",
      "(Epoch 15 / 20) train acc: 0.388000; val_acc: 0.377778\n",
      "(Iteration 151 / 200) loss: 1.407562\n",
      "(Epoch 16 / 20) train acc: 0.402000; val_acc: 0.388889\n",
      "(Iteration 161 / 200) loss: 1.200498\n",
      "(Epoch 17 / 20) train acc: 0.458000; val_acc: 0.436111\n",
      "(Iteration 171 / 200) loss: 1.396963\n",
      "(Epoch 18 / 20) train acc: 0.431000; val_acc: 0.394444\n",
      "(Iteration 181 / 200) loss: 1.219830\n",
      "(Epoch 19 / 20) train acc: 0.457000; val_acc: 0.427778\n",
      "(Iteration 191 / 200) loss: 1.251928\n",
      "(Epoch 20 / 20) train acc: 0.436000; val_acc: 0.469444\n",
      "(Iteration 1 / 200) loss: 3233.038789\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.158333\n",
      "(Epoch 1 / 20) train acc: 0.134000; val_acc: 0.158333\n",
      "(Iteration 11 / 200) loss: 3831.859849\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.155556\n",
      "(Iteration 21 / 200) loss: 3128.836863\n",
      "(Epoch 3 / 20) train acc: 0.117000; val_acc: 0.161111\n",
      "(Iteration 31 / 200) loss: 3280.431907\n",
      "(Epoch 4 / 20) train acc: 0.139000; val_acc: 0.161111\n",
      "(Iteration 41 / 200) loss: 3276.518882\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.161111\n",
      "(Iteration 51 / 200) loss: 3966.793528\n",
      "(Epoch 6 / 20) train acc: 0.119000; val_acc: 0.161111\n",
      "(Iteration 61 / 200) loss: 3352.637529\n",
      "(Epoch 7 / 20) train acc: 0.151000; val_acc: 0.169444\n",
      "(Iteration 71 / 200) loss: 3018.857200\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.166667\n",
      "(Iteration 81 / 200) loss: 2886.357041\n",
      "(Epoch 9 / 20) train acc: 0.138000; val_acc: 0.163889\n",
      "(Iteration 91 / 200) loss: 2800.134338\n",
      "(Epoch 10 / 20) train acc: 0.134000; val_acc: 0.169444\n",
      "(Iteration 101 / 200) loss: 3005.041172\n",
      "(Epoch 11 / 20) train acc: 0.138000; val_acc: 0.163889\n",
      "(Iteration 111 / 200) loss: 2868.194829\n",
      "(Epoch 12 / 20) train acc: 0.134000; val_acc: 0.161111\n",
      "(Iteration 121 / 200) loss: 2301.901904\n",
      "(Epoch 13 / 20) train acc: 0.152000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 2425.913262\n",
      "(Epoch 14 / 20) train acc: 0.154000; val_acc: 0.169444\n",
      "(Iteration 141 / 200) loss: 2248.622964\n",
      "(Epoch 15 / 20) train acc: 0.136000; val_acc: 0.177778\n",
      "(Iteration 151 / 200) loss: 2379.188936\n",
      "(Epoch 16 / 20) train acc: 0.144000; val_acc: 0.186111\n",
      "(Iteration 161 / 200) loss: 2658.557163\n",
      "(Epoch 17 / 20) train acc: 0.128000; val_acc: 0.186111\n",
      "(Iteration 171 / 200) loss: 2317.586013\n",
      "(Epoch 18 / 20) train acc: 0.171000; val_acc: 0.183333\n",
      "(Iteration 181 / 200) loss: 2205.678770\n",
      "(Epoch 19 / 20) train acc: 0.152000; val_acc: 0.186111\n",
      "(Iteration 191 / 200) loss: 1918.696367\n",
      "(Epoch 20 / 20) train acc: 0.149000; val_acc: 0.194444\n",
      "(Iteration 1 / 200) loss: 4.501722\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.132000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 4.396039\n",
      "(Epoch 2 / 20) train acc: 0.166000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 3.519196\n",
      "(Epoch 3 / 20) train acc: 0.239000; val_acc: 0.200000\n",
      "(Iteration 31 / 200) loss: 3.088348\n",
      "(Epoch 4 / 20) train acc: 0.299000; val_acc: 0.263889\n",
      "(Iteration 41 / 200) loss: 2.928447\n",
      "(Epoch 5 / 20) train acc: 0.363000; val_acc: 0.319444\n",
      "(Iteration 51 / 200) loss: 2.584340\n",
      "(Epoch 6 / 20) train acc: 0.448000; val_acc: 0.391667\n",
      "(Iteration 61 / 200) loss: 2.490647\n",
      "(Epoch 7 / 20) train acc: 0.551000; val_acc: 0.491667\n",
      "(Iteration 71 / 200) loss: 2.338886\n",
      "(Epoch 8 / 20) train acc: 0.585000; val_acc: 0.538889\n",
      "(Iteration 81 / 200) loss: 2.199047\n",
      "(Epoch 9 / 20) train acc: 0.600000; val_acc: 0.575000\n",
      "(Iteration 91 / 200) loss: 1.981674\n",
      "(Epoch 10 / 20) train acc: 0.685000; val_acc: 0.611111\n",
      "(Iteration 101 / 200) loss: 1.777915\n",
      "(Epoch 11 / 20) train acc: 0.727000; val_acc: 0.650000\n",
      "(Iteration 111 / 200) loss: 1.798192\n",
      "(Epoch 12 / 20) train acc: 0.756000; val_acc: 0.675000\n",
      "(Iteration 121 / 200) loss: 1.663780\n",
      "(Epoch 13 / 20) train acc: 0.790000; val_acc: 0.702778\n",
      "(Iteration 131 / 200) loss: 1.642703\n",
      "(Epoch 14 / 20) train acc: 0.805000; val_acc: 0.733333\n",
      "(Iteration 141 / 200) loss: 1.505905\n",
      "(Epoch 15 / 20) train acc: 0.835000; val_acc: 0.763889\n",
      "(Iteration 151 / 200) loss: 1.329916\n",
      "(Epoch 16 / 20) train acc: 0.844000; val_acc: 0.769444\n",
      "(Iteration 161 / 200) loss: 1.413488\n",
      "(Epoch 17 / 20) train acc: 0.857000; val_acc: 0.786111\n",
      "(Iteration 171 / 200) loss: 1.303817\n",
      "(Epoch 18 / 20) train acc: 0.872000; val_acc: 0.797222\n",
      "(Iteration 181 / 200) loss: 1.334637\n",
      "(Epoch 19 / 20) train acc: 0.870000; val_acc: 0.811111\n",
      "(Iteration 191 / 200) loss: 1.196285\n",
      "(Epoch 20 / 20) train acc: 0.897000; val_acc: 0.836111\n",
      "(Iteration 1 / 200) loss: 2.310579\n",
      "(Epoch 0 / 20) train acc: 0.123000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.353000; val_acc: 0.302778\n",
      "(Iteration 11 / 200) loss: 2.306279\n",
      "(Epoch 2 / 20) train acc: 0.491000; val_acc: 0.441667\n",
      "(Iteration 21 / 200) loss: 2.302573\n",
      "(Epoch 3 / 20) train acc: 0.457000; val_acc: 0.400000\n",
      "(Iteration 31 / 200) loss: 2.294875\n",
      "(Epoch 4 / 20) train acc: 0.421000; val_acc: 0.405556\n",
      "(Iteration 41 / 200) loss: 2.285215\n",
      "(Epoch 5 / 20) train acc: 0.445000; val_acc: 0.425000\n",
      "(Iteration 51 / 200) loss: 2.272674\n",
      "(Epoch 6 / 20) train acc: 0.457000; val_acc: 0.452778\n",
      "(Iteration 61 / 200) loss: 2.254465\n",
      "(Epoch 7 / 20) train acc: 0.447000; val_acc: 0.463889\n",
      "(Iteration 71 / 200) loss: 2.229008\n",
      "(Epoch 8 / 20) train acc: 0.486000; val_acc: 0.491667\n",
      "(Iteration 81 / 200) loss: 2.192122\n",
      "(Epoch 9 / 20) train acc: 0.440000; val_acc: 0.441667\n",
      "(Iteration 91 / 200) loss: 2.113100\n",
      "(Epoch 10 / 20) train acc: 0.447000; val_acc: 0.461111\n",
      "(Iteration 101 / 200) loss: 2.078846\n",
      "(Epoch 11 / 20) train acc: 0.470000; val_acc: 0.466667\n",
      "(Iteration 111 / 200) loss: 1.976246\n",
      "(Epoch 12 / 20) train acc: 0.478000; val_acc: 0.511111\n",
      "(Iteration 121 / 200) loss: 1.941450\n",
      "(Epoch 13 / 20) train acc: 0.560000; val_acc: 0.583333\n",
      "(Iteration 131 / 200) loss: 1.784630\n",
      "(Epoch 14 / 20) train acc: 0.517000; val_acc: 0.602778\n",
      "(Iteration 141 / 200) loss: 1.704704\n",
      "(Epoch 15 / 20) train acc: 0.618000; val_acc: 0.638889\n",
      "(Iteration 151 / 200) loss: 1.603978\n",
      "(Epoch 16 / 20) train acc: 0.665000; val_acc: 0.683333\n",
      "(Iteration 161 / 200) loss: 1.462562\n",
      "(Epoch 17 / 20) train acc: 0.738000; val_acc: 0.738889\n",
      "(Iteration 171 / 200) loss: 1.326518\n",
      "(Epoch 18 / 20) train acc: 0.747000; val_acc: 0.769444\n",
      "(Iteration 181 / 200) loss: 1.252910\n",
      "(Epoch 19 / 20) train acc: 0.772000; val_acc: 0.775000\n",
      "(Iteration 191 / 200) loss: 1.200090\n",
      "(Epoch 20 / 20) train acc: 0.770000; val_acc: 0.802778\n",
      "(Iteration 1 / 200) loss: 2.302666\n",
      "(Epoch 0 / 20) train acc: 0.150000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302443\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302113\n",
      "(Epoch 3 / 20) train acc: 0.114000; val_acc: 0.113889\n",
      "(Iteration 31 / 200) loss: 2.300235\n",
      "(Epoch 4 / 20) train acc: 0.216000; val_acc: 0.205556\n",
      "(Iteration 41 / 200) loss: 2.293918\n",
      "(Epoch 5 / 20) train acc: 0.214000; val_acc: 0.219444\n",
      "(Iteration 51 / 200) loss: 2.278558\n",
      "(Epoch 6 / 20) train acc: 0.212000; val_acc: 0.211111\n",
      "(Iteration 61 / 200) loss: 2.241322\n",
      "(Epoch 7 / 20) train acc: 0.235000; val_acc: 0.236111\n",
      "(Iteration 71 / 200) loss: 2.185235\n",
      "(Epoch 8 / 20) train acc: 0.267000; val_acc: 0.241667\n",
      "(Iteration 81 / 200) loss: 2.105152\n",
      "(Epoch 9 / 20) train acc: 0.247000; val_acc: 0.238889\n",
      "(Iteration 91 / 200) loss: 2.088345\n",
      "(Epoch 10 / 20) train acc: 0.237000; val_acc: 0.244444\n",
      "(Iteration 101 / 200) loss: 1.989589\n",
      "(Epoch 11 / 20) train acc: 0.251000; val_acc: 0.252778\n",
      "(Iteration 111 / 200) loss: 1.927280\n",
      "(Epoch 12 / 20) train acc: 0.241000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 1.866597\n",
      "(Epoch 13 / 20) train acc: 0.253000; val_acc: 0.263889\n",
      "(Iteration 131 / 200) loss: 1.788876\n",
      "(Epoch 14 / 20) train acc: 0.282000; val_acc: 0.294444\n",
      "(Iteration 141 / 200) loss: 1.739791\n",
      "(Epoch 15 / 20) train acc: 0.307000; val_acc: 0.280556\n",
      "(Iteration 151 / 200) loss: 1.692221\n",
      "(Epoch 16 / 20) train acc: 0.321000; val_acc: 0.327778\n",
      "(Iteration 161 / 200) loss: 1.737093\n",
      "(Epoch 17 / 20) train acc: 0.306000; val_acc: 0.300000\n",
      "(Iteration 171 / 200) loss: 1.744626\n",
      "(Epoch 18 / 20) train acc: 0.336000; val_acc: 0.286111\n",
      "(Iteration 181 / 200) loss: 1.740869\n",
      "(Epoch 19 / 20) train acc: 0.378000; val_acc: 0.325000\n",
      "(Iteration 191 / 200) loss: 1.743403\n",
      "(Epoch 20 / 20) train acc: 0.469000; val_acc: 0.425000\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302560\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302494\n",
      "(Epoch 3 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302626\n",
      "(Epoch 4 / 20) train acc: 0.072000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302329\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.299956\n",
      "(Epoch 6 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.296203\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.279849\n",
      "(Epoch 8 / 20) train acc: 0.133000; val_acc: 0.102778\n",
      "(Iteration 81 / 200) loss: 2.271263\n",
      "(Epoch 9 / 20) train acc: 0.148000; val_acc: 0.119444\n",
      "(Iteration 91 / 200) loss: 2.246470\n",
      "(Epoch 10 / 20) train acc: 0.172000; val_acc: 0.122222\n",
      "(Iteration 101 / 200) loss: 2.164690\n",
      "(Epoch 11 / 20) train acc: 0.149000; val_acc: 0.141667\n",
      "(Iteration 111 / 200) loss: 2.168617\n",
      "(Epoch 12 / 20) train acc: 0.164000; val_acc: 0.172222\n",
      "(Iteration 121 / 200) loss: 2.158646\n",
      "(Epoch 13 / 20) train acc: 0.174000; val_acc: 0.175000\n",
      "(Iteration 131 / 200) loss: 2.141218\n",
      "(Epoch 14 / 20) train acc: 0.194000; val_acc: 0.183333\n",
      "(Iteration 141 / 200) loss: 2.042213\n",
      "(Epoch 15 / 20) train acc: 0.197000; val_acc: 0.188889\n",
      "(Iteration 151 / 200) loss: 2.005874\n",
      "(Epoch 16 / 20) train acc: 0.178000; val_acc: 0.191667\n",
      "(Iteration 161 / 200) loss: 2.041293\n",
      "(Epoch 17 / 20) train acc: 0.197000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 1.975756\n",
      "(Epoch 18 / 20) train acc: 0.196000; val_acc: 0.191667\n",
      "(Iteration 181 / 200) loss: 1.925881\n",
      "(Epoch 19 / 20) train acc: 0.214000; val_acc: 0.197222\n",
      "(Iteration 191 / 200) loss: 1.990061\n",
      "(Epoch 20 / 20) train acc: 0.195000; val_acc: 0.194444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.089000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302546\n",
      "(Epoch 2 / 20) train acc: 0.086000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302538\n",
      "(Epoch 3 / 20) train acc: 0.114000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302469\n",
      "(Epoch 4 / 20) train acc: 0.117000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302402\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.301929\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.299662\n",
      "(Epoch 7 / 20) train acc: 0.084000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.293037\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.086111\n",
      "(Iteration 81 / 200) loss: 2.279107\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Iteration 91 / 200) loss: 2.257070\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Iteration 101 / 200) loss: 2.238431\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 2.210788\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 2.167637\n",
      "(Epoch 13 / 20) train acc: 0.131000; val_acc: 0.105556\n",
      "(Iteration 131 / 200) loss: 2.093984\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.113889\n",
      "(Iteration 141 / 200) loss: 2.026102\n",
      "(Epoch 15 / 20) train acc: 0.179000; val_acc: 0.122222\n",
      "(Iteration 151 / 200) loss: 2.054452\n",
      "(Epoch 16 / 20) train acc: 0.162000; val_acc: 0.127778\n",
      "(Iteration 161 / 200) loss: 2.030252\n",
      "(Epoch 17 / 20) train acc: 0.165000; val_acc: 0.136111\n",
      "(Iteration 171 / 200) loss: 1.984558\n",
      "(Epoch 18 / 20) train acc: 0.165000; val_acc: 0.136111\n",
      "(Iteration 181 / 200) loss: 1.998244\n",
      "(Epoch 19 / 20) train acc: 0.182000; val_acc: 0.150000\n",
      "(Iteration 191 / 200) loss: 2.000994\n",
      "(Epoch 20 / 20) train acc: 0.206000; val_acc: 0.150000\n",
      "(Iteration 1 / 200) loss: 6976.912078\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 7191.325005\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.130556\n",
      "(Iteration 21 / 200) loss: 6128.984199\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.130556\n",
      "(Iteration 31 / 200) loss: 6243.596345\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.133333\n",
      "(Iteration 41 / 200) loss: 6324.448813\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 6423.341199\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.136111\n",
      "(Iteration 61 / 200) loss: 6384.400645\n",
      "(Epoch 7 / 20) train acc: 0.124000; val_acc: 0.136111\n",
      "(Iteration 71 / 200) loss: 6326.332056\n",
      "(Epoch 8 / 20) train acc: 0.134000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 5757.330410\n",
      "(Epoch 9 / 20) train acc: 0.125000; val_acc: 0.138889\n",
      "(Iteration 91 / 200) loss: 5937.410112\n",
      "(Epoch 10 / 20) train acc: 0.139000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 5273.941777\n",
      "(Epoch 11 / 20) train acc: 0.134000; val_acc: 0.152778\n",
      "(Iteration 111 / 200) loss: 5088.354102\n",
      "(Epoch 12 / 20) train acc: 0.130000; val_acc: 0.152778\n",
      "(Iteration 121 / 200) loss: 5682.060012\n",
      "(Epoch 13 / 20) train acc: 0.138000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 5312.438757\n",
      "(Epoch 14 / 20) train acc: 0.137000; val_acc: 0.161111\n",
      "(Iteration 141 / 200) loss: 4270.585637\n",
      "(Epoch 15 / 20) train acc: 0.140000; val_acc: 0.163889\n",
      "(Iteration 151 / 200) loss: 5042.956011\n",
      "(Epoch 16 / 20) train acc: 0.150000; val_acc: 0.166667\n",
      "(Iteration 161 / 200) loss: 4226.852002\n",
      "(Epoch 17 / 20) train acc: 0.128000; val_acc: 0.169444\n",
      "(Iteration 171 / 200) loss: 4355.104590\n",
      "(Epoch 18 / 20) train acc: 0.147000; val_acc: 0.169444\n",
      "(Iteration 181 / 200) loss: 4484.684729\n",
      "(Epoch 19 / 20) train acc: 0.142000; val_acc: 0.166667\n",
      "(Iteration 191 / 200) loss: 4528.728003\n",
      "(Epoch 20 / 20) train acc: 0.134000; val_acc: 0.166667\n",
      "(Iteration 1 / 200) loss: 5.468464\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.122222\n",
      "(Iteration 11 / 200) loss: 4.340318\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.155556\n",
      "(Iteration 21 / 200) loss: 3.698087\n",
      "(Epoch 3 / 20) train acc: 0.197000; val_acc: 0.211111\n",
      "(Iteration 31 / 200) loss: 3.420821\n",
      "(Epoch 4 / 20) train acc: 0.292000; val_acc: 0.300000\n",
      "(Iteration 41 / 200) loss: 3.012736\n",
      "(Epoch 5 / 20) train acc: 0.346000; val_acc: 0.369444\n",
      "(Iteration 51 / 200) loss: 2.762089\n",
      "(Epoch 6 / 20) train acc: 0.398000; val_acc: 0.416667\n",
      "(Iteration 61 / 200) loss: 2.564274\n",
      "(Epoch 7 / 20) train acc: 0.505000; val_acc: 0.488889\n",
      "(Iteration 71 / 200) loss: 2.331393\n",
      "(Epoch 8 / 20) train acc: 0.555000; val_acc: 0.555556\n",
      "(Iteration 81 / 200) loss: 2.071819\n",
      "(Epoch 9 / 20) train acc: 0.623000; val_acc: 0.605556\n",
      "(Iteration 91 / 200) loss: 2.143530\n",
      "(Epoch 10 / 20) train acc: 0.684000; val_acc: 0.661111\n",
      "(Iteration 101 / 200) loss: 2.005818\n",
      "(Epoch 11 / 20) train acc: 0.732000; val_acc: 0.691667\n",
      "(Iteration 111 / 200) loss: 1.737785\n",
      "(Epoch 12 / 20) train acc: 0.756000; val_acc: 0.733333\n",
      "(Iteration 121 / 200) loss: 1.685071\n",
      "(Epoch 13 / 20) train acc: 0.790000; val_acc: 0.755556\n",
      "(Iteration 131 / 200) loss: 1.654400\n",
      "(Epoch 14 / 20) train acc: 0.788000; val_acc: 0.769444\n",
      "(Iteration 141 / 200) loss: 1.424909\n",
      "(Epoch 15 / 20) train acc: 0.855000; val_acc: 0.786111\n",
      "(Iteration 151 / 200) loss: 1.434267\n",
      "(Epoch 16 / 20) train acc: 0.841000; val_acc: 0.794444\n",
      "(Iteration 161 / 200) loss: 1.428297\n",
      "(Epoch 17 / 20) train acc: 0.863000; val_acc: 0.802778\n",
      "(Iteration 171 / 200) loss: 1.447349\n",
      "(Epoch 18 / 20) train acc: 0.877000; val_acc: 0.811111\n",
      "(Iteration 181 / 200) loss: 1.267172\n",
      "(Epoch 19 / 20) train acc: 0.873000; val_acc: 0.827778\n",
      "(Iteration 191 / 200) loss: 1.242140\n",
      "(Epoch 20 / 20) train acc: 0.906000; val_acc: 0.827778\n",
      "(Iteration 1 / 200) loss: 2.310144\n",
      "(Epoch 0 / 20) train acc: 0.117000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.420000; val_acc: 0.375000\n",
      "(Iteration 11 / 200) loss: 2.305412\n",
      "(Epoch 2 / 20) train acc: 0.612000; val_acc: 0.563889\n",
      "(Iteration 21 / 200) loss: 2.299154\n",
      "(Epoch 3 / 20) train acc: 0.602000; val_acc: 0.550000\n",
      "(Iteration 31 / 200) loss: 2.290040\n",
      "(Epoch 4 / 20) train acc: 0.534000; val_acc: 0.522222\n",
      "(Iteration 41 / 200) loss: 2.278466\n",
      "(Epoch 5 / 20) train acc: 0.599000; val_acc: 0.541667\n",
      "(Iteration 51 / 200) loss: 2.259766\n",
      "(Epoch 6 / 20) train acc: 0.619000; val_acc: 0.555556\n",
      "(Iteration 61 / 200) loss: 2.227851\n",
      "(Epoch 7 / 20) train acc: 0.584000; val_acc: 0.541667\n",
      "(Iteration 71 / 200) loss: 2.206971\n",
      "(Epoch 8 / 20) train acc: 0.598000; val_acc: 0.563889\n",
      "(Iteration 81 / 200) loss: 2.171683\n",
      "(Epoch 9 / 20) train acc: 0.597000; val_acc: 0.547222\n",
      "(Iteration 91 / 200) loss: 2.121347\n",
      "(Epoch 10 / 20) train acc: 0.569000; val_acc: 0.533333\n",
      "(Iteration 101 / 200) loss: 2.049212\n",
      "(Epoch 11 / 20) train acc: 0.602000; val_acc: 0.530556\n",
      "(Iteration 111 / 200) loss: 1.965837\n",
      "(Epoch 12 / 20) train acc: 0.605000; val_acc: 0.591667\n",
      "(Iteration 121 / 200) loss: 1.875570\n",
      "(Epoch 13 / 20) train acc: 0.620000; val_acc: 0.616667\n",
      "(Iteration 131 / 200) loss: 1.779176\n",
      "(Epoch 14 / 20) train acc: 0.705000; val_acc: 0.688889\n",
      "(Iteration 141 / 200) loss: 1.689503\n",
      "(Epoch 15 / 20) train acc: 0.662000; val_acc: 0.705556\n",
      "(Iteration 151 / 200) loss: 1.538097\n",
      "(Epoch 16 / 20) train acc: 0.761000; val_acc: 0.733333\n",
      "(Iteration 161 / 200) loss: 1.442896\n",
      "(Epoch 17 / 20) train acc: 0.776000; val_acc: 0.780556\n",
      "(Iteration 171 / 200) loss: 1.321587\n",
      "(Epoch 18 / 20) train acc: 0.779000; val_acc: 0.761111\n",
      "(Iteration 181 / 200) loss: 1.170588\n",
      "(Epoch 19 / 20) train acc: 0.789000; val_acc: 0.786111\n",
      "(Iteration 191 / 200) loss: 1.074343\n",
      "(Epoch 20 / 20) train acc: 0.779000; val_acc: 0.788889\n",
      "(Iteration 1 / 200) loss: 2.302668\n",
      "(Epoch 0 / 20) train acc: 0.170000; val_acc: 0.169444\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302459\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302360\n",
      "(Epoch 3 / 20) train acc: 0.210000; val_acc: 0.186111\n",
      "(Iteration 31 / 200) loss: 2.300967\n",
      "(Epoch 4 / 20) train acc: 0.220000; val_acc: 0.183333\n",
      "(Iteration 41 / 200) loss: 2.296317\n",
      "(Epoch 5 / 20) train acc: 0.255000; val_acc: 0.236111\n",
      "(Iteration 51 / 200) loss: 2.284287\n",
      "(Epoch 6 / 20) train acc: 0.242000; val_acc: 0.202778\n",
      "(Iteration 61 / 200) loss: 2.270244\n",
      "(Epoch 7 / 20) train acc: 0.226000; val_acc: 0.208333\n",
      "(Iteration 71 / 200) loss: 2.236544\n",
      "(Epoch 8 / 20) train acc: 0.186000; val_acc: 0.205556\n",
      "(Iteration 81 / 200) loss: 2.176474\n",
      "(Epoch 9 / 20) train acc: 0.217000; val_acc: 0.208333\n",
      "(Iteration 91 / 200) loss: 2.083338\n",
      "(Epoch 10 / 20) train acc: 0.235000; val_acc: 0.250000\n",
      "(Iteration 101 / 200) loss: 1.985111\n",
      "(Epoch 11 / 20) train acc: 0.300000; val_acc: 0.283333\n",
      "(Iteration 111 / 200) loss: 1.972145\n",
      "(Epoch 12 / 20) train acc: 0.339000; val_acc: 0.325000\n",
      "(Iteration 121 / 200) loss: 1.891200\n",
      "(Epoch 13 / 20) train acc: 0.412000; val_acc: 0.377778\n",
      "(Iteration 131 / 200) loss: 1.775945\n",
      "(Epoch 14 / 20) train acc: 0.419000; val_acc: 0.427778\n",
      "(Iteration 141 / 200) loss: 1.674759\n",
      "(Epoch 15 / 20) train acc: 0.508000; val_acc: 0.480556\n",
      "(Iteration 151 / 200) loss: 1.638624\n",
      "(Epoch 16 / 20) train acc: 0.526000; val_acc: 0.516667\n",
      "(Iteration 161 / 200) loss: 1.516095\n",
      "(Epoch 17 / 20) train acc: 0.560000; val_acc: 0.558333\n",
      "(Iteration 171 / 200) loss: 1.496786\n",
      "(Epoch 18 / 20) train acc: 0.585000; val_acc: 0.588889\n",
      "(Iteration 181 / 200) loss: 1.413842\n",
      "(Epoch 19 / 20) train acc: 0.545000; val_acc: 0.572222\n",
      "(Iteration 191 / 200) loss: 1.361092\n",
      "(Epoch 20 / 20) train acc: 0.533000; val_acc: 0.533333\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302551\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302551\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302481\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302228\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.094444\n",
      "(Iteration 51 / 200) loss: 2.300115\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 2.293392\n",
      "(Epoch 7 / 20) train acc: 0.119000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.276846\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.241134\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.239658\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.094444\n",
      "(Iteration 101 / 200) loss: 2.226217\n",
      "(Epoch 11 / 20) train acc: 0.127000; val_acc: 0.125000\n",
      "(Iteration 111 / 200) loss: 2.184817\n",
      "(Epoch 12 / 20) train acc: 0.150000; val_acc: 0.161111\n",
      "(Iteration 121 / 200) loss: 2.122828\n",
      "(Epoch 13 / 20) train acc: 0.195000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 2.141754\n",
      "(Epoch 14 / 20) train acc: 0.192000; val_acc: 0.163889\n",
      "(Iteration 141 / 200) loss: 2.117546\n",
      "(Epoch 15 / 20) train acc: 0.184000; val_acc: 0.175000\n",
      "(Iteration 151 / 200) loss: 2.151238\n",
      "(Epoch 16 / 20) train acc: 0.197000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 2.111272\n",
      "(Epoch 17 / 20) train acc: 0.181000; val_acc: 0.172222\n",
      "(Iteration 171 / 200) loss: 2.043302\n",
      "(Epoch 18 / 20) train acc: 0.185000; val_acc: 0.177778\n",
      "(Iteration 181 / 200) loss: 2.096277\n",
      "(Epoch 19 / 20) train acc: 0.176000; val_acc: 0.177778\n",
      "(Iteration 191 / 200) loss: 1.995796\n",
      "(Epoch 20 / 20) train acc: 0.188000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302610\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302695\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302536\n",
      "(Epoch 4 / 20) train acc: 0.132000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.301959\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.298465\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.293796\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.284758\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.257542\n",
      "(Epoch 9 / 20) train acc: 0.127000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.244333\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.195763\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.123927\n",
      "(Epoch 12 / 20) train acc: 0.187000; val_acc: 0.163889\n",
      "(Iteration 121 / 200) loss: 2.145976\n",
      "(Epoch 13 / 20) train acc: 0.215000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 2.153669\n",
      "(Epoch 14 / 20) train acc: 0.198000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 2.029708\n",
      "(Epoch 15 / 20) train acc: 0.195000; val_acc: 0.177778\n",
      "(Iteration 151 / 200) loss: 2.112010\n",
      "(Epoch 16 / 20) train acc: 0.190000; val_acc: 0.169444\n",
      "(Iteration 161 / 200) loss: 2.103676\n",
      "(Epoch 17 / 20) train acc: 0.188000; val_acc: 0.177778\n",
      "(Iteration 171 / 200) loss: 2.053717\n",
      "(Epoch 18 / 20) train acc: 0.175000; val_acc: 0.177778\n",
      "(Iteration 181 / 200) loss: 2.112823\n",
      "(Epoch 19 / 20) train acc: 0.207000; val_acc: 0.177778\n",
      "(Iteration 191 / 200) loss: 2.128475\n",
      "(Epoch 20 / 20) train acc: 0.226000; val_acc: 0.186111\n",
      "(Iteration 1 / 200) loss: 3690.837642\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 3899.648835\n",
      "(Epoch 2 / 20) train acc: 0.126000; val_acc: 0.144444\n",
      "(Iteration 21 / 200) loss: 3985.846335\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.150000\n",
      "(Iteration 31 / 200) loss: 3770.836636\n",
      "(Epoch 4 / 20) train acc: 0.158000; val_acc: 0.155556\n",
      "(Iteration 41 / 200) loss: 4153.003198\n",
      "(Epoch 5 / 20) train acc: 0.133000; val_acc: 0.158333\n",
      "(Iteration 51 / 200) loss: 4105.454448\n",
      "(Epoch 6 / 20) train acc: 0.132000; val_acc: 0.163889\n",
      "(Iteration 61 / 200) loss: 3596.189922\n",
      "(Epoch 7 / 20) train acc: 0.151000; val_acc: 0.172222\n",
      "(Iteration 71 / 200) loss: 3463.506726\n",
      "(Epoch 8 / 20) train acc: 0.139000; val_acc: 0.172222\n",
      "(Iteration 81 / 200) loss: 2623.104458\n",
      "(Epoch 9 / 20) train acc: 0.165000; val_acc: 0.175000\n",
      "(Iteration 91 / 200) loss: 3382.115215\n",
      "(Epoch 10 / 20) train acc: 0.158000; val_acc: 0.177778\n",
      "(Iteration 101 / 200) loss: 3446.196138\n",
      "(Epoch 11 / 20) train acc: 0.183000; val_acc: 0.177778\n",
      "(Iteration 111 / 200) loss: 2935.824507\n",
      "(Epoch 12 / 20) train acc: 0.158000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 2484.855078\n",
      "(Epoch 13 / 20) train acc: 0.176000; val_acc: 0.172222\n",
      "(Iteration 131 / 200) loss: 2699.789836\n",
      "(Epoch 14 / 20) train acc: 0.181000; val_acc: 0.169444\n",
      "(Iteration 141 / 200) loss: 2936.399922\n",
      "(Epoch 15 / 20) train acc: 0.158000; val_acc: 0.177778\n",
      "(Iteration 151 / 200) loss: 2677.821011\n",
      "(Epoch 16 / 20) train acc: 0.205000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 2327.873098\n",
      "(Epoch 17 / 20) train acc: 0.192000; val_acc: 0.186111\n",
      "(Iteration 171 / 200) loss: 2548.687119\n",
      "(Epoch 18 / 20) train acc: 0.178000; val_acc: 0.188889\n",
      "(Iteration 181 / 200) loss: 2561.004795\n",
      "(Epoch 19 / 20) train acc: 0.197000; val_acc: 0.191667\n",
      "(Iteration 191 / 200) loss: 2158.103320\n",
      "(Epoch 20 / 20) train acc: 0.183000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 6.759533\n",
      "(Epoch 0 / 20) train acc: 0.144000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.133333\n",
      "(Iteration 11 / 200) loss: 6.182170\n",
      "(Epoch 2 / 20) train acc: 0.164000; val_acc: 0.130556\n",
      "(Iteration 21 / 200) loss: 4.996355\n",
      "(Epoch 3 / 20) train acc: 0.144000; val_acc: 0.130556\n",
      "(Iteration 31 / 200) loss: 4.302758\n",
      "(Epoch 4 / 20) train acc: 0.170000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 3.764660\n",
      "(Epoch 5 / 20) train acc: 0.206000; val_acc: 0.152778\n",
      "(Iteration 51 / 200) loss: 3.084443\n",
      "(Epoch 6 / 20) train acc: 0.280000; val_acc: 0.211111\n",
      "(Iteration 61 / 200) loss: 3.042088\n",
      "(Epoch 7 / 20) train acc: 0.314000; val_acc: 0.286111\n",
      "(Iteration 71 / 200) loss: 2.974222\n",
      "(Epoch 8 / 20) train acc: 0.385000; val_acc: 0.363889\n",
      "(Iteration 81 / 200) loss: 2.505983\n",
      "(Epoch 9 / 20) train acc: 0.441000; val_acc: 0.425000\n",
      "(Iteration 91 / 200) loss: 2.341128\n",
      "(Epoch 10 / 20) train acc: 0.502000; val_acc: 0.491667\n",
      "(Iteration 101 / 200) loss: 2.360843\n",
      "(Epoch 11 / 20) train acc: 0.562000; val_acc: 0.541667\n",
      "(Iteration 111 / 200) loss: 2.358365\n",
      "(Epoch 12 / 20) train acc: 0.599000; val_acc: 0.594444\n",
      "(Iteration 121 / 200) loss: 2.295427\n",
      "(Epoch 13 / 20) train acc: 0.632000; val_acc: 0.661111\n",
      "(Iteration 131 / 200) loss: 2.069303\n",
      "(Epoch 14 / 20) train acc: 0.675000; val_acc: 0.691667\n",
      "(Iteration 141 / 200) loss: 1.826261\n",
      "(Epoch 15 / 20) train acc: 0.728000; val_acc: 0.730556\n",
      "(Iteration 151 / 200) loss: 1.948357\n",
      "(Epoch 16 / 20) train acc: 0.725000; val_acc: 0.752778\n",
      "(Iteration 161 / 200) loss: 1.738574\n",
      "(Epoch 17 / 20) train acc: 0.749000; val_acc: 0.780556\n",
      "(Iteration 171 / 200) loss: 1.643152\n",
      "(Epoch 18 / 20) train acc: 0.776000; val_acc: 0.794444\n",
      "(Iteration 181 / 200) loss: 1.506865\n",
      "(Epoch 19 / 20) train acc: 0.815000; val_acc: 0.794444\n",
      "(Iteration 191 / 200) loss: 1.366413\n",
      "(Epoch 20 / 20) train acc: 0.827000; val_acc: 0.819444\n",
      "(Iteration 1 / 200) loss: 2.311026\n",
      "(Epoch 0 / 20) train acc: 0.169000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.265000; val_acc: 0.258333\n",
      "(Iteration 11 / 200) loss: 2.307047\n",
      "(Epoch 2 / 20) train acc: 0.378000; val_acc: 0.352778\n",
      "(Iteration 21 / 200) loss: 2.302445\n",
      "(Epoch 3 / 20) train acc: 0.400000; val_acc: 0.363889\n",
      "(Iteration 31 / 200) loss: 2.297201\n",
      "(Epoch 4 / 20) train acc: 0.441000; val_acc: 0.427778\n",
      "(Iteration 41 / 200) loss: 2.289382\n",
      "(Epoch 5 / 20) train acc: 0.494000; val_acc: 0.475000\n",
      "(Iteration 51 / 200) loss: 2.275788\n",
      "(Epoch 6 / 20) train acc: 0.551000; val_acc: 0.550000\n",
      "(Iteration 61 / 200) loss: 2.255182\n",
      "(Epoch 7 / 20) train acc: 0.483000; val_acc: 0.433333\n",
      "(Iteration 71 / 200) loss: 2.231326\n",
      "(Epoch 8 / 20) train acc: 0.541000; val_acc: 0.500000\n",
      "(Iteration 81 / 200) loss: 2.181543\n",
      "(Epoch 9 / 20) train acc: 0.511000; val_acc: 0.522222\n",
      "(Iteration 91 / 200) loss: 2.126195\n",
      "(Epoch 10 / 20) train acc: 0.505000; val_acc: 0.522222\n",
      "(Iteration 101 / 200) loss: 2.048599\n",
      "(Epoch 11 / 20) train acc: 0.574000; val_acc: 0.538889\n",
      "(Iteration 111 / 200) loss: 1.977992\n",
      "(Epoch 12 / 20) train acc: 0.581000; val_acc: 0.566667\n",
      "(Iteration 121 / 200) loss: 1.859484\n",
      "(Epoch 13 / 20) train acc: 0.578000; val_acc: 0.572222\n",
      "(Iteration 131 / 200) loss: 1.710568\n",
      "(Epoch 14 / 20) train acc: 0.580000; val_acc: 0.608333\n",
      "(Iteration 141 / 200) loss: 1.650169\n",
      "(Epoch 15 / 20) train acc: 0.665000; val_acc: 0.666667\n",
      "(Iteration 151 / 200) loss: 1.499459\n",
      "(Epoch 16 / 20) train acc: 0.668000; val_acc: 0.713889\n",
      "(Iteration 161 / 200) loss: 1.454030\n",
      "(Epoch 17 / 20) train acc: 0.711000; val_acc: 0.769444\n",
      "(Iteration 171 / 200) loss: 1.279388\n",
      "(Epoch 18 / 20) train acc: 0.755000; val_acc: 0.780556\n",
      "(Iteration 181 / 200) loss: 1.212846\n",
      "(Epoch 19 / 20) train acc: 0.800000; val_acc: 0.805556\n",
      "(Iteration 191 / 200) loss: 1.137280\n",
      "(Epoch 20 / 20) train acc: 0.794000; val_acc: 0.816667\n",
      "(Iteration 1 / 200) loss: 2.302668\n",
      "(Epoch 0 / 20) train acc: 0.186000; val_acc: 0.200000\n",
      "(Epoch 1 / 20) train acc: 0.153000; val_acc: 0.205556\n",
      "(Iteration 11 / 200) loss: 2.302578\n",
      "(Epoch 2 / 20) train acc: 0.219000; val_acc: 0.194444\n",
      "(Iteration 21 / 200) loss: 2.302289\n",
      "(Epoch 3 / 20) train acc: 0.204000; val_acc: 0.205556\n",
      "(Iteration 31 / 200) loss: 2.300948\n",
      "(Epoch 4 / 20) train acc: 0.277000; val_acc: 0.277778\n",
      "(Iteration 41 / 200) loss: 2.296465\n",
      "(Epoch 5 / 20) train acc: 0.259000; val_acc: 0.261111\n",
      "(Iteration 51 / 200) loss: 2.284587\n",
      "(Epoch 6 / 20) train acc: 0.329000; val_acc: 0.297222\n",
      "(Iteration 61 / 200) loss: 2.261354\n",
      "(Epoch 7 / 20) train acc: 0.309000; val_acc: 0.330556\n",
      "(Iteration 71 / 200) loss: 2.223131\n",
      "(Epoch 8 / 20) train acc: 0.329000; val_acc: 0.327778\n",
      "(Iteration 81 / 200) loss: 2.183231\n",
      "(Epoch 9 / 20) train acc: 0.339000; val_acc: 0.344444\n",
      "(Iteration 91 / 200) loss: 2.092676\n",
      "(Epoch 10 / 20) train acc: 0.340000; val_acc: 0.347222\n",
      "(Iteration 101 / 200) loss: 2.016922\n",
      "(Epoch 11 / 20) train acc: 0.304000; val_acc: 0.338889\n",
      "(Iteration 111 / 200) loss: 1.891526\n",
      "(Epoch 12 / 20) train acc: 0.345000; val_acc: 0.363889\n",
      "(Iteration 121 / 200) loss: 1.837515\n",
      "(Epoch 13 / 20) train acc: 0.364000; val_acc: 0.388889\n",
      "(Iteration 131 / 200) loss: 1.828264\n",
      "(Epoch 14 / 20) train acc: 0.378000; val_acc: 0.397222\n",
      "(Iteration 141 / 200) loss: 1.714489\n",
      "(Epoch 15 / 20) train acc: 0.442000; val_acc: 0.416667\n",
      "(Iteration 151 / 200) loss: 1.612572\n",
      "(Epoch 16 / 20) train acc: 0.434000; val_acc: 0.463889\n",
      "(Iteration 161 / 200) loss: 1.502189\n",
      "(Epoch 17 / 20) train acc: 0.480000; val_acc: 0.488889\n",
      "(Iteration 171 / 200) loss: 1.485529\n",
      "(Epoch 18 / 20) train acc: 0.473000; val_acc: 0.488889\n",
      "(Iteration 181 / 200) loss: 1.372660\n",
      "(Epoch 19 / 20) train acc: 0.507000; val_acc: 0.486111\n",
      "(Iteration 191 / 200) loss: 1.335880\n",
      "(Epoch 20 / 20) train acc: 0.504000; val_acc: 0.486111\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302649\n",
      "(Epoch 2 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302635\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302564\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302526\n",
      "(Epoch 5 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.301155\n",
      "(Epoch 6 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.293775\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.122222\n",
      "(Iteration 71 / 200) loss: 2.275355\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.111111\n",
      "(Iteration 81 / 200) loss: 2.261436\n",
      "(Epoch 9 / 20) train acc: 0.140000; val_acc: 0.130556\n",
      "(Iteration 91 / 200) loss: 2.199058\n",
      "(Epoch 10 / 20) train acc: 0.168000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 2.095885\n",
      "(Epoch 11 / 20) train acc: 0.160000; val_acc: 0.150000\n",
      "(Iteration 111 / 200) loss: 2.114400\n",
      "(Epoch 12 / 20) train acc: 0.179000; val_acc: 0.155556\n",
      "(Iteration 121 / 200) loss: 2.074525\n",
      "(Epoch 13 / 20) train acc: 0.202000; val_acc: 0.172222\n",
      "(Iteration 131 / 200) loss: 2.140336\n",
      "(Epoch 14 / 20) train acc: 0.177000; val_acc: 0.197222\n",
      "(Iteration 141 / 200) loss: 1.980518\n",
      "(Epoch 15 / 20) train acc: 0.168000; val_acc: 0.200000\n",
      "(Iteration 151 / 200) loss: 1.965008\n",
      "(Epoch 16 / 20) train acc: 0.207000; val_acc: 0.197222\n",
      "(Iteration 161 / 200) loss: 1.840541\n",
      "(Epoch 17 / 20) train acc: 0.196000; val_acc: 0.211111\n",
      "(Iteration 171 / 200) loss: 2.108777\n",
      "(Epoch 18 / 20) train acc: 0.183000; val_acc: 0.166667\n",
      "(Iteration 181 / 200) loss: 2.017984\n",
      "(Epoch 19 / 20) train acc: 0.208000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 1.949700\n",
      "(Epoch 20 / 20) train acc: 0.189000; val_acc: 0.158333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302569\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302645\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302592\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302580\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302517\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302598\n",
      "(Epoch 7 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302720\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302446\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302635\n",
      "(Epoch 10 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302680\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302424\n",
      "(Epoch 12 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302485\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302682\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.303041\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302877\n",
      "(Epoch 16 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302305\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302384\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.303169\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302665\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 8524.468486\n",
      "(Epoch 0 / 20) train acc: 0.082000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 7236.597788\n",
      "(Epoch 2 / 20) train acc: 0.088000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 7055.367480\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 6652.175990\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 7935.807653\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.094444\n",
      "(Iteration 51 / 200) loss: 6623.600574\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.094444\n",
      "(Iteration 61 / 200) loss: 6856.102265\n",
      "(Epoch 7 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 6258.085836\n",
      "(Epoch 8 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Iteration 81 / 200) loss: 5327.381904\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 5721.259867\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.077778\n",
      "(Iteration 101 / 200) loss: 5589.645345\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.077778\n",
      "(Iteration 111 / 200) loss: 5492.572692\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.075000\n",
      "(Iteration 121 / 200) loss: 5442.046282\n",
      "(Epoch 13 / 20) train acc: 0.091000; val_acc: 0.072222\n",
      "(Iteration 131 / 200) loss: 5510.931773\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.072222\n",
      "(Iteration 141 / 200) loss: 5425.039784\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.072222\n",
      "(Iteration 151 / 200) loss: 5650.028467\n",
      "(Epoch 16 / 20) train acc: 0.095000; val_acc: 0.075000\n",
      "(Iteration 161 / 200) loss: 4753.174059\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.075000\n",
      "(Iteration 171 / 200) loss: 5109.371855\n",
      "(Epoch 18 / 20) train acc: 0.091000; val_acc: 0.072222\n",
      "(Iteration 181 / 200) loss: 4400.871540\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.072222\n",
      "(Iteration 191 / 200) loss: 5094.867795\n",
      "(Epoch 20 / 20) train acc: 0.094000; val_acc: 0.069444\n",
      "(Iteration 1 / 200) loss: 4.293929\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.102778\n",
      "(Iteration 11 / 200) loss: 3.265519\n",
      "(Epoch 2 / 20) train acc: 0.153000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 3.169779\n",
      "(Epoch 3 / 20) train acc: 0.169000; val_acc: 0.136111\n",
      "(Iteration 31 / 200) loss: 2.554842\n",
      "(Epoch 4 / 20) train acc: 0.213000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 2.417519\n",
      "(Epoch 5 / 20) train acc: 0.301000; val_acc: 0.258333\n",
      "(Iteration 51 / 200) loss: 1.992588\n",
      "(Epoch 6 / 20) train acc: 0.365000; val_acc: 0.327778\n",
      "(Iteration 61 / 200) loss: 1.878581\n",
      "(Epoch 7 / 20) train acc: 0.470000; val_acc: 0.394444\n",
      "(Iteration 71 / 200) loss: 1.722601\n",
      "(Epoch 8 / 20) train acc: 0.495000; val_acc: 0.447222\n",
      "(Iteration 81 / 200) loss: 1.386422\n",
      "(Epoch 9 / 20) train acc: 0.587000; val_acc: 0.527778\n",
      "(Iteration 91 / 200) loss: 1.327413\n",
      "(Epoch 10 / 20) train acc: 0.645000; val_acc: 0.613889\n",
      "(Iteration 101 / 200) loss: 1.150029\n",
      "(Epoch 11 / 20) train acc: 0.694000; val_acc: 0.669444\n",
      "(Iteration 111 / 200) loss: 1.183204\n",
      "(Epoch 12 / 20) train acc: 0.744000; val_acc: 0.713889\n",
      "(Iteration 121 / 200) loss: 1.055823\n",
      "(Epoch 13 / 20) train acc: 0.792000; val_acc: 0.769444\n",
      "(Iteration 131 / 200) loss: 0.949097\n",
      "(Epoch 14 / 20) train acc: 0.802000; val_acc: 0.783333\n",
      "(Iteration 141 / 200) loss: 0.792757\n",
      "(Epoch 15 / 20) train acc: 0.849000; val_acc: 0.794444\n",
      "(Iteration 151 / 200) loss: 0.713361\n",
      "(Epoch 16 / 20) train acc: 0.858000; val_acc: 0.813889\n",
      "(Iteration 161 / 200) loss: 0.769117\n",
      "(Epoch 17 / 20) train acc: 0.854000; val_acc: 0.825000\n",
      "(Iteration 171 / 200) loss: 0.602466\n",
      "(Epoch 18 / 20) train acc: 0.841000; val_acc: 0.833333\n",
      "(Iteration 181 / 200) loss: 0.725895\n",
      "(Epoch 19 / 20) train acc: 0.870000; val_acc: 0.841667\n",
      "(Iteration 191 / 200) loss: 0.625082\n",
      "(Epoch 20 / 20) train acc: 0.886000; val_acc: 0.850000\n",
      "(Iteration 1 / 200) loss: 2.303018\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.186000; val_acc: 0.197222\n",
      "(Iteration 11 / 200) loss: 2.301239\n",
      "(Epoch 2 / 20) train acc: 0.403000; val_acc: 0.413889\n",
      "(Iteration 21 / 200) loss: 2.296407\n",
      "(Epoch 3 / 20) train acc: 0.556000; val_acc: 0.530556\n",
      "(Iteration 31 / 200) loss: 2.291670\n",
      "(Epoch 4 / 20) train acc: 0.599000; val_acc: 0.622222\n",
      "(Iteration 41 / 200) loss: 2.281077\n",
      "(Epoch 5 / 20) train acc: 0.689000; val_acc: 0.677778\n",
      "(Iteration 51 / 200) loss: 2.262370\n",
      "(Epoch 6 / 20) train acc: 0.596000; val_acc: 0.616667\n",
      "(Iteration 61 / 200) loss: 2.239363\n",
      "(Epoch 7 / 20) train acc: 0.655000; val_acc: 0.669444\n",
      "(Iteration 71 / 200) loss: 2.208344\n",
      "(Epoch 8 / 20) train acc: 0.669000; val_acc: 0.708333\n",
      "(Iteration 81 / 200) loss: 2.164974\n",
      "(Epoch 9 / 20) train acc: 0.706000; val_acc: 0.697222\n",
      "(Iteration 91 / 200) loss: 2.089066\n",
      "(Epoch 10 / 20) train acc: 0.631000; val_acc: 0.663889\n",
      "(Iteration 101 / 200) loss: 2.002925\n",
      "(Epoch 11 / 20) train acc: 0.602000; val_acc: 0.602778\n",
      "(Iteration 111 / 200) loss: 1.936942\n",
      "(Epoch 12 / 20) train acc: 0.651000; val_acc: 0.672222\n",
      "(Iteration 121 / 200) loss: 1.796916\n",
      "(Epoch 13 / 20) train acc: 0.656000; val_acc: 0.694444\n",
      "(Iteration 131 / 200) loss: 1.670493\n",
      "(Epoch 14 / 20) train acc: 0.721000; val_acc: 0.741667\n",
      "(Iteration 141 / 200) loss: 1.576728\n",
      "(Epoch 15 / 20) train acc: 0.761000; val_acc: 0.755556\n",
      "(Iteration 151 / 200) loss: 1.517530\n",
      "(Epoch 16 / 20) train acc: 0.731000; val_acc: 0.786111\n",
      "(Iteration 161 / 200) loss: 1.300435\n",
      "(Epoch 17 / 20) train acc: 0.745000; val_acc: 0.744444\n",
      "(Iteration 171 / 200) loss: 1.309828\n",
      "(Epoch 18 / 20) train acc: 0.758000; val_acc: 0.775000\n",
      "(Iteration 181 / 200) loss: 1.201681\n",
      "(Epoch 19 / 20) train acc: 0.791000; val_acc: 0.813889\n",
      "(Iteration 191 / 200) loss: 1.013656\n",
      "(Epoch 20 / 20) train acc: 0.805000; val_acc: 0.805556\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.134000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302570\n",
      "(Epoch 2 / 20) train acc: 0.183000; val_acc: 0.152778\n",
      "(Iteration 21 / 200) loss: 2.302097\n",
      "(Epoch 3 / 20) train acc: 0.524000; val_acc: 0.466667\n",
      "(Iteration 31 / 200) loss: 2.300238\n",
      "(Epoch 4 / 20) train acc: 0.499000; val_acc: 0.536111\n",
      "(Iteration 41 / 200) loss: 2.295771\n",
      "(Epoch 5 / 20) train acc: 0.531000; val_acc: 0.530556\n",
      "(Iteration 51 / 200) loss: 2.280946\n",
      "(Epoch 6 / 20) train acc: 0.507000; val_acc: 0.480556\n",
      "(Iteration 61 / 200) loss: 2.260940\n",
      "(Epoch 7 / 20) train acc: 0.507000; val_acc: 0.522222\n",
      "(Iteration 71 / 200) loss: 2.217930\n",
      "(Epoch 8 / 20) train acc: 0.458000; val_acc: 0.433333\n",
      "(Iteration 81 / 200) loss: 2.148971\n",
      "(Epoch 9 / 20) train acc: 0.349000; val_acc: 0.352778\n",
      "(Iteration 91 / 200) loss: 2.066207\n",
      "(Epoch 10 / 20) train acc: 0.393000; val_acc: 0.361111\n",
      "(Iteration 101 / 200) loss: 2.002236\n",
      "(Epoch 11 / 20) train acc: 0.466000; val_acc: 0.458333\n",
      "(Iteration 111 / 200) loss: 1.870939\n",
      "(Epoch 12 / 20) train acc: 0.502000; val_acc: 0.477778\n",
      "(Iteration 121 / 200) loss: 1.755101\n",
      "(Epoch 13 / 20) train acc: 0.423000; val_acc: 0.458333\n",
      "(Iteration 131 / 200) loss: 1.751789\n",
      "(Epoch 14 / 20) train acc: 0.440000; val_acc: 0.450000\n",
      "(Iteration 141 / 200) loss: 1.628655\n",
      "(Epoch 15 / 20) train acc: 0.486000; val_acc: 0.513889\n",
      "(Iteration 151 / 200) loss: 1.534096\n",
      "(Epoch 16 / 20) train acc: 0.539000; val_acc: 0.544444\n",
      "(Iteration 161 / 200) loss: 1.535193\n",
      "(Epoch 17 / 20) train acc: 0.567000; val_acc: 0.555556\n",
      "(Iteration 171 / 200) loss: 1.425137\n",
      "(Epoch 18 / 20) train acc: 0.586000; val_acc: 0.572222\n",
      "(Iteration 181 / 200) loss: 1.446219\n",
      "(Epoch 19 / 20) train acc: 0.587000; val_acc: 0.602778\n",
      "(Iteration 191 / 200) loss: 1.393607\n",
      "(Epoch 20 / 20) train acc: 0.590000; val_acc: 0.608333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302523\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302360\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.300358\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.296953\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.285427\n",
      "(Epoch 6 / 20) train acc: 0.153000; val_acc: 0.130556\n",
      "(Iteration 61 / 200) loss: 2.272257\n",
      "(Epoch 7 / 20) train acc: 0.128000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.240204\n",
      "(Epoch 8 / 20) train acc: 0.163000; val_acc: 0.150000\n",
      "(Iteration 81 / 200) loss: 2.211589\n",
      "(Epoch 9 / 20) train acc: 0.176000; val_acc: 0.169444\n",
      "(Iteration 91 / 200) loss: 2.171564\n",
      "(Epoch 10 / 20) train acc: 0.192000; val_acc: 0.175000\n",
      "(Iteration 101 / 200) loss: 2.111937\n",
      "(Epoch 11 / 20) train acc: 0.208000; val_acc: 0.172222\n",
      "(Iteration 111 / 200) loss: 2.106945\n",
      "(Epoch 12 / 20) train acc: 0.199000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.066124\n",
      "(Epoch 13 / 20) train acc: 0.213000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 2.077092\n",
      "(Epoch 14 / 20) train acc: 0.206000; val_acc: 0.183333\n",
      "(Iteration 141 / 200) loss: 2.083163\n",
      "(Epoch 15 / 20) train acc: 0.196000; val_acc: 0.186111\n",
      "(Iteration 151 / 200) loss: 2.059080\n",
      "(Epoch 16 / 20) train acc: 0.228000; val_acc: 0.186111\n",
      "(Iteration 161 / 200) loss: 2.109525\n",
      "(Epoch 17 / 20) train acc: 0.213000; val_acc: 0.186111\n",
      "(Iteration 171 / 200) loss: 2.050982\n",
      "(Epoch 18 / 20) train acc: 0.202000; val_acc: 0.186111\n",
      "(Iteration 181 / 200) loss: 2.046591\n",
      "(Epoch 19 / 20) train acc: 0.174000; val_acc: 0.200000\n",
      "(Iteration 191 / 200) loss: 1.947643\n",
      "(Epoch 20 / 20) train acc: 0.209000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302557\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302526\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301918\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.299472\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.294838\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.278853\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.260902\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.227239\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.162954\n",
      "(Epoch 10 / 20) train acc: 0.083000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.125165\n",
      "(Epoch 11 / 20) train acc: 0.198000; val_acc: 0.166667\n",
      "(Iteration 111 / 200) loss: 2.159908\n",
      "(Epoch 12 / 20) train acc: 0.156000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.064845\n",
      "(Epoch 13 / 20) train acc: 0.198000; val_acc: 0.183333\n",
      "(Iteration 131 / 200) loss: 2.098614\n",
      "(Epoch 14 / 20) train acc: 0.199000; val_acc: 0.197222\n",
      "(Iteration 141 / 200) loss: 2.008013\n",
      "(Epoch 15 / 20) train acc: 0.213000; val_acc: 0.191667\n",
      "(Iteration 151 / 200) loss: 1.980775\n",
      "(Epoch 16 / 20) train acc: 0.225000; val_acc: 0.194444\n",
      "(Iteration 161 / 200) loss: 2.024287\n",
      "(Epoch 17 / 20) train acc: 0.199000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 2.085836\n",
      "(Epoch 18 / 20) train acc: 0.213000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 2.018977\n",
      "(Epoch 19 / 20) train acc: 0.190000; val_acc: 0.194444\n",
      "(Iteration 191 / 200) loss: 1.961520\n",
      "(Epoch 20 / 20) train acc: 0.189000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 4931.984847\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.113889\n",
      "(Iteration 11 / 200) loss: 4799.658541\n",
      "(Epoch 2 / 20) train acc: 0.140000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 5299.201017\n",
      "(Epoch 3 / 20) train acc: 0.119000; val_acc: 0.113889\n",
      "(Iteration 31 / 200) loss: 4194.248290\n",
      "(Epoch 4 / 20) train acc: 0.143000; val_acc: 0.111111\n",
      "(Iteration 41 / 200) loss: 4239.503096\n",
      "(Epoch 5 / 20) train acc: 0.123000; val_acc: 0.113889\n",
      "(Iteration 51 / 200) loss: 4981.658855\n",
      "(Epoch 6 / 20) train acc: 0.122000; val_acc: 0.113889\n",
      "(Iteration 61 / 200) loss: 3996.363393\n",
      "(Epoch 7 / 20) train acc: 0.118000; val_acc: 0.111111\n",
      "(Iteration 71 / 200) loss: 4144.305751\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.108333\n",
      "(Iteration 81 / 200) loss: 3862.303436\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.111111\n",
      "(Iteration 91 / 200) loss: 4004.637078\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.105556\n",
      "(Iteration 101 / 200) loss: 3893.839772\n",
      "(Epoch 11 / 20) train acc: 0.121000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 3848.401545\n",
      "(Epoch 12 / 20) train acc: 0.127000; val_acc: 0.102778\n",
      "(Iteration 121 / 200) loss: 3784.508001\n",
      "(Epoch 13 / 20) train acc: 0.125000; val_acc: 0.102778\n",
      "(Iteration 131 / 200) loss: 3974.644443\n",
      "(Epoch 14 / 20) train acc: 0.132000; val_acc: 0.102778\n",
      "(Iteration 141 / 200) loss: 3652.550914\n",
      "(Epoch 15 / 20) train acc: 0.129000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 3324.725824\n",
      "(Epoch 16 / 20) train acc: 0.151000; val_acc: 0.111111\n",
      "(Iteration 161 / 200) loss: 3196.662923\n",
      "(Epoch 17 / 20) train acc: 0.154000; val_acc: 0.111111\n",
      "(Iteration 171 / 200) loss: 2826.220640\n",
      "(Epoch 18 / 20) train acc: 0.131000; val_acc: 0.111111\n",
      "(Iteration 181 / 200) loss: 2965.434293\n",
      "(Epoch 19 / 20) train acc: 0.142000; val_acc: 0.111111\n",
      "(Iteration 191 / 200) loss: 3066.168913\n",
      "(Epoch 20 / 20) train acc: 0.138000; val_acc: 0.111111\n",
      "(Iteration 1 / 200) loss: 4.973364\n",
      "(Epoch 0 / 20) train acc: 0.195000; val_acc: 0.186111\n",
      "(Epoch 1 / 20) train acc: 0.239000; val_acc: 0.200000\n",
      "(Iteration 11 / 200) loss: 4.135292\n",
      "(Epoch 2 / 20) train acc: 0.281000; val_acc: 0.255556\n",
      "(Iteration 21 / 200) loss: 3.114234\n",
      "(Epoch 3 / 20) train acc: 0.300000; val_acc: 0.280556\n",
      "(Iteration 31 / 200) loss: 2.504567\n",
      "(Epoch 4 / 20) train acc: 0.330000; val_acc: 0.322222\n",
      "(Iteration 41 / 200) loss: 2.493364\n",
      "(Epoch 5 / 20) train acc: 0.371000; val_acc: 0.372222\n",
      "(Iteration 51 / 200) loss: 1.996527\n",
      "(Epoch 6 / 20) train acc: 0.454000; val_acc: 0.419444\n",
      "(Iteration 61 / 200) loss: 1.670227\n",
      "(Epoch 7 / 20) train acc: 0.520000; val_acc: 0.486111\n",
      "(Iteration 71 / 200) loss: 1.616507\n",
      "(Epoch 8 / 20) train acc: 0.589000; val_acc: 0.541667\n",
      "(Iteration 81 / 200) loss: 1.585014\n",
      "(Epoch 9 / 20) train acc: 0.656000; val_acc: 0.605556\n",
      "(Iteration 91 / 200) loss: 1.333545\n",
      "(Epoch 10 / 20) train acc: 0.693000; val_acc: 0.650000\n",
      "(Iteration 101 / 200) loss: 1.115598\n",
      "(Epoch 11 / 20) train acc: 0.760000; val_acc: 0.708333\n",
      "(Iteration 111 / 200) loss: 0.989552\n",
      "(Epoch 12 / 20) train acc: 0.770000; val_acc: 0.727778\n",
      "(Iteration 121 / 200) loss: 0.868972\n",
      "(Epoch 13 / 20) train acc: 0.790000; val_acc: 0.761111\n",
      "(Iteration 131 / 200) loss: 0.847986\n",
      "(Epoch 14 / 20) train acc: 0.803000; val_acc: 0.766667\n",
      "(Iteration 141 / 200) loss: 0.773452\n",
      "(Epoch 15 / 20) train acc: 0.834000; val_acc: 0.777778\n",
      "(Iteration 151 / 200) loss: 0.734292\n",
      "(Epoch 16 / 20) train acc: 0.867000; val_acc: 0.805556\n",
      "(Iteration 161 / 200) loss: 0.696825\n",
      "(Epoch 17 / 20) train acc: 0.871000; val_acc: 0.808333\n",
      "(Iteration 171 / 200) loss: 0.634174\n",
      "(Epoch 18 / 20) train acc: 0.869000; val_acc: 0.830556\n",
      "(Iteration 181 / 200) loss: 0.580686\n",
      "(Epoch 19 / 20) train acc: 0.887000; val_acc: 0.836111\n",
      "(Iteration 191 / 200) loss: 0.540510\n",
      "(Epoch 20 / 20) train acc: 0.896000; val_acc: 0.844444\n",
      "(Iteration 1 / 200) loss: 2.304052\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.445000; val_acc: 0.444444\n",
      "(Iteration 11 / 200) loss: 2.300061\n",
      "(Epoch 2 / 20) train acc: 0.537000; val_acc: 0.555556\n",
      "(Iteration 21 / 200) loss: 2.295866\n",
      "(Epoch 3 / 20) train acc: 0.500000; val_acc: 0.541667\n",
      "(Iteration 31 / 200) loss: 2.290099\n",
      "(Epoch 4 / 20) train acc: 0.499000; val_acc: 0.561111\n",
      "(Iteration 41 / 200) loss: 2.278158\n",
      "(Epoch 5 / 20) train acc: 0.544000; val_acc: 0.577778\n",
      "(Iteration 51 / 200) loss: 2.264771\n",
      "(Epoch 6 / 20) train acc: 0.528000; val_acc: 0.580556\n",
      "(Iteration 61 / 200) loss: 2.242160\n",
      "(Epoch 7 / 20) train acc: 0.550000; val_acc: 0.588889\n",
      "(Iteration 71 / 200) loss: 2.209795\n",
      "(Epoch 8 / 20) train acc: 0.573000; val_acc: 0.575000\n",
      "(Iteration 81 / 200) loss: 2.159532\n",
      "(Epoch 9 / 20) train acc: 0.599000; val_acc: 0.613889\n",
      "(Iteration 91 / 200) loss: 2.092550\n",
      "(Epoch 10 / 20) train acc: 0.594000; val_acc: 0.611111\n",
      "(Iteration 101 / 200) loss: 2.051731\n",
      "(Epoch 11 / 20) train acc: 0.574000; val_acc: 0.594444\n",
      "(Iteration 111 / 200) loss: 1.940322\n",
      "(Epoch 12 / 20) train acc: 0.555000; val_acc: 0.594444\n",
      "(Iteration 121 / 200) loss: 1.812692\n",
      "(Epoch 13 / 20) train acc: 0.641000; val_acc: 0.633333\n",
      "(Iteration 131 / 200) loss: 1.702463\n",
      "(Epoch 14 / 20) train acc: 0.680000; val_acc: 0.683333\n",
      "(Iteration 141 / 200) loss: 1.624591\n",
      "(Epoch 15 / 20) train acc: 0.740000; val_acc: 0.733333\n",
      "(Iteration 151 / 200) loss: 1.523931\n",
      "(Epoch 16 / 20) train acc: 0.746000; val_acc: 0.755556\n",
      "(Iteration 161 / 200) loss: 1.394129\n",
      "(Epoch 17 / 20) train acc: 0.734000; val_acc: 0.738889\n",
      "(Iteration 171 / 200) loss: 1.304307\n",
      "(Epoch 18 / 20) train acc: 0.760000; val_acc: 0.761111\n",
      "(Iteration 181 / 200) loss: 1.187331\n",
      "(Epoch 19 / 20) train acc: 0.801000; val_acc: 0.763889\n",
      "(Iteration 191 / 200) loss: 1.132515\n",
      "(Epoch 20 / 20) train acc: 0.801000; val_acc: 0.783333\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.266000; val_acc: 0.247222\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302483\n",
      "(Epoch 2 / 20) train acc: 0.298000; val_acc: 0.294444\n",
      "(Iteration 21 / 200) loss: 2.301846\n",
      "(Epoch 3 / 20) train acc: 0.252000; val_acc: 0.236111\n",
      "(Iteration 31 / 200) loss: 2.300456\n",
      "(Epoch 4 / 20) train acc: 0.234000; val_acc: 0.219444\n",
      "(Iteration 41 / 200) loss: 2.295468\n",
      "(Epoch 5 / 20) train acc: 0.254000; val_acc: 0.252778\n",
      "(Iteration 51 / 200) loss: 2.284159\n",
      "(Epoch 6 / 20) train acc: 0.253000; val_acc: 0.275000\n",
      "(Iteration 61 / 200) loss: 2.262283\n",
      "(Epoch 7 / 20) train acc: 0.313000; val_acc: 0.313889\n",
      "(Iteration 71 / 200) loss: 2.205673\n",
      "(Epoch 8 / 20) train acc: 0.300000; val_acc: 0.300000\n",
      "(Iteration 81 / 200) loss: 2.158041\n",
      "(Epoch 9 / 20) train acc: 0.297000; val_acc: 0.333333\n",
      "(Iteration 91 / 200) loss: 2.070703\n",
      "(Epoch 10 / 20) train acc: 0.403000; val_acc: 0.405556\n",
      "(Iteration 101 / 200) loss: 1.992874\n",
      "(Epoch 11 / 20) train acc: 0.385000; val_acc: 0.425000\n",
      "(Iteration 111 / 200) loss: 1.903588\n",
      "(Epoch 12 / 20) train acc: 0.422000; val_acc: 0.430556\n",
      "(Iteration 121 / 200) loss: 1.802371\n",
      "(Epoch 13 / 20) train acc: 0.466000; val_acc: 0.486111\n",
      "(Iteration 131 / 200) loss: 1.730406\n",
      "(Epoch 14 / 20) train acc: 0.449000; val_acc: 0.441667\n",
      "(Iteration 141 / 200) loss: 1.647278\n",
      "(Epoch 15 / 20) train acc: 0.468000; val_acc: 0.472222\n",
      "(Iteration 151 / 200) loss: 1.566288\n",
      "(Epoch 16 / 20) train acc: 0.491000; val_acc: 0.511111\n",
      "(Iteration 161 / 200) loss: 1.448275\n",
      "(Epoch 17 / 20) train acc: 0.507000; val_acc: 0.516667\n",
      "(Iteration 171 / 200) loss: 1.335314\n",
      "(Epoch 18 / 20) train acc: 0.552000; val_acc: 0.536111\n",
      "(Iteration 181 / 200) loss: 1.305782\n",
      "(Epoch 19 / 20) train acc: 0.589000; val_acc: 0.591667\n",
      "(Iteration 191 / 200) loss: 1.196398\n",
      "(Epoch 20 / 20) train acc: 0.601000; val_acc: 0.627778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302602\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302395\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.111111\n",
      "(Iteration 31 / 200) loss: 2.300978\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.102778\n",
      "(Iteration 41 / 200) loss: 2.297935\n",
      "(Epoch 5 / 20) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.285393\n",
      "(Epoch 6 / 20) train acc: 0.089000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.268661\n",
      "(Epoch 7 / 20) train acc: 0.121000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 2.238561\n",
      "(Epoch 8 / 20) train acc: 0.088000; val_acc: 0.105556\n",
      "(Iteration 81 / 200) loss: 2.207232\n",
      "(Epoch 9 / 20) train acc: 0.131000; val_acc: 0.125000\n",
      "(Iteration 91 / 200) loss: 2.172581\n",
      "(Epoch 10 / 20) train acc: 0.154000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 2.114664\n",
      "(Epoch 11 / 20) train acc: 0.161000; val_acc: 0.147222\n",
      "(Iteration 111 / 200) loss: 2.090490\n",
      "(Epoch 12 / 20) train acc: 0.182000; val_acc: 0.188889\n",
      "(Iteration 121 / 200) loss: 2.093094\n",
      "(Epoch 13 / 20) train acc: 0.149000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 2.104139\n",
      "(Epoch 14 / 20) train acc: 0.211000; val_acc: 0.172222\n",
      "(Iteration 141 / 200) loss: 2.038156\n",
      "(Epoch 15 / 20) train acc: 0.152000; val_acc: 0.188889\n",
      "(Iteration 151 / 200) loss: 2.058066\n",
      "(Epoch 16 / 20) train acc: 0.219000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 2.031809\n",
      "(Epoch 17 / 20) train acc: 0.185000; val_acc: 0.169444\n",
      "(Iteration 171 / 200) loss: 2.022047\n",
      "(Epoch 18 / 20) train acc: 0.178000; val_acc: 0.169444\n",
      "(Iteration 181 / 200) loss: 2.045203\n",
      "(Epoch 19 / 20) train acc: 0.192000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 2.127606\n",
      "(Epoch 20 / 20) train acc: 0.204000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302559\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302406\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.301698\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.298324\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.288267\n",
      "(Epoch 6 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.268541\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.238972\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.203410\n",
      "(Epoch 9 / 20) train acc: 0.128000; val_acc: 0.136111\n",
      "(Iteration 91 / 200) loss: 2.155136\n",
      "(Epoch 10 / 20) train acc: 0.125000; val_acc: 0.141667\n",
      "(Iteration 101 / 200) loss: 2.116949\n",
      "(Epoch 11 / 20) train acc: 0.149000; val_acc: 0.133333\n",
      "(Iteration 111 / 200) loss: 2.100869\n",
      "(Epoch 12 / 20) train acc: 0.174000; val_acc: 0.147222\n",
      "(Iteration 121 / 200) loss: 2.001709\n",
      "(Epoch 13 / 20) train acc: 0.190000; val_acc: 0.152778\n",
      "(Iteration 131 / 200) loss: 2.088948\n",
      "(Epoch 14 / 20) train acc: 0.177000; val_acc: 0.144444\n",
      "(Iteration 141 / 200) loss: 2.110731\n",
      "(Epoch 15 / 20) train acc: 0.166000; val_acc: 0.186111\n",
      "(Iteration 151 / 200) loss: 2.031387\n",
      "(Epoch 16 / 20) train acc: 0.195000; val_acc: 0.186111\n",
      "(Iteration 161 / 200) loss: 1.976010\n",
      "(Epoch 17 / 20) train acc: 0.180000; val_acc: 0.211111\n",
      "(Iteration 171 / 200) loss: 1.924782\n",
      "(Epoch 18 / 20) train acc: 0.187000; val_acc: 0.166667\n",
      "(Iteration 181 / 200) loss: 2.004757\n",
      "(Epoch 19 / 20) train acc: 0.207000; val_acc: 0.166667\n",
      "(Iteration 191 / 200) loss: 1.962877\n",
      "(Epoch 20 / 20) train acc: 0.201000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 9854.795287\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.161000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 7613.135460\n",
      "(Epoch 2 / 20) train acc: 0.148000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 7529.834748\n",
      "(Epoch 3 / 20) train acc: 0.145000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 8544.051655\n",
      "(Epoch 4 / 20) train acc: 0.141000; val_acc: 0.177778\n",
      "(Iteration 41 / 200) loss: 6616.318004\n",
      "(Epoch 5 / 20) train acc: 0.148000; val_acc: 0.177778\n",
      "(Iteration 51 / 200) loss: 8070.647510\n",
      "(Epoch 6 / 20) train acc: 0.152000; val_acc: 0.177778\n",
      "(Iteration 61 / 200) loss: 7668.492645\n",
      "(Epoch 7 / 20) train acc: 0.158000; val_acc: 0.183333\n",
      "(Iteration 71 / 200) loss: 7250.961521\n",
      "(Epoch 8 / 20) train acc: 0.142000; val_acc: 0.180556\n",
      "(Iteration 81 / 200) loss: 7796.531717\n",
      "(Epoch 9 / 20) train acc: 0.153000; val_acc: 0.177778\n",
      "(Iteration 91 / 200) loss: 7605.268791\n",
      "(Epoch 10 / 20) train acc: 0.163000; val_acc: 0.177778\n",
      "(Iteration 101 / 200) loss: 7435.864006\n",
      "(Epoch 11 / 20) train acc: 0.139000; val_acc: 0.177778\n",
      "(Iteration 111 / 200) loss: 6581.082986\n",
      "(Epoch 12 / 20) train acc: 0.162000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 6519.176330\n",
      "(Epoch 13 / 20) train acc: 0.167000; val_acc: 0.175000\n",
      "(Iteration 131 / 200) loss: 6977.887804\n",
      "(Epoch 14 / 20) train acc: 0.145000; val_acc: 0.175000\n",
      "(Iteration 141 / 200) loss: 6384.048672\n",
      "(Epoch 15 / 20) train acc: 0.173000; val_acc: 0.175000\n",
      "(Iteration 151 / 200) loss: 7012.752669\n",
      "(Epoch 16 / 20) train acc: 0.159000; val_acc: 0.172222\n",
      "(Iteration 161 / 200) loss: 5578.395430\n",
      "(Epoch 17 / 20) train acc: 0.152000; val_acc: 0.172222\n",
      "(Iteration 171 / 200) loss: 6669.648844\n",
      "(Epoch 18 / 20) train acc: 0.156000; val_acc: 0.172222\n",
      "(Iteration 181 / 200) loss: 6159.397898\n",
      "(Epoch 19 / 20) train acc: 0.128000; val_acc: 0.175000\n",
      "(Iteration 191 / 200) loss: 5445.150106\n",
      "(Epoch 20 / 20) train acc: 0.142000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 4.555254\n",
      "(Epoch 0 / 20) train acc: 0.065000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.078000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 3.689107\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 3.368034\n",
      "(Epoch 3 / 20) train acc: 0.154000; val_acc: 0.130556\n",
      "(Iteration 31 / 200) loss: 3.047478\n",
      "(Epoch 4 / 20) train acc: 0.178000; val_acc: 0.166667\n",
      "(Iteration 41 / 200) loss: 2.798303\n",
      "(Epoch 5 / 20) train acc: 0.219000; val_acc: 0.191667\n",
      "(Iteration 51 / 200) loss: 2.315566\n",
      "(Epoch 6 / 20) train acc: 0.317000; val_acc: 0.288889\n",
      "(Iteration 61 / 200) loss: 2.291868\n",
      "(Epoch 7 / 20) train acc: 0.364000; val_acc: 0.402778\n",
      "(Iteration 71 / 200) loss: 2.010550\n",
      "(Epoch 8 / 20) train acc: 0.468000; val_acc: 0.488889\n",
      "(Iteration 81 / 200) loss: 1.768791\n",
      "(Epoch 9 / 20) train acc: 0.554000; val_acc: 0.544444\n",
      "(Iteration 91 / 200) loss: 1.396862\n",
      "(Epoch 10 / 20) train acc: 0.620000; val_acc: 0.597222\n",
      "(Iteration 101 / 200) loss: 1.460062\n",
      "(Epoch 11 / 20) train acc: 0.673000; val_acc: 0.647222\n",
      "(Iteration 111 / 200) loss: 1.302270\n",
      "(Epoch 12 / 20) train acc: 0.756000; val_acc: 0.700000\n",
      "(Iteration 121 / 200) loss: 1.158165\n",
      "(Epoch 13 / 20) train acc: 0.751000; val_acc: 0.738889\n",
      "(Iteration 131 / 200) loss: 1.102432\n",
      "(Epoch 14 / 20) train acc: 0.798000; val_acc: 0.763889\n",
      "(Iteration 141 / 200) loss: 0.888880\n",
      "(Epoch 15 / 20) train acc: 0.821000; val_acc: 0.780556\n",
      "(Iteration 151 / 200) loss: 0.785955\n",
      "(Epoch 16 / 20) train acc: 0.847000; val_acc: 0.805556\n",
      "(Iteration 161 / 200) loss: 0.842877\n",
      "(Epoch 17 / 20) train acc: 0.835000; val_acc: 0.830556\n",
      "(Iteration 171 / 200) loss: 0.793116\n",
      "(Epoch 18 / 20) train acc: 0.864000; val_acc: 0.838889\n",
      "(Iteration 181 / 200) loss: 0.633891\n",
      "(Epoch 19 / 20) train acc: 0.879000; val_acc: 0.841667\n",
      "(Iteration 191 / 200) loss: 0.654668\n",
      "(Epoch 20 / 20) train acc: 0.902000; val_acc: 0.847222\n",
      "(Iteration 1 / 200) loss: 2.304021\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.189000; val_acc: 0.158333\n",
      "(Iteration 11 / 200) loss: 2.300130\n",
      "(Epoch 2 / 20) train acc: 0.412000; val_acc: 0.336111\n",
      "(Iteration 21 / 200) loss: 2.295457\n",
      "(Epoch 3 / 20) train acc: 0.521000; val_acc: 0.447222\n",
      "(Iteration 31 / 200) loss: 2.289796\n",
      "(Epoch 4 / 20) train acc: 0.559000; val_acc: 0.494444\n",
      "(Iteration 41 / 200) loss: 2.276041\n",
      "(Epoch 5 / 20) train acc: 0.558000; val_acc: 0.511111\n",
      "(Iteration 51 / 200) loss: 2.264890\n",
      "(Epoch 6 / 20) train acc: 0.576000; val_acc: 0.566667\n",
      "(Iteration 61 / 200) loss: 2.237168\n",
      "(Epoch 7 / 20) train acc: 0.631000; val_acc: 0.605556\n",
      "(Iteration 71 / 200) loss: 2.207873\n",
      "(Epoch 8 / 20) train acc: 0.654000; val_acc: 0.625000\n",
      "(Iteration 81 / 200) loss: 2.152645\n",
      "(Epoch 9 / 20) train acc: 0.625000; val_acc: 0.591667\n",
      "(Iteration 91 / 200) loss: 2.104130\n",
      "(Epoch 10 / 20) train acc: 0.664000; val_acc: 0.666667\n",
      "(Iteration 101 / 200) loss: 1.995526\n",
      "(Epoch 11 / 20) train acc: 0.744000; val_acc: 0.719444\n",
      "(Iteration 111 / 200) loss: 1.920512\n",
      "(Epoch 12 / 20) train acc: 0.755000; val_acc: 0.733333\n",
      "(Iteration 121 / 200) loss: 1.798597\n",
      "(Epoch 13 / 20) train acc: 0.751000; val_acc: 0.755556\n",
      "(Iteration 131 / 200) loss: 1.656397\n",
      "(Epoch 14 / 20) train acc: 0.752000; val_acc: 0.747222\n",
      "(Iteration 141 / 200) loss: 1.611294\n",
      "(Epoch 15 / 20) train acc: 0.761000; val_acc: 0.763889\n",
      "(Iteration 151 / 200) loss: 1.433154\n",
      "(Epoch 16 / 20) train acc: 0.756000; val_acc: 0.755556\n",
      "(Iteration 161 / 200) loss: 1.395185\n",
      "(Epoch 17 / 20) train acc: 0.745000; val_acc: 0.783333\n",
      "(Iteration 171 / 200) loss: 1.227016\n",
      "(Epoch 18 / 20) train acc: 0.814000; val_acc: 0.813889\n",
      "(Iteration 181 / 200) loss: 1.264816\n",
      "(Epoch 19 / 20) train acc: 0.793000; val_acc: 0.825000\n",
      "(Iteration 191 / 200) loss: 1.036701\n",
      "(Epoch 20 / 20) train acc: 0.832000; val_acc: 0.825000\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302527\n",
      "(Epoch 2 / 20) train acc: 0.242000; val_acc: 0.211111\n",
      "(Iteration 21 / 200) loss: 2.301995\n",
      "(Epoch 3 / 20) train acc: 0.254000; val_acc: 0.236111\n",
      "(Iteration 31 / 200) loss: 2.300322\n",
      "(Epoch 4 / 20) train acc: 0.188000; val_acc: 0.211111\n",
      "(Iteration 41 / 200) loss: 2.294753\n",
      "(Epoch 5 / 20) train acc: 0.202000; val_acc: 0.186111\n",
      "(Iteration 51 / 200) loss: 2.280377\n",
      "(Epoch 6 / 20) train acc: 0.186000; val_acc: 0.166667\n",
      "(Iteration 61 / 200) loss: 2.242085\n",
      "(Epoch 7 / 20) train acc: 0.173000; val_acc: 0.169444\n",
      "(Iteration 71 / 200) loss: 2.202821\n",
      "(Epoch 8 / 20) train acc: 0.219000; val_acc: 0.222222\n",
      "(Iteration 81 / 200) loss: 2.129725\n",
      "(Epoch 9 / 20) train acc: 0.261000; val_acc: 0.219444\n",
      "(Iteration 91 / 200) loss: 2.071381\n",
      "(Epoch 10 / 20) train acc: 0.244000; val_acc: 0.233333\n",
      "(Iteration 101 / 200) loss: 2.013793\n",
      "(Epoch 11 / 20) train acc: 0.239000; val_acc: 0.230556\n",
      "(Iteration 111 / 200) loss: 1.911138\n",
      "(Epoch 12 / 20) train acc: 0.238000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 1.854386\n",
      "(Epoch 13 / 20) train acc: 0.291000; val_acc: 0.263889\n",
      "(Iteration 131 / 200) loss: 1.738152\n",
      "(Epoch 14 / 20) train acc: 0.289000; val_acc: 0.316667\n",
      "(Iteration 141 / 200) loss: 1.833933\n",
      "(Epoch 15 / 20) train acc: 0.323000; val_acc: 0.352778\n",
      "(Iteration 151 / 200) loss: 1.729846\n",
      "(Epoch 16 / 20) train acc: 0.357000; val_acc: 0.377778\n",
      "(Iteration 161 / 200) loss: 1.703421\n",
      "(Epoch 17 / 20) train acc: 0.421000; val_acc: 0.383333\n",
      "(Iteration 171 / 200) loss: 1.568421\n",
      "(Epoch 18 / 20) train acc: 0.402000; val_acc: 0.388889\n",
      "(Iteration 181 / 200) loss: 1.597676\n",
      "(Epoch 19 / 20) train acc: 0.470000; val_acc: 0.444444\n",
      "(Iteration 191 / 200) loss: 1.590886\n",
      "(Epoch 20 / 20) train acc: 0.458000; val_acc: 0.402778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302405\n",
      "(Epoch 3 / 20) train acc: 0.291000; val_acc: 0.313889\n",
      "(Iteration 31 / 200) loss: 2.301270\n",
      "(Epoch 4 / 20) train acc: 0.200000; val_acc: 0.230556\n",
      "(Iteration 41 / 200) loss: 2.298834\n",
      "(Epoch 5 / 20) train acc: 0.208000; val_acc: 0.252778\n",
      "(Iteration 51 / 200) loss: 2.293031\n",
      "(Epoch 6 / 20) train acc: 0.198000; val_acc: 0.213889\n",
      "(Iteration 61 / 200) loss: 2.280041\n",
      "(Epoch 7 / 20) train acc: 0.184000; val_acc: 0.216667\n",
      "(Iteration 71 / 200) loss: 2.253391\n",
      "(Epoch 8 / 20) train acc: 0.200000; val_acc: 0.222222\n",
      "(Iteration 81 / 200) loss: 2.224513\n",
      "(Epoch 9 / 20) train acc: 0.168000; val_acc: 0.222222\n",
      "(Iteration 91 / 200) loss: 2.174555\n",
      "(Epoch 10 / 20) train acc: 0.202000; val_acc: 0.222222\n",
      "(Iteration 101 / 200) loss: 2.143778\n",
      "(Epoch 11 / 20) train acc: 0.228000; val_acc: 0.252778\n",
      "(Iteration 111 / 200) loss: 2.114046\n",
      "(Epoch 12 / 20) train acc: 0.261000; val_acc: 0.280556\n",
      "(Iteration 121 / 200) loss: 2.041500\n",
      "(Epoch 13 / 20) train acc: 0.224000; val_acc: 0.238889\n",
      "(Iteration 131 / 200) loss: 1.992228\n",
      "(Epoch 14 / 20) train acc: 0.202000; val_acc: 0.216667\n",
      "(Iteration 141 / 200) loss: 1.862504\n",
      "(Epoch 15 / 20) train acc: 0.204000; val_acc: 0.213889\n",
      "(Iteration 151 / 200) loss: 1.868365\n",
      "(Epoch 16 / 20) train acc: 0.198000; val_acc: 0.222222\n",
      "(Iteration 161 / 200) loss: 1.829054\n",
      "(Epoch 17 / 20) train acc: 0.299000; val_acc: 0.311111\n",
      "(Iteration 171 / 200) loss: 1.756084\n",
      "(Epoch 18 / 20) train acc: 0.361000; val_acc: 0.341667\n",
      "(Iteration 181 / 200) loss: 1.703762\n",
      "(Epoch 19 / 20) train acc: 0.366000; val_acc: 0.372222\n",
      "(Iteration 191 / 200) loss: 1.667604\n",
      "(Epoch 20 / 20) train acc: 0.414000; val_acc: 0.400000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302570\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302406\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301894\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.300353\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.294711\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.286260\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 2.262256\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.127778\n",
      "(Iteration 81 / 200) loss: 2.247055\n",
      "(Epoch 9 / 20) train acc: 0.082000; val_acc: 0.127778\n",
      "(Iteration 91 / 200) loss: 2.222786\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.133333\n",
      "(Iteration 101 / 200) loss: 2.199767\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.138889\n",
      "(Iteration 111 / 200) loss: 2.166422\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.158333\n",
      "(Iteration 121 / 200) loss: 2.169567\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 2.147315\n",
      "(Epoch 14 / 20) train acc: 0.121000; val_acc: 0.163889\n",
      "(Iteration 141 / 200) loss: 2.082966\n",
      "(Epoch 15 / 20) train acc: 0.172000; val_acc: 0.205556\n",
      "(Iteration 151 / 200) loss: 2.121026\n",
      "(Epoch 16 / 20) train acc: 0.188000; val_acc: 0.216667\n",
      "(Iteration 161 / 200) loss: 2.077445\n",
      "(Epoch 17 / 20) train acc: 0.185000; val_acc: 0.222222\n",
      "(Iteration 171 / 200) loss: 2.073204\n",
      "(Epoch 18 / 20) train acc: 0.184000; val_acc: 0.216667\n",
      "(Iteration 181 / 200) loss: 2.087539\n",
      "(Epoch 19 / 20) train acc: 0.189000; val_acc: 0.222222\n",
      "(Iteration 191 / 200) loss: 2.073489\n",
      "(Epoch 20 / 20) train acc: 0.213000; val_acc: 0.225000\n",
      "(Iteration 1 / 200) loss: 4281.197181\n",
      "(Epoch 0 / 20) train acc: 0.022000; val_acc: 0.030556\n",
      "(Epoch 1 / 20) train acc: 0.032000; val_acc: 0.033333\n",
      "(Iteration 11 / 200) loss: 4259.381274\n",
      "(Epoch 2 / 20) train acc: 0.029000; val_acc: 0.036111\n",
      "(Iteration 21 / 200) loss: 3996.006671\n",
      "(Epoch 3 / 20) train acc: 0.042000; val_acc: 0.036111\n",
      "(Iteration 31 / 200) loss: 4037.828332\n",
      "(Epoch 4 / 20) train acc: 0.049000; val_acc: 0.041667\n",
      "(Iteration 41 / 200) loss: 4159.186245\n",
      "(Epoch 5 / 20) train acc: 0.044000; val_acc: 0.044444\n",
      "(Iteration 51 / 200) loss: 4022.402911\n",
      "(Epoch 6 / 20) train acc: 0.042000; val_acc: 0.050000\n",
      "(Iteration 61 / 200) loss: 3682.454270\n",
      "(Epoch 7 / 20) train acc: 0.043000; val_acc: 0.061111\n",
      "(Iteration 71 / 200) loss: 3291.399070\n",
      "(Epoch 8 / 20) train acc: 0.045000; val_acc: 0.061111\n",
      "(Iteration 81 / 200) loss: 3134.725431\n",
      "(Epoch 9 / 20) train acc: 0.044000; val_acc: 0.063889\n",
      "(Iteration 91 / 200) loss: 3545.300231\n",
      "(Epoch 10 / 20) train acc: 0.048000; val_acc: 0.069444\n",
      "(Iteration 101 / 200) loss: 3463.164406\n",
      "(Epoch 11 / 20) train acc: 0.066000; val_acc: 0.072222\n",
      "(Iteration 111 / 200) loss: 2823.780458\n",
      "(Epoch 12 / 20) train acc: 0.076000; val_acc: 0.086111\n",
      "(Iteration 121 / 200) loss: 3100.396198\n",
      "(Epoch 13 / 20) train acc: 0.064000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 2893.307251\n",
      "(Epoch 14 / 20) train acc: 0.075000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2855.532993\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 2839.538111\n",
      "(Epoch 16 / 20) train acc: 0.083000; val_acc: 0.111111\n",
      "(Iteration 161 / 200) loss: 3050.003227\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2642.935844\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.127778\n",
      "(Iteration 181 / 200) loss: 2711.230961\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.130556\n",
      "(Iteration 191 / 200) loss: 2734.492328\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.138889\n",
      "(Iteration 1 / 200) loss: 5.262919\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 4.524517\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 3.743850\n",
      "(Epoch 3 / 20) train acc: 0.114000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 3.155948\n",
      "(Epoch 4 / 20) train acc: 0.136000; val_acc: 0.152778\n",
      "(Iteration 41 / 200) loss: 2.755507\n",
      "(Epoch 5 / 20) train acc: 0.212000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 2.847339\n",
      "(Epoch 6 / 20) train acc: 0.233000; val_acc: 0.250000\n",
      "(Iteration 61 / 200) loss: 2.487811\n",
      "(Epoch 7 / 20) train acc: 0.277000; val_acc: 0.283333\n",
      "(Iteration 71 / 200) loss: 2.174231\n",
      "(Epoch 8 / 20) train acc: 0.351000; val_acc: 0.352778\n",
      "(Iteration 81 / 200) loss: 2.006076\n",
      "(Epoch 9 / 20) train acc: 0.427000; val_acc: 0.405556\n",
      "(Iteration 91 / 200) loss: 1.777511\n",
      "(Epoch 10 / 20) train acc: 0.486000; val_acc: 0.444444\n",
      "(Iteration 101 / 200) loss: 1.556438\n",
      "(Epoch 11 / 20) train acc: 0.492000; val_acc: 0.500000\n",
      "(Iteration 111 / 200) loss: 1.506253\n",
      "(Epoch 12 / 20) train acc: 0.562000; val_acc: 0.550000\n",
      "(Iteration 121 / 200) loss: 1.340324\n",
      "(Epoch 13 / 20) train acc: 0.614000; val_acc: 0.605556\n",
      "(Iteration 131 / 200) loss: 1.298136\n",
      "(Epoch 14 / 20) train acc: 0.644000; val_acc: 0.666667\n",
      "(Iteration 141 / 200) loss: 1.104288\n",
      "(Epoch 15 / 20) train acc: 0.717000; val_acc: 0.705556\n",
      "(Iteration 151 / 200) loss: 1.011622\n",
      "(Epoch 16 / 20) train acc: 0.758000; val_acc: 0.758333\n",
      "(Iteration 161 / 200) loss: 0.985449\n",
      "(Epoch 17 / 20) train acc: 0.796000; val_acc: 0.794444\n",
      "(Iteration 171 / 200) loss: 0.975313\n",
      "(Epoch 18 / 20) train acc: 0.824000; val_acc: 0.822222\n",
      "(Iteration 181 / 200) loss: 0.803179\n",
      "(Epoch 19 / 20) train acc: 0.852000; val_acc: 0.844444\n",
      "(Iteration 191 / 200) loss: 0.786274\n",
      "(Epoch 20 / 20) train acc: 0.866000; val_acc: 0.855556\n",
      "(Iteration 1 / 200) loss: 2.302330\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.288000; val_acc: 0.294444\n",
      "(Iteration 11 / 200) loss: 2.299536\n",
      "(Epoch 2 / 20) train acc: 0.487000; val_acc: 0.466667\n",
      "(Iteration 21 / 200) loss: 2.293605\n",
      "(Epoch 3 / 20) train acc: 0.512000; val_acc: 0.491667\n",
      "(Iteration 31 / 200) loss: 2.286753\n",
      "(Epoch 4 / 20) train acc: 0.522000; val_acc: 0.488889\n",
      "(Iteration 41 / 200) loss: 2.277173\n",
      "(Epoch 5 / 20) train acc: 0.543000; val_acc: 0.497222\n",
      "(Iteration 51 / 200) loss: 2.263032\n",
      "(Epoch 6 / 20) train acc: 0.562000; val_acc: 0.530556\n",
      "(Iteration 61 / 200) loss: 2.232483\n",
      "(Epoch 7 / 20) train acc: 0.518000; val_acc: 0.502778\n",
      "(Iteration 71 / 200) loss: 2.209064\n",
      "(Epoch 8 / 20) train acc: 0.532000; val_acc: 0.483333\n",
      "(Iteration 81 / 200) loss: 2.156635\n",
      "(Epoch 9 / 20) train acc: 0.599000; val_acc: 0.561111\n",
      "(Iteration 91 / 200) loss: 2.097488\n",
      "(Epoch 10 / 20) train acc: 0.599000; val_acc: 0.569444\n",
      "(Iteration 101 / 200) loss: 2.021014\n",
      "(Epoch 11 / 20) train acc: 0.622000; val_acc: 0.597222\n",
      "(Iteration 111 / 200) loss: 1.899243\n",
      "(Epoch 12 / 20) train acc: 0.647000; val_acc: 0.636111\n",
      "(Iteration 121 / 200) loss: 1.822151\n",
      "(Epoch 13 / 20) train acc: 0.651000; val_acc: 0.669444\n",
      "(Iteration 131 / 200) loss: 1.738434\n",
      "(Epoch 14 / 20) train acc: 0.679000; val_acc: 0.683333\n",
      "(Iteration 141 / 200) loss: 1.641744\n",
      "(Epoch 15 / 20) train acc: 0.690000; val_acc: 0.694444\n",
      "(Iteration 151 / 200) loss: 1.483073\n",
      "(Epoch 16 / 20) train acc: 0.705000; val_acc: 0.725000\n",
      "(Iteration 161 / 200) loss: 1.283741\n",
      "(Epoch 17 / 20) train acc: 0.719000; val_acc: 0.713889\n",
      "(Iteration 171 / 200) loss: 1.256951\n",
      "(Epoch 18 / 20) train acc: 0.751000; val_acc: 0.769444\n",
      "(Iteration 181 / 200) loss: 1.184402\n",
      "(Epoch 19 / 20) train acc: 0.775000; val_acc: 0.808333\n",
      "(Iteration 191 / 200) loss: 0.995022\n",
      "(Epoch 20 / 20) train acc: 0.792000; val_acc: 0.819444\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.134000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302508\n",
      "(Epoch 2 / 20) train acc: 0.088000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302070\n",
      "(Epoch 3 / 20) train acc: 0.354000; val_acc: 0.350000\n",
      "(Iteration 31 / 200) loss: 2.300254\n",
      "(Epoch 4 / 20) train acc: 0.395000; val_acc: 0.388889\n",
      "(Iteration 41 / 200) loss: 2.295849\n",
      "(Epoch 5 / 20) train acc: 0.297000; val_acc: 0.316667\n",
      "(Iteration 51 / 200) loss: 2.284896\n",
      "(Epoch 6 / 20) train acc: 0.259000; val_acc: 0.280556\n",
      "(Iteration 61 / 200) loss: 2.263901\n",
      "(Epoch 7 / 20) train acc: 0.295000; val_acc: 0.288889\n",
      "(Iteration 71 / 200) loss: 2.238206\n",
      "(Epoch 8 / 20) train acc: 0.261000; val_acc: 0.277778\n",
      "(Iteration 81 / 200) loss: 2.192816\n",
      "(Epoch 9 / 20) train acc: 0.260000; val_acc: 0.291667\n",
      "(Iteration 91 / 200) loss: 2.133964\n",
      "(Epoch 10 / 20) train acc: 0.348000; val_acc: 0.375000\n",
      "(Iteration 101 / 200) loss: 2.050292\n",
      "(Epoch 11 / 20) train acc: 0.232000; val_acc: 0.280556\n",
      "(Iteration 111 / 200) loss: 2.005499\n",
      "(Epoch 12 / 20) train acc: 0.265000; val_acc: 0.272222\n",
      "(Iteration 121 / 200) loss: 1.949090\n",
      "(Epoch 13 / 20) train acc: 0.274000; val_acc: 0.280556\n",
      "(Iteration 131 / 200) loss: 1.811811\n",
      "(Epoch 14 / 20) train acc: 0.342000; val_acc: 0.366667\n",
      "(Iteration 141 / 200) loss: 1.778305\n",
      "(Epoch 15 / 20) train acc: 0.338000; val_acc: 0.322222\n",
      "(Iteration 151 / 200) loss: 1.741449\n",
      "(Epoch 16 / 20) train acc: 0.429000; val_acc: 0.388889\n",
      "(Iteration 161 / 200) loss: 1.642065\n",
      "(Epoch 17 / 20) train acc: 0.503000; val_acc: 0.502778\n",
      "(Iteration 171 / 200) loss: 1.605370\n",
      "(Epoch 18 / 20) train acc: 0.580000; val_acc: 0.588889\n",
      "(Iteration 181 / 200) loss: 1.601556\n",
      "(Epoch 19 / 20) train acc: 0.647000; val_acc: 0.613889\n",
      "(Iteration 191 / 200) loss: 1.602636\n",
      "(Epoch 20 / 20) train acc: 0.611000; val_acc: 0.619444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.165000; val_acc: 0.197222\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.123000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 2.302392\n",
      "(Epoch 3 / 20) train acc: 0.192000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 2.301020\n",
      "(Epoch 4 / 20) train acc: 0.190000; val_acc: 0.177778\n",
      "(Iteration 41 / 200) loss: 2.296538\n",
      "(Epoch 5 / 20) train acc: 0.191000; val_acc: 0.172222\n",
      "(Iteration 51 / 200) loss: 2.289145\n",
      "(Epoch 6 / 20) train acc: 0.198000; val_acc: 0.183333\n",
      "(Iteration 61 / 200) loss: 2.273716\n",
      "(Epoch 7 / 20) train acc: 0.235000; val_acc: 0.208333\n",
      "(Iteration 71 / 200) loss: 2.239721\n",
      "(Epoch 8 / 20) train acc: 0.240000; val_acc: 0.208333\n",
      "(Iteration 81 / 200) loss: 2.195390\n",
      "(Epoch 9 / 20) train acc: 0.204000; val_acc: 0.211111\n",
      "(Iteration 91 / 200) loss: 2.160392\n",
      "(Epoch 10 / 20) train acc: 0.254000; val_acc: 0.233333\n",
      "(Iteration 101 / 200) loss: 2.097147\n",
      "(Epoch 11 / 20) train acc: 0.243000; val_acc: 0.236111\n",
      "(Iteration 111 / 200) loss: 2.005998\n",
      "(Epoch 12 / 20) train acc: 0.249000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 1.957458\n",
      "(Epoch 13 / 20) train acc: 0.294000; val_acc: 0.252778\n",
      "(Iteration 131 / 200) loss: 1.894163\n",
      "(Epoch 14 / 20) train acc: 0.273000; val_acc: 0.250000\n",
      "(Iteration 141 / 200) loss: 1.793215\n",
      "(Epoch 15 / 20) train acc: 0.269000; val_acc: 0.250000\n",
      "(Iteration 151 / 200) loss: 1.905717\n",
      "(Epoch 16 / 20) train acc: 0.291000; val_acc: 0.294444\n",
      "(Iteration 161 / 200) loss: 1.833669\n",
      "(Epoch 17 / 20) train acc: 0.304000; val_acc: 0.300000\n",
      "(Iteration 171 / 200) loss: 1.715989\n",
      "(Epoch 18 / 20) train acc: 0.300000; val_acc: 0.297222\n",
      "(Iteration 181 / 200) loss: 1.680297\n",
      "(Epoch 19 / 20) train acc: 0.336000; val_acc: 0.288889\n",
      "(Iteration 191 / 200) loss: 1.619932\n",
      "(Epoch 20 / 20) train acc: 0.382000; val_acc: 0.297222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302555\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302246\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301748\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 2.297023\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Iteration 51 / 200) loss: 2.287595\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 2.288746\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 2.238118\n",
      "(Epoch 8 / 20) train acc: 0.096000; val_acc: 0.136111\n",
      "(Iteration 81 / 200) loss: 2.199617\n",
      "(Epoch 9 / 20) train acc: 0.134000; val_acc: 0.150000\n",
      "(Iteration 91 / 200) loss: 2.186922\n",
      "(Epoch 10 / 20) train acc: 0.175000; val_acc: 0.177778\n",
      "(Iteration 101 / 200) loss: 2.165645\n",
      "(Epoch 11 / 20) train acc: 0.141000; val_acc: 0.169444\n",
      "(Iteration 111 / 200) loss: 2.151324\n",
      "(Epoch 12 / 20) train acc: 0.166000; val_acc: 0.186111\n",
      "(Iteration 121 / 200) loss: 2.138844\n",
      "(Epoch 13 / 20) train acc: 0.177000; val_acc: 0.200000\n",
      "(Iteration 131 / 200) loss: 2.073252\n",
      "(Epoch 14 / 20) train acc: 0.202000; val_acc: 0.197222\n",
      "(Iteration 141 / 200) loss: 2.064403\n",
      "(Epoch 15 / 20) train acc: 0.186000; val_acc: 0.200000\n",
      "(Iteration 151 / 200) loss: 2.101507\n",
      "(Epoch 16 / 20) train acc: 0.198000; val_acc: 0.208333\n",
      "(Iteration 161 / 200) loss: 2.027005\n",
      "(Epoch 17 / 20) train acc: 0.200000; val_acc: 0.205556\n",
      "(Iteration 171 / 200) loss: 1.999923\n",
      "(Epoch 18 / 20) train acc: 0.200000; val_acc: 0.205556\n",
      "(Iteration 181 / 200) loss: 2.070051\n",
      "(Epoch 19 / 20) train acc: 0.189000; val_acc: 0.205556\n",
      "(Iteration 191 / 200) loss: 2.050005\n",
      "(Epoch 20 / 20) train acc: 0.205000; val_acc: 0.211111\n",
      "(Iteration 1 / 200) loss: 5169.365623\n",
      "(Epoch 0 / 20) train acc: 0.062000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.051000; val_acc: 0.063889\n",
      "(Iteration 11 / 200) loss: 4897.328499\n",
      "(Epoch 2 / 20) train acc: 0.048000; val_acc: 0.069444\n",
      "(Iteration 21 / 200) loss: 4382.675803\n",
      "(Epoch 3 / 20) train acc: 0.055000; val_acc: 0.066667\n",
      "(Iteration 31 / 200) loss: 4117.145299\n",
      "(Epoch 4 / 20) train acc: 0.065000; val_acc: 0.066667\n",
      "(Iteration 41 / 200) loss: 4547.596985\n",
      "(Epoch 5 / 20) train acc: 0.067000; val_acc: 0.069444\n",
      "(Iteration 51 / 200) loss: 3766.024609\n",
      "(Epoch 6 / 20) train acc: 0.063000; val_acc: 0.066667\n",
      "(Iteration 61 / 200) loss: 4194.943170\n",
      "(Epoch 7 / 20) train acc: 0.061000; val_acc: 0.069444\n",
      "(Iteration 71 / 200) loss: 4261.647047\n",
      "(Epoch 8 / 20) train acc: 0.070000; val_acc: 0.072222\n",
      "(Iteration 81 / 200) loss: 3394.539362\n",
      "(Epoch 9 / 20) train acc: 0.082000; val_acc: 0.077778\n",
      "(Iteration 91 / 200) loss: 3619.526988\n",
      "(Epoch 10 / 20) train acc: 0.078000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 3284.442115\n",
      "(Epoch 11 / 20) train acc: 0.078000; val_acc: 0.086111\n",
      "(Iteration 111 / 200) loss: 3979.131931\n",
      "(Epoch 12 / 20) train acc: 0.081000; val_acc: 0.094444\n",
      "(Iteration 121 / 200) loss: 3392.486123\n",
      "(Epoch 13 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 3138.262503\n",
      "(Epoch 14 / 20) train acc: 0.059000; val_acc: 0.102778\n",
      "(Iteration 141 / 200) loss: 3094.576699\n",
      "(Epoch 15 / 20) train acc: 0.075000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 3386.105269\n",
      "(Epoch 16 / 20) train acc: 0.079000; val_acc: 0.105556\n",
      "(Iteration 161 / 200) loss: 3140.926338\n",
      "(Epoch 17 / 20) train acc: 0.081000; val_acc: 0.119444\n",
      "(Iteration 171 / 200) loss: 3198.141160\n",
      "(Epoch 18 / 20) train acc: 0.074000; val_acc: 0.125000\n",
      "(Iteration 181 / 200) loss: 2900.553793\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.122222\n",
      "(Iteration 191 / 200) loss: 3111.169865\n",
      "(Epoch 20 / 20) train acc: 0.090000; val_acc: 0.122222\n",
      "(Iteration 1 / 200) loss: 4.688880\n",
      "(Epoch 0 / 20) train acc: 0.030000; val_acc: 0.022222\n",
      "(Epoch 1 / 20) train acc: 0.067000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 3.906861\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 3.109168\n",
      "(Epoch 3 / 20) train acc: 0.175000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 2.761554\n",
      "(Epoch 4 / 20) train acc: 0.193000; val_acc: 0.255556\n",
      "(Iteration 41 / 200) loss: 2.418759\n",
      "(Epoch 5 / 20) train acc: 0.305000; val_acc: 0.294444\n",
      "(Iteration 51 / 200) loss: 2.111258\n",
      "(Epoch 6 / 20) train acc: 0.353000; val_acc: 0.325000\n",
      "(Iteration 61 / 200) loss: 1.978533\n",
      "(Epoch 7 / 20) train acc: 0.402000; val_acc: 0.383333\n",
      "(Iteration 71 / 200) loss: 1.738891\n",
      "(Epoch 8 / 20) train acc: 0.477000; val_acc: 0.450000\n",
      "(Iteration 81 / 200) loss: 1.552867\n",
      "(Epoch 9 / 20) train acc: 0.564000; val_acc: 0.497222\n",
      "(Iteration 91 / 200) loss: 1.412456\n",
      "(Epoch 10 / 20) train acc: 0.582000; val_acc: 0.561111\n",
      "(Iteration 101 / 200) loss: 1.325589\n",
      "(Epoch 11 / 20) train acc: 0.627000; val_acc: 0.608333\n",
      "(Iteration 111 / 200) loss: 1.259632\n",
      "(Epoch 12 / 20) train acc: 0.677000; val_acc: 0.658333\n",
      "(Iteration 121 / 200) loss: 1.219635\n",
      "(Epoch 13 / 20) train acc: 0.728000; val_acc: 0.697222\n",
      "(Iteration 131 / 200) loss: 1.108788\n",
      "(Epoch 14 / 20) train acc: 0.741000; val_acc: 0.708333\n",
      "(Iteration 141 / 200) loss: 0.846064\n",
      "(Epoch 15 / 20) train acc: 0.786000; val_acc: 0.750000\n",
      "(Iteration 151 / 200) loss: 0.892552\n",
      "(Epoch 16 / 20) train acc: 0.807000; val_acc: 0.769444\n",
      "(Iteration 161 / 200) loss: 0.852297\n",
      "(Epoch 17 / 20) train acc: 0.799000; val_acc: 0.775000\n",
      "(Iteration 171 / 200) loss: 0.806734\n",
      "(Epoch 18 / 20) train acc: 0.859000; val_acc: 0.794444\n",
      "(Iteration 181 / 200) loss: 0.549721\n",
      "(Epoch 19 / 20) train acc: 0.876000; val_acc: 0.805556\n",
      "(Iteration 191 / 200) loss: 0.511716\n",
      "(Epoch 20 / 20) train acc: 0.878000; val_acc: 0.827778\n",
      "(Iteration 1 / 200) loss: 2.302270\n",
      "(Epoch 0 / 20) train acc: 0.144000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.346000; val_acc: 0.319444\n",
      "(Iteration 11 / 200) loss: 2.299119\n",
      "(Epoch 2 / 20) train acc: 0.632000; val_acc: 0.563889\n",
      "(Iteration 21 / 200) loss: 2.294717\n",
      "(Epoch 3 / 20) train acc: 0.705000; val_acc: 0.661111\n",
      "(Iteration 31 / 200) loss: 2.286575\n",
      "(Epoch 4 / 20) train acc: 0.770000; val_acc: 0.744444\n",
      "(Iteration 41 / 200) loss: 2.277078\n",
      "(Epoch 5 / 20) train acc: 0.732000; val_acc: 0.672222\n",
      "(Iteration 51 / 200) loss: 2.261747\n",
      "(Epoch 6 / 20) train acc: 0.678000; val_acc: 0.605556\n",
      "(Iteration 61 / 200) loss: 2.235515\n",
      "(Epoch 7 / 20) train acc: 0.724000; val_acc: 0.686111\n",
      "(Iteration 71 / 200) loss: 2.210367\n",
      "(Epoch 8 / 20) train acc: 0.743000; val_acc: 0.700000\n",
      "(Iteration 81 / 200) loss: 2.151095\n",
      "(Epoch 9 / 20) train acc: 0.681000; val_acc: 0.669444\n",
      "(Iteration 91 / 200) loss: 2.111064\n",
      "(Epoch 10 / 20) train acc: 0.676000; val_acc: 0.647222\n",
      "(Iteration 101 / 200) loss: 2.009047\n",
      "(Epoch 11 / 20) train acc: 0.742000; val_acc: 0.691667\n",
      "(Iteration 111 / 200) loss: 1.902748\n",
      "(Epoch 12 / 20) train acc: 0.719000; val_acc: 0.730556\n",
      "(Iteration 121 / 200) loss: 1.824183\n",
      "(Epoch 13 / 20) train acc: 0.736000; val_acc: 0.719444\n",
      "(Iteration 131 / 200) loss: 1.694422\n",
      "(Epoch 14 / 20) train acc: 0.698000; val_acc: 0.719444\n",
      "(Iteration 141 / 200) loss: 1.574825\n",
      "(Epoch 15 / 20) train acc: 0.732000; val_acc: 0.733333\n",
      "(Iteration 151 / 200) loss: 1.515213\n",
      "(Epoch 16 / 20) train acc: 0.716000; val_acc: 0.750000\n",
      "(Iteration 161 / 200) loss: 1.314133\n",
      "(Epoch 17 / 20) train acc: 0.780000; val_acc: 0.775000\n",
      "(Iteration 171 / 200) loss: 1.196514\n",
      "(Epoch 18 / 20) train acc: 0.781000; val_acc: 0.794444\n",
      "(Iteration 181 / 200) loss: 1.110317\n",
      "(Epoch 19 / 20) train acc: 0.807000; val_acc: 0.808333\n",
      "(Iteration 191 / 200) loss: 1.050888\n",
      "(Epoch 20 / 20) train acc: 0.791000; val_acc: 0.813889\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.137000; val_acc: 0.161111\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302395\n",
      "(Epoch 2 / 20) train acc: 0.200000; val_acc: 0.188889\n",
      "(Iteration 21 / 200) loss: 2.302041\n",
      "(Epoch 3 / 20) train acc: 0.346000; val_acc: 0.352778\n",
      "(Iteration 31 / 200) loss: 2.299409\n",
      "(Epoch 4 / 20) train acc: 0.432000; val_acc: 0.441667\n",
      "(Iteration 41 / 200) loss: 2.293783\n",
      "(Epoch 5 / 20) train acc: 0.417000; val_acc: 0.438889\n",
      "(Iteration 51 / 200) loss: 2.275121\n",
      "(Epoch 6 / 20) train acc: 0.394000; val_acc: 0.377778\n",
      "(Iteration 61 / 200) loss: 2.253528\n",
      "(Epoch 7 / 20) train acc: 0.357000; val_acc: 0.358333\n",
      "(Iteration 71 / 200) loss: 2.217468\n",
      "(Epoch 8 / 20) train acc: 0.364000; val_acc: 0.358333\n",
      "(Iteration 81 / 200) loss: 2.120339\n",
      "(Epoch 9 / 20) train acc: 0.398000; val_acc: 0.391667\n",
      "(Iteration 91 / 200) loss: 2.042638\n",
      "(Epoch 10 / 20) train acc: 0.383000; val_acc: 0.375000\n",
      "(Iteration 101 / 200) loss: 1.973687\n",
      "(Epoch 11 / 20) train acc: 0.433000; val_acc: 0.427778\n",
      "(Iteration 111 / 200) loss: 1.921627\n",
      "(Epoch 12 / 20) train acc: 0.401000; val_acc: 0.391667\n",
      "(Iteration 121 / 200) loss: 1.844817\n",
      "(Epoch 13 / 20) train acc: 0.428000; val_acc: 0.450000\n",
      "(Iteration 131 / 200) loss: 1.781995\n",
      "(Epoch 14 / 20) train acc: 0.472000; val_acc: 0.438889\n",
      "(Iteration 141 / 200) loss: 1.733706\n",
      "(Epoch 15 / 20) train acc: 0.431000; val_acc: 0.402778\n",
      "(Iteration 151 / 200) loss: 1.721736\n",
      "(Epoch 16 / 20) train acc: 0.457000; val_acc: 0.391667\n",
      "(Iteration 161 / 200) loss: 1.698359\n",
      "(Epoch 17 / 20) train acc: 0.498000; val_acc: 0.427778\n",
      "(Iteration 171 / 200) loss: 1.557602\n",
      "(Epoch 18 / 20) train acc: 0.545000; val_acc: 0.513889\n",
      "(Iteration 181 / 200) loss: 1.576689\n",
      "(Epoch 19 / 20) train acc: 0.583000; val_acc: 0.580556\n",
      "(Iteration 191 / 200) loss: 1.510983\n",
      "(Epoch 20 / 20) train acc: 0.605000; val_acc: 0.613889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302566\n",
      "(Epoch 2 / 20) train acc: 0.175000; val_acc: 0.158333\n",
      "(Iteration 21 / 200) loss: 2.302376\n",
      "(Epoch 3 / 20) train acc: 0.181000; val_acc: 0.163889\n",
      "(Iteration 31 / 200) loss: 2.301177\n",
      "(Epoch 4 / 20) train acc: 0.209000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 2.297370\n",
      "(Epoch 5 / 20) train acc: 0.203000; val_acc: 0.225000\n",
      "(Iteration 51 / 200) loss: 2.288382\n",
      "(Epoch 6 / 20) train acc: 0.258000; val_acc: 0.238889\n",
      "(Iteration 61 / 200) loss: 2.271659\n",
      "(Epoch 7 / 20) train acc: 0.247000; val_acc: 0.250000\n",
      "(Iteration 71 / 200) loss: 2.242093\n",
      "(Epoch 8 / 20) train acc: 0.248000; val_acc: 0.250000\n",
      "(Iteration 81 / 200) loss: 2.195789\n",
      "(Epoch 9 / 20) train acc: 0.256000; val_acc: 0.258333\n",
      "(Iteration 91 / 200) loss: 2.135812\n",
      "(Epoch 10 / 20) train acc: 0.232000; val_acc: 0.255556\n",
      "(Iteration 101 / 200) loss: 2.032107\n",
      "(Epoch 11 / 20) train acc: 0.278000; val_acc: 0.238889\n",
      "(Iteration 111 / 200) loss: 1.965319\n",
      "(Epoch 12 / 20) train acc: 0.236000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 1.836352\n",
      "(Epoch 13 / 20) train acc: 0.316000; val_acc: 0.316667\n",
      "(Iteration 131 / 200) loss: 1.845810\n",
      "(Epoch 14 / 20) train acc: 0.325000; val_acc: 0.341667\n",
      "(Iteration 141 / 200) loss: 1.822778\n",
      "(Epoch 15 / 20) train acc: 0.326000; val_acc: 0.347222\n",
      "(Iteration 151 / 200) loss: 1.661118\n",
      "(Epoch 16 / 20) train acc: 0.325000; val_acc: 0.350000\n",
      "(Iteration 161 / 200) loss: 1.697889\n",
      "(Epoch 17 / 20) train acc: 0.301000; val_acc: 0.347222\n",
      "(Iteration 171 / 200) loss: 1.680931\n",
      "(Epoch 18 / 20) train acc: 0.330000; val_acc: 0.350000\n",
      "(Iteration 181 / 200) loss: 1.639975\n",
      "(Epoch 19 / 20) train acc: 0.451000; val_acc: 0.433333\n",
      "(Iteration 191 / 200) loss: 1.633818\n",
      "(Epoch 20 / 20) train acc: 0.500000; val_acc: 0.461111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302593\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302382\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.301440\n",
      "(Epoch 4 / 20) train acc: 0.085000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.298128\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.289167\n",
      "(Epoch 6 / 20) train acc: 0.123000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.280141\n",
      "(Epoch 7 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.252723\n",
      "(Epoch 8 / 20) train acc: 0.135000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.217270\n",
      "(Epoch 9 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.178577\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.166489\n",
      "(Epoch 11 / 20) train acc: 0.145000; val_acc: 0.136111\n",
      "(Iteration 111 / 200) loss: 2.072604\n",
      "(Epoch 12 / 20) train acc: 0.175000; val_acc: 0.152778\n",
      "(Iteration 121 / 200) loss: 2.148659\n",
      "(Epoch 13 / 20) train acc: 0.201000; val_acc: 0.155556\n",
      "(Iteration 131 / 200) loss: 2.003677\n",
      "(Epoch 14 / 20) train acc: 0.165000; val_acc: 0.158333\n",
      "(Iteration 141 / 200) loss: 2.004651\n",
      "(Epoch 15 / 20) train acc: 0.194000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 2.079411\n",
      "(Epoch 16 / 20) train acc: 0.211000; val_acc: 0.166667\n",
      "(Iteration 161 / 200) loss: 2.022813\n",
      "(Epoch 17 / 20) train acc: 0.220000; val_acc: 0.166667\n",
      "(Iteration 171 / 200) loss: 2.109439\n",
      "(Epoch 18 / 20) train acc: 0.200000; val_acc: 0.166667\n",
      "(Iteration 181 / 200) loss: 1.987056\n",
      "(Epoch 19 / 20) train acc: 0.167000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 2.044275\n",
      "(Epoch 20 / 20) train acc: 0.183000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 6188.150448\n",
      "(Epoch 0 / 20) train acc: 0.135000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.138000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 5305.272103\n",
      "(Epoch 2 / 20) train acc: 0.167000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 5733.937550\n",
      "(Epoch 3 / 20) train acc: 0.133000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 4444.323000\n",
      "(Epoch 4 / 20) train acc: 0.171000; val_acc: 0.163889\n",
      "(Iteration 41 / 200) loss: 4747.544392\n",
      "(Epoch 5 / 20) train acc: 0.162000; val_acc: 0.166667\n",
      "(Iteration 51 / 200) loss: 5259.715786\n",
      "(Epoch 6 / 20) train acc: 0.156000; val_acc: 0.172222\n",
      "(Iteration 61 / 200) loss: 3553.754994\n",
      "(Epoch 7 / 20) train acc: 0.187000; val_acc: 0.175000\n",
      "(Iteration 71 / 200) loss: 4637.927954\n",
      "(Epoch 8 / 20) train acc: 0.169000; val_acc: 0.180556\n",
      "(Iteration 81 / 200) loss: 4188.172165\n",
      "(Epoch 9 / 20) train acc: 0.193000; val_acc: 0.183333\n",
      "(Iteration 91 / 200) loss: 3767.656065\n",
      "(Epoch 10 / 20) train acc: 0.189000; val_acc: 0.188889\n",
      "(Iteration 101 / 200) loss: 3276.105905\n",
      "(Epoch 11 / 20) train acc: 0.190000; val_acc: 0.186111\n",
      "(Iteration 111 / 200) loss: 3946.907308\n",
      "(Epoch 12 / 20) train acc: 0.205000; val_acc: 0.188889\n",
      "(Iteration 121 / 200) loss: 4107.461521\n",
      "(Epoch 13 / 20) train acc: 0.205000; val_acc: 0.191667\n",
      "(Iteration 131 / 200) loss: 3527.514173\n",
      "(Epoch 14 / 20) train acc: 0.191000; val_acc: 0.194444\n",
      "(Iteration 141 / 200) loss: 3585.972139\n",
      "(Epoch 15 / 20) train acc: 0.200000; val_acc: 0.191667\n",
      "(Iteration 151 / 200) loss: 3929.284168\n",
      "(Epoch 16 / 20) train acc: 0.196000; val_acc: 0.191667\n",
      "(Iteration 161 / 200) loss: 3928.045572\n",
      "(Epoch 17 / 20) train acc: 0.184000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 3791.973540\n",
      "(Epoch 18 / 20) train acc: 0.217000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 3440.569946\n",
      "(Epoch 19 / 20) train acc: 0.190000; val_acc: 0.197222\n",
      "(Iteration 191 / 200) loss: 3453.075102\n",
      "(Epoch 20 / 20) train acc: 0.216000; val_acc: 0.205556\n",
      "(Iteration 1 / 200) loss: 4.086322\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.131000; val_acc: 0.133333\n",
      "(Iteration 11 / 200) loss: 3.375302\n",
      "(Epoch 2 / 20) train acc: 0.155000; val_acc: 0.144444\n",
      "(Iteration 21 / 200) loss: 3.056385\n",
      "(Epoch 3 / 20) train acc: 0.164000; val_acc: 0.163889\n",
      "(Iteration 31 / 200) loss: 2.608561\n",
      "(Epoch 4 / 20) train acc: 0.247000; val_acc: 0.219444\n",
      "(Iteration 41 / 200) loss: 2.382654\n",
      "(Epoch 5 / 20) train acc: 0.325000; val_acc: 0.291667\n",
      "(Iteration 51 / 200) loss: 2.006253\n",
      "(Epoch 6 / 20) train acc: 0.393000; val_acc: 0.386111\n",
      "(Iteration 61 / 200) loss: 1.747996\n",
      "(Epoch 7 / 20) train acc: 0.481000; val_acc: 0.427778\n",
      "(Iteration 71 / 200) loss: 1.665976\n",
      "(Epoch 8 / 20) train acc: 0.520000; val_acc: 0.497222\n",
      "(Iteration 81 / 200) loss: 1.545364\n",
      "(Epoch 9 / 20) train acc: 0.557000; val_acc: 0.538889\n",
      "(Iteration 91 / 200) loss: 1.491168\n",
      "(Epoch 10 / 20) train acc: 0.620000; val_acc: 0.586111\n",
      "(Iteration 101 / 200) loss: 1.216360\n",
      "(Epoch 11 / 20) train acc: 0.663000; val_acc: 0.608333\n",
      "(Iteration 111 / 200) loss: 1.290566\n",
      "(Epoch 12 / 20) train acc: 0.700000; val_acc: 0.638889\n",
      "(Iteration 121 / 200) loss: 1.192072\n",
      "(Epoch 13 / 20) train acc: 0.709000; val_acc: 0.669444\n",
      "(Iteration 131 / 200) loss: 0.898221\n",
      "(Epoch 14 / 20) train acc: 0.748000; val_acc: 0.691667\n",
      "(Iteration 141 / 200) loss: 0.910586\n",
      "(Epoch 15 / 20) train acc: 0.777000; val_acc: 0.722222\n",
      "(Iteration 151 / 200) loss: 0.894640\n",
      "(Epoch 16 / 20) train acc: 0.787000; val_acc: 0.727778\n",
      "(Iteration 161 / 200) loss: 0.697194\n",
      "(Epoch 17 / 20) train acc: 0.800000; val_acc: 0.736111\n",
      "(Iteration 171 / 200) loss: 0.760323\n",
      "(Epoch 18 / 20) train acc: 0.848000; val_acc: 0.761111\n",
      "(Iteration 181 / 200) loss: 0.727712\n",
      "(Epoch 19 / 20) train acc: 0.845000; val_acc: 0.783333\n",
      "(Iteration 191 / 200) loss: 0.631865\n",
      "(Epoch 20 / 20) train acc: 0.818000; val_acc: 0.797222\n",
      "(Iteration 1 / 200) loss: 2.302822\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.338000; val_acc: 0.302778\n",
      "(Iteration 11 / 200) loss: 2.299718\n",
      "(Epoch 2 / 20) train acc: 0.526000; val_acc: 0.447222\n",
      "(Iteration 21 / 200) loss: 2.296286\n",
      "(Epoch 3 / 20) train acc: 0.363000; val_acc: 0.311111\n",
      "(Iteration 31 / 200) loss: 2.290505\n",
      "(Epoch 4 / 20) train acc: 0.497000; val_acc: 0.400000\n",
      "(Iteration 41 / 200) loss: 2.280816\n",
      "(Epoch 5 / 20) train acc: 0.442000; val_acc: 0.391667\n",
      "(Iteration 51 / 200) loss: 2.265082\n",
      "(Epoch 6 / 20) train acc: 0.532000; val_acc: 0.466667\n",
      "(Iteration 61 / 200) loss: 2.249976\n",
      "(Epoch 7 / 20) train acc: 0.660000; val_acc: 0.572222\n",
      "(Iteration 71 / 200) loss: 2.217937\n",
      "(Epoch 8 / 20) train acc: 0.633000; val_acc: 0.577778\n",
      "(Iteration 81 / 200) loss: 2.166788\n",
      "(Epoch 9 / 20) train acc: 0.621000; val_acc: 0.575000\n",
      "(Iteration 91 / 200) loss: 2.097403\n",
      "(Epoch 10 / 20) train acc: 0.661000; val_acc: 0.591667\n",
      "(Iteration 101 / 200) loss: 2.072764\n",
      "(Epoch 11 / 20) train acc: 0.707000; val_acc: 0.647222\n",
      "(Iteration 111 / 200) loss: 1.954635\n",
      "(Epoch 12 / 20) train acc: 0.742000; val_acc: 0.663889\n",
      "(Iteration 121 / 200) loss: 1.846063\n",
      "(Epoch 13 / 20) train acc: 0.730000; val_acc: 0.672222\n",
      "(Iteration 131 / 200) loss: 1.742022\n",
      "(Epoch 14 / 20) train acc: 0.648000; val_acc: 0.680556\n",
      "(Iteration 141 / 200) loss: 1.638931\n",
      "(Epoch 15 / 20) train acc: 0.669000; val_acc: 0.683333\n",
      "(Iteration 151 / 200) loss: 1.540884\n",
      "(Epoch 16 / 20) train acc: 0.709000; val_acc: 0.725000\n",
      "(Iteration 161 / 200) loss: 1.426797\n",
      "(Epoch 17 / 20) train acc: 0.759000; val_acc: 0.769444\n",
      "(Iteration 171 / 200) loss: 1.364108\n",
      "(Epoch 18 / 20) train acc: 0.772000; val_acc: 0.816667\n",
      "(Iteration 181 / 200) loss: 1.152762\n",
      "(Epoch 19 / 20) train acc: 0.798000; val_acc: 0.819444\n",
      "(Iteration 191 / 200) loss: 1.101572\n",
      "(Epoch 20 / 20) train acc: 0.819000; val_acc: 0.813889\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302558\n",
      "(Epoch 2 / 20) train acc: 0.283000; val_acc: 0.266667\n",
      "(Iteration 21 / 200) loss: 2.302190\n",
      "(Epoch 3 / 20) train acc: 0.263000; val_acc: 0.219444\n",
      "(Iteration 31 / 200) loss: 2.300031\n",
      "(Epoch 4 / 20) train acc: 0.317000; val_acc: 0.275000\n",
      "(Iteration 41 / 200) loss: 2.295714\n",
      "(Epoch 5 / 20) train acc: 0.289000; val_acc: 0.233333\n",
      "(Iteration 51 / 200) loss: 2.284506\n",
      "(Epoch 6 / 20) train acc: 0.318000; val_acc: 0.258333\n",
      "(Iteration 61 / 200) loss: 2.264373\n",
      "(Epoch 7 / 20) train acc: 0.314000; val_acc: 0.236111\n",
      "(Iteration 71 / 200) loss: 2.225261\n",
      "(Epoch 8 / 20) train acc: 0.284000; val_acc: 0.236111\n",
      "(Iteration 81 / 200) loss: 2.173908\n",
      "(Epoch 9 / 20) train acc: 0.324000; val_acc: 0.288889\n",
      "(Iteration 91 / 200) loss: 2.099615\n",
      "(Epoch 10 / 20) train acc: 0.414000; val_acc: 0.363889\n",
      "(Iteration 101 / 200) loss: 2.026158\n",
      "(Epoch 11 / 20) train acc: 0.434000; val_acc: 0.433333\n",
      "(Iteration 111 / 200) loss: 1.951328\n",
      "(Epoch 12 / 20) train acc: 0.539000; val_acc: 0.508333\n",
      "(Iteration 121 / 200) loss: 1.869373\n",
      "(Epoch 13 / 20) train acc: 0.533000; val_acc: 0.533333\n",
      "(Iteration 131 / 200) loss: 1.807079\n",
      "(Epoch 14 / 20) train acc: 0.493000; val_acc: 0.530556\n",
      "(Iteration 141 / 200) loss: 1.705160\n",
      "(Epoch 15 / 20) train acc: 0.450000; val_acc: 0.472222\n",
      "(Iteration 151 / 200) loss: 1.672124\n",
      "(Epoch 16 / 20) train acc: 0.438000; val_acc: 0.475000\n",
      "(Iteration 161 / 200) loss: 1.540642\n",
      "(Epoch 17 / 20) train acc: 0.551000; val_acc: 0.547222\n",
      "(Iteration 171 / 200) loss: 1.580378\n",
      "(Epoch 18 / 20) train acc: 0.559000; val_acc: 0.608333\n",
      "(Iteration 181 / 200) loss: 1.514999\n",
      "(Epoch 19 / 20) train acc: 0.632000; val_acc: 0.666667\n",
      "(Iteration 191 / 200) loss: 1.431898\n",
      "(Epoch 20 / 20) train acc: 0.680000; val_acc: 0.669444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.231000; val_acc: 0.211111\n",
      "(Iteration 11 / 200) loss: 2.302558\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302452\n",
      "(Epoch 3 / 20) train acc: 0.226000; val_acc: 0.219444\n",
      "(Iteration 31 / 200) loss: 2.301712\n",
      "(Epoch 4 / 20) train acc: 0.201000; val_acc: 0.180556\n",
      "(Iteration 41 / 200) loss: 2.299680\n",
      "(Epoch 5 / 20) train acc: 0.228000; val_acc: 0.180556\n",
      "(Iteration 51 / 200) loss: 2.293532\n",
      "(Epoch 6 / 20) train acc: 0.196000; val_acc: 0.180556\n",
      "(Iteration 61 / 200) loss: 2.282722\n",
      "(Epoch 7 / 20) train acc: 0.192000; val_acc: 0.180556\n",
      "(Iteration 71 / 200) loss: 2.268422\n",
      "(Epoch 8 / 20) train acc: 0.219000; val_acc: 0.180556\n",
      "(Iteration 81 / 200) loss: 2.232782\n",
      "(Epoch 9 / 20) train acc: 0.226000; val_acc: 0.180556\n",
      "(Iteration 91 / 200) loss: 2.167237\n",
      "(Epoch 10 / 20) train acc: 0.219000; val_acc: 0.180556\n",
      "(Iteration 101 / 200) loss: 2.111603\n",
      "(Epoch 11 / 20) train acc: 0.189000; val_acc: 0.180556\n",
      "(Iteration 111 / 200) loss: 2.082718\n",
      "(Epoch 12 / 20) train acc: 0.213000; val_acc: 0.180556\n",
      "(Iteration 121 / 200) loss: 1.954781\n",
      "(Epoch 13 / 20) train acc: 0.202000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 1.933786\n",
      "(Epoch 14 / 20) train acc: 0.243000; val_acc: 0.222222\n",
      "(Iteration 141 / 200) loss: 1.880168\n",
      "(Epoch 15 / 20) train acc: 0.286000; val_acc: 0.205556\n",
      "(Iteration 151 / 200) loss: 1.899861\n",
      "(Epoch 16 / 20) train acc: 0.263000; val_acc: 0.244444\n",
      "(Iteration 161 / 200) loss: 1.773044\n",
      "(Epoch 17 / 20) train acc: 0.270000; val_acc: 0.247222\n",
      "(Iteration 171 / 200) loss: 1.794272\n",
      "(Epoch 18 / 20) train acc: 0.298000; val_acc: 0.277778\n",
      "(Iteration 181 / 200) loss: 1.732067\n",
      "(Epoch 19 / 20) train acc: 0.363000; val_acc: 0.338889\n",
      "(Iteration 191 / 200) loss: 1.798516\n",
      "(Epoch 20 / 20) train acc: 0.353000; val_acc: 0.352778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302559\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302489\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302427\n",
      "(Epoch 4 / 20) train acc: 0.110000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 2.302323\n",
      "(Epoch 5 / 20) train acc: 0.132000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.300860\n",
      "(Epoch 6 / 20) train acc: 0.145000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.298511\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 2.294209\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.105556\n",
      "(Iteration 81 / 200) loss: 2.286645\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.108333\n",
      "(Iteration 91 / 200) loss: 2.273285\n",
      "(Epoch 10 / 20) train acc: 0.128000; val_acc: 0.113889\n",
      "(Iteration 101 / 200) loss: 2.264241\n",
      "(Epoch 11 / 20) train acc: 0.140000; val_acc: 0.119444\n",
      "(Iteration 111 / 200) loss: 2.227922\n",
      "(Epoch 12 / 20) train acc: 0.165000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.248502\n",
      "(Epoch 13 / 20) train acc: 0.156000; val_acc: 0.122222\n",
      "(Iteration 131 / 200) loss: 2.179555\n",
      "(Epoch 14 / 20) train acc: 0.172000; val_acc: 0.122222\n",
      "(Iteration 141 / 200) loss: 2.194707\n",
      "(Epoch 15 / 20) train acc: 0.171000; val_acc: 0.122222\n",
      "(Iteration 151 / 200) loss: 2.144084\n",
      "(Epoch 16 / 20) train acc: 0.180000; val_acc: 0.147222\n",
      "(Iteration 161 / 200) loss: 2.137296\n",
      "(Epoch 17 / 20) train acc: 0.170000; val_acc: 0.152778\n",
      "(Iteration 171 / 200) loss: 2.166927\n",
      "(Epoch 18 / 20) train acc: 0.172000; val_acc: 0.152778\n",
      "(Iteration 181 / 200) loss: 2.135437\n",
      "(Epoch 19 / 20) train acc: 0.217000; val_acc: 0.163889\n",
      "(Iteration 191 / 200) loss: 2.068544\n",
      "(Epoch 20 / 20) train acc: 0.196000; val_acc: 0.161111\n",
      "(Iteration 1 / 200) loss: 4594.732395\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 4398.766748\n",
      "(Epoch 2 / 20) train acc: 0.084000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 4142.941729\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.136111\n",
      "(Iteration 31 / 200) loss: 4221.297961\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 3851.626380\n",
      "(Epoch 5 / 20) train acc: 0.100000; val_acc: 0.136111\n",
      "(Iteration 51 / 200) loss: 3569.433549\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.138889\n",
      "(Iteration 61 / 200) loss: 3560.687594\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.138889\n",
      "(Iteration 71 / 200) loss: 3461.540702\n",
      "(Epoch 8 / 20) train acc: 0.128000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 3347.860372\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 3433.643168\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 2801.494400\n",
      "(Epoch 11 / 20) train acc: 0.092000; val_acc: 0.136111\n",
      "(Iteration 111 / 200) loss: 3014.310633\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.141667\n",
      "(Iteration 121 / 200) loss: 2684.021867\n",
      "(Epoch 13 / 20) train acc: 0.115000; val_acc: 0.141667\n",
      "(Iteration 131 / 200) loss: 2676.660600\n",
      "(Epoch 14 / 20) train acc: 0.117000; val_acc: 0.147222\n",
      "(Iteration 141 / 200) loss: 2772.943395\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.147222\n",
      "(Iteration 151 / 200) loss: 2239.202441\n",
      "(Epoch 16 / 20) train acc: 0.138000; val_acc: 0.155556\n",
      "(Iteration 161 / 200) loss: 2208.764611\n",
      "(Epoch 17 / 20) train acc: 0.149000; val_acc: 0.158333\n",
      "(Iteration 171 / 200) loss: 2192.607720\n",
      "(Epoch 18 / 20) train acc: 0.147000; val_acc: 0.161111\n",
      "(Iteration 181 / 200) loss: 2307.797547\n",
      "(Epoch 19 / 20) train acc: 0.149000; val_acc: 0.172222\n",
      "(Iteration 191 / 200) loss: 1893.406592\n",
      "(Epoch 20 / 20) train acc: 0.144000; val_acc: 0.172222\n",
      "(Iteration 1 / 200) loss: 4.663575\n",
      "(Epoch 0 / 20) train acc: 0.127000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.137000; val_acc: 0.150000\n",
      "(Iteration 11 / 200) loss: 4.291083\n",
      "(Epoch 2 / 20) train acc: 0.218000; val_acc: 0.186111\n",
      "(Iteration 21 / 200) loss: 3.780642\n",
      "(Epoch 3 / 20) train acc: 0.246000; val_acc: 0.216667\n",
      "(Iteration 31 / 200) loss: 3.103570\n",
      "(Epoch 4 / 20) train acc: 0.266000; val_acc: 0.236111\n",
      "(Iteration 41 / 200) loss: 2.439027\n",
      "(Epoch 5 / 20) train acc: 0.339000; val_acc: 0.319444\n",
      "(Iteration 51 / 200) loss: 2.125077\n",
      "(Epoch 6 / 20) train acc: 0.395000; val_acc: 0.388889\n",
      "(Iteration 61 / 200) loss: 1.767802\n",
      "(Epoch 7 / 20) train acc: 0.499000; val_acc: 0.483333\n",
      "(Iteration 71 / 200) loss: 1.780155\n",
      "(Epoch 8 / 20) train acc: 0.537000; val_acc: 0.530556\n",
      "(Iteration 81 / 200) loss: 1.468357\n",
      "(Epoch 9 / 20) train acc: 0.584000; val_acc: 0.594444\n",
      "(Iteration 91 / 200) loss: 1.333669\n",
      "(Epoch 10 / 20) train acc: 0.660000; val_acc: 0.622222\n",
      "(Iteration 101 / 200) loss: 1.329775\n",
      "(Epoch 11 / 20) train acc: 0.651000; val_acc: 0.658333\n",
      "(Iteration 111 / 200) loss: 1.193137\n",
      "(Epoch 12 / 20) train acc: 0.724000; val_acc: 0.694444\n",
      "(Iteration 121 / 200) loss: 1.099594\n",
      "(Epoch 13 / 20) train acc: 0.723000; val_acc: 0.705556\n",
      "(Iteration 131 / 200) loss: 0.936280\n",
      "(Epoch 14 / 20) train acc: 0.763000; val_acc: 0.727778\n",
      "(Iteration 141 / 200) loss: 0.884810\n",
      "(Epoch 15 / 20) train acc: 0.802000; val_acc: 0.755556\n",
      "(Iteration 151 / 200) loss: 0.700273\n",
      "(Epoch 16 / 20) train acc: 0.801000; val_acc: 0.772222\n",
      "(Iteration 161 / 200) loss: 0.885233\n",
      "(Epoch 17 / 20) train acc: 0.809000; val_acc: 0.786111\n",
      "(Iteration 171 / 200) loss: 0.762332\n",
      "(Epoch 18 / 20) train acc: 0.830000; val_acc: 0.805556\n",
      "(Iteration 181 / 200) loss: 0.656911\n",
      "(Epoch 19 / 20) train acc: 0.844000; val_acc: 0.816667\n",
      "(Iteration 191 / 200) loss: 0.540084\n",
      "(Epoch 20 / 20) train acc: 0.859000; val_acc: 0.822222\n",
      "(Iteration 1 / 200) loss: 2.302607\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.324000; val_acc: 0.286111\n",
      "(Iteration 11 / 200) loss: 2.299365\n",
      "(Epoch 2 / 20) train acc: 0.459000; val_acc: 0.363889\n",
      "(Iteration 21 / 200) loss: 2.297004\n",
      "(Epoch 3 / 20) train acc: 0.426000; val_acc: 0.388889\n",
      "(Iteration 31 / 200) loss: 2.287808\n",
      "(Epoch 4 / 20) train acc: 0.368000; val_acc: 0.358333\n",
      "(Iteration 41 / 200) loss: 2.279334\n",
      "(Epoch 5 / 20) train acc: 0.307000; val_acc: 0.291667\n",
      "(Iteration 51 / 200) loss: 2.267889\n",
      "(Epoch 6 / 20) train acc: 0.375000; val_acc: 0.336111\n",
      "(Iteration 61 / 200) loss: 2.241239\n",
      "(Epoch 7 / 20) train acc: 0.469000; val_acc: 0.455556\n",
      "(Iteration 71 / 200) loss: 2.193202\n",
      "(Epoch 8 / 20) train acc: 0.456000; val_acc: 0.430556\n",
      "(Iteration 81 / 200) loss: 2.135039\n",
      "(Epoch 9 / 20) train acc: 0.462000; val_acc: 0.455556\n",
      "(Iteration 91 / 200) loss: 2.088703\n",
      "(Epoch 10 / 20) train acc: 0.527000; val_acc: 0.500000\n",
      "(Iteration 101 / 200) loss: 2.012828\n",
      "(Epoch 11 / 20) train acc: 0.643000; val_acc: 0.638889\n",
      "(Iteration 111 / 200) loss: 1.930269\n",
      "(Epoch 12 / 20) train acc: 0.657000; val_acc: 0.647222\n",
      "(Iteration 121 / 200) loss: 1.762056\n",
      "(Epoch 13 / 20) train acc: 0.640000; val_acc: 0.619444\n",
      "(Iteration 131 / 200) loss: 1.705793\n",
      "(Epoch 14 / 20) train acc: 0.673000; val_acc: 0.702778\n",
      "(Iteration 141 / 200) loss: 1.567004\n",
      "(Epoch 15 / 20) train acc: 0.731000; val_acc: 0.727778\n",
      "(Iteration 151 / 200) loss: 1.487656\n",
      "(Epoch 16 / 20) train acc: 0.735000; val_acc: 0.727778\n",
      "(Iteration 161 / 200) loss: 1.388556\n",
      "(Epoch 17 / 20) train acc: 0.759000; val_acc: 0.744444\n",
      "(Iteration 171 / 200) loss: 1.271193\n",
      "(Epoch 18 / 20) train acc: 0.767000; val_acc: 0.780556\n",
      "(Iteration 181 / 200) loss: 1.242966\n",
      "(Epoch 19 / 20) train acc: 0.765000; val_acc: 0.783333\n",
      "(Iteration 191 / 200) loss: 1.210975\n",
      "(Epoch 20 / 20) train acc: 0.788000; val_acc: 0.777778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.319000; val_acc: 0.283333\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302458\n",
      "(Epoch 2 / 20) train acc: 0.212000; val_acc: 0.158333\n",
      "(Iteration 21 / 200) loss: 2.301965\n",
      "(Epoch 3 / 20) train acc: 0.218000; val_acc: 0.161111\n",
      "(Iteration 31 / 200) loss: 2.299900\n",
      "(Epoch 4 / 20) train acc: 0.251000; val_acc: 0.191667\n",
      "(Iteration 41 / 200) loss: 2.296007\n",
      "(Epoch 5 / 20) train acc: 0.263000; val_acc: 0.194444\n",
      "(Iteration 51 / 200) loss: 2.282832\n",
      "(Epoch 6 / 20) train acc: 0.310000; val_acc: 0.283333\n",
      "(Iteration 61 / 200) loss: 2.270330\n",
      "(Epoch 7 / 20) train acc: 0.394000; val_acc: 0.338889\n",
      "(Iteration 71 / 200) loss: 2.211959\n",
      "(Epoch 8 / 20) train acc: 0.389000; val_acc: 0.325000\n",
      "(Iteration 81 / 200) loss: 2.175332\n",
      "(Epoch 9 / 20) train acc: 0.314000; val_acc: 0.269444\n",
      "(Iteration 91 / 200) loss: 2.075420\n",
      "(Epoch 10 / 20) train acc: 0.327000; val_acc: 0.305556\n",
      "(Iteration 101 / 200) loss: 2.019743\n",
      "(Epoch 11 / 20) train acc: 0.317000; val_acc: 0.313889\n",
      "(Iteration 111 / 200) loss: 1.949907\n",
      "(Epoch 12 / 20) train acc: 0.342000; val_acc: 0.325000\n",
      "(Iteration 121 / 200) loss: 1.877470\n",
      "(Epoch 13 / 20) train acc: 0.362000; val_acc: 0.355556\n",
      "(Iteration 131 / 200) loss: 1.728588\n",
      "(Epoch 14 / 20) train acc: 0.400000; val_acc: 0.380556\n",
      "(Iteration 141 / 200) loss: 1.739012\n",
      "(Epoch 15 / 20) train acc: 0.479000; val_acc: 0.502778\n",
      "(Iteration 151 / 200) loss: 1.615405\n",
      "(Epoch 16 / 20) train acc: 0.499000; val_acc: 0.508333\n",
      "(Iteration 161 / 200) loss: 1.564707\n",
      "(Epoch 17 / 20) train acc: 0.576000; val_acc: 0.541667\n",
      "(Iteration 171 / 200) loss: 1.451935\n",
      "(Epoch 18 / 20) train acc: 0.561000; val_acc: 0.569444\n",
      "(Iteration 181 / 200) loss: 1.496264\n",
      "(Epoch 19 / 20) train acc: 0.612000; val_acc: 0.611111\n",
      "(Iteration 191 / 200) loss: 1.359800\n",
      "(Epoch 20 / 20) train acc: 0.614000; val_acc: 0.611111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302502\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302300\n",
      "(Epoch 3 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.300750\n",
      "(Epoch 4 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.297138\n",
      "(Epoch 5 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.291730\n",
      "(Epoch 6 / 20) train acc: 0.184000; val_acc: 0.150000\n",
      "(Iteration 61 / 200) loss: 2.271980\n",
      "(Epoch 7 / 20) train acc: 0.185000; val_acc: 0.219444\n",
      "(Iteration 71 / 200) loss: 2.255913\n",
      "(Epoch 8 / 20) train acc: 0.143000; val_acc: 0.130556\n",
      "(Iteration 81 / 200) loss: 2.210165\n",
      "(Epoch 9 / 20) train acc: 0.150000; val_acc: 0.136111\n",
      "(Iteration 91 / 200) loss: 2.206722\n",
      "(Epoch 10 / 20) train acc: 0.176000; val_acc: 0.136111\n",
      "(Iteration 101 / 200) loss: 2.161904\n",
      "(Epoch 11 / 20) train acc: 0.187000; val_acc: 0.194444\n",
      "(Iteration 111 / 200) loss: 2.124063\n",
      "(Epoch 12 / 20) train acc: 0.173000; val_acc: 0.200000\n",
      "(Iteration 121 / 200) loss: 2.156272\n",
      "(Epoch 13 / 20) train acc: 0.191000; val_acc: 0.216667\n",
      "(Iteration 131 / 200) loss: 2.098627\n",
      "(Epoch 14 / 20) train acc: 0.173000; val_acc: 0.219444\n",
      "(Iteration 141 / 200) loss: 2.100962\n",
      "(Epoch 15 / 20) train acc: 0.191000; val_acc: 0.219444\n",
      "(Iteration 151 / 200) loss: 2.126258\n",
      "(Epoch 16 / 20) train acc: 0.190000; val_acc: 0.222222\n",
      "(Iteration 161 / 200) loss: 2.065006\n",
      "(Epoch 17 / 20) train acc: 0.168000; val_acc: 0.222222\n",
      "(Iteration 171 / 200) loss: 2.017415\n",
      "(Epoch 18 / 20) train acc: 0.181000; val_acc: 0.225000\n",
      "(Iteration 181 / 200) loss: 2.005066\n",
      "(Epoch 19 / 20) train acc: 0.220000; val_acc: 0.233333\n",
      "(Iteration 191 / 200) loss: 1.975901\n",
      "(Epoch 20 / 20) train acc: 0.188000; val_acc: 0.238889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.144000; val_acc: 0.147222\n",
      "(Iteration 11 / 200) loss: 2.302569\n",
      "(Epoch 2 / 20) train acc: 0.141000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 2.302482\n",
      "(Epoch 3 / 20) train acc: 0.183000; val_acc: 0.158333\n",
      "(Iteration 31 / 200) loss: 2.302088\n",
      "(Epoch 4 / 20) train acc: 0.135000; val_acc: 0.122222\n",
      "(Iteration 41 / 200) loss: 2.300548\n",
      "(Epoch 5 / 20) train acc: 0.153000; val_acc: 0.147222\n",
      "(Iteration 51 / 200) loss: 2.296306\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.144444\n",
      "(Iteration 61 / 200) loss: 2.288429\n",
      "(Epoch 7 / 20) train acc: 0.126000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 2.278782\n",
      "(Epoch 8 / 20) train acc: 0.149000; val_acc: 0.119444\n",
      "(Iteration 81 / 200) loss: 2.241900\n",
      "(Epoch 9 / 20) train acc: 0.126000; val_acc: 0.125000\n",
      "(Iteration 91 / 200) loss: 2.207684\n",
      "(Epoch 10 / 20) train acc: 0.147000; val_acc: 0.150000\n",
      "(Iteration 101 / 200) loss: 2.210140\n",
      "(Epoch 11 / 20) train acc: 0.150000; val_acc: 0.144444\n",
      "(Iteration 111 / 200) loss: 2.164764\n",
      "(Epoch 12 / 20) train acc: 0.163000; val_acc: 0.186111\n",
      "(Iteration 121 / 200) loss: 2.156126\n",
      "(Epoch 13 / 20) train acc: 0.177000; val_acc: 0.163889\n",
      "(Iteration 131 / 200) loss: 2.104736\n",
      "(Epoch 14 / 20) train acc: 0.196000; val_acc: 0.166667\n",
      "(Iteration 141 / 200) loss: 2.136523\n",
      "(Epoch 15 / 20) train acc: 0.198000; val_acc: 0.205556\n",
      "(Iteration 151 / 200) loss: 2.117225\n",
      "(Epoch 16 / 20) train acc: 0.158000; val_acc: 0.202778\n",
      "(Iteration 161 / 200) loss: 2.179323\n",
      "(Epoch 17 / 20) train acc: 0.200000; val_acc: 0.205556\n",
      "(Iteration 171 / 200) loss: 2.033211\n",
      "(Epoch 18 / 20) train acc: 0.200000; val_acc: 0.205556\n",
      "(Iteration 181 / 200) loss: 2.046521\n",
      "(Epoch 19 / 20) train acc: 0.199000; val_acc: 0.205556\n",
      "(Iteration 191 / 200) loss: 2.026218\n",
      "(Epoch 20 / 20) train acc: 0.214000; val_acc: 0.205556\n",
      "(Iteration 1 / 200) loss: 3412.834499\n",
      "(Epoch 0 / 20) train acc: 0.170000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.151000; val_acc: 0.150000\n",
      "(Iteration 11 / 200) loss: 3481.742285\n",
      "(Epoch 2 / 20) train acc: 0.210000; val_acc: 0.152778\n",
      "(Iteration 21 / 200) loss: 3571.746638\n",
      "(Epoch 3 / 20) train acc: 0.170000; val_acc: 0.155556\n",
      "(Iteration 31 / 200) loss: 3029.464742\n",
      "(Epoch 4 / 20) train acc: 0.201000; val_acc: 0.158333\n",
      "(Iteration 41 / 200) loss: 2996.350347\n",
      "(Epoch 5 / 20) train acc: 0.157000; val_acc: 0.155556\n",
      "(Iteration 51 / 200) loss: 3130.830326\n",
      "(Epoch 6 / 20) train acc: 0.166000; val_acc: 0.152778\n",
      "(Iteration 61 / 200) loss: 2663.186868\n",
      "(Epoch 7 / 20) train acc: 0.172000; val_acc: 0.155556\n",
      "(Iteration 71 / 200) loss: 2432.183879\n",
      "(Epoch 8 / 20) train acc: 0.180000; val_acc: 0.155556\n",
      "(Iteration 81 / 200) loss: 3254.694328\n",
      "(Epoch 9 / 20) train acc: 0.181000; val_acc: 0.161111\n",
      "(Iteration 91 / 200) loss: 2839.060245\n",
      "(Epoch 10 / 20) train acc: 0.194000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 2718.684913\n",
      "(Epoch 11 / 20) train acc: 0.189000; val_acc: 0.161111\n",
      "(Iteration 111 / 200) loss: 2544.302081\n",
      "(Epoch 12 / 20) train acc: 0.199000; val_acc: 0.166667\n",
      "(Iteration 121 / 200) loss: 2511.973781\n",
      "(Epoch 13 / 20) train acc: 0.179000; val_acc: 0.166667\n",
      "(Iteration 131 / 200) loss: 2237.477824\n",
      "(Epoch 14 / 20) train acc: 0.200000; val_acc: 0.169444\n",
      "(Iteration 141 / 200) loss: 1968.036711\n",
      "(Epoch 15 / 20) train acc: 0.205000; val_acc: 0.180556\n",
      "(Iteration 151 / 200) loss: 1977.957785\n",
      "(Epoch 16 / 20) train acc: 0.192000; val_acc: 0.183333\n",
      "(Iteration 161 / 200) loss: 2265.870891\n",
      "(Epoch 17 / 20) train acc: 0.208000; val_acc: 0.183333\n",
      "(Iteration 171 / 200) loss: 1810.804310\n",
      "(Epoch 18 / 20) train acc: 0.210000; val_acc: 0.183333\n",
      "(Iteration 181 / 200) loss: 2093.512729\n",
      "(Epoch 19 / 20) train acc: 0.232000; val_acc: 0.183333\n",
      "(Iteration 191 / 200) loss: 1906.805366\n",
      "(Epoch 20 / 20) train acc: 0.251000; val_acc: 0.188889\n",
      "(Iteration 1 / 200) loss: 5.189629\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 3.905778\n",
      "(Epoch 2 / 20) train acc: 0.136000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 4.266973\n",
      "(Epoch 3 / 20) train acc: 0.161000; val_acc: 0.144444\n",
      "(Iteration 31 / 200) loss: 3.372863\n",
      "(Epoch 4 / 20) train acc: 0.171000; val_acc: 0.180556\n",
      "(Iteration 41 / 200) loss: 3.099185\n",
      "(Epoch 5 / 20) train acc: 0.213000; val_acc: 0.202778\n",
      "(Iteration 51 / 200) loss: 2.505765\n",
      "(Epoch 6 / 20) train acc: 0.235000; val_acc: 0.241667\n",
      "(Iteration 61 / 200) loss: 2.338339\n",
      "(Epoch 7 / 20) train acc: 0.287000; val_acc: 0.302778\n",
      "(Iteration 71 / 200) loss: 2.099851\n",
      "(Epoch 8 / 20) train acc: 0.353000; val_acc: 0.350000\n",
      "(Iteration 81 / 200) loss: 1.827279\n",
      "(Epoch 9 / 20) train acc: 0.442000; val_acc: 0.411111\n",
      "(Iteration 91 / 200) loss: 1.673720\n",
      "(Epoch 10 / 20) train acc: 0.487000; val_acc: 0.483333\n",
      "(Iteration 101 / 200) loss: 1.415274\n",
      "(Epoch 11 / 20) train acc: 0.584000; val_acc: 0.569444\n",
      "(Iteration 111 / 200) loss: 1.286707\n",
      "(Epoch 12 / 20) train acc: 0.610000; val_acc: 0.627778\n",
      "(Iteration 121 / 200) loss: 1.278768\n",
      "(Epoch 13 / 20) train acc: 0.680000; val_acc: 0.655556\n",
      "(Iteration 131 / 200) loss: 1.108234\n",
      "(Epoch 14 / 20) train acc: 0.709000; val_acc: 0.702778\n",
      "(Iteration 141 / 200) loss: 1.035241\n",
      "(Epoch 15 / 20) train acc: 0.746000; val_acc: 0.736111\n",
      "(Iteration 151 / 200) loss: 0.957378\n",
      "(Epoch 16 / 20) train acc: 0.766000; val_acc: 0.763889\n",
      "(Iteration 161 / 200) loss: 0.915914\n",
      "(Epoch 17 / 20) train acc: 0.803000; val_acc: 0.786111\n",
      "(Iteration 171 / 200) loss: 0.711606\n",
      "(Epoch 18 / 20) train acc: 0.823000; val_acc: 0.797222\n",
      "(Iteration 181 / 200) loss: 0.596642\n",
      "(Epoch 19 / 20) train acc: 0.826000; val_acc: 0.813889\n",
      "(Iteration 191 / 200) loss: 0.783943\n",
      "(Epoch 20 / 20) train acc: 0.857000; val_acc: 0.822222\n",
      "(Iteration 1 / 200) loss: 2.302554\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.275000; val_acc: 0.233333\n",
      "(Iteration 11 / 200) loss: 2.299436\n",
      "(Epoch 2 / 20) train acc: 0.314000; val_acc: 0.302778\n",
      "(Iteration 21 / 200) loss: 2.295581\n",
      "(Epoch 3 / 20) train acc: 0.550000; val_acc: 0.513889\n",
      "(Iteration 31 / 200) loss: 2.289153\n",
      "(Epoch 4 / 20) train acc: 0.650000; val_acc: 0.580556\n",
      "(Iteration 41 / 200) loss: 2.280468\n",
      "(Epoch 5 / 20) train acc: 0.683000; val_acc: 0.641667\n",
      "(Iteration 51 / 200) loss: 2.264579\n",
      "(Epoch 6 / 20) train acc: 0.647000; val_acc: 0.577778\n",
      "(Iteration 61 / 200) loss: 2.243838\n",
      "(Epoch 7 / 20) train acc: 0.591000; val_acc: 0.580556\n",
      "(Iteration 71 / 200) loss: 2.206959\n",
      "(Epoch 8 / 20) train acc: 0.642000; val_acc: 0.633333\n",
      "(Iteration 81 / 200) loss: 2.158436\n",
      "(Epoch 9 / 20) train acc: 0.646000; val_acc: 0.627778\n",
      "(Iteration 91 / 200) loss: 2.105638\n",
      "(Epoch 10 / 20) train acc: 0.684000; val_acc: 0.636111\n",
      "(Iteration 101 / 200) loss: 2.027596\n",
      "(Epoch 11 / 20) train acc: 0.694000; val_acc: 0.652778\n",
      "(Iteration 111 / 200) loss: 1.914217\n",
      "(Epoch 12 / 20) train acc: 0.679000; val_acc: 0.641667\n",
      "(Iteration 121 / 200) loss: 1.828869\n",
      "(Epoch 13 / 20) train acc: 0.701000; val_acc: 0.675000\n",
      "(Iteration 131 / 200) loss: 1.705228\n",
      "(Epoch 14 / 20) train acc: 0.697000; val_acc: 0.677778\n",
      "(Iteration 141 / 200) loss: 1.586005\n",
      "(Epoch 15 / 20) train acc: 0.688000; val_acc: 0.694444\n",
      "(Iteration 151 / 200) loss: 1.524506\n",
      "(Epoch 16 / 20) train acc: 0.763000; val_acc: 0.780556\n",
      "(Iteration 161 / 200) loss: 1.475310\n",
      "(Epoch 17 / 20) train acc: 0.776000; val_acc: 0.800000\n",
      "(Iteration 171 / 200) loss: 1.264128\n",
      "(Epoch 18 / 20) train acc: 0.785000; val_acc: 0.802778\n",
      "(Iteration 181 / 200) loss: 1.176712\n",
      "(Epoch 19 / 20) train acc: 0.776000; val_acc: 0.797222\n",
      "(Iteration 191 / 200) loss: 1.099533\n",
      "(Epoch 20 / 20) train acc: 0.800000; val_acc: 0.797222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.224000; val_acc: 0.191667\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302491\n",
      "(Epoch 2 / 20) train acc: 0.201000; val_acc: 0.191667\n",
      "(Iteration 21 / 200) loss: 2.301908\n",
      "(Epoch 3 / 20) train acc: 0.255000; val_acc: 0.225000\n",
      "(Iteration 31 / 200) loss: 2.299640\n",
      "(Epoch 4 / 20) train acc: 0.326000; val_acc: 0.291667\n",
      "(Iteration 41 / 200) loss: 2.294396\n",
      "(Epoch 5 / 20) train acc: 0.368000; val_acc: 0.333333\n",
      "(Iteration 51 / 200) loss: 2.281885\n",
      "(Epoch 6 / 20) train acc: 0.371000; val_acc: 0.350000\n",
      "(Iteration 61 / 200) loss: 2.259233\n",
      "(Epoch 7 / 20) train acc: 0.368000; val_acc: 0.391667\n",
      "(Iteration 71 / 200) loss: 2.206224\n",
      "(Epoch 8 / 20) train acc: 0.414000; val_acc: 0.413889\n",
      "(Iteration 81 / 200) loss: 2.149416\n",
      "(Epoch 9 / 20) train acc: 0.426000; val_acc: 0.447222\n",
      "(Iteration 91 / 200) loss: 2.081168\n",
      "(Epoch 10 / 20) train acc: 0.428000; val_acc: 0.463889\n",
      "(Iteration 101 / 200) loss: 1.989623\n",
      "(Epoch 11 / 20) train acc: 0.397000; val_acc: 0.402778\n",
      "(Iteration 111 / 200) loss: 1.882522\n",
      "(Epoch 12 / 20) train acc: 0.386000; val_acc: 0.394444\n",
      "(Iteration 121 / 200) loss: 1.822369\n",
      "(Epoch 13 / 20) train acc: 0.404000; val_acc: 0.383333\n",
      "(Iteration 131 / 200) loss: 1.719507\n",
      "(Epoch 14 / 20) train acc: 0.446000; val_acc: 0.427778\n",
      "(Iteration 141 / 200) loss: 1.699774\n",
      "(Epoch 15 / 20) train acc: 0.404000; val_acc: 0.436111\n",
      "(Iteration 151 / 200) loss: 1.674617\n",
      "(Epoch 16 / 20) train acc: 0.467000; val_acc: 0.477778\n",
      "(Iteration 161 / 200) loss: 1.543581\n",
      "(Epoch 17 / 20) train acc: 0.494000; val_acc: 0.500000\n",
      "(Iteration 171 / 200) loss: 1.540175\n",
      "(Epoch 18 / 20) train acc: 0.508000; val_acc: 0.497222\n",
      "(Iteration 181 / 200) loss: 1.411307\n",
      "(Epoch 19 / 20) train acc: 0.522000; val_acc: 0.519444\n",
      "(Iteration 191 / 200) loss: 1.446306\n",
      "(Epoch 20 / 20) train acc: 0.539000; val_acc: 0.544444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.204000; val_acc: 0.213889\n",
      "(Epoch 1 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302570\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302418\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.300997\n",
      "(Epoch 4 / 20) train acc: 0.137000; val_acc: 0.111111\n",
      "(Iteration 41 / 200) loss: 2.299190\n",
      "(Epoch 5 / 20) train acc: 0.153000; val_acc: 0.138889\n",
      "(Iteration 51 / 200) loss: 2.292922\n",
      "(Epoch 6 / 20) train acc: 0.150000; val_acc: 0.163889\n",
      "(Iteration 61 / 200) loss: 2.268888\n",
      "(Epoch 7 / 20) train acc: 0.195000; val_acc: 0.175000\n",
      "(Iteration 71 / 200) loss: 2.248445\n",
      "(Epoch 8 / 20) train acc: 0.239000; val_acc: 0.208333\n",
      "(Iteration 81 / 200) loss: 2.234247\n",
      "(Epoch 9 / 20) train acc: 0.214000; val_acc: 0.216667\n",
      "(Iteration 91 / 200) loss: 2.185561\n",
      "(Epoch 10 / 20) train acc: 0.234000; val_acc: 0.213889\n",
      "(Iteration 101 / 200) loss: 2.157053\n",
      "(Epoch 11 / 20) train acc: 0.240000; val_acc: 0.247222\n",
      "(Iteration 111 / 200) loss: 2.132878\n",
      "(Epoch 12 / 20) train acc: 0.266000; val_acc: 0.261111\n",
      "(Iteration 121 / 200) loss: 2.084215\n",
      "(Epoch 13 / 20) train acc: 0.238000; val_acc: 0.266667\n",
      "(Iteration 131 / 200) loss: 2.076983\n",
      "(Epoch 14 / 20) train acc: 0.303000; val_acc: 0.325000\n",
      "(Iteration 141 / 200) loss: 1.984168\n",
      "(Epoch 15 / 20) train acc: 0.344000; val_acc: 0.338889\n",
      "(Iteration 151 / 200) loss: 1.965120\n",
      "(Epoch 16 / 20) train acc: 0.341000; val_acc: 0.361111\n",
      "(Iteration 161 / 200) loss: 1.936280\n",
      "(Epoch 17 / 20) train acc: 0.362000; val_acc: 0.375000\n",
      "(Iteration 171 / 200) loss: 1.926784\n",
      "(Epoch 18 / 20) train acc: 0.337000; val_acc: 0.391667\n",
      "(Iteration 181 / 200) loss: 1.825543\n",
      "(Epoch 19 / 20) train acc: 0.383000; val_acc: 0.400000\n",
      "(Iteration 191 / 200) loss: 1.777176\n",
      "(Epoch 20 / 20) train acc: 0.405000; val_acc: 0.408333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302635\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302367\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.125000\n",
      "(Iteration 31 / 200) loss: 2.302005\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.298816\n",
      "(Epoch 5 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.296037\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.279368\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.254853\n",
      "(Epoch 8 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.220697\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.166667\n",
      "(Iteration 91 / 200) loss: 2.199208\n",
      "(Epoch 10 / 20) train acc: 0.172000; val_acc: 0.172222\n",
      "(Iteration 101 / 200) loss: 2.132600\n",
      "(Epoch 11 / 20) train acc: 0.151000; val_acc: 0.200000\n",
      "(Iteration 111 / 200) loss: 2.155236\n",
      "(Epoch 12 / 20) train acc: 0.175000; val_acc: 0.213889\n",
      "(Iteration 121 / 200) loss: 2.079112\n",
      "(Epoch 13 / 20) train acc: 0.182000; val_acc: 0.194444\n",
      "(Iteration 131 / 200) loss: 2.023509\n",
      "(Epoch 14 / 20) train acc: 0.167000; val_acc: 0.211111\n",
      "(Iteration 141 / 200) loss: 2.089660\n",
      "(Epoch 15 / 20) train acc: 0.188000; val_acc: 0.208333\n",
      "(Iteration 151 / 200) loss: 2.025820\n",
      "(Epoch 16 / 20) train acc: 0.181000; val_acc: 0.208333\n",
      "(Iteration 161 / 200) loss: 1.978177\n",
      "(Epoch 17 / 20) train acc: 0.203000; val_acc: 0.213889\n",
      "(Iteration 171 / 200) loss: 1.998295\n",
      "(Epoch 18 / 20) train acc: 0.188000; val_acc: 0.216667\n",
      "(Iteration 181 / 200) loss: 1.882277\n",
      "(Epoch 19 / 20) train acc: 0.180000; val_acc: 0.219444\n",
      "(Iteration 191 / 200) loss: 1.998106\n",
      "(Epoch 20 / 20) train acc: 0.203000; val_acc: 0.219444\n",
      "(Iteration 1 / 200) loss: 4726.669953\n",
      "(Epoch 0 / 20) train acc: 0.036000; val_acc: 0.027778\n",
      "(Epoch 1 / 20) train acc: 0.024000; val_acc: 0.033333\n",
      "(Iteration 11 / 200) loss: 4046.429309\n",
      "(Epoch 2 / 20) train acc: 0.031000; val_acc: 0.033333\n",
      "(Iteration 21 / 200) loss: 4602.795231\n",
      "(Epoch 3 / 20) train acc: 0.033000; val_acc: 0.025000\n",
      "(Iteration 31 / 200) loss: 3869.148654\n",
      "(Epoch 4 / 20) train acc: 0.029000; val_acc: 0.019444\n",
      "(Iteration 41 / 200) loss: 3744.798952\n",
      "(Epoch 5 / 20) train acc: 0.032000; val_acc: 0.025000\n",
      "(Iteration 51 / 200) loss: 3502.754875\n",
      "(Epoch 6 / 20) train acc: 0.032000; val_acc: 0.036111\n",
      "(Iteration 61 / 200) loss: 3383.336423\n",
      "(Epoch 7 / 20) train acc: 0.036000; val_acc: 0.038889\n",
      "(Iteration 71 / 200) loss: 3365.841721\n",
      "(Epoch 8 / 20) train acc: 0.040000; val_acc: 0.044444\n",
      "(Iteration 81 / 200) loss: 2756.933269\n",
      "(Epoch 9 / 20) train acc: 0.052000; val_acc: 0.050000\n",
      "(Iteration 91 / 200) loss: 2945.082942\n",
      "(Epoch 10 / 20) train acc: 0.051000; val_acc: 0.063889\n",
      "(Iteration 101 / 200) loss: 2931.465115\n",
      "(Epoch 11 / 20) train acc: 0.067000; val_acc: 0.066667\n",
      "(Iteration 111 / 200) loss: 2929.714163\n",
      "(Epoch 12 / 20) train acc: 0.070000; val_acc: 0.072222\n",
      "(Iteration 121 / 200) loss: 2636.059461\n",
      "(Epoch 13 / 20) train acc: 0.063000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2687.430696\n",
      "(Epoch 14 / 20) train acc: 0.074000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 2416.261150\n",
      "(Epoch 15 / 20) train acc: 0.079000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 2626.029417\n",
      "(Epoch 16 / 20) train acc: 0.081000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 2357.625652\n",
      "(Epoch 17 / 20) train acc: 0.094000; val_acc: 0.105556\n",
      "(Iteration 171 / 200) loss: 2706.946888\n",
      "(Epoch 18 / 20) train acc: 0.078000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2323.213904\n",
      "(Epoch 19 / 20) train acc: 0.082000; val_acc: 0.119444\n",
      "(Iteration 191 / 200) loss: 2380.079670\n",
      "(Epoch 20 / 20) train acc: 0.086000; val_acc: 0.119444\n",
      "(Iteration 1 / 200) loss: 5.943575\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.159000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 4.989056\n",
      "(Epoch 2 / 20) train acc: 0.180000; val_acc: 0.158333\n",
      "(Iteration 21 / 200) loss: 3.589909\n",
      "(Epoch 3 / 20) train acc: 0.211000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 3.196992\n",
      "(Epoch 4 / 20) train acc: 0.240000; val_acc: 0.247222\n",
      "(Iteration 41 / 200) loss: 2.984549\n",
      "(Epoch 5 / 20) train acc: 0.283000; val_acc: 0.272222\n",
      "(Iteration 51 / 200) loss: 2.641676\n",
      "(Epoch 6 / 20) train acc: 0.323000; val_acc: 0.294444\n",
      "(Iteration 61 / 200) loss: 2.219983\n",
      "(Epoch 7 / 20) train acc: 0.330000; val_acc: 0.327778\n",
      "(Iteration 71 / 200) loss: 2.030181\n",
      "(Epoch 8 / 20) train acc: 0.429000; val_acc: 0.388889\n",
      "(Iteration 81 / 200) loss: 1.882198\n",
      "(Epoch 9 / 20) train acc: 0.460000; val_acc: 0.458333\n",
      "(Iteration 91 / 200) loss: 1.760346\n",
      "(Epoch 10 / 20) train acc: 0.560000; val_acc: 0.488889\n",
      "(Iteration 101 / 200) loss: 1.405298\n",
      "(Epoch 11 / 20) train acc: 0.553000; val_acc: 0.544444\n",
      "(Iteration 111 / 200) loss: 1.446984\n",
      "(Epoch 12 / 20) train acc: 0.628000; val_acc: 0.600000\n",
      "(Iteration 121 / 200) loss: 1.198017\n",
      "(Epoch 13 / 20) train acc: 0.688000; val_acc: 0.644444\n",
      "(Iteration 131 / 200) loss: 1.066204\n",
      "(Epoch 14 / 20) train acc: 0.709000; val_acc: 0.694444\n",
      "(Iteration 141 / 200) loss: 0.942544\n",
      "(Epoch 15 / 20) train acc: 0.772000; val_acc: 0.730556\n",
      "(Iteration 151 / 200) loss: 0.985759\n",
      "(Epoch 16 / 20) train acc: 0.806000; val_acc: 0.758333\n",
      "(Iteration 161 / 200) loss: 0.848078\n",
      "(Epoch 17 / 20) train acc: 0.793000; val_acc: 0.761111\n",
      "(Iteration 171 / 200) loss: 0.912492\n",
      "(Epoch 18 / 20) train acc: 0.810000; val_acc: 0.780556\n",
      "(Iteration 181 / 200) loss: 0.726698\n",
      "(Epoch 19 / 20) train acc: 0.835000; val_acc: 0.797222\n",
      "(Iteration 191 / 200) loss: 0.587330\n",
      "(Epoch 20 / 20) train acc: 0.857000; val_acc: 0.808333\n",
      "(Iteration 1 / 200) loss: 2.303142\n",
      "(Epoch 0 / 20) train acc: 0.154000; val_acc: 0.136111\n",
      "(Epoch 1 / 20) train acc: 0.248000; val_acc: 0.194444\n",
      "(Iteration 11 / 200) loss: 2.299880\n",
      "(Epoch 2 / 20) train acc: 0.421000; val_acc: 0.383333\n",
      "(Iteration 21 / 200) loss: 2.294789\n",
      "(Epoch 3 / 20) train acc: 0.427000; val_acc: 0.369444\n",
      "(Iteration 31 / 200) loss: 2.288787\n",
      "(Epoch 4 / 20) train acc: 0.471000; val_acc: 0.419444\n",
      "(Iteration 41 / 200) loss: 2.278123\n",
      "(Epoch 5 / 20) train acc: 0.552000; val_acc: 0.466667\n",
      "(Iteration 51 / 200) loss: 2.262219\n",
      "(Epoch 6 / 20) train acc: 0.524000; val_acc: 0.497222\n",
      "(Iteration 61 / 200) loss: 2.246198\n",
      "(Epoch 7 / 20) train acc: 0.530000; val_acc: 0.463889\n",
      "(Iteration 71 / 200) loss: 2.218302\n",
      "(Epoch 8 / 20) train acc: 0.507000; val_acc: 0.483333\n",
      "(Iteration 81 / 200) loss: 2.164278\n",
      "(Epoch 9 / 20) train acc: 0.540000; val_acc: 0.522222\n",
      "(Iteration 91 / 200) loss: 2.121967\n",
      "(Epoch 10 / 20) train acc: 0.634000; val_acc: 0.627778\n",
      "(Iteration 101 / 200) loss: 2.052810\n",
      "(Epoch 11 / 20) train acc: 0.630000; val_acc: 0.647222\n",
      "(Iteration 111 / 200) loss: 1.958062\n",
      "(Epoch 12 / 20) train acc: 0.632000; val_acc: 0.638889\n",
      "(Iteration 121 / 200) loss: 1.853486\n",
      "(Epoch 13 / 20) train acc: 0.607000; val_acc: 0.663889\n",
      "(Iteration 131 / 200) loss: 1.755817\n",
      "(Epoch 14 / 20) train acc: 0.612000; val_acc: 0.672222\n",
      "(Iteration 141 / 200) loss: 1.733437\n",
      "(Epoch 15 / 20) train acc: 0.697000; val_acc: 0.688889\n",
      "(Iteration 151 / 200) loss: 1.556596\n",
      "(Epoch 16 / 20) train acc: 0.715000; val_acc: 0.694444\n",
      "(Iteration 161 / 200) loss: 1.415762\n",
      "(Epoch 17 / 20) train acc: 0.717000; val_acc: 0.730556\n",
      "(Iteration 171 / 200) loss: 1.429575\n",
      "(Epoch 18 / 20) train acc: 0.702000; val_acc: 0.708333\n",
      "(Iteration 181 / 200) loss: 1.313866\n",
      "(Epoch 19 / 20) train acc: 0.715000; val_acc: 0.750000\n",
      "(Iteration 191 / 200) loss: 1.122025\n",
      "(Epoch 20 / 20) train acc: 0.769000; val_acc: 0.772222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302499\n",
      "(Epoch 2 / 20) train acc: 0.293000; val_acc: 0.236111\n",
      "(Iteration 21 / 200) loss: 2.301931\n",
      "(Epoch 3 / 20) train acc: 0.410000; val_acc: 0.377778\n",
      "(Iteration 31 / 200) loss: 2.300167\n",
      "(Epoch 4 / 20) train acc: 0.455000; val_acc: 0.380556\n",
      "(Iteration 41 / 200) loss: 2.295332\n",
      "(Epoch 5 / 20) train acc: 0.489000; val_acc: 0.427778\n",
      "(Iteration 51 / 200) loss: 2.283947\n",
      "(Epoch 6 / 20) train acc: 0.564000; val_acc: 0.486111\n",
      "(Iteration 61 / 200) loss: 2.263360\n",
      "(Epoch 7 / 20) train acc: 0.527000; val_acc: 0.491667\n",
      "(Iteration 71 / 200) loss: 2.230554\n",
      "(Epoch 8 / 20) train acc: 0.569000; val_acc: 0.508333\n",
      "(Iteration 81 / 200) loss: 2.178903\n",
      "(Epoch 9 / 20) train acc: 0.528000; val_acc: 0.522222\n",
      "(Iteration 91 / 200) loss: 2.118724\n",
      "(Epoch 10 / 20) train acc: 0.513000; val_acc: 0.500000\n",
      "(Iteration 101 / 200) loss: 2.016368\n",
      "(Epoch 11 / 20) train acc: 0.488000; val_acc: 0.500000\n",
      "(Iteration 111 / 200) loss: 1.915125\n",
      "(Epoch 12 / 20) train acc: 0.470000; val_acc: 0.486111\n",
      "(Iteration 121 / 200) loss: 1.826773\n",
      "(Epoch 13 / 20) train acc: 0.496000; val_acc: 0.483333\n",
      "(Iteration 131 / 200) loss: 1.707622\n",
      "(Epoch 14 / 20) train acc: 0.543000; val_acc: 0.527778\n",
      "(Iteration 141 / 200) loss: 1.621602\n",
      "(Epoch 15 / 20) train acc: 0.550000; val_acc: 0.541667\n",
      "(Iteration 151 / 200) loss: 1.598458\n",
      "(Epoch 16 / 20) train acc: 0.584000; val_acc: 0.600000\n",
      "(Iteration 161 / 200) loss: 1.499667\n",
      "(Epoch 17 / 20) train acc: 0.644000; val_acc: 0.652778\n",
      "(Iteration 171 / 200) loss: 1.426807\n",
      "(Epoch 18 / 20) train acc: 0.676000; val_acc: 0.691667\n",
      "(Iteration 181 / 200) loss: 1.307864\n",
      "(Epoch 19 / 20) train acc: 0.654000; val_acc: 0.661111\n",
      "(Iteration 191 / 200) loss: 1.223412\n",
      "(Epoch 20 / 20) train acc: 0.677000; val_acc: 0.705556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302537\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 2.302442\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.133333\n",
      "(Iteration 31 / 200) loss: 2.301404\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.111111\n",
      "(Iteration 41 / 200) loss: 2.298580\n",
      "(Epoch 5 / 20) train acc: 0.125000; val_acc: 0.125000\n",
      "(Iteration 51 / 200) loss: 2.290578\n",
      "(Epoch 6 / 20) train acc: 0.153000; val_acc: 0.138889\n",
      "(Iteration 61 / 200) loss: 2.276986\n",
      "(Epoch 7 / 20) train acc: 0.124000; val_acc: 0.136111\n",
      "(Iteration 71 / 200) loss: 2.260393\n",
      "(Epoch 8 / 20) train acc: 0.177000; val_acc: 0.177778\n",
      "(Iteration 81 / 200) loss: 2.216314\n",
      "(Epoch 9 / 20) train acc: 0.198000; val_acc: 0.211111\n",
      "(Iteration 91 / 200) loss: 2.199549\n",
      "(Epoch 10 / 20) train acc: 0.200000; val_acc: 0.177778\n",
      "(Iteration 101 / 200) loss: 2.149051\n",
      "(Epoch 11 / 20) train acc: 0.230000; val_acc: 0.177778\n",
      "(Iteration 111 / 200) loss: 2.133660\n",
      "(Epoch 12 / 20) train acc: 0.194000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 2.068621\n",
      "(Epoch 13 / 20) train acc: 0.214000; val_acc: 0.202778\n",
      "(Iteration 131 / 200) loss: 2.097987\n",
      "(Epoch 14 / 20) train acc: 0.294000; val_acc: 0.283333\n",
      "(Iteration 141 / 200) loss: 1.987716\n",
      "(Epoch 15 / 20) train acc: 0.272000; val_acc: 0.305556\n",
      "(Iteration 151 / 200) loss: 1.948040\n",
      "(Epoch 16 / 20) train acc: 0.247000; val_acc: 0.291667\n",
      "(Iteration 161 / 200) loss: 1.903484\n",
      "(Epoch 17 / 20) train acc: 0.290000; val_acc: 0.311111\n",
      "(Iteration 171 / 200) loss: 1.922128\n",
      "(Epoch 18 / 20) train acc: 0.336000; val_acc: 0.347222\n",
      "(Iteration 181 / 200) loss: 1.787507\n",
      "(Epoch 19 / 20) train acc: 0.387000; val_acc: 0.369444\n",
      "(Iteration 191 / 200) loss: 1.724876\n",
      "(Epoch 20 / 20) train acc: 0.403000; val_acc: 0.369444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302528\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 2.302316\n",
      "(Epoch 3 / 20) train acc: 0.119000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.301724\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.298477\n",
      "(Epoch 5 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.289482\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.267008\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.242242\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.253918\n",
      "(Epoch 9 / 20) train acc: 0.088000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.202140\n",
      "(Epoch 10 / 20) train acc: 0.118000; val_acc: 0.105556\n",
      "(Iteration 101 / 200) loss: 2.165410\n",
      "(Epoch 11 / 20) train acc: 0.145000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.118198\n",
      "(Epoch 12 / 20) train acc: 0.128000; val_acc: 0.122222\n",
      "(Iteration 121 / 200) loss: 2.131866\n",
      "(Epoch 13 / 20) train acc: 0.174000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 2.099819\n",
      "(Epoch 14 / 20) train acc: 0.180000; val_acc: 0.141667\n",
      "(Iteration 141 / 200) loss: 2.026072\n",
      "(Epoch 15 / 20) train acc: 0.178000; val_acc: 0.150000\n",
      "(Iteration 151 / 200) loss: 2.096436\n",
      "(Epoch 16 / 20) train acc: 0.202000; val_acc: 0.163889\n",
      "(Iteration 161 / 200) loss: 1.964213\n",
      "(Epoch 17 / 20) train acc: 0.179000; val_acc: 0.163889\n",
      "(Iteration 171 / 200) loss: 1.999042\n",
      "(Epoch 18 / 20) train acc: 0.204000; val_acc: 0.163889\n",
      "(Iteration 181 / 200) loss: 1.945725\n",
      "(Epoch 19 / 20) train acc: 0.184000; val_acc: 0.163889\n",
      "(Iteration 191 / 200) loss: 1.954011\n",
      "(Epoch 20 / 20) train acc: 0.177000; val_acc: 0.175000\n",
      "(Iteration 1 / 200) loss: 4870.984307\n",
      "(Epoch 0 / 20) train acc: 0.063000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.066000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 4691.736372\n",
      "(Epoch 2 / 20) train acc: 0.058000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 5058.890625\n",
      "(Epoch 3 / 20) train acc: 0.047000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 4867.792759\n",
      "(Epoch 4 / 20) train acc: 0.057000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 4250.634937\n",
      "(Epoch 5 / 20) train acc: 0.059000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 4671.186194\n",
      "(Epoch 6 / 20) train acc: 0.069000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 4492.132773\n",
      "(Epoch 7 / 20) train acc: 0.048000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 5023.967168\n",
      "(Epoch 8 / 20) train acc: 0.054000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 5001.371245\n",
      "(Epoch 9 / 20) train acc: 0.060000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 4003.297812\n",
      "(Epoch 10 / 20) train acc: 0.053000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 4785.105312\n",
      "(Epoch 11 / 20) train acc: 0.062000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 4116.677522\n",
      "(Epoch 12 / 20) train acc: 0.062000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 4717.664731\n",
      "(Epoch 13 / 20) train acc: 0.062000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 4309.333499\n",
      "(Epoch 14 / 20) train acc: 0.068000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 4875.074790\n",
      "(Epoch 15 / 20) train acc: 0.055000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 4523.583242\n",
      "(Epoch 16 / 20) train acc: 0.057000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 4791.295449\n",
      "(Epoch 17 / 20) train acc: 0.072000; val_acc: 0.094444\n",
      "(Iteration 171 / 200) loss: 4500.850781\n",
      "(Epoch 18 / 20) train acc: 0.054000; val_acc: 0.094444\n",
      "(Iteration 181 / 200) loss: 4577.074248\n",
      "(Epoch 19 / 20) train acc: 0.050000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 4602.899277\n",
      "(Epoch 20 / 20) train acc: 0.057000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 5.725950\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.108333\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 5.594503\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 5.351410\n",
      "(Epoch 3 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 5.234885\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.102778\n",
      "(Iteration 41 / 200) loss: 4.940316\n",
      "(Epoch 5 / 20) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 5.371467\n",
      "(Epoch 6 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 5.476307\n",
      "(Epoch 7 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 5.036752\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 4.711998\n",
      "(Epoch 9 / 20) train acc: 0.084000; val_acc: 0.108333\n",
      "(Iteration 91 / 200) loss: 4.601623\n",
      "(Epoch 10 / 20) train acc: 0.083000; val_acc: 0.108333\n",
      "(Iteration 101 / 200) loss: 4.876664\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.111111\n",
      "(Iteration 111 / 200) loss: 4.927244\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.111111\n",
      "(Iteration 121 / 200) loss: 4.793436\n",
      "(Epoch 13 / 20) train acc: 0.077000; val_acc: 0.111111\n",
      "(Iteration 131 / 200) loss: 4.692472\n",
      "(Epoch 14 / 20) train acc: 0.086000; val_acc: 0.111111\n",
      "(Iteration 141 / 200) loss: 4.532846\n",
      "(Epoch 15 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 4.361808\n",
      "(Epoch 16 / 20) train acc: 0.089000; val_acc: 0.122222\n",
      "(Iteration 161 / 200) loss: 4.626118\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.125000\n",
      "(Iteration 171 / 200) loss: 4.449380\n",
      "(Epoch 18 / 20) train acc: 0.094000; val_acc: 0.125000\n",
      "(Iteration 181 / 200) loss: 4.481162\n",
      "(Epoch 19 / 20) train acc: 0.117000; val_acc: 0.127778\n",
      "(Iteration 191 / 200) loss: 4.210041\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 2.310757\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.133000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.309910\n",
      "(Epoch 2 / 20) train acc: 0.168000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 2.310110\n",
      "(Epoch 3 / 20) train acc: 0.197000; val_acc: 0.180556\n",
      "(Iteration 31 / 200) loss: 2.309729\n",
      "(Epoch 4 / 20) train acc: 0.222000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 2.309103\n",
      "(Epoch 5 / 20) train acc: 0.227000; val_acc: 0.200000\n",
      "(Iteration 51 / 200) loss: 2.308689\n",
      "(Epoch 6 / 20) train acc: 0.238000; val_acc: 0.200000\n",
      "(Iteration 61 / 200) loss: 2.307968\n",
      "(Epoch 7 / 20) train acc: 0.243000; val_acc: 0.205556\n",
      "(Iteration 71 / 200) loss: 2.307439\n",
      "(Epoch 8 / 20) train acc: 0.245000; val_acc: 0.202778\n",
      "(Iteration 81 / 200) loss: 2.306789\n",
      "(Epoch 9 / 20) train acc: 0.245000; val_acc: 0.216667\n",
      "(Iteration 91 / 200) loss: 2.307291\n",
      "(Epoch 10 / 20) train acc: 0.279000; val_acc: 0.225000\n",
      "(Iteration 101 / 200) loss: 2.306410\n",
      "(Epoch 11 / 20) train acc: 0.292000; val_acc: 0.238889\n",
      "(Iteration 111 / 200) loss: 2.305933\n",
      "(Epoch 12 / 20) train acc: 0.307000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 2.304803\n",
      "(Epoch 13 / 20) train acc: 0.306000; val_acc: 0.258333\n",
      "(Iteration 131 / 200) loss: 2.305362\n",
      "(Epoch 14 / 20) train acc: 0.314000; val_acc: 0.294444\n",
      "(Iteration 141 / 200) loss: 2.303737\n",
      "(Epoch 15 / 20) train acc: 0.363000; val_acc: 0.322222\n",
      "(Iteration 151 / 200) loss: 2.303304\n",
      "(Epoch 16 / 20) train acc: 0.377000; val_acc: 0.327778\n",
      "(Iteration 161 / 200) loss: 2.301481\n",
      "(Epoch 17 / 20) train acc: 0.399000; val_acc: 0.333333\n",
      "(Iteration 171 / 200) loss: 2.301426\n",
      "(Epoch 18 / 20) train acc: 0.403000; val_acc: 0.350000\n",
      "(Iteration 181 / 200) loss: 2.300698\n",
      "(Epoch 19 / 20) train acc: 0.402000; val_acc: 0.358333\n",
      "(Iteration 191 / 200) loss: 2.299074\n",
      "(Epoch 20 / 20) train acc: 0.406000; val_acc: 0.363889\n",
      "(Iteration 1 / 200) loss: 2.302665\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302649\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302642\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302629\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302625\n",
      "(Epoch 5 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302615\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302580\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302613\n",
      "(Epoch 8 / 20) train acc: 0.182000; val_acc: 0.175000\n",
      "(Iteration 81 / 200) loss: 2.302576\n",
      "(Epoch 9 / 20) train acc: 0.229000; val_acc: 0.202778\n",
      "(Iteration 91 / 200) loss: 2.302575\n",
      "(Epoch 10 / 20) train acc: 0.218000; val_acc: 0.202778\n",
      "(Iteration 101 / 200) loss: 2.302478\n",
      "(Epoch 11 / 20) train acc: 0.303000; val_acc: 0.288889\n",
      "(Iteration 111 / 200) loss: 2.302491\n",
      "(Epoch 12 / 20) train acc: 0.292000; val_acc: 0.258333\n",
      "(Iteration 121 / 200) loss: 2.302358\n",
      "(Epoch 13 / 20) train acc: 0.306000; val_acc: 0.283333\n",
      "(Iteration 131 / 200) loss: 2.302297\n",
      "(Epoch 14 / 20) train acc: 0.327000; val_acc: 0.308333\n",
      "(Iteration 141 / 200) loss: 2.302210\n",
      "(Epoch 15 / 20) train acc: 0.340000; val_acc: 0.316667\n",
      "(Iteration 151 / 200) loss: 2.302024\n",
      "(Epoch 16 / 20) train acc: 0.320000; val_acc: 0.319444\n",
      "(Iteration 161 / 200) loss: 2.301719\n",
      "(Epoch 17 / 20) train acc: 0.340000; val_acc: 0.316667\n",
      "(Iteration 171 / 200) loss: 2.301377\n",
      "(Epoch 18 / 20) train acc: 0.325000; val_acc: 0.322222\n",
      "(Iteration 181 / 200) loss: 2.300951\n",
      "(Epoch 19 / 20) train acc: 0.310000; val_acc: 0.311111\n",
      "(Iteration 191 / 200) loss: 2.300549\n",
      "(Epoch 20 / 20) train acc: 0.330000; val_acc: 0.333333\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302582\n",
      "(Epoch 5 / 20) train acc: 0.091000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302586\n",
      "(Epoch 6 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.302594\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.302608\n",
      "(Epoch 8 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.302575\n",
      "(Epoch 9 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.302617\n",
      "(Epoch 10 / 20) train acc: 0.128000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302567\n",
      "(Epoch 11 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302573\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302567\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302586\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302592\n",
      "(Epoch 15 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302487\n",
      "(Epoch 16 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302562\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302603\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302612\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302560\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.122000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302576\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302591\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302567\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302591\n",
      "(Epoch 6 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302562\n",
      "(Epoch 7 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302604\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302559\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302573\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302563\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302505\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302534\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302605\n",
      "(Epoch 14 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302571\n",
      "(Epoch 15 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302555\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302442\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302428\n",
      "(Epoch 18 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302393\n",
      "(Epoch 19 / 20) train acc: 0.118000; val_acc: 0.111111\n",
      "(Iteration 191 / 200) loss: 2.301767\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 4183.515125\n",
      "(Epoch 0 / 20) train acc: 0.137000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 4336.155933\n",
      "(Epoch 2 / 20) train acc: 0.128000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 4497.434897\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.119444\n",
      "(Iteration 31 / 200) loss: 4158.079524\n",
      "(Epoch 4 / 20) train acc: 0.129000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 3944.401707\n",
      "(Epoch 5 / 20) train acc: 0.128000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 4373.322310\n",
      "(Epoch 6 / 20) train acc: 0.122000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 4608.195400\n",
      "(Epoch 7 / 20) train acc: 0.155000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 4431.203181\n",
      "(Epoch 8 / 20) train acc: 0.135000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 4064.067224\n",
      "(Epoch 9 / 20) train acc: 0.128000; val_acc: 0.122222\n",
      "(Iteration 91 / 200) loss: 3950.671899\n",
      "(Epoch 10 / 20) train acc: 0.136000; val_acc: 0.122222\n",
      "(Iteration 101 / 200) loss: 4830.035044\n",
      "(Epoch 11 / 20) train acc: 0.137000; val_acc: 0.122222\n",
      "(Iteration 111 / 200) loss: 4563.777864\n",
      "(Epoch 12 / 20) train acc: 0.130000; val_acc: 0.122222\n",
      "(Iteration 121 / 200) loss: 4529.543799\n",
      "(Epoch 13 / 20) train acc: 0.136000; val_acc: 0.122222\n",
      "(Iteration 131 / 200) loss: 4004.027249\n",
      "(Epoch 14 / 20) train acc: 0.138000; val_acc: 0.122222\n",
      "(Iteration 141 / 200) loss: 4051.723516\n",
      "(Epoch 15 / 20) train acc: 0.152000; val_acc: 0.122222\n",
      "(Iteration 151 / 200) loss: 4063.149441\n",
      "(Epoch 16 / 20) train acc: 0.147000; val_acc: 0.125000\n",
      "(Iteration 161 / 200) loss: 4078.941313\n",
      "(Epoch 17 / 20) train acc: 0.127000; val_acc: 0.125000\n",
      "(Iteration 171 / 200) loss: 3993.404734\n",
      "(Epoch 18 / 20) train acc: 0.139000; val_acc: 0.125000\n",
      "(Iteration 181 / 200) loss: 3699.437529\n",
      "(Epoch 19 / 20) train acc: 0.117000; val_acc: 0.127778\n",
      "(Iteration 191 / 200) loss: 4030.167192\n",
      "(Epoch 20 / 20) train acc: 0.129000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 6.807253\n",
      "(Epoch 0 / 20) train acc: 0.058000; val_acc: 0.050000\n",
      "(Epoch 1 / 20) train acc: 0.055000; val_acc: 0.050000\n",
      "(Iteration 11 / 200) loss: 6.531621\n",
      "(Epoch 2 / 20) train acc: 0.061000; val_acc: 0.055556\n",
      "(Iteration 21 / 200) loss: 5.970517\n",
      "(Epoch 3 / 20) train acc: 0.060000; val_acc: 0.058333\n",
      "(Iteration 31 / 200) loss: 6.182279\n",
      "(Epoch 4 / 20) train acc: 0.059000; val_acc: 0.063889\n",
      "(Iteration 41 / 200) loss: 6.215512\n",
      "(Epoch 5 / 20) train acc: 0.046000; val_acc: 0.063889\n",
      "(Iteration 51 / 200) loss: 6.244442\n",
      "(Epoch 6 / 20) train acc: 0.082000; val_acc: 0.066667\n",
      "(Iteration 61 / 200) loss: 5.824392\n",
      "(Epoch 7 / 20) train acc: 0.089000; val_acc: 0.066667\n",
      "(Iteration 71 / 200) loss: 5.484666\n",
      "(Epoch 8 / 20) train acc: 0.075000; val_acc: 0.066667\n",
      "(Iteration 81 / 200) loss: 5.519157\n",
      "(Epoch 9 / 20) train acc: 0.090000; val_acc: 0.072222\n",
      "(Iteration 91 / 200) loss: 5.462051\n",
      "(Epoch 10 / 20) train acc: 0.084000; val_acc: 0.075000\n",
      "(Iteration 101 / 200) loss: 4.888383\n",
      "(Epoch 11 / 20) train acc: 0.083000; val_acc: 0.077778\n",
      "(Iteration 111 / 200) loss: 5.149674\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.077778\n",
      "(Iteration 121 / 200) loss: 5.067480\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 5.414031\n",
      "(Epoch 14 / 20) train acc: 0.112000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 5.075305\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Iteration 151 / 200) loss: 5.110353\n",
      "(Epoch 16 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 4.769437\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 4.591989\n",
      "(Epoch 18 / 20) train acc: 0.122000; val_acc: 0.105556\n",
      "(Iteration 181 / 200) loss: 4.688235\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.105556\n",
      "(Iteration 191 / 200) loss: 4.887044\n",
      "(Epoch 20 / 20) train acc: 0.122000; val_acc: 0.111111\n",
      "(Iteration 1 / 200) loss: 2.310629\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.310056\n",
      "(Epoch 2 / 20) train acc: 0.146000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.309677\n",
      "(Epoch 3 / 20) train acc: 0.127000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.309235\n",
      "(Epoch 4 / 20) train acc: 0.144000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 2.308588\n",
      "(Epoch 5 / 20) train acc: 0.140000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 2.308613\n",
      "(Epoch 6 / 20) train acc: 0.165000; val_acc: 0.102778\n",
      "(Iteration 61 / 200) loss: 2.307659\n",
      "(Epoch 7 / 20) train acc: 0.145000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.307683\n",
      "(Epoch 8 / 20) train acc: 0.173000; val_acc: 0.102778\n",
      "(Iteration 81 / 200) loss: 2.307332\n",
      "(Epoch 9 / 20) train acc: 0.152000; val_acc: 0.119444\n",
      "(Iteration 91 / 200) loss: 2.306197\n",
      "(Epoch 10 / 20) train acc: 0.182000; val_acc: 0.133333\n",
      "(Iteration 101 / 200) loss: 2.306129\n",
      "(Epoch 11 / 20) train acc: 0.196000; val_acc: 0.155556\n",
      "(Iteration 111 / 200) loss: 2.305466\n",
      "(Epoch 12 / 20) train acc: 0.229000; val_acc: 0.186111\n",
      "(Iteration 121 / 200) loss: 2.304901\n",
      "(Epoch 13 / 20) train acc: 0.281000; val_acc: 0.205556\n",
      "(Iteration 131 / 200) loss: 2.304843\n",
      "(Epoch 14 / 20) train acc: 0.273000; val_acc: 0.222222\n",
      "(Iteration 141 / 200) loss: 2.303704\n",
      "(Epoch 15 / 20) train acc: 0.279000; val_acc: 0.227778\n",
      "(Iteration 151 / 200) loss: 2.302309\n",
      "(Epoch 16 / 20) train acc: 0.287000; val_acc: 0.238889\n",
      "(Iteration 161 / 200) loss: 2.302091\n",
      "(Epoch 17 / 20) train acc: 0.338000; val_acc: 0.241667\n",
      "(Iteration 171 / 200) loss: 2.302162\n",
      "(Epoch 18 / 20) train acc: 0.318000; val_acc: 0.258333\n",
      "(Iteration 181 / 200) loss: 2.300050\n",
      "(Epoch 19 / 20) train acc: 0.323000; val_acc: 0.275000\n",
      "(Iteration 191 / 200) loss: 2.298802\n",
      "(Epoch 20 / 20) train acc: 0.362000; val_acc: 0.291667\n",
      "(Iteration 1 / 200) loss: 2.302666\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302654\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302644\n",
      "(Epoch 3 / 20) train acc: 0.248000; val_acc: 0.263889\n",
      "(Iteration 31 / 200) loss: 2.302641\n",
      "(Epoch 4 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302629\n",
      "(Epoch 5 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302618\n",
      "(Epoch 6 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302595\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302575\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302553\n",
      "(Epoch 9 / 20) train acc: 0.261000; val_acc: 0.211111\n",
      "(Iteration 91 / 200) loss: 2.302563\n",
      "(Epoch 10 / 20) train acc: 0.300000; val_acc: 0.241667\n",
      "(Iteration 101 / 200) loss: 2.302500\n",
      "(Epoch 11 / 20) train acc: 0.279000; val_acc: 0.216667\n",
      "(Iteration 111 / 200) loss: 2.302430\n",
      "(Epoch 12 / 20) train acc: 0.199000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 2.302324\n",
      "(Epoch 13 / 20) train acc: 0.241000; val_acc: 0.188889\n",
      "(Iteration 131 / 200) loss: 2.302112\n",
      "(Epoch 14 / 20) train acc: 0.264000; val_acc: 0.247222\n",
      "(Iteration 141 / 200) loss: 2.301992\n",
      "(Epoch 15 / 20) train acc: 0.305000; val_acc: 0.277778\n",
      "(Iteration 151 / 200) loss: 2.301654\n",
      "(Epoch 16 / 20) train acc: 0.314000; val_acc: 0.294444\n",
      "(Iteration 161 / 200) loss: 2.301325\n",
      "(Epoch 17 / 20) train acc: 0.298000; val_acc: 0.313889\n",
      "(Iteration 171 / 200) loss: 2.300847\n",
      "(Epoch 18 / 20) train acc: 0.348000; val_acc: 0.313889\n",
      "(Iteration 181 / 200) loss: 2.300340\n",
      "(Epoch 19 / 20) train acc: 0.383000; val_acc: 0.319444\n",
      "(Iteration 191 / 200) loss: 2.299762\n",
      "(Epoch 20 / 20) train acc: 0.377000; val_acc: 0.300000\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302590\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302592\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302564\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302586\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302596\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302588\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302567\n",
      "(Epoch 10 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302543\n",
      "(Epoch 11 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302573\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302584\n",
      "(Epoch 13 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302581\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302581\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302601\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302573\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302607\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302545\n",
      "(Epoch 19 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302514\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302589\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302588\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302572\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302581\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302589\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302569\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302579\n",
      "(Epoch 9 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302606\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302567\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302584\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302571\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302568\n",
      "(Epoch 14 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302594\n",
      "(Epoch 15 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302622\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302550\n",
      "(Epoch 17 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302534\n",
      "(Epoch 18 / 20) train acc: 0.079000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302595\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302599\n",
      "(Epoch 20 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 4251.207192\n",
      "(Epoch 0 / 20) train acc: 0.139000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 4644.845315\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 3841.920991\n",
      "(Epoch 3 / 20) train acc: 0.142000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 3911.859524\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.119444\n",
      "(Iteration 41 / 200) loss: 4168.255925\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.119444\n",
      "(Iteration 51 / 200) loss: 4435.307329\n",
      "(Epoch 6 / 20) train acc: 0.134000; val_acc: 0.119444\n",
      "(Iteration 61 / 200) loss: 4236.966533\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.119444\n",
      "(Iteration 71 / 200) loss: 4158.074199\n",
      "(Epoch 8 / 20) train acc: 0.119000; val_acc: 0.119444\n",
      "(Iteration 81 / 200) loss: 4049.793418\n",
      "(Epoch 9 / 20) train acc: 0.133000; val_acc: 0.119444\n",
      "(Iteration 91 / 200) loss: 4135.626694\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.119444\n",
      "(Iteration 101 / 200) loss: 4242.112788\n",
      "(Epoch 11 / 20) train acc: 0.125000; val_acc: 0.119444\n",
      "(Iteration 111 / 200) loss: 4092.157622\n",
      "(Epoch 12 / 20) train acc: 0.148000; val_acc: 0.119444\n",
      "(Iteration 121 / 200) loss: 4296.712449\n",
      "(Epoch 13 / 20) train acc: 0.132000; val_acc: 0.122222\n",
      "(Iteration 131 / 200) loss: 3979.303506\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.119444\n",
      "(Iteration 141 / 200) loss: 3579.497695\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.119444\n",
      "(Iteration 151 / 200) loss: 4474.788149\n",
      "(Epoch 16 / 20) train acc: 0.122000; val_acc: 0.119444\n",
      "(Iteration 161 / 200) loss: 3729.756094\n",
      "(Epoch 17 / 20) train acc: 0.125000; val_acc: 0.119444\n",
      "(Iteration 171 / 200) loss: 4201.535935\n",
      "(Epoch 18 / 20) train acc: 0.125000; val_acc: 0.119444\n",
      "(Iteration 181 / 200) loss: 3873.873594\n",
      "(Epoch 19 / 20) train acc: 0.152000; val_acc: 0.122222\n",
      "(Iteration 191 / 200) loss: 4384.185605\n",
      "(Epoch 20 / 20) train acc: 0.121000; val_acc: 0.122222\n",
      "(Iteration 1 / 200) loss: 6.359496\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 6.885397\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 6.848936\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.111111\n",
      "(Iteration 31 / 200) loss: 5.608991\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.111111\n",
      "(Iteration 41 / 200) loss: 5.945002\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.111111\n",
      "(Iteration 51 / 200) loss: 6.871596\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.113889\n",
      "(Iteration 61 / 200) loss: 5.817025\n",
      "(Epoch 7 / 20) train acc: 0.116000; val_acc: 0.113889\n",
      "(Iteration 71 / 200) loss: 5.520409\n",
      "(Epoch 8 / 20) train acc: 0.126000; val_acc: 0.113889\n",
      "(Iteration 81 / 200) loss: 5.879419\n",
      "(Epoch 9 / 20) train acc: 0.123000; val_acc: 0.119444\n",
      "(Iteration 91 / 200) loss: 5.417640\n",
      "(Epoch 10 / 20) train acc: 0.131000; val_acc: 0.119444\n",
      "(Iteration 101 / 200) loss: 6.228750\n",
      "(Epoch 11 / 20) train acc: 0.131000; val_acc: 0.122222\n",
      "(Iteration 111 / 200) loss: 6.147524\n",
      "(Epoch 12 / 20) train acc: 0.127000; val_acc: 0.125000\n",
      "(Iteration 121 / 200) loss: 5.076593\n",
      "(Epoch 13 / 20) train acc: 0.138000; val_acc: 0.130556\n",
      "(Iteration 131 / 200) loss: 5.486234\n",
      "(Epoch 14 / 20) train acc: 0.145000; val_acc: 0.133333\n",
      "(Iteration 141 / 200) loss: 5.462604\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.136111\n",
      "(Iteration 151 / 200) loss: 5.081019\n",
      "(Epoch 16 / 20) train acc: 0.139000; val_acc: 0.136111\n",
      "(Iteration 161 / 200) loss: 5.513397\n",
      "(Epoch 17 / 20) train acc: 0.135000; val_acc: 0.138889\n",
      "(Iteration 171 / 200) loss: 4.927135\n",
      "(Epoch 18 / 20) train acc: 0.150000; val_acc: 0.141667\n",
      "(Iteration 181 / 200) loss: 4.774866\n",
      "(Epoch 19 / 20) train acc: 0.143000; val_acc: 0.144444\n",
      "(Iteration 191 / 200) loss: 4.742618\n",
      "(Epoch 20 / 20) train acc: 0.140000; val_acc: 0.147222\n",
      "(Iteration 1 / 200) loss: 2.310631\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.310023\n",
      "(Epoch 2 / 20) train acc: 0.155000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.310002\n",
      "(Epoch 3 / 20) train acc: 0.219000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 2.309582\n",
      "(Epoch 4 / 20) train acc: 0.233000; val_acc: 0.219444\n",
      "(Iteration 41 / 200) loss: 2.308997\n",
      "(Epoch 5 / 20) train acc: 0.276000; val_acc: 0.275000\n",
      "(Iteration 51 / 200) loss: 2.308491\n",
      "(Epoch 6 / 20) train acc: 0.348000; val_acc: 0.297222\n",
      "(Iteration 61 / 200) loss: 2.308337\n",
      "(Epoch 7 / 20) train acc: 0.395000; val_acc: 0.341667\n",
      "(Iteration 71 / 200) loss: 2.307485\n",
      "(Epoch 8 / 20) train acc: 0.409000; val_acc: 0.363889\n",
      "(Iteration 81 / 200) loss: 2.307207\n",
      "(Epoch 9 / 20) train acc: 0.452000; val_acc: 0.391667\n",
      "(Iteration 91 / 200) loss: 2.306580\n",
      "(Epoch 10 / 20) train acc: 0.473000; val_acc: 0.436111\n",
      "(Iteration 101 / 200) loss: 2.306545\n",
      "(Epoch 11 / 20) train acc: 0.504000; val_acc: 0.494444\n",
      "(Iteration 111 / 200) loss: 2.306026\n",
      "(Epoch 12 / 20) train acc: 0.546000; val_acc: 0.527778\n",
      "(Iteration 121 / 200) loss: 2.305085\n",
      "(Epoch 13 / 20) train acc: 0.570000; val_acc: 0.572222\n",
      "(Iteration 131 / 200) loss: 2.304994\n",
      "(Epoch 14 / 20) train acc: 0.593000; val_acc: 0.594444\n",
      "(Iteration 141 / 200) loss: 2.304494\n",
      "(Epoch 15 / 20) train acc: 0.600000; val_acc: 0.600000\n",
      "(Iteration 151 / 200) loss: 2.304454\n",
      "(Epoch 16 / 20) train acc: 0.631000; val_acc: 0.611111\n",
      "(Iteration 161 / 200) loss: 2.302589\n",
      "(Epoch 17 / 20) train acc: 0.620000; val_acc: 0.619444\n",
      "(Iteration 171 / 200) loss: 2.302769\n",
      "(Epoch 18 / 20) train acc: 0.621000; val_acc: 0.625000\n",
      "(Iteration 181 / 200) loss: 2.300461\n",
      "(Epoch 19 / 20) train acc: 0.614000; val_acc: 0.625000\n",
      "(Iteration 191 / 200) loss: 2.300139\n",
      "(Epoch 20 / 20) train acc: 0.633000; val_acc: 0.622222\n",
      "(Iteration 1 / 200) loss: 2.302667\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302653\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302639\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302644\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302623\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302609\n",
      "(Epoch 6 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302593\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302565\n",
      "(Epoch 8 / 20) train acc: 0.127000; val_acc: 0.088889\n",
      "(Iteration 81 / 200) loss: 2.302584\n",
      "(Epoch 9 / 20) train acc: 0.196000; val_acc: 0.155556\n",
      "(Iteration 91 / 200) loss: 2.302531\n",
      "(Epoch 10 / 20) train acc: 0.204000; val_acc: 0.161111\n",
      "(Iteration 101 / 200) loss: 2.302512\n",
      "(Epoch 11 / 20) train acc: 0.198000; val_acc: 0.163889\n",
      "(Iteration 111 / 200) loss: 2.302435\n",
      "(Epoch 12 / 20) train acc: 0.208000; val_acc: 0.163889\n",
      "(Iteration 121 / 200) loss: 2.302306\n",
      "(Epoch 13 / 20) train acc: 0.236000; val_acc: 0.188889\n",
      "(Iteration 131 / 200) loss: 2.302241\n",
      "(Epoch 14 / 20) train acc: 0.299000; val_acc: 0.247222\n",
      "(Iteration 141 / 200) loss: 2.302083\n",
      "(Epoch 15 / 20) train acc: 0.294000; val_acc: 0.252778\n",
      "(Iteration 151 / 200) loss: 2.301746\n",
      "(Epoch 16 / 20) train acc: 0.279000; val_acc: 0.244444\n",
      "(Iteration 161 / 200) loss: 2.301486\n",
      "(Epoch 17 / 20) train acc: 0.284000; val_acc: 0.247222\n",
      "(Iteration 171 / 200) loss: 2.301211\n",
      "(Epoch 18 / 20) train acc: 0.261000; val_acc: 0.247222\n",
      "(Iteration 181 / 200) loss: 2.300549\n",
      "(Epoch 19 / 20) train acc: 0.283000; val_acc: 0.233333\n",
      "(Iteration 191 / 200) loss: 2.299696\n",
      "(Epoch 20 / 20) train acc: 0.298000; val_acc: 0.263889\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302589\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302586\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302570\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302586\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302577\n",
      "(Epoch 9 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302558\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302604\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302531\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302558\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.302571\n",
      "(Epoch 14 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.302622\n",
      "(Epoch 15 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.302578\n",
      "(Epoch 16 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Iteration 161 / 200) loss: 2.302632\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 2.302569\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 181 / 200) loss: 2.302597\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 191 / 200) loss: 2.302564\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302576\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302584\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302579\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302596\n",
      "(Epoch 7 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302603\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302593\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302593\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302559\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302605\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302603\n",
      "(Epoch 13 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302570\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302496\n",
      "(Epoch 15 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302468\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302413\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302236\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302142\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301849\n",
      "(Epoch 20 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2808.899578\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 3011.799336\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.111111\n",
      "(Iteration 21 / 200) loss: 2977.853196\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.111111\n",
      "(Iteration 31 / 200) loss: 3213.447685\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 2954.965612\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.113889\n",
      "(Iteration 51 / 200) loss: 3320.521667\n",
      "(Epoch 6 / 20) train acc: 0.094000; val_acc: 0.113889\n",
      "(Iteration 61 / 200) loss: 3170.517410\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.113889\n",
      "(Iteration 71 / 200) loss: 3291.570338\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.113889\n",
      "(Iteration 81 / 200) loss: 3142.514831\n",
      "(Epoch 9 / 20) train acc: 0.089000; val_acc: 0.113889\n",
      "(Iteration 91 / 200) loss: 3128.546510\n",
      "(Epoch 10 / 20) train acc: 0.098000; val_acc: 0.113889\n",
      "(Iteration 101 / 200) loss: 3331.625376\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.113889\n",
      "(Iteration 111 / 200) loss: 2736.877367\n",
      "(Epoch 12 / 20) train acc: 0.082000; val_acc: 0.111111\n",
      "(Iteration 121 / 200) loss: 2798.992797\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.111111\n",
      "(Iteration 131 / 200) loss: 3151.560102\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.111111\n",
      "(Iteration 141 / 200) loss: 2512.344123\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.111111\n",
      "(Iteration 151 / 200) loss: 2990.531271\n",
      "(Epoch 16 / 20) train acc: 0.093000; val_acc: 0.108333\n",
      "(Iteration 161 / 200) loss: 3114.949824\n",
      "(Epoch 17 / 20) train acc: 0.081000; val_acc: 0.108333\n",
      "(Iteration 171 / 200) loss: 3046.739629\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.105556\n",
      "(Iteration 181 / 200) loss: 2775.365684\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.102778\n",
      "(Iteration 191 / 200) loss: 3019.371116\n",
      "(Epoch 20 / 20) train acc: 0.087000; val_acc: 0.102778\n",
      "(Iteration 1 / 200) loss: 3.319289\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 3.156531\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 3.010043\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 2.890050\n",
      "(Epoch 4 / 20) train acc: 0.140000; val_acc: 0.108333\n",
      "(Iteration 41 / 200) loss: 2.865393\n",
      "(Epoch 5 / 20) train acc: 0.128000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.938759\n",
      "(Epoch 6 / 20) train acc: 0.150000; val_acc: 0.125000\n",
      "(Iteration 61 / 200) loss: 2.942869\n",
      "(Epoch 7 / 20) train acc: 0.149000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 3.020519\n",
      "(Epoch 8 / 20) train acc: 0.146000; val_acc: 0.141667\n",
      "(Iteration 81 / 200) loss: 2.900300\n",
      "(Epoch 9 / 20) train acc: 0.183000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 2.751313\n",
      "(Epoch 10 / 20) train acc: 0.174000; val_acc: 0.141667\n",
      "(Iteration 101 / 200) loss: 2.747828\n",
      "(Epoch 11 / 20) train acc: 0.179000; val_acc: 0.147222\n",
      "(Iteration 111 / 200) loss: 2.708039\n",
      "(Epoch 12 / 20) train acc: 0.208000; val_acc: 0.150000\n",
      "(Iteration 121 / 200) loss: 2.830695\n",
      "(Epoch 13 / 20) train acc: 0.172000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 2.529592\n",
      "(Epoch 14 / 20) train acc: 0.159000; val_acc: 0.166667\n",
      "(Iteration 141 / 200) loss: 2.579448\n",
      "(Epoch 15 / 20) train acc: 0.179000; val_acc: 0.169444\n",
      "(Iteration 151 / 200) loss: 2.417439\n",
      "(Epoch 16 / 20) train acc: 0.197000; val_acc: 0.172222\n",
      "(Iteration 161 / 200) loss: 2.473616\n",
      "(Epoch 17 / 20) train acc: 0.178000; val_acc: 0.180556\n",
      "(Iteration 171 / 200) loss: 2.377236\n",
      "(Epoch 18 / 20) train acc: 0.213000; val_acc: 0.188889\n",
      "(Iteration 181 / 200) loss: 2.613734\n",
      "(Epoch 19 / 20) train acc: 0.216000; val_acc: 0.213889\n",
      "(Iteration 191 / 200) loss: 2.648887\n",
      "(Epoch 20 / 20) train acc: 0.229000; val_acc: 0.216667\n",
      "(Iteration 1 / 200) loss: 2.302945\n",
      "(Epoch 0 / 20) train acc: 0.078000; val_acc: 0.055556\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302629\n",
      "(Epoch 2 / 20) train acc: 0.131000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302500\n",
      "(Epoch 3 / 20) train acc: 0.170000; val_acc: 0.166667\n",
      "(Iteration 31 / 200) loss: 2.302764\n",
      "(Epoch 4 / 20) train acc: 0.209000; val_acc: 0.211111\n",
      "(Iteration 41 / 200) loss: 2.302172\n",
      "(Epoch 5 / 20) train acc: 0.252000; val_acc: 0.261111\n",
      "(Iteration 51 / 200) loss: 2.301154\n",
      "(Epoch 6 / 20) train acc: 0.311000; val_acc: 0.325000\n",
      "(Iteration 61 / 200) loss: 2.300944\n",
      "(Epoch 7 / 20) train acc: 0.346000; val_acc: 0.405556\n",
      "(Iteration 71 / 200) loss: 2.300677\n",
      "(Epoch 8 / 20) train acc: 0.450000; val_acc: 0.450000\n",
      "(Iteration 81 / 200) loss: 2.300239\n",
      "(Epoch 9 / 20) train acc: 0.480000; val_acc: 0.488889\n",
      "(Iteration 91 / 200) loss: 2.299321\n",
      "(Epoch 10 / 20) train acc: 0.569000; val_acc: 0.519444\n",
      "(Iteration 101 / 200) loss: 2.298075\n",
      "(Epoch 11 / 20) train acc: 0.612000; val_acc: 0.536111\n",
      "(Iteration 111 / 200) loss: 2.297918\n",
      "(Epoch 12 / 20) train acc: 0.608000; val_acc: 0.563889\n",
      "(Iteration 121 / 200) loss: 2.298102\n",
      "(Epoch 13 / 20) train acc: 0.629000; val_acc: 0.586111\n",
      "(Iteration 131 / 200) loss: 2.296804\n",
      "(Epoch 14 / 20) train acc: 0.636000; val_acc: 0.613889\n",
      "(Iteration 141 / 200) loss: 2.295396\n",
      "(Epoch 15 / 20) train acc: 0.646000; val_acc: 0.622222\n",
      "(Iteration 151 / 200) loss: 2.292786\n",
      "(Epoch 16 / 20) train acc: 0.649000; val_acc: 0.627778\n",
      "(Iteration 161 / 200) loss: 2.294788\n",
      "(Epoch 17 / 20) train acc: 0.647000; val_acc: 0.630556\n",
      "(Iteration 171 / 200) loss: 2.293618\n",
      "(Epoch 18 / 20) train acc: 0.675000; val_acc: 0.622222\n",
      "(Iteration 181 / 200) loss: 2.291190\n",
      "(Epoch 19 / 20) train acc: 0.686000; val_acc: 0.630556\n",
      "(Iteration 191 / 200) loss: 2.292175\n",
      "(Epoch 20 / 20) train acc: 0.671000; val_acc: 0.638889\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302591\n",
      "(Epoch 2 / 20) train acc: 0.192000; val_acc: 0.208333\n",
      "(Iteration 21 / 200) loss: 2.302588\n",
      "(Epoch 3 / 20) train acc: 0.202000; val_acc: 0.194444\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.208000; val_acc: 0.200000\n",
      "(Iteration 41 / 200) loss: 2.302572\n",
      "(Epoch 5 / 20) train acc: 0.204000; val_acc: 0.191667\n",
      "(Iteration 51 / 200) loss: 2.302559\n",
      "(Epoch 6 / 20) train acc: 0.227000; val_acc: 0.200000\n",
      "(Iteration 61 / 200) loss: 2.302540\n",
      "(Epoch 7 / 20) train acc: 0.266000; val_acc: 0.263889\n",
      "(Iteration 71 / 200) loss: 2.302490\n",
      "(Epoch 8 / 20) train acc: 0.277000; val_acc: 0.258333\n",
      "(Iteration 81 / 200) loss: 2.302494\n",
      "(Epoch 9 / 20) train acc: 0.285000; val_acc: 0.236111\n",
      "(Iteration 91 / 200) loss: 2.302424\n",
      "(Epoch 10 / 20) train acc: 0.223000; val_acc: 0.163889\n",
      "(Iteration 101 / 200) loss: 2.302378\n",
      "(Epoch 11 / 20) train acc: 0.231000; val_acc: 0.161111\n",
      "(Iteration 111 / 200) loss: 2.302236\n",
      "(Epoch 12 / 20) train acc: 0.214000; val_acc: 0.163889\n",
      "(Iteration 121 / 200) loss: 2.302067\n",
      "(Epoch 13 / 20) train acc: 0.224000; val_acc: 0.183333\n",
      "(Iteration 131 / 200) loss: 2.301877\n",
      "(Epoch 14 / 20) train acc: 0.302000; val_acc: 0.236111\n",
      "(Iteration 141 / 200) loss: 2.301663\n",
      "(Epoch 15 / 20) train acc: 0.380000; val_acc: 0.319444\n",
      "(Iteration 151 / 200) loss: 2.301457\n",
      "(Epoch 16 / 20) train acc: 0.436000; val_acc: 0.430556\n",
      "(Iteration 161 / 200) loss: 2.301218\n",
      "(Epoch 17 / 20) train acc: 0.517000; val_acc: 0.475000\n",
      "(Iteration 171 / 200) loss: 2.300641\n",
      "(Epoch 18 / 20) train acc: 0.471000; val_acc: 0.469444\n",
      "(Iteration 181 / 200) loss: 2.300119\n",
      "(Epoch 19 / 20) train acc: 0.416000; val_acc: 0.427778\n",
      "(Iteration 191 / 200) loss: 2.299855\n",
      "(Epoch 20 / 20) train acc: 0.475000; val_acc: 0.447222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.087000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302573\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302566\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302574\n",
      "(Epoch 6 / 20) train acc: 0.185000; val_acc: 0.194444\n",
      "(Iteration 61 / 200) loss: 2.302564\n",
      "(Epoch 7 / 20) train acc: 0.200000; val_acc: 0.194444\n",
      "(Iteration 71 / 200) loss: 2.302535\n",
      "(Epoch 8 / 20) train acc: 0.209000; val_acc: 0.197222\n",
      "(Iteration 81 / 200) loss: 2.302475\n",
      "(Epoch 9 / 20) train acc: 0.182000; val_acc: 0.158333\n",
      "(Iteration 91 / 200) loss: 2.302418\n",
      "(Epoch 10 / 20) train acc: 0.207000; val_acc: 0.155556\n",
      "(Iteration 101 / 200) loss: 2.302261\n",
      "(Epoch 11 / 20) train acc: 0.225000; val_acc: 0.194444\n",
      "(Iteration 111 / 200) loss: 2.302129\n",
      "(Epoch 12 / 20) train acc: 0.192000; val_acc: 0.155556\n",
      "(Iteration 121 / 200) loss: 2.301952\n",
      "(Epoch 13 / 20) train acc: 0.217000; val_acc: 0.172222\n",
      "(Iteration 131 / 200) loss: 2.301776\n",
      "(Epoch 14 / 20) train acc: 0.184000; val_acc: 0.175000\n",
      "(Iteration 141 / 200) loss: 2.301247\n",
      "(Epoch 15 / 20) train acc: 0.190000; val_acc: 0.172222\n",
      "(Iteration 151 / 200) loss: 2.301073\n",
      "(Epoch 16 / 20) train acc: 0.221000; val_acc: 0.180556\n",
      "(Iteration 161 / 200) loss: 2.300761\n",
      "(Epoch 17 / 20) train acc: 0.244000; val_acc: 0.247222\n",
      "(Iteration 171 / 200) loss: 2.300029\n",
      "(Epoch 18 / 20) train acc: 0.268000; val_acc: 0.244444\n",
      "(Iteration 181 / 200) loss: 2.299979\n",
      "(Epoch 19 / 20) train acc: 0.254000; val_acc: 0.236111\n",
      "(Iteration 191 / 200) loss: 2.297921\n",
      "(Epoch 20 / 20) train acc: 0.247000; val_acc: 0.241667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302580\n",
      "(Epoch 4 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302590\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302586\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302605\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302574\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302594\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302557\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302449\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302412\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.302411\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302125\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302087\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.301645\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.301625\n",
      "(Epoch 17 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 2.300726\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.300916\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.300353\n",
      "(Epoch 20 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 4124.724917\n",
      "(Epoch 0 / 20) train acc: 0.139000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 4123.505357\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 4408.667705\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 3065.152252\n",
      "(Epoch 4 / 20) train acc: 0.114000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 3983.599617\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 4391.662918\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 3945.225906\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 4080.310458\n",
      "(Epoch 8 / 20) train acc: 0.122000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 4064.463444\n",
      "(Epoch 9 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 3612.417684\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 3973.252860\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 3797.363034\n",
      "(Epoch 12 / 20) train acc: 0.118000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 3920.201334\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 4260.183697\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.091667\n",
      "(Iteration 141 / 200) loss: 4254.596999\n",
      "(Epoch 15 / 20) train acc: 0.116000; val_acc: 0.091667\n",
      "(Iteration 151 / 200) loss: 3121.788738\n",
      "(Epoch 16 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Iteration 161 / 200) loss: 3658.905479\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 171 / 200) loss: 4321.660345\n",
      "(Epoch 18 / 20) train acc: 0.126000; val_acc: 0.091667\n",
      "(Iteration 181 / 200) loss: 3725.618646\n",
      "(Epoch 19 / 20) train acc: 0.136000; val_acc: 0.091667\n",
      "(Iteration 191 / 200) loss: 3830.620385\n",
      "(Epoch 20 / 20) train acc: 0.128000; val_acc: 0.094444\n",
      "(Iteration 1 / 200) loss: 5.471208\n",
      "(Epoch 0 / 20) train acc: 0.024000; val_acc: 0.033333\n",
      "(Epoch 1 / 20) train acc: 0.026000; val_acc: 0.027778\n",
      "(Iteration 11 / 200) loss: 5.531617\n",
      "(Epoch 2 / 20) train acc: 0.034000; val_acc: 0.025000\n",
      "(Iteration 21 / 200) loss: 5.142489\n",
      "(Epoch 3 / 20) train acc: 0.032000; val_acc: 0.030556\n",
      "(Iteration 31 / 200) loss: 5.487947\n",
      "(Epoch 4 / 20) train acc: 0.021000; val_acc: 0.030556\n",
      "(Iteration 41 / 200) loss: 5.179160\n",
      "(Epoch 5 / 20) train acc: 0.038000; val_acc: 0.030556\n",
      "(Iteration 51 / 200) loss: 5.066159\n",
      "(Epoch 6 / 20) train acc: 0.027000; val_acc: 0.036111\n",
      "(Iteration 61 / 200) loss: 4.203301\n",
      "(Epoch 7 / 20) train acc: 0.035000; val_acc: 0.036111\n",
      "(Iteration 71 / 200) loss: 4.754139\n",
      "(Epoch 8 / 20) train acc: 0.049000; val_acc: 0.036111\n",
      "(Iteration 81 / 200) loss: 3.986127\n",
      "(Epoch 9 / 20) train acc: 0.045000; val_acc: 0.038889\n",
      "(Iteration 91 / 200) loss: 4.211889\n",
      "(Epoch 10 / 20) train acc: 0.038000; val_acc: 0.050000\n",
      "(Iteration 101 / 200) loss: 4.286236\n",
      "(Epoch 11 / 20) train acc: 0.053000; val_acc: 0.052778\n",
      "(Iteration 111 / 200) loss: 4.230191\n",
      "(Epoch 12 / 20) train acc: 0.058000; val_acc: 0.058333\n",
      "(Iteration 121 / 200) loss: 3.996911\n",
      "(Epoch 13 / 20) train acc: 0.054000; val_acc: 0.058333\n",
      "(Iteration 131 / 200) loss: 4.315563\n",
      "(Epoch 14 / 20) train acc: 0.067000; val_acc: 0.061111\n",
      "(Iteration 141 / 200) loss: 4.442354\n",
      "(Epoch 15 / 20) train acc: 0.067000; val_acc: 0.063889\n",
      "(Iteration 151 / 200) loss: 4.210806\n",
      "(Epoch 16 / 20) train acc: 0.081000; val_acc: 0.075000\n",
      "(Iteration 161 / 200) loss: 3.846794\n",
      "(Epoch 17 / 20) train acc: 0.075000; val_acc: 0.077778\n",
      "(Iteration 171 / 200) loss: 3.837926\n",
      "(Epoch 18 / 20) train acc: 0.077000; val_acc: 0.086111\n",
      "(Iteration 181 / 200) loss: 3.362907\n",
      "(Epoch 19 / 20) train acc: 0.085000; val_acc: 0.088889\n",
      "(Iteration 191 / 200) loss: 3.814787\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 2.302242\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.303247\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.301849\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302177\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.300350\n",
      "(Epoch 5 / 20) train acc: 0.148000; val_acc: 0.102778\n",
      "(Iteration 51 / 200) loss: 2.301859\n",
      "(Epoch 6 / 20) train acc: 0.122000; val_acc: 0.108333\n",
      "(Iteration 61 / 200) loss: 2.299960\n",
      "(Epoch 7 / 20) train acc: 0.145000; val_acc: 0.119444\n",
      "(Iteration 71 / 200) loss: 2.300620\n",
      "(Epoch 8 / 20) train acc: 0.161000; val_acc: 0.125000\n",
      "(Iteration 81 / 200) loss: 2.299120\n",
      "(Epoch 9 / 20) train acc: 0.170000; val_acc: 0.133333\n",
      "(Iteration 91 / 200) loss: 2.298401\n",
      "(Epoch 10 / 20) train acc: 0.178000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 2.298553\n",
      "(Epoch 11 / 20) train acc: 0.192000; val_acc: 0.155556\n",
      "(Iteration 111 / 200) loss: 2.298761\n",
      "(Epoch 12 / 20) train acc: 0.192000; val_acc: 0.155556\n",
      "(Iteration 121 / 200) loss: 2.298482\n",
      "(Epoch 13 / 20) train acc: 0.217000; val_acc: 0.166667\n",
      "(Iteration 131 / 200) loss: 2.297022\n",
      "(Epoch 14 / 20) train acc: 0.225000; val_acc: 0.172222\n",
      "(Iteration 141 / 200) loss: 2.295482\n",
      "(Epoch 15 / 20) train acc: 0.273000; val_acc: 0.200000\n",
      "(Iteration 151 / 200) loss: 2.294924\n",
      "(Epoch 16 / 20) train acc: 0.258000; val_acc: 0.205556\n",
      "(Iteration 161 / 200) loss: 2.295730\n",
      "(Epoch 17 / 20) train acc: 0.286000; val_acc: 0.227778\n",
      "(Iteration 171 / 200) loss: 2.295354\n",
      "(Epoch 18 / 20) train acc: 0.284000; val_acc: 0.236111\n",
      "(Iteration 181 / 200) loss: 2.292933\n",
      "(Epoch 19 / 20) train acc: 0.298000; val_acc: 0.241667\n",
      "(Iteration 191 / 200) loss: 2.293371\n",
      "(Epoch 20 / 20) train acc: 0.295000; val_acc: 0.255556\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.138000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 2.302593\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302570\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302578\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302563\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302540\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302510\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302490\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302470\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302338\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302272\n",
      "(Epoch 11 / 20) train acc: 0.208000; val_acc: 0.147222\n",
      "(Iteration 111 / 200) loss: 2.302144\n",
      "(Epoch 12 / 20) train acc: 0.216000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.301989\n",
      "(Epoch 13 / 20) train acc: 0.291000; val_acc: 0.213889\n",
      "(Iteration 131 / 200) loss: 2.301756\n",
      "(Epoch 14 / 20) train acc: 0.266000; val_acc: 0.225000\n",
      "(Iteration 141 / 200) loss: 2.301473\n",
      "(Epoch 15 / 20) train acc: 0.294000; val_acc: 0.227778\n",
      "(Iteration 151 / 200) loss: 2.301080\n",
      "(Epoch 16 / 20) train acc: 0.309000; val_acc: 0.233333\n",
      "(Iteration 161 / 200) loss: 2.300567\n",
      "(Epoch 17 / 20) train acc: 0.272000; val_acc: 0.230556\n",
      "(Iteration 171 / 200) loss: 2.299789\n",
      "(Epoch 18 / 20) train acc: 0.286000; val_acc: 0.227778\n",
      "(Iteration 181 / 200) loss: 2.299257\n",
      "(Epoch 19 / 20) train acc: 0.270000; val_acc: 0.219444\n",
      "(Iteration 191 / 200) loss: 2.298154\n",
      "(Epoch 20 / 20) train acc: 0.252000; val_acc: 0.194444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302585\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302577\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302569\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302557\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.302544\n",
      "(Epoch 8 / 20) train acc: 0.123000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.302459\n",
      "(Epoch 9 / 20) train acc: 0.137000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.302366\n",
      "(Epoch 10 / 20) train acc: 0.131000; val_acc: 0.094444\n",
      "(Iteration 101 / 200) loss: 2.302322\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.094444\n",
      "(Iteration 111 / 200) loss: 2.301973\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 2.301834\n",
      "(Epoch 13 / 20) train acc: 0.127000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 2.301796\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 2.301112\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 2.300373\n",
      "(Epoch 16 / 20) train acc: 0.143000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 2.299633\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.094444\n",
      "(Iteration 171 / 200) loss: 2.299544\n",
      "(Epoch 18 / 20) train acc: 0.124000; val_acc: 0.094444\n",
      "(Iteration 181 / 200) loss: 2.298214\n",
      "(Epoch 19 / 20) train acc: 0.142000; val_acc: 0.108333\n",
      "(Iteration 191 / 200) loss: 2.297220\n",
      "(Epoch 20 / 20) train acc: 0.135000; val_acc: 0.108333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302579\n",
      "(Epoch 4 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302579\n",
      "(Epoch 5 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302590\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302574\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302591\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302575\n",
      "(Epoch 9 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302567\n",
      "(Epoch 10 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302568\n",
      "(Epoch 11 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302573\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302555\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302475\n",
      "(Epoch 14 / 20) train acc: 0.124000; val_acc: 0.136111\n",
      "(Iteration 141 / 200) loss: 2.302446\n",
      "(Epoch 15 / 20) train acc: 0.125000; val_acc: 0.111111\n",
      "(Iteration 151 / 200) loss: 2.302344\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.102778\n",
      "(Iteration 161 / 200) loss: 2.302279\n",
      "(Epoch 17 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.302065\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.302293\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.301988\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 6940.783697\n",
      "(Epoch 0 / 20) train acc: 0.029000; val_acc: 0.038889\n",
      "(Epoch 1 / 20) train acc: 0.039000; val_acc: 0.038889\n",
      "(Iteration 11 / 200) loss: 6514.275949\n",
      "(Epoch 2 / 20) train acc: 0.033000; val_acc: 0.038889\n",
      "(Iteration 21 / 200) loss: 7015.865115\n",
      "(Epoch 3 / 20) train acc: 0.032000; val_acc: 0.038889\n",
      "(Iteration 31 / 200) loss: 6710.541793\n",
      "(Epoch 4 / 20) train acc: 0.037000; val_acc: 0.038889\n",
      "(Iteration 41 / 200) loss: 6238.832853\n",
      "(Epoch 5 / 20) train acc: 0.035000; val_acc: 0.038889\n",
      "(Iteration 51 / 200) loss: 6473.318914\n",
      "(Epoch 6 / 20) train acc: 0.044000; val_acc: 0.038889\n",
      "(Iteration 61 / 200) loss: 7471.933100\n",
      "(Epoch 7 / 20) train acc: 0.047000; val_acc: 0.038889\n",
      "(Iteration 71 / 200) loss: 6672.332910\n",
      "(Epoch 8 / 20) train acc: 0.037000; val_acc: 0.038889\n",
      "(Iteration 81 / 200) loss: 6117.701472\n",
      "(Epoch 9 / 20) train acc: 0.035000; val_acc: 0.038889\n",
      "(Iteration 91 / 200) loss: 6524.061909\n",
      "(Epoch 10 / 20) train acc: 0.045000; val_acc: 0.038889\n",
      "(Iteration 101 / 200) loss: 6220.206721\n",
      "(Epoch 11 / 20) train acc: 0.032000; val_acc: 0.038889\n",
      "(Iteration 111 / 200) loss: 5998.397159\n",
      "(Epoch 12 / 20) train acc: 0.037000; val_acc: 0.038889\n",
      "(Iteration 121 / 200) loss: 6793.768847\n",
      "(Epoch 13 / 20) train acc: 0.042000; val_acc: 0.038889\n",
      "(Iteration 131 / 200) loss: 6535.739286\n",
      "(Epoch 14 / 20) train acc: 0.060000; val_acc: 0.041667\n",
      "(Iteration 141 / 200) loss: 6693.564726\n",
      "(Epoch 15 / 20) train acc: 0.033000; val_acc: 0.041667\n",
      "(Iteration 151 / 200) loss: 5865.661414\n",
      "(Epoch 16 / 20) train acc: 0.041000; val_acc: 0.041667\n",
      "(Iteration 161 / 200) loss: 5758.678729\n",
      "(Epoch 17 / 20) train acc: 0.043000; val_acc: 0.041667\n",
      "(Iteration 171 / 200) loss: 5439.472293\n",
      "(Epoch 18 / 20) train acc: 0.028000; val_acc: 0.041667\n",
      "(Iteration 181 / 200) loss: 6486.992733\n",
      "(Epoch 19 / 20) train acc: 0.052000; val_acc: 0.041667\n",
      "(Iteration 191 / 200) loss: 6735.408172\n",
      "(Epoch 20 / 20) train acc: 0.039000; val_acc: 0.041667\n",
      "(Iteration 1 / 200) loss: 4.164412\n",
      "(Epoch 0 / 20) train acc: 0.127000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.132000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 3.920045\n",
      "(Epoch 2 / 20) train acc: 0.132000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 3.677801\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 3.746810\n",
      "(Epoch 4 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 3.535764\n",
      "(Epoch 5 / 20) train acc: 0.124000; val_acc: 0.125000\n",
      "(Iteration 51 / 200) loss: 3.700589\n",
      "(Epoch 6 / 20) train acc: 0.141000; val_acc: 0.130556\n",
      "(Iteration 61 / 200) loss: 3.303560\n",
      "(Epoch 7 / 20) train acc: 0.131000; val_acc: 0.136111\n",
      "(Iteration 71 / 200) loss: 3.394629\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 3.259838\n",
      "(Epoch 9 / 20) train acc: 0.118000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 3.464996\n",
      "(Epoch 10 / 20) train acc: 0.154000; val_acc: 0.147222\n",
      "(Iteration 101 / 200) loss: 3.349373\n",
      "(Epoch 11 / 20) train acc: 0.128000; val_acc: 0.150000\n",
      "(Iteration 111 / 200) loss: 3.193762\n",
      "(Epoch 12 / 20) train acc: 0.151000; val_acc: 0.155556\n",
      "(Iteration 121 / 200) loss: 3.233213\n",
      "(Epoch 13 / 20) train acc: 0.143000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 3.370716\n",
      "(Epoch 14 / 20) train acc: 0.153000; val_acc: 0.169444\n",
      "(Iteration 141 / 200) loss: 3.051092\n",
      "(Epoch 15 / 20) train acc: 0.162000; val_acc: 0.180556\n",
      "(Iteration 151 / 200) loss: 2.955567\n",
      "(Epoch 16 / 20) train acc: 0.167000; val_acc: 0.183333\n",
      "(Iteration 161 / 200) loss: 2.839690\n",
      "(Epoch 17 / 20) train acc: 0.160000; val_acc: 0.188889\n",
      "(Iteration 171 / 200) loss: 2.711880\n",
      "(Epoch 18 / 20) train acc: 0.178000; val_acc: 0.188889\n",
      "(Iteration 181 / 200) loss: 2.987844\n",
      "(Epoch 19 / 20) train acc: 0.191000; val_acc: 0.194444\n",
      "(Iteration 191 / 200) loss: 2.717132\n",
      "(Epoch 20 / 20) train acc: 0.184000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 2.303351\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.127000; val_acc: 0.150000\n",
      "(Iteration 11 / 200) loss: 2.302575\n",
      "(Epoch 2 / 20) train acc: 0.170000; val_acc: 0.169444\n",
      "(Iteration 21 / 200) loss: 2.302798\n",
      "(Epoch 3 / 20) train acc: 0.193000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 2.302514\n",
      "(Epoch 4 / 20) train acc: 0.233000; val_acc: 0.227778\n",
      "(Iteration 41 / 200) loss: 2.301998\n",
      "(Epoch 5 / 20) train acc: 0.268000; val_acc: 0.258333\n",
      "(Iteration 51 / 200) loss: 2.301373\n",
      "(Epoch 6 / 20) train acc: 0.295000; val_acc: 0.302778\n",
      "(Iteration 61 / 200) loss: 2.301071\n",
      "(Epoch 7 / 20) train acc: 0.334000; val_acc: 0.316667\n",
      "(Iteration 71 / 200) loss: 2.300952\n",
      "(Epoch 8 / 20) train acc: 0.327000; val_acc: 0.336111\n",
      "(Iteration 81 / 200) loss: 2.300234\n",
      "(Epoch 9 / 20) train acc: 0.386000; val_acc: 0.352778\n",
      "(Iteration 91 / 200) loss: 2.300559\n",
      "(Epoch 10 / 20) train acc: 0.355000; val_acc: 0.363889\n",
      "(Iteration 101 / 200) loss: 2.299196\n",
      "(Epoch 11 / 20) train acc: 0.412000; val_acc: 0.380556\n",
      "(Iteration 111 / 200) loss: 2.299247\n",
      "(Epoch 12 / 20) train acc: 0.420000; val_acc: 0.397222\n",
      "(Iteration 121 / 200) loss: 2.298931\n",
      "(Epoch 13 / 20) train acc: 0.402000; val_acc: 0.405556\n",
      "(Iteration 131 / 200) loss: 2.297256\n",
      "(Epoch 14 / 20) train acc: 0.437000; val_acc: 0.425000\n",
      "(Iteration 141 / 200) loss: 2.297664\n",
      "(Epoch 15 / 20) train acc: 0.447000; val_acc: 0.430556\n",
      "(Iteration 151 / 200) loss: 2.297165\n",
      "(Epoch 16 / 20) train acc: 0.488000; val_acc: 0.447222\n",
      "(Iteration 161 / 200) loss: 2.296615\n",
      "(Epoch 17 / 20) train acc: 0.486000; val_acc: 0.466667\n",
      "(Iteration 171 / 200) loss: 2.295420\n",
      "(Epoch 18 / 20) train acc: 0.523000; val_acc: 0.491667\n",
      "(Iteration 181 / 200) loss: 2.294638\n",
      "(Epoch 19 / 20) train acc: 0.508000; val_acc: 0.502778\n",
      "(Iteration 191 / 200) loss: 2.293897\n",
      "(Epoch 20 / 20) train acc: 0.533000; val_acc: 0.497222\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.164000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 2.302591\n",
      "(Epoch 2 / 20) train acc: 0.217000; val_acc: 0.172222\n",
      "(Iteration 21 / 200) loss: 2.302593\n",
      "(Epoch 3 / 20) train acc: 0.246000; val_acc: 0.180556\n",
      "(Iteration 31 / 200) loss: 2.302569\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302574\n",
      "(Epoch 5 / 20) train acc: 0.125000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302561\n",
      "(Epoch 6 / 20) train acc: 0.213000; val_acc: 0.152778\n",
      "(Iteration 61 / 200) loss: 2.302513\n",
      "(Epoch 7 / 20) train acc: 0.243000; val_acc: 0.227778\n",
      "(Iteration 71 / 200) loss: 2.302515\n",
      "(Epoch 8 / 20) train acc: 0.384000; val_acc: 0.313889\n",
      "(Iteration 81 / 200) loss: 2.302470\n",
      "(Epoch 9 / 20) train acc: 0.335000; val_acc: 0.283333\n",
      "(Iteration 91 / 200) loss: 2.302391\n",
      "(Epoch 10 / 20) train acc: 0.339000; val_acc: 0.308333\n",
      "(Iteration 101 / 200) loss: 2.302318\n",
      "(Epoch 11 / 20) train acc: 0.490000; val_acc: 0.416667\n",
      "(Iteration 111 / 200) loss: 2.302125\n",
      "(Epoch 12 / 20) train acc: 0.461000; val_acc: 0.441667\n",
      "(Iteration 121 / 200) loss: 2.302151\n",
      "(Epoch 13 / 20) train acc: 0.510000; val_acc: 0.433333\n",
      "(Iteration 131 / 200) loss: 2.301752\n",
      "(Epoch 14 / 20) train acc: 0.524000; val_acc: 0.494444\n",
      "(Iteration 141 / 200) loss: 2.301602\n",
      "(Epoch 15 / 20) train acc: 0.528000; val_acc: 0.491667\n",
      "(Iteration 151 / 200) loss: 2.301293\n",
      "(Epoch 16 / 20) train acc: 0.550000; val_acc: 0.541667\n",
      "(Iteration 161 / 200) loss: 2.301071\n",
      "(Epoch 17 / 20) train acc: 0.592000; val_acc: 0.550000\n",
      "(Iteration 171 / 200) loss: 2.300660\n",
      "(Epoch 18 / 20) train acc: 0.565000; val_acc: 0.544444\n",
      "(Iteration 181 / 200) loss: 2.299968\n",
      "(Epoch 19 / 20) train acc: 0.606000; val_acc: 0.563889\n",
      "(Iteration 191 / 200) loss: 2.299171\n",
      "(Epoch 20 / 20) train acc: 0.606000; val_acc: 0.597222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302585\n",
      "(Epoch 5 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302575\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302555\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302525\n",
      "(Epoch 8 / 20) train acc: 0.187000; val_acc: 0.188889\n",
      "(Iteration 81 / 200) loss: 2.302489\n",
      "(Epoch 9 / 20) train acc: 0.186000; val_acc: 0.197222\n",
      "(Iteration 91 / 200) loss: 2.302406\n",
      "(Epoch 10 / 20) train acc: 0.218000; val_acc: 0.197222\n",
      "(Iteration 101 / 200) loss: 2.302296\n",
      "(Epoch 11 / 20) train acc: 0.198000; val_acc: 0.200000\n",
      "(Iteration 111 / 200) loss: 2.302069\n",
      "(Epoch 12 / 20) train acc: 0.192000; val_acc: 0.213889\n",
      "(Iteration 121 / 200) loss: 2.301937\n",
      "(Epoch 13 / 20) train acc: 0.220000; val_acc: 0.219444\n",
      "(Iteration 131 / 200) loss: 2.301745\n",
      "(Epoch 14 / 20) train acc: 0.242000; val_acc: 0.247222\n",
      "(Iteration 141 / 200) loss: 2.301269\n",
      "(Epoch 15 / 20) train acc: 0.284000; val_acc: 0.308333\n",
      "(Iteration 151 / 200) loss: 2.301006\n",
      "(Epoch 16 / 20) train acc: 0.325000; val_acc: 0.305556\n",
      "(Iteration 161 / 200) loss: 2.300477\n",
      "(Epoch 17 / 20) train acc: 0.256000; val_acc: 0.300000\n",
      "(Iteration 171 / 200) loss: 2.299622\n",
      "(Epoch 18 / 20) train acc: 0.305000; val_acc: 0.308333\n",
      "(Iteration 181 / 200) loss: 2.299039\n",
      "(Epoch 19 / 20) train acc: 0.293000; val_acc: 0.311111\n",
      "(Iteration 191 / 200) loss: 2.298129\n",
      "(Epoch 20 / 20) train acc: 0.275000; val_acc: 0.311111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302586\n",
      "(Epoch 3 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302594\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302600\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302575\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302576\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302531\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302547\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302508\n",
      "(Epoch 10 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302433\n",
      "(Epoch 11 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302372\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302086\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302247\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 2.301770\n",
      "(Epoch 15 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301459\n",
      "(Epoch 16 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.300982\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.301903\n",
      "(Epoch 18 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.299366\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.299951\n",
      "(Epoch 20 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 5030.892476\n",
      "(Epoch 0 / 20) train acc: 0.051000; val_acc: 0.055556\n",
      "(Epoch 1 / 20) train acc: 0.074000; val_acc: 0.055556\n",
      "(Iteration 11 / 200) loss: 6744.900891\n",
      "(Epoch 2 / 20) train acc: 0.072000; val_acc: 0.055556\n",
      "(Iteration 21 / 200) loss: 6556.540875\n",
      "(Epoch 3 / 20) train acc: 0.055000; val_acc: 0.055556\n",
      "(Iteration 31 / 200) loss: 6901.771483\n",
      "(Epoch 4 / 20) train acc: 0.051000; val_acc: 0.055556\n",
      "(Iteration 41 / 200) loss: 6156.875842\n",
      "(Epoch 5 / 20) train acc: 0.068000; val_acc: 0.055556\n",
      "(Iteration 51 / 200) loss: 6718.436451\n",
      "(Epoch 6 / 20) train acc: 0.059000; val_acc: 0.055556\n",
      "(Iteration 61 / 200) loss: 6480.717060\n",
      "(Epoch 7 / 20) train acc: 0.069000; val_acc: 0.055556\n",
      "(Iteration 71 / 200) loss: 5843.900793\n",
      "(Epoch 8 / 20) train acc: 0.066000; val_acc: 0.055556\n",
      "(Iteration 81 / 200) loss: 5441.524527\n",
      "(Epoch 9 / 20) train acc: 0.082000; val_acc: 0.055556\n",
      "(Iteration 91 / 200) loss: 5582.708886\n",
      "(Epoch 10 / 20) train acc: 0.053000; val_acc: 0.055556\n",
      "(Iteration 101 / 200) loss: 5839.625745\n",
      "(Epoch 11 / 20) train acc: 0.058000; val_acc: 0.052778\n",
      "(Iteration 111 / 200) loss: 6192.813229\n",
      "(Epoch 12 / 20) train acc: 0.068000; val_acc: 0.052778\n",
      "(Iteration 121 / 200) loss: 5859.686964\n",
      "(Epoch 13 / 20) train acc: 0.057000; val_acc: 0.052778\n",
      "(Iteration 131 / 200) loss: 5983.555698\n",
      "(Epoch 14 / 20) train acc: 0.061000; val_acc: 0.052778\n",
      "(Iteration 141 / 200) loss: 5728.855682\n",
      "(Epoch 15 / 20) train acc: 0.069000; val_acc: 0.052778\n",
      "(Iteration 151 / 200) loss: 5908.573166\n",
      "(Epoch 16 / 20) train acc: 0.064000; val_acc: 0.052778\n",
      "(Iteration 161 / 200) loss: 5276.808150\n",
      "(Epoch 17 / 20) train acc: 0.068000; val_acc: 0.052778\n",
      "(Iteration 171 / 200) loss: 6574.606259\n",
      "(Epoch 18 / 20) train acc: 0.056000; val_acc: 0.052778\n",
      "(Iteration 181 / 200) loss: 6326.850618\n",
      "(Epoch 19 / 20) train acc: 0.064000; val_acc: 0.052778\n",
      "(Iteration 191 / 200) loss: 5630.994978\n",
      "(Epoch 20 / 20) train acc: 0.072000; val_acc: 0.050000\n",
      "(Iteration 1 / 200) loss: 5.865526\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.147000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 4.783208\n",
      "(Epoch 2 / 20) train acc: 0.139000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 5.139359\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.119444\n",
      "(Iteration 31 / 200) loss: 4.634730\n",
      "(Epoch 4 / 20) train acc: 0.158000; val_acc: 0.136111\n",
      "(Iteration 41 / 200) loss: 4.557536\n",
      "(Epoch 5 / 20) train acc: 0.139000; val_acc: 0.138889\n",
      "(Iteration 51 / 200) loss: 4.853503\n",
      "(Epoch 6 / 20) train acc: 0.158000; val_acc: 0.136111\n",
      "(Iteration 61 / 200) loss: 4.940986\n",
      "(Epoch 7 / 20) train acc: 0.140000; val_acc: 0.133333\n",
      "(Iteration 71 / 200) loss: 4.562618\n",
      "(Epoch 8 / 20) train acc: 0.141000; val_acc: 0.127778\n",
      "(Iteration 81 / 200) loss: 5.075210\n",
      "(Epoch 9 / 20) train acc: 0.139000; val_acc: 0.130556\n",
      "(Iteration 91 / 200) loss: 4.781237\n",
      "(Epoch 10 / 20) train acc: 0.156000; val_acc: 0.130556\n",
      "(Iteration 101 / 200) loss: 4.053444\n",
      "(Epoch 11 / 20) train acc: 0.143000; val_acc: 0.133333\n",
      "(Iteration 111 / 200) loss: 3.917382\n",
      "(Epoch 12 / 20) train acc: 0.189000; val_acc: 0.136111\n",
      "(Iteration 121 / 200) loss: 3.749271\n",
      "(Epoch 13 / 20) train acc: 0.154000; val_acc: 0.141667\n",
      "(Iteration 131 / 200) loss: 4.622898\n",
      "(Epoch 14 / 20) train acc: 0.164000; val_acc: 0.144444\n",
      "(Iteration 141 / 200) loss: 4.033310\n",
      "(Epoch 15 / 20) train acc: 0.171000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 4.203379\n",
      "(Epoch 16 / 20) train acc: 0.179000; val_acc: 0.147222\n",
      "(Iteration 161 / 200) loss: 3.959812\n",
      "(Epoch 17 / 20) train acc: 0.165000; val_acc: 0.155556\n",
      "(Iteration 171 / 200) loss: 3.951949\n",
      "(Epoch 18 / 20) train acc: 0.167000; val_acc: 0.152778\n",
      "(Iteration 181 / 200) loss: 3.929808\n",
      "(Epoch 19 / 20) train acc: 0.194000; val_acc: 0.155556\n",
      "(Iteration 191 / 200) loss: 3.891685\n",
      "(Epoch 20 / 20) train acc: 0.169000; val_acc: 0.152778\n",
      "(Iteration 1 / 200) loss: 2.302860\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.115000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302220\n",
      "(Epoch 2 / 20) train acc: 0.155000; val_acc: 0.172222\n",
      "(Iteration 21 / 200) loss: 2.301927\n",
      "(Epoch 3 / 20) train acc: 0.180000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 2.301242\n",
      "(Epoch 4 / 20) train acc: 0.207000; val_acc: 0.219444\n",
      "(Iteration 41 / 200) loss: 2.300826\n",
      "(Epoch 5 / 20) train acc: 0.227000; val_acc: 0.219444\n",
      "(Iteration 51 / 200) loss: 2.300656\n",
      "(Epoch 6 / 20) train acc: 0.212000; val_acc: 0.233333\n",
      "(Iteration 61 / 200) loss: 2.300655\n",
      "(Epoch 7 / 20) train acc: 0.265000; val_acc: 0.255556\n",
      "(Iteration 71 / 200) loss: 2.299969\n",
      "(Epoch 8 / 20) train acc: 0.292000; val_acc: 0.283333\n",
      "(Iteration 81 / 200) loss: 2.299173\n",
      "(Epoch 9 / 20) train acc: 0.298000; val_acc: 0.291667\n",
      "(Iteration 91 / 200) loss: 2.299377\n",
      "(Epoch 10 / 20) train acc: 0.320000; val_acc: 0.313889\n",
      "(Iteration 101 / 200) loss: 2.298134\n",
      "(Epoch 11 / 20) train acc: 0.361000; val_acc: 0.316667\n",
      "(Iteration 111 / 200) loss: 2.297474\n",
      "(Epoch 12 / 20) train acc: 0.363000; val_acc: 0.330556\n",
      "(Iteration 121 / 200) loss: 2.297139\n",
      "(Epoch 13 / 20) train acc: 0.445000; val_acc: 0.363889\n",
      "(Iteration 131 / 200) loss: 2.297691\n",
      "(Epoch 14 / 20) train acc: 0.423000; val_acc: 0.369444\n",
      "(Iteration 141 / 200) loss: 2.296032\n",
      "(Epoch 15 / 20) train acc: 0.397000; val_acc: 0.347222\n",
      "(Iteration 151 / 200) loss: 2.294401\n",
      "(Epoch 16 / 20) train acc: 0.409000; val_acc: 0.347222\n",
      "(Iteration 161 / 200) loss: 2.294447\n",
      "(Epoch 17 / 20) train acc: 0.456000; val_acc: 0.355556\n",
      "(Iteration 171 / 200) loss: 2.293603\n",
      "(Epoch 18 / 20) train acc: 0.469000; val_acc: 0.386111\n",
      "(Iteration 181 / 200) loss: 2.293777\n",
      "(Epoch 19 / 20) train acc: 0.463000; val_acc: 0.408333\n",
      "(Iteration 191 / 200) loss: 2.293303\n",
      "(Epoch 20 / 20) train acc: 0.498000; val_acc: 0.427778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.169000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2.302579\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302565\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302567\n",
      "(Epoch 4 / 20) train acc: 0.211000; val_acc: 0.183333\n",
      "(Iteration 41 / 200) loss: 2.302547\n",
      "(Epoch 5 / 20) train acc: 0.214000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 2.302544\n",
      "(Epoch 6 / 20) train acc: 0.229000; val_acc: 0.213889\n",
      "(Iteration 61 / 200) loss: 2.302515\n",
      "(Epoch 7 / 20) train acc: 0.266000; val_acc: 0.263889\n",
      "(Iteration 71 / 200) loss: 2.302478\n",
      "(Epoch 8 / 20) train acc: 0.300000; val_acc: 0.280556\n",
      "(Iteration 81 / 200) loss: 2.302413\n",
      "(Epoch 9 / 20) train acc: 0.230000; val_acc: 0.225000\n",
      "(Iteration 91 / 200) loss: 2.302324\n",
      "(Epoch 10 / 20) train acc: 0.213000; val_acc: 0.208333\n",
      "(Iteration 101 / 200) loss: 2.302283\n",
      "(Epoch 11 / 20) train acc: 0.293000; val_acc: 0.269444\n",
      "(Iteration 111 / 200) loss: 2.302162\n",
      "(Epoch 12 / 20) train acc: 0.328000; val_acc: 0.333333\n",
      "(Iteration 121 / 200) loss: 2.302008\n",
      "(Epoch 13 / 20) train acc: 0.407000; val_acc: 0.405556\n",
      "(Iteration 131 / 200) loss: 2.301745\n",
      "(Epoch 14 / 20) train acc: 0.373000; val_acc: 0.391667\n",
      "(Iteration 141 / 200) loss: 2.301314\n",
      "(Epoch 15 / 20) train acc: 0.368000; val_acc: 0.363889\n",
      "(Iteration 151 / 200) loss: 2.301015\n",
      "(Epoch 16 / 20) train acc: 0.383000; val_acc: 0.369444\n",
      "(Iteration 161 / 200) loss: 2.300722\n",
      "(Epoch 17 / 20) train acc: 0.449000; val_acc: 0.402778\n",
      "(Iteration 171 / 200) loss: 2.299844\n",
      "(Epoch 18 / 20) train acc: 0.431000; val_acc: 0.388889\n",
      "(Iteration 181 / 200) loss: 2.299140\n",
      "(Epoch 19 / 20) train acc: 0.403000; val_acc: 0.383333\n",
      "(Iteration 191 / 200) loss: 2.298183\n",
      "(Epoch 20 / 20) train acc: 0.419000; val_acc: 0.397222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.123000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302590\n",
      "(Epoch 3 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302588\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302558\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302571\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.108333\n",
      "(Iteration 61 / 200) loss: 2.302550\n",
      "(Epoch 7 / 20) train acc: 0.123000; val_acc: 0.113889\n",
      "(Iteration 71 / 200) loss: 2.302484\n",
      "(Epoch 8 / 20) train acc: 0.138000; val_acc: 0.113889\n",
      "(Iteration 81 / 200) loss: 2.302504\n",
      "(Epoch 9 / 20) train acc: 0.118000; val_acc: 0.113889\n",
      "(Iteration 91 / 200) loss: 2.302411\n",
      "(Epoch 10 / 20) train acc: 0.149000; val_acc: 0.125000\n",
      "(Iteration 101 / 200) loss: 2.302292\n",
      "(Epoch 11 / 20) train acc: 0.150000; val_acc: 0.127778\n",
      "(Iteration 111 / 200) loss: 2.302049\n",
      "(Epoch 12 / 20) train acc: 0.136000; val_acc: 0.127778\n",
      "(Iteration 121 / 200) loss: 2.301809\n",
      "(Epoch 13 / 20) train acc: 0.151000; val_acc: 0.130556\n",
      "(Iteration 131 / 200) loss: 2.301406\n",
      "(Epoch 14 / 20) train acc: 0.162000; val_acc: 0.138889\n",
      "(Iteration 141 / 200) loss: 2.301495\n",
      "(Epoch 15 / 20) train acc: 0.164000; val_acc: 0.141667\n",
      "(Iteration 151 / 200) loss: 2.300474\n",
      "(Epoch 16 / 20) train acc: 0.150000; val_acc: 0.141667\n",
      "(Iteration 161 / 200) loss: 2.299666\n",
      "(Epoch 17 / 20) train acc: 0.150000; val_acc: 0.141667\n",
      "(Iteration 171 / 200) loss: 2.300121\n",
      "(Epoch 18 / 20) train acc: 0.159000; val_acc: 0.141667\n",
      "(Iteration 181 / 200) loss: 2.298718\n",
      "(Epoch 19 / 20) train acc: 0.158000; val_acc: 0.141667\n",
      "(Iteration 191 / 200) loss: 2.297380\n",
      "(Epoch 20 / 20) train acc: 0.153000; val_acc: 0.141667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302585\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.202000; val_acc: 0.152778\n",
      "(Iteration 51 / 200) loss: 2.302559\n",
      "(Epoch 6 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302586\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302524\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302520\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302496\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302394\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302304\n",
      "(Epoch 12 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302141\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302085\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.301856\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301605\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.300807\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.300735\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.300118\n",
      "(Epoch 19 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.299530\n",
      "(Epoch 20 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 1 / 200) loss: 3731.662745\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.132000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2971.855847\n",
      "(Epoch 2 / 20) train acc: 0.129000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 3525.262391\n",
      "(Epoch 3 / 20) train acc: 0.135000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 3108.271435\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.172222\n",
      "(Iteration 41 / 200) loss: 3306.336105\n",
      "(Epoch 5 / 20) train acc: 0.132000; val_acc: 0.172222\n",
      "(Iteration 51 / 200) loss: 2835.381400\n",
      "(Epoch 6 / 20) train acc: 0.125000; val_acc: 0.172222\n",
      "(Iteration 61 / 200) loss: 3047.020132\n",
      "(Epoch 7 / 20) train acc: 0.132000; val_acc: 0.172222\n",
      "(Iteration 71 / 200) loss: 3532.955427\n",
      "(Epoch 8 / 20) train acc: 0.122000; val_acc: 0.172222\n",
      "(Iteration 81 / 200) loss: 3658.447910\n",
      "(Epoch 9 / 20) train acc: 0.131000; val_acc: 0.172222\n",
      "(Iteration 91 / 200) loss: 3633.036017\n",
      "(Epoch 10 / 20) train acc: 0.133000; val_acc: 0.172222\n",
      "(Iteration 101 / 200) loss: 2491.404906\n",
      "(Epoch 11 / 20) train acc: 0.127000; val_acc: 0.172222\n",
      "(Iteration 111 / 200) loss: 3099.994419\n",
      "(Epoch 12 / 20) train acc: 0.147000; val_acc: 0.172222\n",
      "(Iteration 121 / 200) loss: 2956.127527\n",
      "(Epoch 13 / 20) train acc: 0.148000; val_acc: 0.172222\n",
      "(Iteration 131 / 200) loss: 3486.224697\n",
      "(Epoch 14 / 20) train acc: 0.137000; val_acc: 0.172222\n",
      "(Iteration 141 / 200) loss: 3402.912804\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.172222\n",
      "(Iteration 151 / 200) loss: 3585.422162\n",
      "(Epoch 16 / 20) train acc: 0.149000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 2819.272457\n",
      "(Epoch 17 / 20) train acc: 0.126000; val_acc: 0.175000\n",
      "(Iteration 171 / 200) loss: 2930.924314\n",
      "(Epoch 18 / 20) train acc: 0.127000; val_acc: 0.177778\n",
      "(Iteration 181 / 200) loss: 3042.424922\n",
      "(Epoch 19 / 20) train acc: 0.139000; val_acc: 0.177778\n",
      "(Iteration 191 / 200) loss: 2972.725217\n",
      "(Epoch 20 / 20) train acc: 0.127000; val_acc: 0.177778\n",
      "(Iteration 1 / 200) loss: 4.329547\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.108333\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 3.925564\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.108333\n",
      "(Iteration 21 / 200) loss: 3.926474\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 4.478521\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 3.840786\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 3.911994\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 3.687817\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 4.209371\n",
      "(Epoch 8 / 20) train acc: 0.124000; val_acc: 0.119444\n",
      "(Iteration 81 / 200) loss: 3.654417\n",
      "(Epoch 9 / 20) train acc: 0.140000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 3.515537\n",
      "(Epoch 10 / 20) train acc: 0.128000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 3.317425\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 3.209747\n",
      "(Epoch 12 / 20) train acc: 0.133000; val_acc: 0.119444\n",
      "(Iteration 121 / 200) loss: 3.520402\n",
      "(Epoch 13 / 20) train acc: 0.127000; val_acc: 0.122222\n",
      "(Iteration 131 / 200) loss: 3.449405\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.119444\n",
      "(Iteration 141 / 200) loss: 3.392857\n",
      "(Epoch 15 / 20) train acc: 0.130000; val_acc: 0.122222\n",
      "(Iteration 151 / 200) loss: 3.448766\n",
      "(Epoch 16 / 20) train acc: 0.143000; val_acc: 0.122222\n",
      "(Iteration 161 / 200) loss: 3.356638\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.122222\n",
      "(Iteration 171 / 200) loss: 3.065747\n",
      "(Epoch 18 / 20) train acc: 0.138000; val_acc: 0.122222\n",
      "(Iteration 181 / 200) loss: 3.575414\n",
      "(Epoch 19 / 20) train acc: 0.141000; val_acc: 0.125000\n",
      "(Iteration 191 / 200) loss: 3.067517\n",
      "(Epoch 20 / 20) train acc: 0.140000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 2.302850\n",
      "(Epoch 0 / 20) train acc: 0.084000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302363\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.108333\n",
      "(Iteration 21 / 200) loss: 2.301652\n",
      "(Epoch 3 / 20) train acc: 0.134000; val_acc: 0.125000\n",
      "(Iteration 31 / 200) loss: 2.301567\n",
      "(Epoch 4 / 20) train acc: 0.159000; val_acc: 0.158333\n",
      "(Iteration 41 / 200) loss: 2.301293\n",
      "(Epoch 5 / 20) train acc: 0.214000; val_acc: 0.175000\n",
      "(Iteration 51 / 200) loss: 2.300974\n",
      "(Epoch 6 / 20) train acc: 0.220000; val_acc: 0.219444\n",
      "(Iteration 61 / 200) loss: 2.300292\n",
      "(Epoch 7 / 20) train acc: 0.246000; val_acc: 0.250000\n",
      "(Iteration 71 / 200) loss: 2.300187\n",
      "(Epoch 8 / 20) train acc: 0.300000; val_acc: 0.283333\n",
      "(Iteration 81 / 200) loss: 2.300245\n",
      "(Epoch 9 / 20) train acc: 0.360000; val_acc: 0.308333\n",
      "(Iteration 91 / 200) loss: 2.299757\n",
      "(Epoch 10 / 20) train acc: 0.355000; val_acc: 0.327778\n",
      "(Iteration 101 / 200) loss: 2.298416\n",
      "(Epoch 11 / 20) train acc: 0.394000; val_acc: 0.338889\n",
      "(Iteration 111 / 200) loss: 2.299418\n",
      "(Epoch 12 / 20) train acc: 0.382000; val_acc: 0.355556\n",
      "(Iteration 121 / 200) loss: 2.298610\n",
      "(Epoch 13 / 20) train acc: 0.425000; val_acc: 0.369444\n",
      "(Iteration 131 / 200) loss: 2.298103\n",
      "(Epoch 14 / 20) train acc: 0.446000; val_acc: 0.402778\n",
      "(Iteration 141 / 200) loss: 2.296672\n",
      "(Epoch 15 / 20) train acc: 0.468000; val_acc: 0.430556\n",
      "(Iteration 151 / 200) loss: 2.296518\n",
      "(Epoch 16 / 20) train acc: 0.460000; val_acc: 0.447222\n",
      "(Iteration 161 / 200) loss: 2.296520\n",
      "(Epoch 17 / 20) train acc: 0.497000; val_acc: 0.452778\n",
      "(Iteration 171 / 200) loss: 2.295807\n",
      "(Epoch 18 / 20) train acc: 0.509000; val_acc: 0.452778\n",
      "(Iteration 181 / 200) loss: 2.295699\n",
      "(Epoch 19 / 20) train acc: 0.494000; val_acc: 0.458333\n",
      "(Iteration 191 / 200) loss: 2.293408\n",
      "(Epoch 20 / 20) train acc: 0.482000; val_acc: 0.463889\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.134000; val_acc: 0.180556\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.149000; val_acc: 0.155556\n",
      "(Iteration 21 / 200) loss: 2.302574\n",
      "(Epoch 3 / 20) train acc: 0.147000; val_acc: 0.150000\n",
      "(Iteration 31 / 200) loss: 2.302570\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302554\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302562\n",
      "(Epoch 6 / 20) train acc: 0.132000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 2.302514\n",
      "(Epoch 7 / 20) train acc: 0.210000; val_acc: 0.183333\n",
      "(Iteration 71 / 200) loss: 2.302518\n",
      "(Epoch 8 / 20) train acc: 0.204000; val_acc: 0.211111\n",
      "(Iteration 81 / 200) loss: 2.302476\n",
      "(Epoch 9 / 20) train acc: 0.383000; val_acc: 0.355556\n",
      "(Iteration 91 / 200) loss: 2.302404\n",
      "(Epoch 10 / 20) train acc: 0.468000; val_acc: 0.436111\n",
      "(Iteration 101 / 200) loss: 2.302337\n",
      "(Epoch 11 / 20) train acc: 0.441000; val_acc: 0.427778\n",
      "(Iteration 111 / 200) loss: 2.302243\n",
      "(Epoch 12 / 20) train acc: 0.417000; val_acc: 0.447222\n",
      "(Iteration 121 / 200) loss: 2.302033\n",
      "(Epoch 13 / 20) train acc: 0.473000; val_acc: 0.472222\n",
      "(Iteration 131 / 200) loss: 2.301932\n",
      "(Epoch 14 / 20) train acc: 0.536000; val_acc: 0.555556\n",
      "(Iteration 141 / 200) loss: 2.301616\n",
      "(Epoch 15 / 20) train acc: 0.592000; val_acc: 0.566667\n",
      "(Iteration 151 / 200) loss: 2.301375\n",
      "(Epoch 16 / 20) train acc: 0.549000; val_acc: 0.569444\n",
      "(Iteration 161 / 200) loss: 2.301091\n",
      "(Epoch 17 / 20) train acc: 0.600000; val_acc: 0.586111\n",
      "(Iteration 171 / 200) loss: 2.300499\n",
      "(Epoch 18 / 20) train acc: 0.648000; val_acc: 0.622222\n",
      "(Iteration 181 / 200) loss: 2.300191\n",
      "(Epoch 19 / 20) train acc: 0.632000; val_acc: 0.611111\n",
      "(Iteration 191 / 200) loss: 2.299667\n",
      "(Epoch 20 / 20) train acc: 0.605000; val_acc: 0.588889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302575\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302583\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302566\n",
      "(Epoch 6 / 20) train acc: 0.176000; val_acc: 0.147222\n",
      "(Iteration 61 / 200) loss: 2.302561\n",
      "(Epoch 7 / 20) train acc: 0.217000; val_acc: 0.163889\n",
      "(Iteration 71 / 200) loss: 2.302513\n",
      "(Epoch 8 / 20) train acc: 0.192000; val_acc: 0.158333\n",
      "(Iteration 81 / 200) loss: 2.302517\n",
      "(Epoch 9 / 20) train acc: 0.218000; val_acc: 0.158333\n",
      "(Iteration 91 / 200) loss: 2.302403\n",
      "(Epoch 10 / 20) train acc: 0.203000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 2.302361\n",
      "(Epoch 11 / 20) train acc: 0.191000; val_acc: 0.158333\n",
      "(Iteration 111 / 200) loss: 2.302197\n",
      "(Epoch 12 / 20) train acc: 0.202000; val_acc: 0.158333\n",
      "(Iteration 121 / 200) loss: 2.302026\n",
      "(Epoch 13 / 20) train acc: 0.223000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 2.301708\n",
      "(Epoch 14 / 20) train acc: 0.202000; val_acc: 0.158333\n",
      "(Iteration 141 / 200) loss: 2.301608\n",
      "(Epoch 15 / 20) train acc: 0.225000; val_acc: 0.158333\n",
      "(Iteration 151 / 200) loss: 2.301083\n",
      "(Epoch 16 / 20) train acc: 0.192000; val_acc: 0.158333\n",
      "(Iteration 161 / 200) loss: 2.300637\n",
      "(Epoch 17 / 20) train acc: 0.239000; val_acc: 0.158333\n",
      "(Iteration 171 / 200) loss: 2.300340\n",
      "(Epoch 18 / 20) train acc: 0.246000; val_acc: 0.205556\n",
      "(Iteration 181 / 200) loss: 2.299409\n",
      "(Epoch 19 / 20) train acc: 0.243000; val_acc: 0.208333\n",
      "(Iteration 191 / 200) loss: 2.298930\n",
      "(Epoch 20 / 20) train acc: 0.256000; val_acc: 0.191667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.084000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302562\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302555\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302561\n",
      "(Epoch 9 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302496\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302435\n",
      "(Epoch 11 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302382\n",
      "(Epoch 12 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302305\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302081\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302020\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302016\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301762\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301210\n",
      "(Epoch 18 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301420\n",
      "(Epoch 19 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.300525\n",
      "(Epoch 20 / 20) train acc: 0.129000; val_acc: 0.094444\n",
      "(Iteration 1 / 200) loss: 5917.591344\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 5694.758825\n",
      "(Epoch 2 / 20) train acc: 0.076000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 5262.305683\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 6022.541917\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 5955.096901\n",
      "(Epoch 5 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 6389.688760\n",
      "(Epoch 6 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 6184.468744\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 5723.505603\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 5696.333712\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 5411.730571\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 5539.440555\n",
      "(Epoch 11 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 5702.663664\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 5670.459273\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 5574.094883\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 5926.062367\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 5495.253601\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 6060.740460\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 5814.177945\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 6267.626679\n",
      "(Epoch 19 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 5467.632913\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 6.768084\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 6.657424\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 6.839850\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 5.811703\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 6.497237\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 6.184573\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 7.008888\n",
      "(Epoch 7 / 20) train acc: 0.119000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 5.664242\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.088889\n",
      "(Iteration 81 / 200) loss: 6.370837\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.088889\n",
      "(Iteration 91 / 200) loss: 5.097824\n",
      "(Epoch 10 / 20) train acc: 0.098000; val_acc: 0.088889\n",
      "(Iteration 101 / 200) loss: 5.619447\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 5.221163\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 4.583088\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 4.734087\n",
      "(Epoch 14 / 20) train acc: 0.112000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 5.038127\n",
      "(Epoch 15 / 20) train acc: 0.121000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 4.595137\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 161 / 200) loss: 4.441250\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.102778\n",
      "(Iteration 171 / 200) loss: 4.278085\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.102778\n",
      "(Iteration 181 / 200) loss: 4.352190\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.105556\n",
      "(Iteration 191 / 200) loss: 4.420687\n",
      "(Epoch 20 / 20) train acc: 0.129000; val_acc: 0.105556\n",
      "(Iteration 1 / 200) loss: 2.302658\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302169\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 2.302132\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 2.301777\n",
      "(Epoch 4 / 20) train acc: 0.126000; val_acc: 0.108333\n",
      "(Iteration 41 / 200) loss: 2.301379\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.113889\n",
      "(Iteration 51 / 200) loss: 2.301266\n",
      "(Epoch 6 / 20) train acc: 0.144000; val_acc: 0.130556\n",
      "(Iteration 61 / 200) loss: 2.300757\n",
      "(Epoch 7 / 20) train acc: 0.174000; val_acc: 0.155556\n",
      "(Iteration 71 / 200) loss: 2.300184\n",
      "(Epoch 8 / 20) train acc: 0.181000; val_acc: 0.186111\n",
      "(Iteration 81 / 200) loss: 2.299820\n",
      "(Epoch 9 / 20) train acc: 0.241000; val_acc: 0.216667\n",
      "(Iteration 91 / 200) loss: 2.298930\n",
      "(Epoch 10 / 20) train acc: 0.285000; val_acc: 0.230556\n",
      "(Iteration 101 / 200) loss: 2.299258\n",
      "(Epoch 11 / 20) train acc: 0.315000; val_acc: 0.272222\n",
      "(Iteration 111 / 200) loss: 2.298367\n",
      "(Epoch 12 / 20) train acc: 0.330000; val_acc: 0.297222\n",
      "(Iteration 121 / 200) loss: 2.297685\n",
      "(Epoch 13 / 20) train acc: 0.382000; val_acc: 0.355556\n",
      "(Iteration 131 / 200) loss: 2.297529\n",
      "(Epoch 14 / 20) train acc: 0.427000; val_acc: 0.402778\n",
      "(Iteration 141 / 200) loss: 2.296610\n",
      "(Epoch 15 / 20) train acc: 0.487000; val_acc: 0.427778\n",
      "(Iteration 151 / 200) loss: 2.295768\n",
      "(Epoch 16 / 20) train acc: 0.492000; val_acc: 0.466667\n",
      "(Iteration 161 / 200) loss: 2.296018\n",
      "(Epoch 17 / 20) train acc: 0.515000; val_acc: 0.486111\n",
      "(Iteration 171 / 200) loss: 2.294293\n",
      "(Epoch 18 / 20) train acc: 0.561000; val_acc: 0.497222\n",
      "(Iteration 181 / 200) loss: 2.293345\n",
      "(Epoch 19 / 20) train acc: 0.555000; val_acc: 0.508333\n",
      "(Iteration 191 / 200) loss: 2.292974\n",
      "(Epoch 20 / 20) train acc: 0.551000; val_acc: 0.508333\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.129000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302580\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302575\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302570\n",
      "(Epoch 4 / 20) train acc: 0.168000; val_acc: 0.144444\n",
      "(Iteration 41 / 200) loss: 2.302551\n",
      "(Epoch 5 / 20) train acc: 0.324000; val_acc: 0.255556\n",
      "(Iteration 51 / 200) loss: 2.302547\n",
      "(Epoch 6 / 20) train acc: 0.395000; val_acc: 0.352778\n",
      "(Iteration 61 / 200) loss: 2.302520\n",
      "(Epoch 7 / 20) train acc: 0.338000; val_acc: 0.302778\n",
      "(Iteration 71 / 200) loss: 2.302469\n",
      "(Epoch 8 / 20) train acc: 0.355000; val_acc: 0.327778\n",
      "(Iteration 81 / 200) loss: 2.302417\n",
      "(Epoch 9 / 20) train acc: 0.364000; val_acc: 0.311111\n",
      "(Iteration 91 / 200) loss: 2.302344\n",
      "(Epoch 10 / 20) train acc: 0.311000; val_acc: 0.308333\n",
      "(Iteration 101 / 200) loss: 2.302255\n",
      "(Epoch 11 / 20) train acc: 0.324000; val_acc: 0.266667\n",
      "(Iteration 111 / 200) loss: 2.302119\n",
      "(Epoch 12 / 20) train acc: 0.281000; val_acc: 0.233333\n",
      "(Iteration 121 / 200) loss: 2.302042\n",
      "(Epoch 13 / 20) train acc: 0.305000; val_acc: 0.272222\n",
      "(Iteration 131 / 200) loss: 2.301815\n",
      "(Epoch 14 / 20) train acc: 0.371000; val_acc: 0.352778\n",
      "(Iteration 141 / 200) loss: 2.301635\n",
      "(Epoch 15 / 20) train acc: 0.441000; val_acc: 0.366667\n",
      "(Iteration 151 / 200) loss: 2.301181\n",
      "(Epoch 16 / 20) train acc: 0.405000; val_acc: 0.377778\n",
      "(Iteration 161 / 200) loss: 2.300958\n",
      "(Epoch 17 / 20) train acc: 0.437000; val_acc: 0.402778\n",
      "(Iteration 171 / 200) loss: 2.300365\n",
      "(Epoch 18 / 20) train acc: 0.402000; val_acc: 0.394444\n",
      "(Iteration 181 / 200) loss: 2.300003\n",
      "(Epoch 19 / 20) train acc: 0.437000; val_acc: 0.366667\n",
      "(Iteration 191 / 200) loss: 2.299382\n",
      "(Epoch 20 / 20) train acc: 0.376000; val_acc: 0.366667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302592\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302591\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302581\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302587\n",
      "(Epoch 5 / 20) train acc: 0.158000; val_acc: 0.166667\n",
      "(Iteration 51 / 200) loss: 2.302579\n",
      "(Epoch 6 / 20) train acc: 0.229000; val_acc: 0.191667\n",
      "(Iteration 61 / 200) loss: 2.302553\n",
      "(Epoch 7 / 20) train acc: 0.212000; val_acc: 0.194444\n",
      "(Iteration 71 / 200) loss: 2.302532\n",
      "(Epoch 8 / 20) train acc: 0.255000; val_acc: 0.219444\n",
      "(Iteration 81 / 200) loss: 2.302471\n",
      "(Epoch 9 / 20) train acc: 0.180000; val_acc: 0.188889\n",
      "(Iteration 91 / 200) loss: 2.302399\n",
      "(Epoch 10 / 20) train acc: 0.146000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 2.302372\n",
      "(Epoch 11 / 20) train acc: 0.149000; val_acc: 0.136111\n",
      "(Iteration 111 / 200) loss: 2.302260\n",
      "(Epoch 12 / 20) train acc: 0.148000; val_acc: 0.141667\n",
      "(Iteration 121 / 200) loss: 2.302101\n",
      "(Epoch 13 / 20) train acc: 0.193000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 2.301841\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 2.301572\n",
      "(Epoch 15 / 20) train acc: 0.139000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 2.301228\n",
      "(Epoch 16 / 20) train acc: 0.138000; val_acc: 0.138889\n",
      "(Iteration 161 / 200) loss: 2.300829\n",
      "(Epoch 17 / 20) train acc: 0.156000; val_acc: 0.138889\n",
      "(Iteration 171 / 200) loss: 2.300337\n",
      "(Epoch 18 / 20) train acc: 0.144000; val_acc: 0.141667\n",
      "(Iteration 181 / 200) loss: 2.300201\n",
      "(Epoch 19 / 20) train acc: 0.163000; val_acc: 0.147222\n",
      "(Iteration 191 / 200) loss: 2.299304\n",
      "(Epoch 20 / 20) train acc: 0.133000; val_acc: 0.141667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302591\n",
      "(Epoch 4 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302568\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 2.302583\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302565\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302578\n",
      "(Epoch 8 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302475\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302483\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302429\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302393\n",
      "(Epoch 12 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302152\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.301705\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.301918\n",
      "(Epoch 15 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301613\n",
      "(Epoch 16 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.300864\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.300286\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.299254\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Iteration 191 / 200) loss: 2.299046\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.086111\n",
      "(Iteration 1 / 200) loss: 6124.679951\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 5836.877449\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 5321.786197\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 5677.149320\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 5889.746193\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 6013.901816\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 6099.390564\n",
      "(Epoch 7 / 20) train acc: 0.126000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 5713.248687\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Iteration 81 / 200) loss: 5753.717435\n",
      "(Epoch 9 / 20) train acc: 0.126000; val_acc: 0.088889\n",
      "(Iteration 91 / 200) loss: 5824.183683\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.088889\n",
      "(Iteration 101 / 200) loss: 5598.763681\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 5483.931179\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Iteration 121 / 200) loss: 5749.104927\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 5257.126175\n",
      "(Epoch 14 / 20) train acc: 0.134000; val_acc: 0.088889\n",
      "(Iteration 141 / 200) loss: 5922.366173\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.088889\n",
      "(Iteration 151 / 200) loss: 6181.031796\n",
      "(Epoch 16 / 20) train acc: 0.122000; val_acc: 0.088889\n",
      "(Iteration 161 / 200) loss: 6087.205545\n",
      "(Epoch 17 / 20) train acc: 0.127000; val_acc: 0.088889\n",
      "(Iteration 171 / 200) loss: 5932.946793\n",
      "(Epoch 18 / 20) train acc: 0.125000; val_acc: 0.088889\n",
      "(Iteration 181 / 200) loss: 6337.770541\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.088889\n",
      "(Iteration 191 / 200) loss: 5724.105539\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.088889\n",
      "(Iteration 1 / 200) loss: 5.434739\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.082000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 5.335512\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 5.461792\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 5.410628\n",
      "(Epoch 4 / 20) train acc: 0.073000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 5.510340\n",
      "(Epoch 5 / 20) train acc: 0.067000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 5.151911\n",
      "(Epoch 6 / 20) train acc: 0.084000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 5.158049\n",
      "(Epoch 7 / 20) train acc: 0.068000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 4.127712\n",
      "(Epoch 8 / 20) train acc: 0.074000; val_acc: 0.086111\n",
      "(Iteration 81 / 200) loss: 4.904007\n",
      "(Epoch 9 / 20) train acc: 0.082000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 5.087184\n",
      "(Epoch 10 / 20) train acc: 0.077000; val_acc: 0.086111\n",
      "(Iteration 101 / 200) loss: 4.547815\n",
      "(Epoch 11 / 20) train acc: 0.074000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 4.513703\n",
      "(Epoch 12 / 20) train acc: 0.071000; val_acc: 0.094444\n",
      "(Iteration 121 / 200) loss: 4.402067\n",
      "(Epoch 13 / 20) train acc: 0.066000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 4.037304\n",
      "(Epoch 14 / 20) train acc: 0.078000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 3.911513\n",
      "(Epoch 15 / 20) train acc: 0.082000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 3.807231\n",
      "(Epoch 16 / 20) train acc: 0.083000; val_acc: 0.108333\n",
      "(Iteration 161 / 200) loss: 4.022162\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.113889\n",
      "(Iteration 171 / 200) loss: 3.698460\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 3.483928\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.122222\n",
      "(Iteration 191 / 200) loss: 3.809159\n",
      "(Epoch 20 / 20) train acc: 0.116000; val_acc: 0.125000\n",
      "(Iteration 1 / 200) loss: 2.302024\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.066667\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.066667\n",
      "(Iteration 11 / 200) loss: 2.302251\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.301721\n",
      "(Epoch 3 / 20) train acc: 0.124000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 2.301590\n",
      "(Epoch 4 / 20) train acc: 0.131000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.301072\n",
      "(Epoch 5 / 20) train acc: 0.131000; val_acc: 0.122222\n",
      "(Iteration 51 / 200) loss: 2.300779\n",
      "(Epoch 6 / 20) train acc: 0.175000; val_acc: 0.136111\n",
      "(Iteration 61 / 200) loss: 2.300647\n",
      "(Epoch 7 / 20) train acc: 0.178000; val_acc: 0.147222\n",
      "(Iteration 71 / 200) loss: 2.299932\n",
      "(Epoch 8 / 20) train acc: 0.194000; val_acc: 0.155556\n",
      "(Iteration 81 / 200) loss: 2.299144\n",
      "(Epoch 9 / 20) train acc: 0.184000; val_acc: 0.177778\n",
      "(Iteration 91 / 200) loss: 2.298975\n",
      "(Epoch 10 / 20) train acc: 0.219000; val_acc: 0.188889\n",
      "(Iteration 101 / 200) loss: 2.298365\n",
      "(Epoch 11 / 20) train acc: 0.239000; val_acc: 0.205556\n",
      "(Iteration 111 / 200) loss: 2.298026\n",
      "(Epoch 12 / 20) train acc: 0.261000; val_acc: 0.236111\n",
      "(Iteration 121 / 200) loss: 2.297653\n",
      "(Epoch 13 / 20) train acc: 0.269000; val_acc: 0.252778\n",
      "(Iteration 131 / 200) loss: 2.296822\n",
      "(Epoch 14 / 20) train acc: 0.294000; val_acc: 0.252778\n",
      "(Iteration 141 / 200) loss: 2.296170\n",
      "(Epoch 15 / 20) train acc: 0.301000; val_acc: 0.269444\n",
      "(Iteration 151 / 200) loss: 2.295119\n",
      "(Epoch 16 / 20) train acc: 0.330000; val_acc: 0.288889\n",
      "(Iteration 161 / 200) loss: 2.295016\n",
      "(Epoch 17 / 20) train acc: 0.354000; val_acc: 0.319444\n",
      "(Iteration 171 / 200) loss: 2.293875\n",
      "(Epoch 18 / 20) train acc: 0.392000; val_acc: 0.361111\n",
      "(Iteration 181 / 200) loss: 2.293170\n",
      "(Epoch 19 / 20) train acc: 0.434000; val_acc: 0.400000\n",
      "(Iteration 191 / 200) loss: 2.292532\n",
      "(Epoch 20 / 20) train acc: 0.450000; val_acc: 0.441667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.199000; val_acc: 0.158333\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302578\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302571\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302549\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302530\n",
      "(Epoch 6 / 20) train acc: 0.183000; val_acc: 0.147222\n",
      "(Iteration 61 / 200) loss: 2.302510\n",
      "(Epoch 7 / 20) train acc: 0.217000; val_acc: 0.163889\n",
      "(Iteration 71 / 200) loss: 2.302509\n",
      "(Epoch 8 / 20) train acc: 0.200000; val_acc: 0.166667\n",
      "(Iteration 81 / 200) loss: 2.302425\n",
      "(Epoch 9 / 20) train acc: 0.262000; val_acc: 0.200000\n",
      "(Iteration 91 / 200) loss: 2.302333\n",
      "(Epoch 10 / 20) train acc: 0.262000; val_acc: 0.225000\n",
      "(Iteration 101 / 200) loss: 2.302231\n",
      "(Epoch 11 / 20) train acc: 0.294000; val_acc: 0.275000\n",
      "(Iteration 111 / 200) loss: 2.302142\n",
      "(Epoch 12 / 20) train acc: 0.360000; val_acc: 0.319444\n",
      "(Iteration 121 / 200) loss: 2.301914\n",
      "(Epoch 13 / 20) train acc: 0.389000; val_acc: 0.336111\n",
      "(Iteration 131 / 200) loss: 2.301710\n",
      "(Epoch 14 / 20) train acc: 0.396000; val_acc: 0.347222\n",
      "(Iteration 141 / 200) loss: 2.301514\n",
      "(Epoch 15 / 20) train acc: 0.404000; val_acc: 0.361111\n",
      "(Iteration 151 / 200) loss: 2.301017\n",
      "(Epoch 16 / 20) train acc: 0.391000; val_acc: 0.361111\n",
      "(Iteration 161 / 200) loss: 2.300824\n",
      "(Epoch 17 / 20) train acc: 0.412000; val_acc: 0.394444\n",
      "(Iteration 171 / 200) loss: 2.300329\n",
      "(Epoch 18 / 20) train acc: 0.435000; val_acc: 0.408333\n",
      "(Iteration 181 / 200) loss: 2.299687\n",
      "(Epoch 19 / 20) train acc: 0.413000; val_acc: 0.375000\n",
      "(Iteration 191 / 200) loss: 2.298888\n",
      "(Epoch 20 / 20) train acc: 0.374000; val_acc: 0.363889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.165000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.123000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302578\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302568\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302574\n",
      "(Epoch 6 / 20) train acc: 0.210000; val_acc: 0.200000\n",
      "(Iteration 61 / 200) loss: 2.302543\n",
      "(Epoch 7 / 20) train acc: 0.216000; val_acc: 0.200000\n",
      "(Iteration 71 / 200) loss: 2.302485\n",
      "(Epoch 8 / 20) train acc: 0.210000; val_acc: 0.163889\n",
      "(Iteration 81 / 200) loss: 2.302440\n",
      "(Epoch 9 / 20) train acc: 0.216000; val_acc: 0.163889\n",
      "(Iteration 91 / 200) loss: 2.302394\n",
      "(Epoch 10 / 20) train acc: 0.195000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 2.302275\n",
      "(Epoch 11 / 20) train acc: 0.194000; val_acc: 0.152778\n",
      "(Iteration 111 / 200) loss: 2.302191\n",
      "(Epoch 12 / 20) train acc: 0.194000; val_acc: 0.152778\n",
      "(Iteration 121 / 200) loss: 2.301847\n",
      "(Epoch 13 / 20) train acc: 0.200000; val_acc: 0.163889\n",
      "(Iteration 131 / 200) loss: 2.301629\n",
      "(Epoch 14 / 20) train acc: 0.268000; val_acc: 0.233333\n",
      "(Iteration 141 / 200) loss: 2.301335\n",
      "(Epoch 15 / 20) train acc: 0.344000; val_acc: 0.305556\n",
      "(Iteration 151 / 200) loss: 2.300744\n",
      "(Epoch 16 / 20) train acc: 0.337000; val_acc: 0.300000\n",
      "(Iteration 161 / 200) loss: 2.300430\n",
      "(Epoch 17 / 20) train acc: 0.342000; val_acc: 0.300000\n",
      "(Iteration 171 / 200) loss: 2.299658\n",
      "(Epoch 18 / 20) train acc: 0.351000; val_acc: 0.313889\n",
      "(Iteration 181 / 200) loss: 2.298611\n",
      "(Epoch 19 / 20) train acc: 0.298000; val_acc: 0.288889\n",
      "(Iteration 191 / 200) loss: 2.298129\n",
      "(Epoch 20 / 20) train acc: 0.281000; val_acc: 0.269444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302579\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302584\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302593\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302564\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302561\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302524\n",
      "(Epoch 8 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302485\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302449\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302393\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302178\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302367\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.301759\n",
      "(Epoch 14 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301540\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.301006\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.300996\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.300873\n",
      "(Epoch 18 / 20) train acc: 0.165000; val_acc: 0.144444\n",
      "(Iteration 181 / 200) loss: 2.300692\n",
      "(Epoch 19 / 20) train acc: 0.132000; val_acc: 0.119444\n",
      "(Iteration 191 / 200) loss: 2.299251\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.113889\n",
      "(Iteration 1 / 200) loss: 4473.086098\n",
      "(Epoch 0 / 20) train acc: 0.163000; val_acc: 0.169444\n",
      "(Epoch 1 / 20) train acc: 0.167000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 4534.003596\n",
      "(Epoch 2 / 20) train acc: 0.204000; val_acc: 0.169444\n",
      "(Iteration 21 / 200) loss: 4142.806094\n",
      "(Epoch 3 / 20) train acc: 0.175000; val_acc: 0.169444\n",
      "(Iteration 31 / 200) loss: 3892.021717\n",
      "(Epoch 4 / 20) train acc: 0.196000; val_acc: 0.169444\n",
      "(Iteration 41 / 200) loss: 3637.394216\n",
      "(Epoch 5 / 20) train acc: 0.165000; val_acc: 0.169444\n",
      "(Iteration 51 / 200) loss: 4372.440151\n",
      "(Epoch 6 / 20) train acc: 0.188000; val_acc: 0.169444\n",
      "(Iteration 61 / 200) loss: 4212.982650\n",
      "(Epoch 7 / 20) train acc: 0.160000; val_acc: 0.169444\n",
      "(Iteration 71 / 200) loss: 4171.329836\n",
      "(Epoch 8 / 20) train acc: 0.177000; val_acc: 0.169444\n",
      "(Iteration 81 / 200) loss: 4092.036084\n",
      "(Epoch 9 / 20) train acc: 0.178000; val_acc: 0.169444\n",
      "(Iteration 91 / 200) loss: 4100.312957\n",
      "(Epoch 10 / 20) train acc: 0.176000; val_acc: 0.169444\n",
      "(Iteration 101 / 200) loss: 4411.070455\n",
      "(Epoch 11 / 20) train acc: 0.185000; val_acc: 0.169444\n",
      "(Iteration 111 / 200) loss: 3970.844829\n",
      "(Epoch 12 / 20) train acc: 0.154000; val_acc: 0.169444\n",
      "(Iteration 121 / 200) loss: 4518.437639\n",
      "(Epoch 13 / 20) train acc: 0.193000; val_acc: 0.169444\n",
      "(Iteration 131 / 200) loss: 3668.855138\n",
      "(Epoch 14 / 20) train acc: 0.178000; val_acc: 0.169444\n",
      "(Iteration 141 / 200) loss: 3870.505761\n",
      "(Epoch 15 / 20) train acc: 0.192000; val_acc: 0.169444\n",
      "(Iteration 151 / 200) loss: 4204.095447\n",
      "(Epoch 16 / 20) train acc: 0.183000; val_acc: 0.169444\n",
      "(Iteration 161 / 200) loss: 4357.313258\n",
      "(Epoch 17 / 20) train acc: 0.174000; val_acc: 0.169444\n",
      "(Iteration 171 / 200) loss: 3879.120443\n",
      "(Epoch 18 / 20) train acc: 0.166000; val_acc: 0.169444\n",
      "(Iteration 181 / 200) loss: 4350.604504\n",
      "(Epoch 19 / 20) train acc: 0.159000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 4015.755752\n",
      "(Epoch 20 / 20) train acc: 0.184000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 6.112942\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.126000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 6.380815\n",
      "(Epoch 2 / 20) train acc: 0.116000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 6.448674\n",
      "(Epoch 3 / 20) train acc: 0.114000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 6.883228\n",
      "(Epoch 4 / 20) train acc: 0.127000; val_acc: 0.125000\n",
      "(Iteration 41 / 200) loss: 6.239531\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.125000\n",
      "(Iteration 51 / 200) loss: 5.716754\n",
      "(Epoch 6 / 20) train acc: 0.119000; val_acc: 0.125000\n",
      "(Iteration 61 / 200) loss: 6.382260\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.130556\n",
      "(Iteration 71 / 200) loss: 5.587299\n",
      "(Epoch 8 / 20) train acc: 0.133000; val_acc: 0.130556\n",
      "(Iteration 81 / 200) loss: 5.488399\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.130556\n",
      "(Iteration 91 / 200) loss: 5.533157\n",
      "(Epoch 10 / 20) train acc: 0.122000; val_acc: 0.136111\n",
      "(Iteration 101 / 200) loss: 5.496098\n",
      "(Epoch 11 / 20) train acc: 0.120000; val_acc: 0.133333\n",
      "(Iteration 111 / 200) loss: 5.815187\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.133333\n",
      "(Iteration 121 / 200) loss: 5.375398\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.133333\n",
      "(Iteration 131 / 200) loss: 5.528182\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.133333\n",
      "(Iteration 141 / 200) loss: 5.127538\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.127778\n",
      "(Iteration 151 / 200) loss: 4.550382\n",
      "(Epoch 16 / 20) train acc: 0.125000; val_acc: 0.130556\n",
      "(Iteration 161 / 200) loss: 4.564200\n",
      "(Epoch 17 / 20) train acc: 0.129000; val_acc: 0.133333\n",
      "(Iteration 171 / 200) loss: 3.616992\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.130556\n",
      "(Iteration 181 / 200) loss: 5.001054\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.130556\n",
      "(Iteration 191 / 200) loss: 4.444235\n",
      "(Epoch 20 / 20) train acc: 0.134000; val_acc: 0.130556\n",
      "(Iteration 1 / 200) loss: 2.303396\n",
      "(Epoch 0 / 20) train acc: 0.033000; val_acc: 0.044444\n",
      "(Epoch 1 / 20) train acc: 0.064000; val_acc: 0.063889\n",
      "(Iteration 11 / 200) loss: 2.302937\n",
      "(Epoch 2 / 20) train acc: 0.086000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302561\n",
      "(Epoch 3 / 20) train acc: 0.134000; val_acc: 0.144444\n",
      "(Iteration 31 / 200) loss: 2.302516\n",
      "(Epoch 4 / 20) train acc: 0.161000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 2.302117\n",
      "(Epoch 5 / 20) train acc: 0.239000; val_acc: 0.219444\n",
      "(Iteration 51 / 200) loss: 2.301800\n",
      "(Epoch 6 / 20) train acc: 0.266000; val_acc: 0.255556\n",
      "(Iteration 61 / 200) loss: 2.301309\n",
      "(Epoch 7 / 20) train acc: 0.311000; val_acc: 0.275000\n",
      "(Iteration 71 / 200) loss: 2.301137\n",
      "(Epoch 8 / 20) train acc: 0.331000; val_acc: 0.311111\n",
      "(Iteration 81 / 200) loss: 2.300848\n",
      "(Epoch 9 / 20) train acc: 0.342000; val_acc: 0.336111\n",
      "(Iteration 91 / 200) loss: 2.299714\n",
      "(Epoch 10 / 20) train acc: 0.381000; val_acc: 0.350000\n",
      "(Iteration 101 / 200) loss: 2.299868\n",
      "(Epoch 11 / 20) train acc: 0.411000; val_acc: 0.375000\n",
      "(Iteration 111 / 200) loss: 2.299626\n",
      "(Epoch 12 / 20) train acc: 0.445000; val_acc: 0.416667\n",
      "(Iteration 121 / 200) loss: 2.299089\n",
      "(Epoch 13 / 20) train acc: 0.475000; val_acc: 0.450000\n",
      "(Iteration 131 / 200) loss: 2.298745\n",
      "(Epoch 14 / 20) train acc: 0.554000; val_acc: 0.486111\n",
      "(Iteration 141 / 200) loss: 2.298333\n",
      "(Epoch 15 / 20) train acc: 0.597000; val_acc: 0.541667\n",
      "(Iteration 151 / 200) loss: 2.298114\n",
      "(Epoch 16 / 20) train acc: 0.635000; val_acc: 0.577778\n",
      "(Iteration 161 / 200) loss: 2.296748\n",
      "(Epoch 17 / 20) train acc: 0.622000; val_acc: 0.613889\n",
      "(Iteration 171 / 200) loss: 2.295875\n",
      "(Epoch 18 / 20) train acc: 0.599000; val_acc: 0.616667\n",
      "(Iteration 181 / 200) loss: 2.295474\n",
      "(Epoch 19 / 20) train acc: 0.669000; val_acc: 0.627778\n",
      "(Iteration 191 / 200) loss: 2.294443\n",
      "(Epoch 20 / 20) train acc: 0.659000; val_acc: 0.641667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.155000; val_acc: 0.169444\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 2.302577\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302571\n",
      "(Epoch 3 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302562\n",
      "(Epoch 4 / 20) train acc: 0.234000; val_acc: 0.188889\n",
      "(Iteration 41 / 200) loss: 2.302551\n",
      "(Epoch 5 / 20) train acc: 0.270000; val_acc: 0.238889\n",
      "(Iteration 51 / 200) loss: 2.302550\n",
      "(Epoch 6 / 20) train acc: 0.317000; val_acc: 0.261111\n",
      "(Iteration 61 / 200) loss: 2.302497\n",
      "(Epoch 7 / 20) train acc: 0.305000; val_acc: 0.252778\n",
      "(Iteration 71 / 200) loss: 2.302459\n",
      "(Epoch 8 / 20) train acc: 0.286000; val_acc: 0.250000\n",
      "(Iteration 81 / 200) loss: 2.302431\n",
      "(Epoch 9 / 20) train acc: 0.303000; val_acc: 0.252778\n",
      "(Iteration 91 / 200) loss: 2.302399\n",
      "(Epoch 10 / 20) train acc: 0.289000; val_acc: 0.255556\n",
      "(Iteration 101 / 200) loss: 2.302285\n",
      "(Epoch 11 / 20) train acc: 0.308000; val_acc: 0.255556\n",
      "(Iteration 111 / 200) loss: 2.302086\n",
      "(Epoch 12 / 20) train acc: 0.285000; val_acc: 0.261111\n",
      "(Iteration 121 / 200) loss: 2.301923\n",
      "(Epoch 13 / 20) train acc: 0.319000; val_acc: 0.280556\n",
      "(Iteration 131 / 200) loss: 2.301624\n",
      "(Epoch 14 / 20) train acc: 0.353000; val_acc: 0.305556\n",
      "(Iteration 141 / 200) loss: 2.301476\n",
      "(Epoch 15 / 20) train acc: 0.392000; val_acc: 0.308333\n",
      "(Iteration 151 / 200) loss: 2.301103\n",
      "(Epoch 16 / 20) train acc: 0.388000; val_acc: 0.322222\n",
      "(Iteration 161 / 200) loss: 2.300879\n",
      "(Epoch 17 / 20) train acc: 0.395000; val_acc: 0.363889\n",
      "(Iteration 171 / 200) loss: 2.300127\n",
      "(Epoch 18 / 20) train acc: 0.426000; val_acc: 0.372222\n",
      "(Iteration 181 / 200) loss: 2.299163\n",
      "(Epoch 19 / 20) train acc: 0.404000; val_acc: 0.369444\n",
      "(Iteration 191 / 200) loss: 2.298596\n",
      "(Epoch 20 / 20) train acc: 0.405000; val_acc: 0.366667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302572\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302573\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302578\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302570\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302575\n",
      "(Epoch 7 / 20) train acc: 0.154000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 2.302551\n",
      "(Epoch 8 / 20) train acc: 0.204000; val_acc: 0.194444\n",
      "(Iteration 81 / 200) loss: 2.302523\n",
      "(Epoch 9 / 20) train acc: 0.237000; val_acc: 0.213889\n",
      "(Iteration 91 / 200) loss: 2.302469\n",
      "(Epoch 10 / 20) train acc: 0.241000; val_acc: 0.236111\n",
      "(Iteration 101 / 200) loss: 2.302354\n",
      "(Epoch 11 / 20) train acc: 0.232000; val_acc: 0.225000\n",
      "(Iteration 111 / 200) loss: 2.302203\n",
      "(Epoch 12 / 20) train acc: 0.255000; val_acc: 0.225000\n",
      "(Iteration 121 / 200) loss: 2.302113\n",
      "(Epoch 13 / 20) train acc: 0.233000; val_acc: 0.225000\n",
      "(Iteration 131 / 200) loss: 2.301911\n",
      "(Epoch 14 / 20) train acc: 0.237000; val_acc: 0.202778\n",
      "(Iteration 141 / 200) loss: 2.301883\n",
      "(Epoch 15 / 20) train acc: 0.238000; val_acc: 0.216667\n",
      "(Iteration 151 / 200) loss: 2.301496\n",
      "(Epoch 16 / 20) train acc: 0.212000; val_acc: 0.188889\n",
      "(Iteration 161 / 200) loss: 2.300944\n",
      "(Epoch 17 / 20) train acc: 0.200000; val_acc: 0.175000\n",
      "(Iteration 171 / 200) loss: 2.300130\n",
      "(Epoch 18 / 20) train acc: 0.206000; val_acc: 0.191667\n",
      "(Iteration 181 / 200) loss: 2.300249\n",
      "(Epoch 19 / 20) train acc: 0.204000; val_acc: 0.194444\n",
      "(Iteration 191 / 200) loss: 2.298509\n",
      "(Epoch 20 / 20) train acc: 0.215000; val_acc: 0.205556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302578\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302576\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302591\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302571\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302551\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302480\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302444\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302339\n",
      "(Epoch 10 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302212\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.119444\n",
      "(Iteration 111 / 200) loss: 2.302333\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.301949\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.301572\n",
      "(Epoch 14 / 20) train acc: 0.116000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.301657\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.301048\n",
      "(Epoch 16 / 20) train acc: 0.088000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.300548\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.300279\n",
      "(Epoch 18 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.299113\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.299436\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 5428.532794\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.087000; val_acc: 0.061111\n",
      "(Iteration 11 / 200) loss: 5520.509042\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.061111\n",
      "(Iteration 21 / 200) loss: 5096.381540\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.061111\n",
      "(Iteration 31 / 200) loss: 4944.304038\n",
      "(Epoch 4 / 20) train acc: 0.110000; val_acc: 0.061111\n",
      "(Iteration 41 / 200) loss: 5160.523099\n",
      "(Epoch 5 / 20) train acc: 0.098000; val_acc: 0.061111\n",
      "(Iteration 51 / 200) loss: 4785.303410\n",
      "(Epoch 6 / 20) train acc: 0.086000; val_acc: 0.058333\n",
      "(Iteration 61 / 200) loss: 4934.279033\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.058333\n",
      "(Iteration 71 / 200) loss: 4807.554031\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.058333\n",
      "(Iteration 81 / 200) loss: 5499.240280\n",
      "(Epoch 9 / 20) train acc: 0.085000; val_acc: 0.058333\n",
      "(Iteration 91 / 200) loss: 5438.730903\n",
      "(Epoch 10 / 20) train acc: 0.084000; val_acc: 0.061111\n",
      "(Iteration 101 / 200) loss: 4743.007151\n",
      "(Epoch 11 / 20) train acc: 0.089000; val_acc: 0.061111\n",
      "(Iteration 111 / 200) loss: 5213.012150\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.061111\n",
      "(Iteration 121 / 200) loss: 4944.383710\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.063889\n",
      "(Iteration 131 / 200) loss: 4823.582146\n",
      "(Epoch 14 / 20) train acc: 0.078000; val_acc: 0.063889\n",
      "(Iteration 141 / 200) loss: 4520.607770\n",
      "(Epoch 15 / 20) train acc: 0.085000; val_acc: 0.063889\n",
      "(Iteration 151 / 200) loss: 4990.430580\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.063889\n",
      "(Iteration 161 / 200) loss: 4830.657141\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.063889\n",
      "(Iteration 171 / 200) loss: 6017.637765\n",
      "(Epoch 18 / 20) train acc: 0.084000; val_acc: 0.063889\n",
      "(Iteration 181 / 200) loss: 4425.821825\n",
      "(Epoch 19 / 20) train acc: 0.094000; val_acc: 0.063889\n",
      "(Iteration 191 / 200) loss: 4623.276824\n",
      "(Epoch 20 / 20) train acc: 0.082000; val_acc: 0.063889\n",
      "(Iteration 1 / 200) loss: 4.781068\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.122222\n",
      "(Iteration 11 / 200) loss: 4.967485\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 4.690189\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.133333\n",
      "(Iteration 31 / 200) loss: 4.399269\n",
      "(Epoch 4 / 20) train acc: 0.110000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 4.113487\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 4.314032\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.138889\n",
      "(Iteration 61 / 200) loss: 4.219835\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.138889\n",
      "(Iteration 71 / 200) loss: 3.989954\n",
      "(Epoch 8 / 20) train acc: 0.119000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 3.502306\n",
      "(Epoch 9 / 20) train acc: 0.123000; val_acc: 0.138889\n",
      "(Iteration 91 / 200) loss: 3.702394\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 4.154438\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.141667\n",
      "(Iteration 111 / 200) loss: 3.582956\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.141667\n",
      "(Iteration 121 / 200) loss: 3.357919\n",
      "(Epoch 13 / 20) train acc: 0.125000; val_acc: 0.141667\n",
      "(Iteration 131 / 200) loss: 3.227766\n",
      "(Epoch 14 / 20) train acc: 0.149000; val_acc: 0.147222\n",
      "(Iteration 141 / 200) loss: 3.401083\n",
      "(Epoch 15 / 20) train acc: 0.135000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 3.467253\n",
      "(Epoch 16 / 20) train acc: 0.139000; val_acc: 0.144444\n",
      "(Iteration 161 / 200) loss: 3.313280\n",
      "(Epoch 17 / 20) train acc: 0.142000; val_acc: 0.144444\n",
      "(Iteration 171 / 200) loss: 3.485777\n",
      "(Epoch 18 / 20) train acc: 0.133000; val_acc: 0.147222\n",
      "(Iteration 181 / 200) loss: 3.145935\n",
      "(Epoch 19 / 20) train acc: 0.131000; val_acc: 0.150000\n",
      "(Iteration 191 / 200) loss: 3.224415\n",
      "(Epoch 20 / 20) train acc: 0.139000; val_acc: 0.152778\n",
      "(Iteration 1 / 200) loss: 2.302660\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.134000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 2.302541\n",
      "(Epoch 2 / 20) train acc: 0.161000; val_acc: 0.133333\n",
      "(Iteration 21 / 200) loss: 2.302126\n",
      "(Epoch 3 / 20) train acc: 0.160000; val_acc: 0.133333\n",
      "(Iteration 31 / 200) loss: 2.301370\n",
      "(Epoch 4 / 20) train acc: 0.175000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 2.301529\n",
      "(Epoch 5 / 20) train acc: 0.174000; val_acc: 0.138889\n",
      "(Iteration 51 / 200) loss: 2.300814\n",
      "(Epoch 6 / 20) train acc: 0.191000; val_acc: 0.138889\n",
      "(Iteration 61 / 200) loss: 2.300223\n",
      "(Epoch 7 / 20) train acc: 0.190000; val_acc: 0.161111\n",
      "(Iteration 71 / 200) loss: 2.300612\n",
      "(Epoch 8 / 20) train acc: 0.230000; val_acc: 0.177778\n",
      "(Iteration 81 / 200) loss: 2.298558\n",
      "(Epoch 9 / 20) train acc: 0.221000; val_acc: 0.191667\n",
      "(Iteration 91 / 200) loss: 2.298741\n",
      "(Epoch 10 / 20) train acc: 0.244000; val_acc: 0.211111\n",
      "(Iteration 101 / 200) loss: 2.298305\n",
      "(Epoch 11 / 20) train acc: 0.231000; val_acc: 0.230556\n",
      "(Iteration 111 / 200) loss: 2.297461\n",
      "(Epoch 12 / 20) train acc: 0.278000; val_acc: 0.236111\n",
      "(Iteration 121 / 200) loss: 2.296613\n",
      "(Epoch 13 / 20) train acc: 0.280000; val_acc: 0.247222\n",
      "(Iteration 131 / 200) loss: 2.294970\n",
      "(Epoch 14 / 20) train acc: 0.319000; val_acc: 0.272222\n",
      "(Iteration 141 / 200) loss: 2.295976\n",
      "(Epoch 15 / 20) train acc: 0.333000; val_acc: 0.308333\n",
      "(Iteration 151 / 200) loss: 2.294768\n",
      "(Epoch 16 / 20) train acc: 0.363000; val_acc: 0.327778\n",
      "(Iteration 161 / 200) loss: 2.293827\n",
      "(Epoch 17 / 20) train acc: 0.366000; val_acc: 0.330556\n",
      "(Iteration 171 / 200) loss: 2.292434\n",
      "(Epoch 18 / 20) train acc: 0.402000; val_acc: 0.363889\n",
      "(Iteration 181 / 200) loss: 2.290210\n",
      "(Epoch 19 / 20) train acc: 0.421000; val_acc: 0.383333\n",
      "(Iteration 191 / 200) loss: 2.291480\n",
      "(Epoch 20 / 20) train acc: 0.407000; val_acc: 0.408333\n",
      "(Iteration 1 / 200) loss: 2.302586\n",
      "(Epoch 0 / 20) train acc: 0.047000; val_acc: 0.025000\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302564\n",
      "(Epoch 4 / 20) train acc: 0.166000; val_acc: 0.141667\n",
      "(Iteration 41 / 200) loss: 2.302551\n",
      "(Epoch 5 / 20) train acc: 0.209000; val_acc: 0.155556\n",
      "(Iteration 51 / 200) loss: 2.302555\n",
      "(Epoch 6 / 20) train acc: 0.223000; val_acc: 0.175000\n",
      "(Iteration 61 / 200) loss: 2.302510\n",
      "(Epoch 7 / 20) train acc: 0.194000; val_acc: 0.175000\n",
      "(Iteration 71 / 200) loss: 2.302489\n",
      "(Epoch 8 / 20) train acc: 0.191000; val_acc: 0.172222\n",
      "(Iteration 81 / 200) loss: 2.302444\n",
      "(Epoch 9 / 20) train acc: 0.284000; val_acc: 0.258333\n",
      "(Iteration 91 / 200) loss: 2.302380\n",
      "(Epoch 10 / 20) train acc: 0.337000; val_acc: 0.308333\n",
      "(Iteration 101 / 200) loss: 2.302319\n",
      "(Epoch 11 / 20) train acc: 0.305000; val_acc: 0.305556\n",
      "(Iteration 111 / 200) loss: 2.302145\n",
      "(Epoch 12 / 20) train acc: 0.328000; val_acc: 0.305556\n",
      "(Iteration 121 / 200) loss: 2.301960\n",
      "(Epoch 13 / 20) train acc: 0.322000; val_acc: 0.322222\n",
      "(Iteration 131 / 200) loss: 2.301848\n",
      "(Epoch 14 / 20) train acc: 0.334000; val_acc: 0.333333\n",
      "(Iteration 141 / 200) loss: 2.301469\n",
      "(Epoch 15 / 20) train acc: 0.358000; val_acc: 0.344444\n",
      "(Iteration 151 / 200) loss: 2.301258\n",
      "(Epoch 16 / 20) train acc: 0.428000; val_acc: 0.383333\n",
      "(Iteration 161 / 200) loss: 2.300672\n",
      "(Epoch 17 / 20) train acc: 0.473000; val_acc: 0.436111\n",
      "(Iteration 171 / 200) loss: 2.300625\n",
      "(Epoch 18 / 20) train acc: 0.458000; val_acc: 0.422222\n",
      "(Iteration 181 / 200) loss: 2.299905\n",
      "(Epoch 19 / 20) train acc: 0.476000; val_acc: 0.444444\n",
      "(Iteration 191 / 200) loss: 2.299006\n",
      "(Epoch 20 / 20) train acc: 0.471000; val_acc: 0.425000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302589\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302589\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302566\n",
      "(Epoch 4 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302573\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302539\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302528\n",
      "(Epoch 8 / 20) train acc: 0.156000; val_acc: 0.122222\n",
      "(Iteration 81 / 200) loss: 2.302487\n",
      "(Epoch 9 / 20) train acc: 0.218000; val_acc: 0.172222\n",
      "(Iteration 91 / 200) loss: 2.302446\n",
      "(Epoch 10 / 20) train acc: 0.203000; val_acc: 0.172222\n",
      "(Iteration 101 / 200) loss: 2.302355\n",
      "(Epoch 11 / 20) train acc: 0.193000; val_acc: 0.172222\n",
      "(Iteration 111 / 200) loss: 2.302236\n",
      "(Epoch 12 / 20) train acc: 0.208000; val_acc: 0.172222\n",
      "(Iteration 121 / 200) loss: 2.302073\n",
      "(Epoch 13 / 20) train acc: 0.195000; val_acc: 0.172222\n",
      "(Iteration 131 / 200) loss: 2.301885\n",
      "(Epoch 14 / 20) train acc: 0.225000; val_acc: 0.177778\n",
      "(Iteration 141 / 200) loss: 2.301659\n",
      "(Epoch 15 / 20) train acc: 0.287000; val_acc: 0.255556\n",
      "(Iteration 151 / 200) loss: 2.301339\n",
      "(Epoch 16 / 20) train acc: 0.253000; val_acc: 0.238889\n",
      "(Iteration 161 / 200) loss: 2.300983\n",
      "(Epoch 17 / 20) train acc: 0.254000; val_acc: 0.233333\n",
      "(Iteration 171 / 200) loss: 2.300432\n",
      "(Epoch 18 / 20) train acc: 0.225000; val_acc: 0.230556\n",
      "(Iteration 181 / 200) loss: 2.300164\n",
      "(Epoch 19 / 20) train acc: 0.239000; val_acc: 0.216667\n",
      "(Iteration 191 / 200) loss: 2.298845\n",
      "(Epoch 20 / 20) train acc: 0.227000; val_acc: 0.213889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302591\n",
      "(Epoch 4 / 20) train acc: 0.124000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302582\n",
      "(Epoch 5 / 20) train acc: 0.220000; val_acc: 0.155556\n",
      "(Iteration 51 / 200) loss: 2.302581\n",
      "(Epoch 6 / 20) train acc: 0.116000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 2.302569\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 2.302528\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.086111\n",
      "(Iteration 81 / 200) loss: 2.302473\n",
      "(Epoch 9 / 20) train acc: 0.139000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 2.302453\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 2.302265\n",
      "(Epoch 11 / 20) train acc: 0.122000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 2.302179\n",
      "(Epoch 12 / 20) train acc: 0.112000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 2.302041\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 2.301801\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.088889\n",
      "(Iteration 141 / 200) loss: 2.301374\n",
      "(Epoch 15 / 20) train acc: 0.123000; val_acc: 0.088889\n",
      "(Iteration 151 / 200) loss: 2.301180\n",
      "(Epoch 16 / 20) train acc: 0.130000; val_acc: 0.088889\n",
      "(Iteration 161 / 200) loss: 2.300053\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.088889\n",
      "(Iteration 171 / 200) loss: 2.300729\n",
      "(Epoch 18 / 20) train acc: 0.115000; val_acc: 0.088889\n",
      "(Iteration 181 / 200) loss: 2.299224\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.088889\n",
      "(Iteration 191 / 200) loss: 2.299786\n",
      "(Epoch 20 / 20) train acc: 0.156000; val_acc: 0.141667\n",
      "(Iteration 1 / 200) loss: 30134.125798\n",
      "(Epoch 0 / 20) train acc: 0.076000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.519000; val_acc: 0.505556\n",
      "(Iteration 11 / 200) loss: 2450.508394\n",
      "(Epoch 2 / 20) train acc: 0.761000; val_acc: 0.738889\n",
      "(Iteration 21 / 200) loss: 623.684185\n",
      "(Epoch 3 / 20) train acc: 0.865000; val_acc: 0.827778\n",
      "(Iteration 31 / 200) loss: 671.262061\n",
      "(Epoch 4 / 20) train acc: 0.926000; val_acc: 0.855556\n",
      "(Iteration 41 / 200) loss: 383.485908\n",
      "(Epoch 5 / 20) train acc: 0.932000; val_acc: 0.852778\n",
      "(Iteration 51 / 200) loss: 234.704670\n",
      "(Epoch 6 / 20) train acc: 0.954000; val_acc: 0.897222\n",
      "(Iteration 61 / 200) loss: 165.103975\n",
      "(Epoch 7 / 20) train acc: 0.953000; val_acc: 0.872222\n",
      "(Iteration 71 / 200) loss: 199.913125\n",
      "(Epoch 8 / 20) train acc: 0.980000; val_acc: 0.891667\n",
      "(Iteration 81 / 200) loss: 116.874004\n",
      "(Epoch 9 / 20) train acc: 0.983000; val_acc: 0.891667\n",
      "(Iteration 91 / 200) loss: 155.601067\n",
      "(Epoch 10 / 20) train acc: 0.979000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 125.117695\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.900000\n",
      "(Iteration 111 / 200) loss: 177.667512\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.911111\n",
      "(Iteration 121 / 200) loss: 123.570830\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.911111\n",
      "(Iteration 131 / 200) loss: 114.422939\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.925000\n",
      "(Iteration 141 / 200) loss: 112.860425\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.905556\n",
      "(Iteration 151 / 200) loss: 119.051171\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.911111\n",
      "(Iteration 161 / 200) loss: 117.830496\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 111.498394\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.916667\n",
      "(Iteration 181 / 200) loss: 111.095161\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.913889\n",
      "(Iteration 191 / 200) loss: 110.715730\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 4.999716\n",
      "(Epoch 0 / 20) train acc: 0.188000; val_acc: 0.166667\n",
      "(Epoch 1 / 20) train acc: 0.729000; val_acc: 0.711111\n",
      "(Iteration 11 / 200) loss: 1.926162\n",
      "(Epoch 2 / 20) train acc: 0.893000; val_acc: 0.866667\n",
      "(Iteration 21 / 200) loss: 1.218065\n",
      "(Epoch 3 / 20) train acc: 0.954000; val_acc: 0.930556\n",
      "(Iteration 31 / 200) loss: 0.994049\n",
      "(Epoch 4 / 20) train acc: 0.953000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.955660\n",
      "(Epoch 5 / 20) train acc: 0.961000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.802576\n",
      "(Epoch 6 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.693216\n",
      "(Epoch 7 / 20) train acc: 0.985000; val_acc: 0.966667\n",
      "(Iteration 71 / 200) loss: 0.616295\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.568739\n",
      "(Epoch 9 / 20) train acc: 0.993000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.497515\n",
      "(Epoch 10 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.475317\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.980556\n",
      "(Iteration 111 / 200) loss: 0.434882\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.401501\n",
      "(Epoch 13 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.384819\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.365066\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.955556\n",
      "(Iteration 151 / 200) loss: 0.337642\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.325191\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.283772\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.354622\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.260053\n",
      "(Epoch 20 / 20) train acc: 0.985000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.315893\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.155000; val_acc: 0.191667\n",
      "(Iteration 11 / 200) loss: 2.231239\n",
      "(Epoch 2 / 20) train acc: 0.430000; val_acc: 0.408333\n",
      "(Iteration 21 / 200) loss: 1.703054\n",
      "(Epoch 3 / 20) train acc: 0.563000; val_acc: 0.572222\n",
      "(Iteration 31 / 200) loss: 1.581420\n",
      "(Epoch 4 / 20) train acc: 0.635000; val_acc: 0.627778\n",
      "(Iteration 41 / 200) loss: 1.162183\n",
      "(Epoch 5 / 20) train acc: 0.790000; val_acc: 0.802778\n",
      "(Iteration 51 / 200) loss: 0.755984\n",
      "(Epoch 6 / 20) train acc: 0.838000; val_acc: 0.863889\n",
      "(Iteration 61 / 200) loss: 0.516689\n",
      "(Epoch 7 / 20) train acc: 0.892000; val_acc: 0.847222\n",
      "(Iteration 71 / 200) loss: 0.519315\n",
      "(Epoch 8 / 20) train acc: 0.941000; val_acc: 0.911111\n",
      "(Iteration 81 / 200) loss: 0.640638\n",
      "(Epoch 9 / 20) train acc: 0.929000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 0.443841\n",
      "(Epoch 10 / 20) train acc: 0.934000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.384221\n",
      "(Epoch 11 / 20) train acc: 0.968000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.280856\n",
      "(Epoch 12 / 20) train acc: 0.980000; val_acc: 0.936111\n",
      "(Iteration 121 / 200) loss: 0.265604\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.224745\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.239607\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.318993\n",
      "(Epoch 16 / 20) train acc: 0.972000; val_acc: 0.952778\n",
      "(Iteration 161 / 200) loss: 0.182108\n",
      "(Epoch 17 / 20) train acc: 0.965000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 0.260644\n",
      "(Epoch 18 / 20) train acc: 0.845000; val_acc: 0.850000\n",
      "(Iteration 181 / 200) loss: 0.440520\n",
      "(Epoch 19 / 20) train acc: 0.985000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.221720\n",
      "(Epoch 20 / 20) train acc: 0.980000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302717\n",
      "(Epoch 0 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.139000; val_acc: 0.141667\n",
      "(Iteration 11 / 200) loss: 2.285489\n",
      "(Epoch 2 / 20) train acc: 0.211000; val_acc: 0.227778\n",
      "(Iteration 21 / 200) loss: 1.940979\n",
      "(Epoch 3 / 20) train acc: 0.277000; val_acc: 0.313889\n",
      "(Iteration 31 / 200) loss: 1.907213\n",
      "(Epoch 4 / 20) train acc: 0.321000; val_acc: 0.336111\n",
      "(Iteration 41 / 200) loss: 1.577326\n",
      "(Epoch 5 / 20) train acc: 0.316000; val_acc: 0.347222\n",
      "(Iteration 51 / 200) loss: 1.607480\n",
      "(Epoch 6 / 20) train acc: 0.533000; val_acc: 0.502778\n",
      "(Iteration 61 / 200) loss: 1.219355\n",
      "(Epoch 7 / 20) train acc: 0.718000; val_acc: 0.697222\n",
      "(Iteration 71 / 200) loss: 0.808103\n",
      "(Epoch 8 / 20) train acc: 0.849000; val_acc: 0.827778\n",
      "(Iteration 81 / 200) loss: 0.975350\n",
      "(Epoch 9 / 20) train acc: 0.880000; val_acc: 0.830556\n",
      "(Iteration 91 / 200) loss: 0.608704\n",
      "(Epoch 10 / 20) train acc: 0.916000; val_acc: 0.883333\n",
      "(Iteration 101 / 200) loss: 0.477097\n",
      "(Epoch 11 / 20) train acc: 0.943000; val_acc: 0.925000\n",
      "(Iteration 111 / 200) loss: 0.345820\n",
      "(Epoch 12 / 20) train acc: 0.932000; val_acc: 0.911111\n",
      "(Iteration 121 / 200) loss: 0.357584\n",
      "(Epoch 13 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 0.248347\n",
      "(Epoch 14 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 141 / 200) loss: 0.283280\n",
      "(Epoch 15 / 20) train acc: 0.938000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.341540\n",
      "(Epoch 16 / 20) train acc: 0.985000; val_acc: 0.927778\n",
      "(Iteration 161 / 200) loss: 0.267757\n",
      "(Epoch 17 / 20) train acc: 0.976000; val_acc: 0.933333\n",
      "(Iteration 171 / 200) loss: 0.268218\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.267001\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.184139\n",
      "(Epoch 20 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.154070\n",
      "(Epoch 2 / 20) train acc: 0.242000; val_acc: 0.180556\n",
      "(Iteration 21 / 200) loss: 1.973579\n",
      "(Epoch 3 / 20) train acc: 0.231000; val_acc: 0.227778\n",
      "(Iteration 31 / 200) loss: 1.759106\n",
      "(Epoch 4 / 20) train acc: 0.310000; val_acc: 0.294444\n",
      "(Iteration 41 / 200) loss: 1.673715\n",
      "(Epoch 5 / 20) train acc: 0.392000; val_acc: 0.405556\n",
      "(Iteration 51 / 200) loss: 1.663283\n",
      "(Epoch 6 / 20) train acc: 0.437000; val_acc: 0.425000\n",
      "(Iteration 61 / 200) loss: 1.214458\n",
      "(Epoch 7 / 20) train acc: 0.646000; val_acc: 0.633333\n",
      "(Iteration 71 / 200) loss: 0.939873\n",
      "(Epoch 8 / 20) train acc: 0.750000; val_acc: 0.747222\n",
      "(Iteration 81 / 200) loss: 0.812367\n",
      "(Epoch 9 / 20) train acc: 0.821000; val_acc: 0.802778\n",
      "(Iteration 91 / 200) loss: 0.708139\n",
      "(Epoch 10 / 20) train acc: 0.904000; val_acc: 0.883333\n",
      "(Iteration 101 / 200) loss: 0.365453\n",
      "(Epoch 11 / 20) train acc: 0.917000; val_acc: 0.911111\n",
      "(Iteration 111 / 200) loss: 0.331221\n",
      "(Epoch 12 / 20) train acc: 0.960000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 0.391836\n",
      "(Epoch 13 / 20) train acc: 0.935000; val_acc: 0.905556\n",
      "(Iteration 131 / 200) loss: 0.397996\n",
      "(Epoch 14 / 20) train acc: 0.964000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.314482\n",
      "(Epoch 15 / 20) train acc: 0.957000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.307574\n",
      "(Epoch 16 / 20) train acc: 0.930000; val_acc: 0.850000\n",
      "(Iteration 161 / 200) loss: 0.381765\n",
      "(Epoch 17 / 20) train acc: 0.981000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.242108\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.266401\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.279866\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 2.060558\n",
      "(Epoch 2 / 20) train acc: 0.246000; val_acc: 0.208333\n",
      "(Iteration 21 / 200) loss: 1.837242\n",
      "(Epoch 3 / 20) train acc: 0.212000; val_acc: 0.191667\n",
      "(Iteration 31 / 200) loss: 1.706206\n",
      "(Epoch 4 / 20) train acc: 0.217000; val_acc: 0.200000\n",
      "(Iteration 41 / 200) loss: 1.748871\n",
      "(Epoch 5 / 20) train acc: 0.482000; val_acc: 0.466667\n",
      "(Iteration 51 / 200) loss: 1.385522\n",
      "(Epoch 6 / 20) train acc: 0.675000; val_acc: 0.647222\n",
      "(Iteration 61 / 200) loss: 1.038224\n",
      "(Epoch 7 / 20) train acc: 0.715000; val_acc: 0.708333\n",
      "(Iteration 71 / 200) loss: 0.852139\n",
      "(Epoch 8 / 20) train acc: 0.820000; val_acc: 0.788889\n",
      "(Iteration 81 / 200) loss: 0.698070\n",
      "(Epoch 9 / 20) train acc: 0.846000; val_acc: 0.827778\n",
      "(Iteration 91 / 200) loss: 0.488657\n",
      "(Epoch 10 / 20) train acc: 0.872000; val_acc: 0.869444\n",
      "(Iteration 101 / 200) loss: 0.579668\n",
      "(Epoch 11 / 20) train acc: 0.928000; val_acc: 0.900000\n",
      "(Iteration 111 / 200) loss: 0.486281\n",
      "(Epoch 12 / 20) train acc: 0.903000; val_acc: 0.877778\n",
      "(Iteration 121 / 200) loss: 0.450410\n",
      "(Epoch 13 / 20) train acc: 0.947000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.387045\n",
      "(Epoch 14 / 20) train acc: 0.972000; val_acc: 0.944444\n",
      "(Iteration 141 / 200) loss: 0.260558\n",
      "(Epoch 15 / 20) train acc: 0.987000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.230353\n",
      "(Epoch 16 / 20) train acc: 0.987000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.226743\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.260899\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.187164\n",
      "(Epoch 19 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.216699\n",
      "(Epoch 20 / 20) train acc: 0.985000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 41570.635764\n",
      "(Epoch 0 / 20) train acc: 0.076000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.525000; val_acc: 0.472222\n",
      "(Iteration 11 / 200) loss: 2662.339165\n",
      "(Epoch 2 / 20) train acc: 0.779000; val_acc: 0.711111\n",
      "(Iteration 21 / 200) loss: 839.203730\n",
      "(Epoch 3 / 20) train acc: 0.896000; val_acc: 0.800000\n",
      "(Iteration 31 / 200) loss: 692.210801\n",
      "(Epoch 4 / 20) train acc: 0.923000; val_acc: 0.858333\n",
      "(Iteration 41 / 200) loss: 500.685232\n",
      "(Epoch 5 / 20) train acc: 0.933000; val_acc: 0.877778\n",
      "(Iteration 51 / 200) loss: 205.377129\n",
      "(Epoch 6 / 20) train acc: 0.920000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 329.432573\n",
      "(Epoch 7 / 20) train acc: 0.957000; val_acc: 0.911111\n",
      "(Iteration 71 / 200) loss: 224.943711\n",
      "(Epoch 8 / 20) train acc: 0.971000; val_acc: 0.905556\n",
      "(Iteration 81 / 200) loss: 180.642761\n",
      "(Epoch 9 / 20) train acc: 0.969000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 131.182747\n",
      "(Epoch 10 / 20) train acc: 0.972000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 128.436611\n",
      "(Epoch 11 / 20) train acc: 0.974000; val_acc: 0.908333\n",
      "(Iteration 111 / 200) loss: 144.003550\n",
      "(Epoch 12 / 20) train acc: 0.981000; val_acc: 0.911111\n",
      "(Iteration 121 / 200) loss: 155.808306\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.916667\n",
      "(Iteration 131 / 200) loss: 117.865647\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.922222\n",
      "(Iteration 141 / 200) loss: 117.415605\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 116.992886\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.922222\n",
      "(Iteration 161 / 200) loss: 116.604951\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.925000\n",
      "(Iteration 171 / 200) loss: 116.243652\n",
      "(Epoch 18 / 20) train acc: 0.988000; val_acc: 0.922222\n",
      "(Iteration 181 / 200) loss: 122.143452\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.922222\n",
      "(Iteration 191 / 200) loss: 115.550771\n",
      "(Epoch 20 / 20) train acc: 0.977000; val_acc: 0.925000\n",
      "(Iteration 1 / 200) loss: 4.526124\n",
      "(Epoch 0 / 20) train acc: 0.437000; val_acc: 0.400000\n",
      "(Epoch 1 / 20) train acc: 0.859000; val_acc: 0.833333\n",
      "(Iteration 11 / 200) loss: 1.356212\n",
      "(Epoch 2 / 20) train acc: 0.949000; val_acc: 0.930556\n",
      "(Iteration 21 / 200) loss: 1.110753\n",
      "(Epoch 3 / 20) train acc: 0.949000; val_acc: 0.944444\n",
      "(Iteration 31 / 200) loss: 0.875318\n",
      "(Epoch 4 / 20) train acc: 0.981000; val_acc: 0.961111\n",
      "(Iteration 41 / 200) loss: 0.771726\n",
      "(Epoch 5 / 20) train acc: 0.975000; val_acc: 0.955556\n",
      "(Iteration 51 / 200) loss: 0.652920\n",
      "(Epoch 6 / 20) train acc: 0.982000; val_acc: 0.958333\n",
      "(Iteration 61 / 200) loss: 0.581549\n",
      "(Epoch 7 / 20) train acc: 0.969000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.565827\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.444503\n",
      "(Epoch 9 / 20) train acc: 0.971000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.477853\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.400630\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.361079\n",
      "(Epoch 12 / 20) train acc: 0.966000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.351361\n",
      "(Epoch 13 / 20) train acc: 0.982000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.341952\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.958333\n",
      "(Iteration 141 / 200) loss: 0.304439\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.282061\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.276732\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.250234\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.988889\n",
      "(Iteration 181 / 200) loss: 0.214762\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.206417\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 2.315778\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.253967\n",
      "(Epoch 2 / 20) train acc: 0.298000; val_acc: 0.238889\n",
      "(Iteration 21 / 200) loss: 1.892421\n",
      "(Epoch 3 / 20) train acc: 0.386000; val_acc: 0.311111\n",
      "(Iteration 31 / 200) loss: 1.738149\n",
      "(Epoch 4 / 20) train acc: 0.578000; val_acc: 0.541667\n",
      "(Iteration 41 / 200) loss: 1.154846\n",
      "(Epoch 5 / 20) train acc: 0.700000; val_acc: 0.694444\n",
      "(Iteration 51 / 200) loss: 1.077262\n",
      "(Epoch 6 / 20) train acc: 0.785000; val_acc: 0.752778\n",
      "(Iteration 61 / 200) loss: 0.794162\n",
      "(Epoch 7 / 20) train acc: 0.889000; val_acc: 0.883333\n",
      "(Iteration 71 / 200) loss: 0.528107\n",
      "(Epoch 8 / 20) train acc: 0.936000; val_acc: 0.913889\n",
      "(Iteration 81 / 200) loss: 0.336125\n",
      "(Epoch 9 / 20) train acc: 0.930000; val_acc: 0.905556\n",
      "(Iteration 91 / 200) loss: 0.440957\n",
      "(Epoch 10 / 20) train acc: 0.967000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 0.423014\n",
      "(Epoch 11 / 20) train acc: 0.946000; val_acc: 0.936111\n",
      "(Iteration 111 / 200) loss: 0.320683\n",
      "(Epoch 12 / 20) train acc: 0.985000; val_acc: 0.950000\n",
      "(Iteration 121 / 200) loss: 0.253660\n",
      "(Epoch 13 / 20) train acc: 0.981000; val_acc: 0.944444\n",
      "(Iteration 131 / 200) loss: 0.227358\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.227672\n",
      "(Epoch 15 / 20) train acc: 0.984000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.217429\n",
      "(Epoch 16 / 20) train acc: 0.975000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 0.222561\n",
      "(Epoch 17 / 20) train acc: 0.962000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.268071\n",
      "(Epoch 18 / 20) train acc: 0.973000; val_acc: 0.944444\n",
      "(Iteration 181 / 200) loss: 0.176407\n",
      "(Epoch 19 / 20) train acc: 0.970000; val_acc: 0.936111\n",
      "(Iteration 191 / 200) loss: 0.236186\n",
      "(Epoch 20 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302720\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.306721\n",
      "(Epoch 2 / 20) train acc: 0.189000; val_acc: 0.177778\n",
      "(Iteration 21 / 200) loss: 2.130503\n",
      "(Epoch 3 / 20) train acc: 0.214000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 1.928138\n",
      "(Epoch 4 / 20) train acc: 0.296000; val_acc: 0.313889\n",
      "(Iteration 41 / 200) loss: 1.565644\n",
      "(Epoch 5 / 20) train acc: 0.433000; val_acc: 0.405556\n",
      "(Iteration 51 / 200) loss: 1.456006\n",
      "(Epoch 6 / 20) train acc: 0.530000; val_acc: 0.536111\n",
      "(Iteration 61 / 200) loss: 1.262113\n",
      "(Epoch 7 / 20) train acc: 0.705000; val_acc: 0.686111\n",
      "(Iteration 71 / 200) loss: 0.974041\n",
      "(Epoch 8 / 20) train acc: 0.795000; val_acc: 0.763889\n",
      "(Iteration 81 / 200) loss: 0.777496\n",
      "(Epoch 9 / 20) train acc: 0.864000; val_acc: 0.852778\n",
      "(Iteration 91 / 200) loss: 0.416888\n",
      "(Epoch 10 / 20) train acc: 0.902000; val_acc: 0.855556\n",
      "(Iteration 101 / 200) loss: 0.500804\n",
      "(Epoch 11 / 20) train acc: 0.958000; val_acc: 0.911111\n",
      "(Iteration 111 / 200) loss: 0.366691\n",
      "(Epoch 12 / 20) train acc: 0.945000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 0.398302\n",
      "(Epoch 13 / 20) train acc: 0.946000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 0.373568\n",
      "(Epoch 14 / 20) train acc: 0.959000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 0.313091\n",
      "(Epoch 15 / 20) train acc: 0.958000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 0.263642\n",
      "(Epoch 16 / 20) train acc: 0.969000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.228823\n",
      "(Epoch 17 / 20) train acc: 0.972000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.301043\n",
      "(Epoch 18 / 20) train acc: 0.966000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.305050\n",
      "(Epoch 19 / 20) train acc: 0.976000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 0.240911\n",
      "(Epoch 20 / 20) train acc: 0.967000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.089530\n",
      "(Epoch 2 / 20) train acc: 0.224000; val_acc: 0.216667\n",
      "(Iteration 21 / 200) loss: 2.021747\n",
      "(Epoch 3 / 20) train acc: 0.223000; val_acc: 0.211111\n",
      "(Iteration 31 / 200) loss: 1.877106\n",
      "(Epoch 4 / 20) train acc: 0.374000; val_acc: 0.363889\n",
      "(Iteration 41 / 200) loss: 1.532336\n",
      "(Epoch 5 / 20) train acc: 0.470000; val_acc: 0.436111\n",
      "(Iteration 51 / 200) loss: 1.400198\n",
      "(Epoch 6 / 20) train acc: 0.593000; val_acc: 0.550000\n",
      "(Iteration 61 / 200) loss: 1.106291\n",
      "(Epoch 7 / 20) train acc: 0.746000; val_acc: 0.672222\n",
      "(Iteration 71 / 200) loss: 1.008965\n",
      "(Epoch 8 / 20) train acc: 0.834000; val_acc: 0.763889\n",
      "(Iteration 81 / 200) loss: 0.649606\n",
      "(Epoch 9 / 20) train acc: 0.902000; val_acc: 0.877778\n",
      "(Iteration 91 / 200) loss: 0.545403\n",
      "(Epoch 10 / 20) train acc: 0.942000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.405189\n",
      "(Epoch 11 / 20) train acc: 0.961000; val_acc: 0.902778\n",
      "(Iteration 111 / 200) loss: 0.468517\n",
      "(Epoch 12 / 20) train acc: 0.962000; val_acc: 0.930556\n",
      "(Iteration 121 / 200) loss: 0.384849\n",
      "(Epoch 13 / 20) train acc: 0.976000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 0.342769\n",
      "(Epoch 14 / 20) train acc: 0.945000; val_acc: 0.902778\n",
      "(Iteration 141 / 200) loss: 0.547878\n",
      "(Epoch 15 / 20) train acc: 0.963000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.240451\n",
      "(Epoch 16 / 20) train acc: 0.968000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 0.398202\n",
      "(Epoch 17 / 20) train acc: 0.979000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.323073\n",
      "(Epoch 18 / 20) train acc: 0.966000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.254804\n",
      "(Epoch 19 / 20) train acc: 0.962000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.322478\n",
      "(Epoch 20 / 20) train acc: 0.970000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.300763\n",
      "(Epoch 2 / 20) train acc: 0.171000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 2.091018\n",
      "(Epoch 3 / 20) train acc: 0.233000; val_acc: 0.241667\n",
      "(Iteration 31 / 200) loss: 1.924516\n",
      "(Epoch 4 / 20) train acc: 0.266000; val_acc: 0.236111\n",
      "(Iteration 41 / 200) loss: 1.702820\n",
      "(Epoch 5 / 20) train acc: 0.276000; val_acc: 0.300000\n",
      "(Iteration 51 / 200) loss: 1.595116\n",
      "(Epoch 6 / 20) train acc: 0.443000; val_acc: 0.430556\n",
      "(Iteration 61 / 200) loss: 1.407760\n",
      "(Epoch 7 / 20) train acc: 0.639000; val_acc: 0.625000\n",
      "(Iteration 71 / 200) loss: 1.179029\n",
      "(Epoch 8 / 20) train acc: 0.743000; val_acc: 0.713889\n",
      "(Iteration 81 / 200) loss: 0.821246\n",
      "(Epoch 9 / 20) train acc: 0.902000; val_acc: 0.880556\n",
      "(Iteration 91 / 200) loss: 0.550420\n",
      "(Epoch 10 / 20) train acc: 0.920000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.404183\n",
      "(Epoch 11 / 20) train acc: 0.934000; val_acc: 0.886111\n",
      "(Iteration 111 / 200) loss: 0.413719\n",
      "(Epoch 12 / 20) train acc: 0.958000; val_acc: 0.916667\n",
      "(Iteration 121 / 200) loss: 0.367983\n",
      "(Epoch 13 / 20) train acc: 0.910000; val_acc: 0.905556\n",
      "(Iteration 131 / 200) loss: 0.397319\n",
      "(Epoch 14 / 20) train acc: 0.969000; val_acc: 0.938889\n",
      "(Iteration 141 / 200) loss: 0.254525\n",
      "(Epoch 15 / 20) train acc: 0.976000; val_acc: 0.922222\n",
      "(Iteration 151 / 200) loss: 0.359678\n",
      "(Epoch 16 / 20) train acc: 0.960000; val_acc: 0.922222\n",
      "(Iteration 161 / 200) loss: 0.346072\n",
      "(Epoch 17 / 20) train acc: 0.962000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 0.360646\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.938889\n",
      "(Iteration 181 / 200) loss: 0.350003\n",
      "(Epoch 19 / 20) train acc: 0.971000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 0.236381\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.941667\n",
      "(Iteration 1 / 200) loss: 32358.357917\n",
      "(Epoch 0 / 20) train acc: 0.168000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.491000; val_acc: 0.425000\n",
      "(Iteration 11 / 200) loss: 3770.703428\n",
      "(Epoch 2 / 20) train acc: 0.739000; val_acc: 0.675000\n",
      "(Iteration 21 / 200) loss: 1194.728625\n",
      "(Epoch 3 / 20) train acc: 0.849000; val_acc: 0.802778\n",
      "(Iteration 31 / 200) loss: 513.003047\n",
      "(Epoch 4 / 20) train acc: 0.919000; val_acc: 0.866667\n",
      "(Iteration 41 / 200) loss: 447.672246\n",
      "(Epoch 5 / 20) train acc: 0.940000; val_acc: 0.880556\n",
      "(Iteration 51 / 200) loss: 275.592542\n",
      "(Epoch 6 / 20) train acc: 0.950000; val_acc: 0.888889\n",
      "(Iteration 61 / 200) loss: 254.560918\n",
      "(Epoch 7 / 20) train acc: 0.952000; val_acc: 0.880556\n",
      "(Iteration 71 / 200) loss: 172.265259\n",
      "(Epoch 8 / 20) train acc: 0.958000; val_acc: 0.883333\n",
      "(Iteration 81 / 200) loss: 123.437129\n",
      "(Epoch 9 / 20) train acc: 0.965000; val_acc: 0.911111\n",
      "(Iteration 91 / 200) loss: 168.987717\n",
      "(Epoch 10 / 20) train acc: 0.968000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 126.928738\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.902778\n",
      "(Iteration 111 / 200) loss: 197.672510\n",
      "(Epoch 12 / 20) train acc: 0.978000; val_acc: 0.911111\n",
      "(Iteration 121 / 200) loss: 115.663901\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.916667\n",
      "(Iteration 131 / 200) loss: 115.105249\n",
      "(Epoch 14 / 20) train acc: 0.989000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 125.176997\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.916667\n",
      "(Iteration 151 / 200) loss: 134.716128\n",
      "(Epoch 16 / 20) train acc: 0.990000; val_acc: 0.927778\n",
      "(Iteration 161 / 200) loss: 115.359387\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.927778\n",
      "(Iteration 171 / 200) loss: 113.188191\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.919444\n",
      "(Iteration 181 / 200) loss: 112.771465\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.916667\n",
      "(Iteration 191 / 200) loss: 113.462144\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.913889\n",
      "(Iteration 1 / 200) loss: 4.487105\n",
      "(Epoch 0 / 20) train acc: 0.207000; val_acc: 0.227778\n",
      "(Epoch 1 / 20) train acc: 0.679000; val_acc: 0.727778\n",
      "(Iteration 11 / 200) loss: 2.004452\n",
      "(Epoch 2 / 20) train acc: 0.879000; val_acc: 0.875000\n",
      "(Iteration 21 / 200) loss: 1.229532\n",
      "(Epoch 3 / 20) train acc: 0.951000; val_acc: 0.911111\n",
      "(Iteration 31 / 200) loss: 0.962439\n",
      "(Epoch 4 / 20) train acc: 0.966000; val_acc: 0.916667\n",
      "(Iteration 41 / 200) loss: 0.860673\n",
      "(Epoch 5 / 20) train acc: 0.967000; val_acc: 0.938889\n",
      "(Iteration 51 / 200) loss: 0.738807\n",
      "(Epoch 6 / 20) train acc: 0.973000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 0.670579\n",
      "(Epoch 7 / 20) train acc: 0.963000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.679675\n",
      "(Epoch 8 / 20) train acc: 0.972000; val_acc: 0.952778\n",
      "(Iteration 81 / 200) loss: 0.504235\n",
      "(Epoch 9 / 20) train acc: 0.971000; val_acc: 0.933333\n",
      "(Iteration 91 / 200) loss: 0.554518\n",
      "(Epoch 10 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.441765\n",
      "(Epoch 11 / 20) train acc: 0.980000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.401308\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.373021\n",
      "(Epoch 13 / 20) train acc: 0.984000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.332772\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.348430\n",
      "(Epoch 15 / 20) train acc: 0.991000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.309129\n",
      "(Epoch 16 / 20) train acc: 0.984000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.305153\n",
      "(Epoch 17 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.288123\n",
      "(Epoch 18 / 20) train acc: 0.977000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.288578\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.263579\n",
      "(Epoch 20 / 20) train acc: 0.969000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.315934\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.146000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 2.205888\n",
      "(Epoch 2 / 20) train acc: 0.202000; val_acc: 0.200000\n",
      "(Iteration 21 / 200) loss: 1.910122\n",
      "(Epoch 3 / 20) train acc: 0.232000; val_acc: 0.208333\n",
      "(Iteration 31 / 200) loss: 1.824099\n",
      "(Epoch 4 / 20) train acc: 0.395000; val_acc: 0.391667\n",
      "(Iteration 41 / 200) loss: 1.625643\n",
      "(Epoch 5 / 20) train acc: 0.595000; val_acc: 0.550000\n",
      "(Iteration 51 / 200) loss: 1.225524\n",
      "(Epoch 6 / 20) train acc: 0.800000; val_acc: 0.833333\n",
      "(Iteration 61 / 200) loss: 0.750955\n",
      "(Epoch 7 / 20) train acc: 0.887000; val_acc: 0.861111\n",
      "(Iteration 71 / 200) loss: 0.614475\n",
      "(Epoch 8 / 20) train acc: 0.940000; val_acc: 0.919444\n",
      "(Iteration 81 / 200) loss: 0.423028\n",
      "(Epoch 9 / 20) train acc: 0.968000; val_acc: 0.947222\n",
      "(Iteration 91 / 200) loss: 0.362898\n",
      "(Epoch 10 / 20) train acc: 0.969000; val_acc: 0.919444\n",
      "(Iteration 101 / 200) loss: 0.289389\n",
      "(Epoch 11 / 20) train acc: 0.963000; val_acc: 0.941667\n",
      "(Iteration 111 / 200) loss: 0.287490\n",
      "(Epoch 12 / 20) train acc: 0.975000; val_acc: 0.950000\n",
      "(Iteration 121 / 200) loss: 0.336628\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.947222\n",
      "(Iteration 131 / 200) loss: 0.306991\n",
      "(Epoch 14 / 20) train acc: 0.984000; val_acc: 0.955556\n",
      "(Iteration 141 / 200) loss: 0.255783\n",
      "(Epoch 15 / 20) train acc: 0.966000; val_acc: 0.944444\n",
      "(Iteration 151 / 200) loss: 0.300699\n",
      "(Epoch 16 / 20) train acc: 0.947000; val_acc: 0.919444\n",
      "(Iteration 161 / 200) loss: 0.270760\n",
      "(Epoch 17 / 20) train acc: 0.978000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.232217\n",
      "(Epoch 18 / 20) train acc: 0.973000; val_acc: 0.936111\n",
      "(Iteration 181 / 200) loss: 0.212600\n",
      "(Epoch 19 / 20) train acc: 0.984000; val_acc: 0.966667\n",
      "(Iteration 191 / 200) loss: 0.223786\n",
      "(Epoch 20 / 20) train acc: 0.973000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 2.302719\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.203707\n",
      "(Epoch 2 / 20) train acc: 0.214000; val_acc: 0.200000\n",
      "(Iteration 21 / 200) loss: 2.037781\n",
      "(Epoch 3 / 20) train acc: 0.282000; val_acc: 0.275000\n",
      "(Iteration 31 / 200) loss: 1.893222\n",
      "(Epoch 4 / 20) train acc: 0.446000; val_acc: 0.466667\n",
      "(Iteration 41 / 200) loss: 1.643357\n",
      "(Epoch 5 / 20) train acc: 0.578000; val_acc: 0.516667\n",
      "(Iteration 51 / 200) loss: 1.132368\n",
      "(Epoch 6 / 20) train acc: 0.679000; val_acc: 0.697222\n",
      "(Iteration 61 / 200) loss: 0.881107\n",
      "(Epoch 7 / 20) train acc: 0.807000; val_acc: 0.825000\n",
      "(Iteration 71 / 200) loss: 0.760389\n",
      "(Epoch 8 / 20) train acc: 0.889000; val_acc: 0.869444\n",
      "(Iteration 81 / 200) loss: 0.651021\n",
      "(Epoch 9 / 20) train acc: 0.950000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 0.322031\n",
      "(Epoch 10 / 20) train acc: 0.961000; val_acc: 0.927778\n",
      "(Iteration 101 / 200) loss: 0.317782\n",
      "(Epoch 11 / 20) train acc: 0.939000; val_acc: 0.944444\n",
      "(Iteration 111 / 200) loss: 0.360365\n",
      "(Epoch 12 / 20) train acc: 0.963000; val_acc: 0.927778\n",
      "(Iteration 121 / 200) loss: 0.384345\n",
      "(Epoch 13 / 20) train acc: 0.952000; val_acc: 0.938889\n",
      "(Iteration 131 / 200) loss: 0.284288\n",
      "(Epoch 14 / 20) train acc: 0.940000; val_acc: 0.913889\n",
      "(Iteration 141 / 200) loss: 0.309457\n",
      "(Epoch 15 / 20) train acc: 0.983000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.293055\n",
      "(Epoch 16 / 20) train acc: 0.968000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.316592\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 171 / 200) loss: 0.217553\n",
      "(Epoch 18 / 20) train acc: 0.983000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.188315\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.197254\n",
      "(Epoch 20 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.194000; val_acc: 0.211111\n",
      "(Iteration 11 / 200) loss: 2.223468\n",
      "(Epoch 2 / 20) train acc: 0.205000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 1.968928\n",
      "(Epoch 3 / 20) train acc: 0.221000; val_acc: 0.247222\n",
      "(Iteration 31 / 200) loss: 1.822232\n",
      "(Epoch 4 / 20) train acc: 0.236000; val_acc: 0.238889\n",
      "(Iteration 41 / 200) loss: 1.733277\n",
      "(Epoch 5 / 20) train acc: 0.431000; val_acc: 0.413889\n",
      "(Iteration 51 / 200) loss: 1.413828\n",
      "(Epoch 6 / 20) train acc: 0.484000; val_acc: 0.463889\n",
      "(Iteration 61 / 200) loss: 1.192612\n",
      "(Epoch 7 / 20) train acc: 0.715000; val_acc: 0.661111\n",
      "(Iteration 71 / 200) loss: 0.968445\n",
      "(Epoch 8 / 20) train acc: 0.743000; val_acc: 0.797222\n",
      "(Iteration 81 / 200) loss: 1.109509\n",
      "(Epoch 9 / 20) train acc: 0.837000; val_acc: 0.852778\n",
      "(Iteration 91 / 200) loss: 0.616551\n",
      "(Epoch 10 / 20) train acc: 0.915000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 0.411372\n",
      "(Epoch 11 / 20) train acc: 0.953000; val_acc: 0.905556\n",
      "(Iteration 111 / 200) loss: 0.358071\n",
      "(Epoch 12 / 20) train acc: 0.933000; val_acc: 0.913889\n",
      "(Iteration 121 / 200) loss: 0.361871\n",
      "(Epoch 13 / 20) train acc: 0.976000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.363024\n",
      "(Epoch 14 / 20) train acc: 0.956000; val_acc: 0.927778\n",
      "(Iteration 141 / 200) loss: 0.327238\n",
      "(Epoch 15 / 20) train acc: 0.954000; val_acc: 0.916667\n",
      "(Iteration 151 / 200) loss: 0.326190\n",
      "(Epoch 16 / 20) train acc: 0.988000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.246161\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.383429\n",
      "(Epoch 18 / 20) train acc: 0.983000; val_acc: 0.950000\n",
      "(Iteration 181 / 200) loss: 0.220500\n",
      "(Epoch 19 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.187461\n",
      "(Epoch 20 / 20) train acc: 0.980000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.119000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.082341\n",
      "(Epoch 2 / 20) train acc: 0.204000; val_acc: 0.205556\n",
      "(Iteration 21 / 200) loss: 2.031120\n",
      "(Epoch 3 / 20) train acc: 0.303000; val_acc: 0.277778\n",
      "(Iteration 31 / 200) loss: 1.837585\n",
      "(Epoch 4 / 20) train acc: 0.439000; val_acc: 0.397222\n",
      "(Iteration 41 / 200) loss: 1.371808\n",
      "(Epoch 5 / 20) train acc: 0.549000; val_acc: 0.555556\n",
      "(Iteration 51 / 200) loss: 1.666492\n",
      "(Epoch 6 / 20) train acc: 0.580000; val_acc: 0.602778\n",
      "(Iteration 61 / 200) loss: 1.070421\n",
      "(Epoch 7 / 20) train acc: 0.676000; val_acc: 0.688889\n",
      "(Iteration 71 / 200) loss: 0.985176\n",
      "(Epoch 8 / 20) train acc: 0.737000; val_acc: 0.730556\n",
      "(Iteration 81 / 200) loss: 0.957238\n",
      "(Epoch 9 / 20) train acc: 0.829000; val_acc: 0.813889\n",
      "(Iteration 91 / 200) loss: 0.552007\n",
      "(Epoch 10 / 20) train acc: 0.912000; val_acc: 0.852778\n",
      "(Iteration 101 / 200) loss: 0.451710\n",
      "(Epoch 11 / 20) train acc: 0.938000; val_acc: 0.913889\n",
      "(Iteration 111 / 200) loss: 0.569341\n",
      "(Epoch 12 / 20) train acc: 0.973000; val_acc: 0.916667\n",
      "(Iteration 121 / 200) loss: 0.363506\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 0.350871\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.235906\n",
      "(Epoch 15 / 20) train acc: 0.939000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.392089\n",
      "(Epoch 16 / 20) train acc: 0.958000; val_acc: 0.911111\n",
      "(Iteration 161 / 200) loss: 0.344042\n",
      "(Epoch 17 / 20) train acc: 0.977000; val_acc: 0.947222\n",
      "(Iteration 171 / 200) loss: 0.235045\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.207426\n",
      "(Epoch 19 / 20) train acc: 0.963000; val_acc: 0.930556\n",
      "(Iteration 191 / 200) loss: 0.313245\n",
      "(Epoch 20 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 29012.508915\n",
      "(Epoch 0 / 20) train acc: 0.070000; val_acc: 0.069444\n",
      "(Epoch 1 / 20) train acc: 0.529000; val_acc: 0.452778\n",
      "(Iteration 11 / 200) loss: 2780.115757\n",
      "(Epoch 2 / 20) train acc: 0.787000; val_acc: 0.733333\n",
      "(Iteration 21 / 200) loss: 588.604919\n",
      "(Epoch 3 / 20) train acc: 0.879000; val_acc: 0.827778\n",
      "(Iteration 31 / 200) loss: 579.000097\n",
      "(Epoch 4 / 20) train acc: 0.897000; val_acc: 0.863889\n",
      "(Iteration 41 / 200) loss: 337.256016\n",
      "(Epoch 5 / 20) train acc: 0.933000; val_acc: 0.863889\n",
      "(Iteration 51 / 200) loss: 250.968218\n",
      "(Epoch 6 / 20) train acc: 0.938000; val_acc: 0.886111\n",
      "(Iteration 61 / 200) loss: 290.860472\n",
      "(Epoch 7 / 20) train acc: 0.942000; val_acc: 0.877778\n",
      "(Iteration 71 / 200) loss: 40.295824\n",
      "(Epoch 8 / 20) train acc: 0.950000; val_acc: 0.888889\n",
      "(Iteration 81 / 200) loss: 108.613898\n",
      "(Epoch 9 / 20) train acc: 0.966000; val_acc: 0.902778\n",
      "(Iteration 91 / 200) loss: 24.585604\n",
      "(Epoch 10 / 20) train acc: 0.978000; val_acc: 0.919444\n",
      "(Iteration 101 / 200) loss: 52.757780\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.913889\n",
      "(Iteration 111 / 200) loss: 18.331975\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 11.643458\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.922222\n",
      "(Iteration 131 / 200) loss: 12.120903\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.913889\n",
      "(Iteration 141 / 200) loss: 18.717308\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.925000\n",
      "(Iteration 151 / 200) loss: 11.552046\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 11.528783\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.925000\n",
      "(Iteration 171 / 200) loss: 12.520925\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 15.286961\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.919444\n",
      "(Iteration 191 / 200) loss: 14.569060\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.938889\n",
      "(Iteration 1 / 200) loss: 4.334030\n",
      "(Epoch 0 / 20) train acc: 0.278000; val_acc: 0.275000\n",
      "(Epoch 1 / 20) train acc: 0.827000; val_acc: 0.808333\n",
      "(Iteration 11 / 200) loss: 0.730539\n",
      "(Epoch 2 / 20) train acc: 0.927000; val_acc: 0.900000\n",
      "(Iteration 21 / 200) loss: 0.371174\n",
      "(Epoch 3 / 20) train acc: 0.944000; val_acc: 0.919444\n",
      "(Iteration 31 / 200) loss: 0.226494\n",
      "(Epoch 4 / 20) train acc: 0.962000; val_acc: 0.927778\n",
      "(Iteration 41 / 200) loss: 0.218440\n",
      "(Epoch 5 / 20) train acc: 0.966000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 0.331128\n",
      "(Epoch 6 / 20) train acc: 0.955000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.212133\n",
      "(Epoch 7 / 20) train acc: 0.981000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.155369\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.132578\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.938889\n",
      "(Iteration 91 / 200) loss: 0.132772\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.966667\n",
      "(Iteration 101 / 200) loss: 0.119826\n",
      "(Epoch 11 / 20) train acc: 0.985000; val_acc: 0.952778\n",
      "(Iteration 111 / 200) loss: 0.134172\n",
      "(Epoch 12 / 20) train acc: 0.982000; val_acc: 0.952778\n",
      "(Iteration 121 / 200) loss: 0.159363\n",
      "(Epoch 13 / 20) train acc: 0.984000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 0.132563\n",
      "(Epoch 14 / 20) train acc: 0.979000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.226596\n",
      "(Epoch 15 / 20) train acc: 0.969000; val_acc: 0.936111\n",
      "(Iteration 151 / 200) loss: 0.164052\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.145738\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.096032\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.130075\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.177569\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.303896\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.233000; val_acc: 0.200000\n",
      "(Iteration 11 / 200) loss: 2.015111\n",
      "(Epoch 2 / 20) train acc: 0.434000; val_acc: 0.436111\n",
      "(Iteration 21 / 200) loss: 1.346750\n",
      "(Epoch 3 / 20) train acc: 0.570000; val_acc: 0.550000\n",
      "(Iteration 31 / 200) loss: 1.044091\n",
      "(Epoch 4 / 20) train acc: 0.668000; val_acc: 0.658333\n",
      "(Iteration 41 / 200) loss: 0.728099\n",
      "(Epoch 5 / 20) train acc: 0.799000; val_acc: 0.786111\n",
      "(Iteration 51 / 200) loss: 0.548672\n",
      "(Epoch 6 / 20) train acc: 0.888000; val_acc: 0.852778\n",
      "(Iteration 61 / 200) loss: 0.407611\n",
      "(Epoch 7 / 20) train acc: 0.957000; val_acc: 0.900000\n",
      "(Iteration 71 / 200) loss: 0.250574\n",
      "(Epoch 8 / 20) train acc: 0.952000; val_acc: 0.925000\n",
      "(Iteration 81 / 200) loss: 0.094325\n",
      "(Epoch 9 / 20) train acc: 0.964000; val_acc: 0.938889\n",
      "(Iteration 91 / 200) loss: 0.164374\n",
      "(Epoch 10 / 20) train acc: 0.964000; val_acc: 0.941667\n",
      "(Iteration 101 / 200) loss: 0.167952\n",
      "(Epoch 11 / 20) train acc: 0.961000; val_acc: 0.941667\n",
      "(Iteration 111 / 200) loss: 0.161769\n",
      "(Epoch 12 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.070076\n",
      "(Epoch 13 / 20) train acc: 0.943000; val_acc: 0.930556\n",
      "(Iteration 131 / 200) loss: 0.245448\n",
      "(Epoch 14 / 20) train acc: 0.971000; val_acc: 0.955556\n",
      "(Iteration 141 / 200) loss: 0.070269\n",
      "(Epoch 15 / 20) train acc: 0.969000; val_acc: 0.936111\n",
      "(Iteration 151 / 200) loss: 0.081630\n",
      "(Epoch 16 / 20) train acc: 0.963000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.156690\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.941667\n",
      "(Iteration 171 / 200) loss: 0.116021\n",
      "(Epoch 18 / 20) train acc: 0.976000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.123228\n",
      "(Epoch 19 / 20) train acc: 0.980000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.108239\n",
      "(Epoch 20 / 20) train acc: 0.974000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.108333\n",
      "(Epoch 1 / 20) train acc: 0.178000; val_acc: 0.219444\n",
      "(Iteration 11 / 200) loss: 2.141720\n",
      "(Epoch 2 / 20) train acc: 0.330000; val_acc: 0.344444\n",
      "(Iteration 21 / 200) loss: 1.732278\n",
      "(Epoch 3 / 20) train acc: 0.457000; val_acc: 0.444444\n",
      "(Iteration 31 / 200) loss: 1.437321\n",
      "(Epoch 4 / 20) train acc: 0.635000; val_acc: 0.661111\n",
      "(Iteration 41 / 200) loss: 0.903779\n",
      "(Epoch 5 / 20) train acc: 0.700000; val_acc: 0.741667\n",
      "(Iteration 51 / 200) loss: 0.793134\n",
      "(Epoch 6 / 20) train acc: 0.823000; val_acc: 0.825000\n",
      "(Iteration 61 / 200) loss: 0.498329\n",
      "(Epoch 7 / 20) train acc: 0.893000; val_acc: 0.858333\n",
      "(Iteration 71 / 200) loss: 0.477734\n",
      "(Epoch 8 / 20) train acc: 0.905000; val_acc: 0.877778\n",
      "(Iteration 81 / 200) loss: 0.361342\n",
      "(Epoch 9 / 20) train acc: 0.957000; val_acc: 0.888889\n",
      "(Iteration 91 / 200) loss: 0.300989\n",
      "(Epoch 10 / 20) train acc: 0.929000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 0.191694\n",
      "(Epoch 11 / 20) train acc: 0.953000; val_acc: 0.927778\n",
      "(Iteration 111 / 200) loss: 0.213837\n",
      "(Epoch 12 / 20) train acc: 0.964000; val_acc: 0.944444\n",
      "(Iteration 121 / 200) loss: 0.112648\n",
      "(Epoch 13 / 20) train acc: 0.968000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.131785\n",
      "(Epoch 14 / 20) train acc: 0.947000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.136758\n",
      "(Epoch 15 / 20) train acc: 0.938000; val_acc: 0.913889\n",
      "(Iteration 151 / 200) loss: 0.236242\n",
      "(Epoch 16 / 20) train acc: 0.926000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 0.251443\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.114864\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.149679\n",
      "(Epoch 19 / 20) train acc: 0.983000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.084173\n",
      "(Epoch 20 / 20) train acc: 0.976000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.306544\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.108333\n",
      "(Iteration 21 / 200) loss: 2.297845\n",
      "(Epoch 3 / 20) train acc: 0.177000; val_acc: 0.233333\n",
      "(Iteration 31 / 200) loss: 1.956601\n",
      "(Epoch 4 / 20) train acc: 0.244000; val_acc: 0.277778\n",
      "(Iteration 41 / 200) loss: 1.842109\n",
      "(Epoch 5 / 20) train acc: 0.238000; val_acc: 0.269444\n",
      "(Iteration 51 / 200) loss: 1.628718\n",
      "(Epoch 6 / 20) train acc: 0.383000; val_acc: 0.369444\n",
      "(Iteration 61 / 200) loss: 1.502212\n",
      "(Epoch 7 / 20) train acc: 0.545000; val_acc: 0.530556\n",
      "(Iteration 71 / 200) loss: 1.256350\n",
      "(Epoch 8 / 20) train acc: 0.681000; val_acc: 0.688889\n",
      "(Iteration 81 / 200) loss: 0.900257\n",
      "(Epoch 9 / 20) train acc: 0.787000; val_acc: 0.808333\n",
      "(Iteration 91 / 200) loss: 0.642050\n",
      "(Epoch 10 / 20) train acc: 0.838000; val_acc: 0.825000\n",
      "(Iteration 101 / 200) loss: 0.453511\n",
      "(Epoch 11 / 20) train acc: 0.875000; val_acc: 0.863889\n",
      "(Iteration 111 / 200) loss: 0.359435\n",
      "(Epoch 12 / 20) train acc: 0.893000; val_acc: 0.838889\n",
      "(Iteration 121 / 200) loss: 0.315898\n",
      "(Epoch 13 / 20) train acc: 0.940000; val_acc: 0.919444\n",
      "(Iteration 131 / 200) loss: 0.234761\n",
      "(Epoch 14 / 20) train acc: 0.927000; val_acc: 0.913889\n",
      "(Iteration 141 / 200) loss: 0.209795\n",
      "(Epoch 15 / 20) train acc: 0.951000; val_acc: 0.938889\n",
      "(Iteration 151 / 200) loss: 0.200459\n",
      "(Epoch 16 / 20) train acc: 0.948000; val_acc: 0.916667\n",
      "(Iteration 161 / 200) loss: 0.248628\n",
      "(Epoch 17 / 20) train acc: 0.950000; val_acc: 0.933333\n",
      "(Iteration 171 / 200) loss: 0.211776\n",
      "(Epoch 18 / 20) train acc: 0.946000; val_acc: 0.927778\n",
      "(Iteration 181 / 200) loss: 0.155485\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.122687\n",
      "(Epoch 20 / 20) train acc: 0.970000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.176450\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.155556\n",
      "(Iteration 21 / 200) loss: 1.938352\n",
      "(Epoch 3 / 20) train acc: 0.209000; val_acc: 0.175000\n",
      "(Iteration 31 / 200) loss: 2.092097\n",
      "(Epoch 4 / 20) train acc: 0.345000; val_acc: 0.355556\n",
      "(Iteration 41 / 200) loss: 1.860957\n",
      "(Epoch 5 / 20) train acc: 0.408000; val_acc: 0.430556\n",
      "(Iteration 51 / 200) loss: 1.307207\n",
      "(Epoch 6 / 20) train acc: 0.635000; val_acc: 0.658333\n",
      "(Iteration 61 / 200) loss: 1.041533\n",
      "(Epoch 7 / 20) train acc: 0.866000; val_acc: 0.813889\n",
      "(Iteration 71 / 200) loss: 0.486436\n",
      "(Epoch 8 / 20) train acc: 0.914000; val_acc: 0.883333\n",
      "(Iteration 81 / 200) loss: 0.201211\n",
      "(Epoch 9 / 20) train acc: 0.849000; val_acc: 0.833333\n",
      "(Iteration 91 / 200) loss: 0.595541\n",
      "(Epoch 10 / 20) train acc: 0.948000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.163035\n",
      "(Epoch 11 / 20) train acc: 0.958000; val_acc: 0.916667\n",
      "(Iteration 111 / 200) loss: 0.238216\n",
      "(Epoch 12 / 20) train acc: 0.974000; val_acc: 0.930556\n",
      "(Iteration 121 / 200) loss: 0.268431\n",
      "(Epoch 13 / 20) train acc: 0.971000; val_acc: 0.944444\n",
      "(Iteration 131 / 200) loss: 0.142364\n",
      "(Epoch 14 / 20) train acc: 0.971000; val_acc: 0.922222\n",
      "(Iteration 141 / 200) loss: 0.090515\n",
      "(Epoch 15 / 20) train acc: 0.974000; val_acc: 0.933333\n",
      "(Iteration 151 / 200) loss: 0.125066\n",
      "(Epoch 16 / 20) train acc: 0.982000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.163194\n",
      "(Epoch 17 / 20) train acc: 0.980000; val_acc: 0.941667\n",
      "(Iteration 171 / 200) loss: 0.074213\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.060368\n",
      "(Epoch 19 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 191 / 200) loss: 0.067777\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 39183.863006\n",
      "(Epoch 0 / 20) train acc: 0.075000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.366000; val_acc: 0.361111\n",
      "(Iteration 11 / 200) loss: 6111.988465\n",
      "(Epoch 2 / 20) train acc: 0.729000; val_acc: 0.713889\n",
      "(Iteration 21 / 200) loss: 1997.284578\n",
      "(Epoch 3 / 20) train acc: 0.817000; val_acc: 0.775000\n",
      "(Iteration 31 / 200) loss: 827.316729\n",
      "(Epoch 4 / 20) train acc: 0.868000; val_acc: 0.858333\n",
      "(Iteration 41 / 200) loss: 364.884387\n",
      "(Epoch 5 / 20) train acc: 0.919000; val_acc: 0.866667\n",
      "(Iteration 51 / 200) loss: 377.319400\n",
      "(Epoch 6 / 20) train acc: 0.923000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 304.718990\n",
      "(Epoch 7 / 20) train acc: 0.922000; val_acc: 0.880556\n",
      "(Iteration 71 / 200) loss: 139.695637\n",
      "(Epoch 8 / 20) train acc: 0.936000; val_acc: 0.902778\n",
      "(Iteration 81 / 200) loss: 43.490616\n",
      "(Epoch 9 / 20) train acc: 0.959000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 244.352798\n",
      "(Epoch 10 / 20) train acc: 0.970000; val_acc: 0.908333\n",
      "(Iteration 101 / 200) loss: 81.273291\n",
      "(Epoch 11 / 20) train acc: 0.973000; val_acc: 0.911111\n",
      "(Iteration 111 / 200) loss: 40.906720\n",
      "(Epoch 12 / 20) train acc: 0.975000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 80.509655\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.911111\n",
      "(Iteration 131 / 200) loss: 80.055771\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.911111\n",
      "(Iteration 141 / 200) loss: 29.400417\n",
      "(Epoch 15 / 20) train acc: 0.971000; val_acc: 0.927778\n",
      "(Iteration 151 / 200) loss: 129.982306\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 11.564762\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.925000\n",
      "(Iteration 171 / 200) loss: 18.997673\n",
      "(Epoch 18 / 20) train acc: 0.985000; val_acc: 0.936111\n",
      "(Iteration 181 / 200) loss: 32.051954\n",
      "(Epoch 19 / 20) train acc: 0.984000; val_acc: 0.930556\n",
      "(Iteration 191 / 200) loss: 16.643947\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 4.311062\n",
      "(Epoch 0 / 20) train acc: 0.206000; val_acc: 0.177778\n",
      "(Epoch 1 / 20) train acc: 0.737000; val_acc: 0.725000\n",
      "(Iteration 11 / 200) loss: 1.035590\n",
      "(Epoch 2 / 20) train acc: 0.911000; val_acc: 0.877778\n",
      "(Iteration 21 / 200) loss: 0.410719\n",
      "(Epoch 3 / 20) train acc: 0.952000; val_acc: 0.894444\n",
      "(Iteration 31 / 200) loss: 0.268394\n",
      "(Epoch 4 / 20) train acc: 0.939000; val_acc: 0.933333\n",
      "(Iteration 41 / 200) loss: 0.208481\n",
      "(Epoch 5 / 20) train acc: 0.959000; val_acc: 0.927778\n",
      "(Iteration 51 / 200) loss: 0.225235\n",
      "(Epoch 6 / 20) train acc: 0.966000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.213242\n",
      "(Epoch 7 / 20) train acc: 0.979000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.162254\n",
      "(Epoch 8 / 20) train acc: 0.979000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.194217\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.191481\n",
      "(Epoch 10 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.138504\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.156741\n",
      "(Epoch 12 / 20) train acc: 0.990000; val_acc: 0.980556\n",
      "(Iteration 121 / 200) loss: 0.132902\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.125853\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.111317\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.986111\n",
      "(Iteration 151 / 200) loss: 0.290878\n",
      "(Epoch 16 / 20) train acc: 0.980000; val_acc: 0.952778\n",
      "(Iteration 161 / 200) loss: 0.106920\n",
      "(Epoch 17 / 20) train acc: 0.966000; val_acc: 0.927778\n",
      "(Iteration 171 / 200) loss: 0.250946\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.114121\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.947222\n",
      "(Iteration 191 / 200) loss: 0.109706\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.303928\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.260000; val_acc: 0.266667\n",
      "(Iteration 11 / 200) loss: 1.666861\n",
      "(Epoch 2 / 20) train acc: 0.652000; val_acc: 0.647222\n",
      "(Iteration 21 / 200) loss: 1.064807\n",
      "(Epoch 3 / 20) train acc: 0.750000; val_acc: 0.761111\n",
      "(Iteration 31 / 200) loss: 0.513439\n",
      "(Epoch 4 / 20) train acc: 0.857000; val_acc: 0.827778\n",
      "(Iteration 41 / 200) loss: 0.458929\n",
      "(Epoch 5 / 20) train acc: 0.942000; val_acc: 0.916667\n",
      "(Iteration 51 / 200) loss: 0.294352\n",
      "(Epoch 6 / 20) train acc: 0.952000; val_acc: 0.916667\n",
      "(Iteration 61 / 200) loss: 0.185384\n",
      "(Epoch 7 / 20) train acc: 0.958000; val_acc: 0.911111\n",
      "(Iteration 71 / 200) loss: 0.264201\n",
      "(Epoch 8 / 20) train acc: 0.966000; val_acc: 0.933333\n",
      "(Iteration 81 / 200) loss: 0.069113\n",
      "(Epoch 9 / 20) train acc: 0.955000; val_acc: 0.930556\n",
      "(Iteration 91 / 200) loss: 0.243058\n",
      "(Epoch 10 / 20) train acc: 0.960000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.120646\n",
      "(Epoch 11 / 20) train acc: 0.974000; val_acc: 0.944444\n",
      "(Iteration 111 / 200) loss: 0.082587\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.124768\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.106473\n",
      "(Epoch 14 / 20) train acc: 0.981000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.094549\n",
      "(Epoch 15 / 20) train acc: 0.975000; val_acc: 0.944444\n",
      "(Iteration 151 / 200) loss: 0.184794\n",
      "(Epoch 16 / 20) train acc: 0.986000; val_acc: 0.961111\n",
      "(Iteration 161 / 200) loss: 0.107475\n",
      "(Epoch 17 / 20) train acc: 0.985000; val_acc: 0.947222\n",
      "(Iteration 171 / 200) loss: 0.116412\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.061224\n",
      "(Epoch 19 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.080778\n",
      "(Epoch 20 / 20) train acc: 0.975000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302598\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.283000; val_acc: 0.322222\n",
      "(Iteration 11 / 200) loss: 1.759166\n",
      "(Epoch 2 / 20) train acc: 0.506000; val_acc: 0.483333\n",
      "(Iteration 21 / 200) loss: 1.375934\n",
      "(Epoch 3 / 20) train acc: 0.492000; val_acc: 0.508333\n",
      "(Iteration 31 / 200) loss: 1.135814\n",
      "(Epoch 4 / 20) train acc: 0.732000; val_acc: 0.727778\n",
      "(Iteration 41 / 200) loss: 0.654341\n",
      "(Epoch 5 / 20) train acc: 0.855000; val_acc: 0.825000\n",
      "(Iteration 51 / 200) loss: 0.499308\n",
      "(Epoch 6 / 20) train acc: 0.909000; val_acc: 0.866667\n",
      "(Iteration 61 / 200) loss: 0.310437\n",
      "(Epoch 7 / 20) train acc: 0.922000; val_acc: 0.902778\n",
      "(Iteration 71 / 200) loss: 0.243479\n",
      "(Epoch 8 / 20) train acc: 0.951000; val_acc: 0.944444\n",
      "(Iteration 81 / 200) loss: 0.301393\n",
      "(Epoch 9 / 20) train acc: 0.970000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.115258\n",
      "(Epoch 10 / 20) train acc: 0.966000; val_acc: 0.941667\n",
      "(Iteration 101 / 200) loss: 0.097102\n",
      "(Epoch 11 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.057699\n",
      "(Epoch 12 / 20) train acc: 0.961000; val_acc: 0.958333\n",
      "(Iteration 121 / 200) loss: 0.228414\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.098722\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.147650\n",
      "(Epoch 15 / 20) train acc: 0.985000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.151984\n",
      "(Epoch 16 / 20) train acc: 0.978000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.172183\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.073618\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.073737\n",
      "(Epoch 19 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.096772\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.301137\n",
      "(Epoch 2 / 20) train acc: 0.199000; val_acc: 0.180556\n",
      "(Iteration 21 / 200) loss: 2.212298\n",
      "(Epoch 3 / 20) train acc: 0.193000; val_acc: 0.241667\n",
      "(Iteration 31 / 200) loss: 1.983346\n",
      "(Epoch 4 / 20) train acc: 0.249000; val_acc: 0.263889\n",
      "(Iteration 41 / 200) loss: 1.690194\n",
      "(Epoch 5 / 20) train acc: 0.518000; val_acc: 0.511111\n",
      "(Iteration 51 / 200) loss: 1.596809\n",
      "(Epoch 6 / 20) train acc: 0.665000; val_acc: 0.666667\n",
      "(Iteration 61 / 200) loss: 0.867318\n",
      "(Epoch 7 / 20) train acc: 0.771000; val_acc: 0.761111\n",
      "(Iteration 71 / 200) loss: 0.404271\n",
      "(Epoch 8 / 20) train acc: 0.832000; val_acc: 0.791667\n",
      "(Iteration 81 / 200) loss: 0.409359\n",
      "(Epoch 9 / 20) train acc: 0.898000; val_acc: 0.883333\n",
      "(Iteration 91 / 200) loss: 0.331293\n",
      "(Epoch 10 / 20) train acc: 0.942000; val_acc: 0.916667\n",
      "(Iteration 101 / 200) loss: 0.155958\n",
      "(Epoch 11 / 20) train acc: 0.957000; val_acc: 0.936111\n",
      "(Iteration 111 / 200) loss: 0.159476\n",
      "(Epoch 12 / 20) train acc: 0.957000; val_acc: 0.927778\n",
      "(Iteration 121 / 200) loss: 0.112018\n",
      "(Epoch 13 / 20) train acc: 0.931000; val_acc: 0.913889\n",
      "(Iteration 131 / 200) loss: 0.227078\n",
      "(Epoch 14 / 20) train acc: 0.958000; val_acc: 0.922222\n",
      "(Iteration 141 / 200) loss: 0.132503\n",
      "(Epoch 15 / 20) train acc: 0.927000; val_acc: 0.911111\n",
      "(Iteration 151 / 200) loss: 0.173524\n",
      "(Epoch 16 / 20) train acc: 0.960000; val_acc: 0.941667\n",
      "(Iteration 161 / 200) loss: 0.327716\n",
      "(Epoch 17 / 20) train acc: 0.946000; val_acc: 0.911111\n",
      "(Iteration 171 / 200) loss: 0.307397\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.944444\n",
      "(Iteration 181 / 200) loss: 0.112565\n",
      "(Epoch 19 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.123832\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.167000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 2.169944\n",
      "(Epoch 2 / 20) train acc: 0.141000; val_acc: 0.163889\n",
      "(Iteration 21 / 200) loss: 2.122360\n",
      "(Epoch 3 / 20) train acc: 0.184000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 2.037304\n",
      "(Epoch 4 / 20) train acc: 0.253000; val_acc: 0.247222\n",
      "(Iteration 41 / 200) loss: 1.595654\n",
      "(Epoch 5 / 20) train acc: 0.376000; val_acc: 0.394444\n",
      "(Iteration 51 / 200) loss: 1.378407\n",
      "(Epoch 6 / 20) train acc: 0.561000; val_acc: 0.547222\n",
      "(Iteration 61 / 200) loss: 1.061167\n",
      "(Epoch 7 / 20) train acc: 0.672000; val_acc: 0.672222\n",
      "(Iteration 71 / 200) loss: 0.814957\n",
      "(Epoch 8 / 20) train acc: 0.802000; val_acc: 0.783333\n",
      "(Iteration 81 / 200) loss: 0.425259\n",
      "(Epoch 9 / 20) train acc: 0.884000; val_acc: 0.883333\n",
      "(Iteration 91 / 200) loss: 0.320255\n",
      "(Epoch 10 / 20) train acc: 0.914000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 0.181537\n",
      "(Epoch 11 / 20) train acc: 0.882000; val_acc: 0.872222\n",
      "(Iteration 111 / 200) loss: 0.360729\n",
      "(Epoch 12 / 20) train acc: 0.929000; val_acc: 0.916667\n",
      "(Iteration 121 / 200) loss: 0.322529\n",
      "(Epoch 13 / 20) train acc: 0.971000; val_acc: 0.938889\n",
      "(Iteration 131 / 200) loss: 0.130943\n",
      "(Epoch 14 / 20) train acc: 0.961000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.148972\n",
      "(Epoch 15 / 20) train acc: 0.951000; val_acc: 0.933333\n",
      "(Iteration 151 / 200) loss: 0.098257\n",
      "(Epoch 16 / 20) train acc: 0.982000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 0.104907\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.075467\n",
      "(Epoch 18 / 20) train acc: 0.971000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.194662\n",
      "(Epoch 19 / 20) train acc: 0.981000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.072830\n",
      "(Epoch 20 / 20) train acc: 0.971000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 19799.353579\n",
      "(Epoch 0 / 20) train acc: 0.147000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.655000; val_acc: 0.602778\n",
      "(Iteration 11 / 200) loss: 1690.048565\n",
      "(Epoch 2 / 20) train acc: 0.826000; val_acc: 0.819444\n",
      "(Iteration 21 / 200) loss: 524.696452\n",
      "(Epoch 3 / 20) train acc: 0.879000; val_acc: 0.850000\n",
      "(Iteration 31 / 200) loss: 381.228292\n",
      "(Epoch 4 / 20) train acc: 0.916000; val_acc: 0.883333\n",
      "(Iteration 41 / 200) loss: 556.835503\n",
      "(Epoch 5 / 20) train acc: 0.947000; val_acc: 0.894444\n",
      "(Iteration 51 / 200) loss: 557.988105\n",
      "(Epoch 6 / 20) train acc: 0.957000; val_acc: 0.877778\n",
      "(Iteration 61 / 200) loss: 120.897188\n",
      "(Epoch 7 / 20) train acc: 0.981000; val_acc: 0.913889\n",
      "(Iteration 71 / 200) loss: 15.141694\n",
      "(Epoch 8 / 20) train acc: 0.975000; val_acc: 0.905556\n",
      "(Iteration 81 / 200) loss: 21.351215\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.908333\n",
      "(Iteration 91 / 200) loss: 119.987703\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.888889\n",
      "(Iteration 101 / 200) loss: 22.489504\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.933333\n",
      "(Iteration 111 / 200) loss: 30.753588\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.933333\n",
      "(Iteration 121 / 200) loss: 11.698995\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 13.395348\n",
      "(Epoch 14 / 20) train acc: 0.977000; val_acc: 0.894444\n",
      "(Iteration 141 / 200) loss: 31.389145\n",
      "(Epoch 15 / 20) train acc: 0.985000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 41.004188\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 11.573611\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 11.552219\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.927778\n",
      "(Iteration 181 / 200) loss: 16.499002\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 15.898369\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.933333\n",
      "(Iteration 1 / 200) loss: 5.037004\n",
      "(Epoch 0 / 20) train acc: 0.130000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.733000; val_acc: 0.738889\n",
      "(Iteration 11 / 200) loss: 0.939435\n",
      "(Epoch 2 / 20) train acc: 0.904000; val_acc: 0.883333\n",
      "(Iteration 21 / 200) loss: 0.385973\n",
      "(Epoch 3 / 20) train acc: 0.928000; val_acc: 0.888889\n",
      "(Iteration 31 / 200) loss: 0.319087\n",
      "(Epoch 4 / 20) train acc: 0.942000; val_acc: 0.938889\n",
      "(Iteration 41 / 200) loss: 0.337304\n",
      "(Epoch 5 / 20) train acc: 0.956000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 0.178693\n",
      "(Epoch 6 / 20) train acc: 0.970000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.177806\n",
      "(Epoch 7 / 20) train acc: 0.982000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.162380\n",
      "(Epoch 8 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 0.148081\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.139741\n",
      "(Epoch 10 / 20) train acc: 0.985000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.149577\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.196645\n",
      "(Epoch 12 / 20) train acc: 0.987000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.133185\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.129211\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.102552\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.119268\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 0.100674\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.106221\n",
      "(Epoch 18 / 20) train acc: 0.986000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.154330\n",
      "(Epoch 19 / 20) train acc: 0.982000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.100371\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.303951\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.228000; val_acc: 0.283333\n",
      "(Iteration 11 / 200) loss: 1.975376\n",
      "(Epoch 2 / 20) train acc: 0.619000; val_acc: 0.630556\n",
      "(Iteration 21 / 200) loss: 1.018027\n",
      "(Epoch 3 / 20) train acc: 0.783000; val_acc: 0.811111\n",
      "(Iteration 31 / 200) loss: 0.552039\n",
      "(Epoch 4 / 20) train acc: 0.763000; val_acc: 0.800000\n",
      "(Iteration 41 / 200) loss: 0.635868\n",
      "(Epoch 5 / 20) train acc: 0.887000; val_acc: 0.877778\n",
      "(Iteration 51 / 200) loss: 0.350148\n",
      "(Epoch 6 / 20) train acc: 0.923000; val_acc: 0.897222\n",
      "(Iteration 61 / 200) loss: 0.309195\n",
      "(Epoch 7 / 20) train acc: 0.908000; val_acc: 0.886111\n",
      "(Iteration 71 / 200) loss: 0.188083\n",
      "(Epoch 8 / 20) train acc: 0.943000; val_acc: 0.930556\n",
      "(Iteration 81 / 200) loss: 0.234452\n",
      "(Epoch 9 / 20) train acc: 0.965000; val_acc: 0.936111\n",
      "(Iteration 91 / 200) loss: 0.076215\n",
      "(Epoch 10 / 20) train acc: 0.957000; val_acc: 0.941667\n",
      "(Iteration 101 / 200) loss: 0.092310\n",
      "(Epoch 11 / 20) train acc: 0.964000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.245518\n",
      "(Epoch 12 / 20) train acc: 0.931000; val_acc: 0.905556\n",
      "(Iteration 121 / 200) loss: 0.360039\n",
      "(Epoch 13 / 20) train acc: 0.956000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 0.226113\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.088276\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.091815\n",
      "(Epoch 16 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.107233\n",
      "(Epoch 17 / 20) train acc: 0.987000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.069406\n",
      "(Epoch 18 / 20) train acc: 0.986000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.061074\n",
      "(Epoch 19 / 20) train acc: 0.989000; val_acc: 0.988889\n",
      "(Iteration 191 / 200) loss: 0.104942\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302598\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.198000; val_acc: 0.225000\n",
      "(Iteration 11 / 200) loss: 1.898856\n",
      "(Epoch 2 / 20) train acc: 0.554000; val_acc: 0.561111\n",
      "(Iteration 21 / 200) loss: 1.403654\n",
      "(Epoch 3 / 20) train acc: 0.585000; val_acc: 0.600000\n",
      "(Iteration 31 / 200) loss: 1.109790\n",
      "(Epoch 4 / 20) train acc: 0.693000; val_acc: 0.641667\n",
      "(Iteration 41 / 200) loss: 0.775741\n",
      "(Epoch 5 / 20) train acc: 0.829000; val_acc: 0.844444\n",
      "(Iteration 51 / 200) loss: 0.490874\n",
      "(Epoch 6 / 20) train acc: 0.880000; val_acc: 0.883333\n",
      "(Iteration 61 / 200) loss: 0.261211\n",
      "(Epoch 7 / 20) train acc: 0.925000; val_acc: 0.880556\n",
      "(Iteration 71 / 200) loss: 0.401229\n",
      "(Epoch 8 / 20) train acc: 0.943000; val_acc: 0.925000\n",
      "(Iteration 81 / 200) loss: 0.139332\n",
      "(Epoch 9 / 20) train acc: 0.948000; val_acc: 0.927778\n",
      "(Iteration 91 / 200) loss: 0.197485\n",
      "(Epoch 10 / 20) train acc: 0.965000; val_acc: 0.936111\n",
      "(Iteration 101 / 200) loss: 0.262137\n",
      "(Epoch 11 / 20) train acc: 0.970000; val_acc: 0.938889\n",
      "(Iteration 111 / 200) loss: 0.152267\n",
      "(Epoch 12 / 20) train acc: 0.982000; val_acc: 0.952778\n",
      "(Iteration 121 / 200) loss: 0.092407\n",
      "(Epoch 13 / 20) train acc: 0.967000; val_acc: 0.930556\n",
      "(Iteration 131 / 200) loss: 0.138348\n",
      "(Epoch 14 / 20) train acc: 0.949000; val_acc: 0.933333\n",
      "(Iteration 141 / 200) loss: 0.084675\n",
      "(Epoch 15 / 20) train acc: 0.976000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.113491\n",
      "(Epoch 16 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 0.112212\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.084122\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.076313\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.063614\n",
      "(Epoch 20 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.246805\n",
      "(Epoch 2 / 20) train acc: 0.196000; val_acc: 0.205556\n",
      "(Iteration 21 / 200) loss: 2.032522\n",
      "(Epoch 3 / 20) train acc: 0.204000; val_acc: 0.188889\n",
      "(Iteration 31 / 200) loss: 2.010259\n",
      "(Epoch 4 / 20) train acc: 0.288000; val_acc: 0.269444\n",
      "(Iteration 41 / 200) loss: 1.623729\n",
      "(Epoch 5 / 20) train acc: 0.507000; val_acc: 0.497222\n",
      "(Iteration 51 / 200) loss: 1.278287\n",
      "(Epoch 6 / 20) train acc: 0.635000; val_acc: 0.611111\n",
      "(Iteration 61 / 200) loss: 0.949903\n",
      "(Epoch 7 / 20) train acc: 0.704000; val_acc: 0.680556\n",
      "(Iteration 71 / 200) loss: 0.831390\n",
      "(Epoch 8 / 20) train acc: 0.857000; val_acc: 0.847222\n",
      "(Iteration 81 / 200) loss: 0.369493\n",
      "(Epoch 9 / 20) train acc: 0.797000; val_acc: 0.786111\n",
      "(Iteration 91 / 200) loss: 0.699320\n",
      "(Epoch 10 / 20) train acc: 0.932000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 0.186738\n",
      "(Epoch 11 / 20) train acc: 0.968000; val_acc: 0.955556\n",
      "(Iteration 111 / 200) loss: 0.119088\n",
      "(Epoch 12 / 20) train acc: 0.948000; val_acc: 0.916667\n",
      "(Iteration 121 / 200) loss: 0.274630\n",
      "(Epoch 13 / 20) train acc: 0.930000; val_acc: 0.900000\n",
      "(Iteration 131 / 200) loss: 0.199114\n",
      "(Epoch 14 / 20) train acc: 0.967000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.134344\n",
      "(Epoch 15 / 20) train acc: 0.949000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.263596\n",
      "(Epoch 16 / 20) train acc: 0.983000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.127771\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.110033\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.087727\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.941667\n",
      "(Iteration 191 / 200) loss: 0.121997\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.214000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 2.254426\n",
      "(Epoch 2 / 20) train acc: 0.213000; val_acc: 0.200000\n",
      "(Iteration 21 / 200) loss: 1.926947\n",
      "(Epoch 3 / 20) train acc: 0.248000; val_acc: 0.233333\n",
      "(Iteration 31 / 200) loss: 1.748366\n",
      "(Epoch 4 / 20) train acc: 0.361000; val_acc: 0.341667\n",
      "(Iteration 41 / 200) loss: 1.473419\n",
      "(Epoch 5 / 20) train acc: 0.454000; val_acc: 0.419444\n",
      "(Iteration 51 / 200) loss: 1.264134\n",
      "(Epoch 6 / 20) train acc: 0.590000; val_acc: 0.572222\n",
      "(Iteration 61 / 200) loss: 1.075249\n",
      "(Epoch 7 / 20) train acc: 0.720000; val_acc: 0.733333\n",
      "(Iteration 71 / 200) loss: 0.798418\n",
      "(Epoch 8 / 20) train acc: 0.893000; val_acc: 0.858333\n",
      "(Iteration 81 / 200) loss: 0.482421\n",
      "(Epoch 9 / 20) train acc: 0.913000; val_acc: 0.891667\n",
      "(Iteration 91 / 200) loss: 0.240807\n",
      "(Epoch 10 / 20) train acc: 0.931000; val_acc: 0.900000\n",
      "(Iteration 101 / 200) loss: 0.373031\n",
      "(Epoch 11 / 20) train acc: 0.945000; val_acc: 0.919444\n",
      "(Iteration 111 / 200) loss: 0.259923\n",
      "(Epoch 12 / 20) train acc: 0.945000; val_acc: 0.902778\n",
      "(Iteration 121 / 200) loss: 0.269107\n",
      "(Epoch 13 / 20) train acc: 0.972000; val_acc: 0.933333\n",
      "(Iteration 131 / 200) loss: 0.105324\n",
      "(Epoch 14 / 20) train acc: 0.980000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.082909\n",
      "(Epoch 15 / 20) train acc: 0.987000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.093895\n",
      "(Epoch 16 / 20) train acc: 0.984000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.059582\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.060754\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.089685\n",
      "(Epoch 19 / 20) train acc: 0.978000; val_acc: 0.969444\n",
      "(Iteration 191 / 200) loss: 0.105546\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 25676.676160\n",
      "(Epoch 0 / 20) train acc: 0.215000; val_acc: 0.205556\n",
      "(Epoch 1 / 20) train acc: 0.545000; val_acc: 0.538889\n",
      "(Iteration 11 / 200) loss: 2984.611912\n",
      "(Epoch 2 / 20) train acc: 0.807000; val_acc: 0.752778\n",
      "(Iteration 21 / 200) loss: 610.205038\n",
      "(Epoch 3 / 20) train acc: 0.907000; val_acc: 0.841667\n",
      "(Iteration 31 / 200) loss: 300.378282\n",
      "(Epoch 4 / 20) train acc: 0.947000; val_acc: 0.902778\n",
      "(Iteration 41 / 200) loss: 163.443027\n",
      "(Epoch 5 / 20) train acc: 0.961000; val_acc: 0.913889\n",
      "(Iteration 51 / 200) loss: 206.326285\n",
      "(Epoch 6 / 20) train acc: 0.972000; val_acc: 0.913889\n",
      "(Iteration 61 / 200) loss: 237.956768\n",
      "(Epoch 7 / 20) train acc: 0.984000; val_acc: 0.922222\n",
      "(Iteration 71 / 200) loss: 132.590551\n",
      "(Epoch 8 / 20) train acc: 0.969000; val_acc: 0.930556\n",
      "(Iteration 81 / 200) loss: 31.725810\n",
      "(Epoch 9 / 20) train acc: 0.975000; val_acc: 0.938889\n",
      "(Iteration 91 / 200) loss: 1.195812\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.927778\n",
      "(Iteration 101 / 200) loss: 6.256262\n",
      "(Epoch 11 / 20) train acc: 0.981000; val_acc: 0.938889\n",
      "(Iteration 111 / 200) loss: 1.188460\n",
      "(Epoch 12 / 20) train acc: 0.997000; val_acc: 0.936111\n",
      "(Iteration 121 / 200) loss: 11.385829\n",
      "(Epoch 13 / 20) train acc: 0.982000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 1.182918\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.927778\n",
      "(Iteration 141 / 200) loss: 4.936363\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.936111\n",
      "(Iteration 151 / 200) loss: 1.178751\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 62.378297\n",
      "(Epoch 17 / 20) train acc: 0.986000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 39.720244\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 12.017947\n",
      "(Epoch 19 / 20) train acc: 0.985000; val_acc: 0.930556\n",
      "(Iteration 191 / 200) loss: 14.693314\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.925000\n",
      "(Iteration 1 / 200) loss: 3.891081\n",
      "(Epoch 0 / 20) train acc: 0.271000; val_acc: 0.211111\n",
      "(Epoch 1 / 20) train acc: 0.789000; val_acc: 0.744444\n",
      "(Iteration 11 / 200) loss: 0.537712\n",
      "(Epoch 2 / 20) train acc: 0.931000; val_acc: 0.925000\n",
      "(Iteration 21 / 200) loss: 0.183298\n",
      "(Epoch 3 / 20) train acc: 0.941000; val_acc: 0.930556\n",
      "(Iteration 31 / 200) loss: 0.255122\n",
      "(Epoch 4 / 20) train acc: 0.953000; val_acc: 0.919444\n",
      "(Iteration 41 / 200) loss: 0.136114\n",
      "(Epoch 5 / 20) train acc: 0.956000; val_acc: 0.938889\n",
      "(Iteration 51 / 200) loss: 0.194200\n",
      "(Epoch 6 / 20) train acc: 0.974000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.043014\n",
      "(Epoch 7 / 20) train acc: 0.985000; val_acc: 0.986111\n",
      "(Iteration 71 / 200) loss: 0.030787\n",
      "(Epoch 8 / 20) train acc: 0.990000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 0.032442\n",
      "(Epoch 9 / 20) train acc: 0.979000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.054931\n",
      "(Epoch 10 / 20) train acc: 0.967000; val_acc: 0.919444\n",
      "(Iteration 101 / 200) loss: 0.035873\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.117693\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.986111\n",
      "(Iteration 121 / 200) loss: 0.066799\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.037746\n",
      "(Epoch 14 / 20) train acc: 0.972000; val_acc: 0.944444\n",
      "(Iteration 141 / 200) loss: 0.094915\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.026202\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.044862\n",
      "(Epoch 17 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.048390\n",
      "(Epoch 18 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.118630\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.017345\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302782\n",
      "(Epoch 0 / 20) train acc: 0.127000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.226000; val_acc: 0.236111\n",
      "(Iteration 11 / 200) loss: 1.908226\n",
      "(Epoch 2 / 20) train acc: 0.615000; val_acc: 0.641667\n",
      "(Iteration 21 / 200) loss: 0.957723\n",
      "(Epoch 3 / 20) train acc: 0.744000; val_acc: 0.758333\n",
      "(Iteration 31 / 200) loss: 0.556135\n",
      "(Epoch 4 / 20) train acc: 0.865000; val_acc: 0.819444\n",
      "(Iteration 41 / 200) loss: 0.446377\n",
      "(Epoch 5 / 20) train acc: 0.888000; val_acc: 0.872222\n",
      "(Iteration 51 / 200) loss: 0.241300\n",
      "(Epoch 6 / 20) train acc: 0.967000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.128481\n",
      "(Epoch 7 / 20) train acc: 0.970000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.069212\n",
      "(Epoch 8 / 20) train acc: 0.966000; val_acc: 0.938889\n",
      "(Iteration 81 / 200) loss: 0.170957\n",
      "(Epoch 9 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 91 / 200) loss: 0.150880\n",
      "(Epoch 10 / 20) train acc: 0.973000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 0.142542\n",
      "(Epoch 11 / 20) train acc: 0.976000; val_acc: 0.938889\n",
      "(Iteration 111 / 200) loss: 0.186543\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.950000\n",
      "(Iteration 121 / 200) loss: 0.029343\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 131 / 200) loss: 0.172998\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.063307\n",
      "(Epoch 15 / 20) train acc: 0.987000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.022656\n",
      "(Epoch 16 / 20) train acc: 0.970000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 0.021629\n",
      "(Epoch 17 / 20) train acc: 0.965000; val_acc: 0.944444\n",
      "(Iteration 171 / 200) loss: 0.063701\n",
      "(Epoch 18 / 20) train acc: 0.940000; val_acc: 0.902778\n",
      "(Iteration 181 / 200) loss: 0.178044\n",
      "(Epoch 19 / 20) train acc: 0.976000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.046230\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.930556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.188000; val_acc: 0.194444\n",
      "(Iteration 11 / 200) loss: 1.964970\n",
      "(Epoch 2 / 20) train acc: 0.258000; val_acc: 0.305556\n",
      "(Iteration 21 / 200) loss: 1.905103\n",
      "(Epoch 3 / 20) train acc: 0.540000; val_acc: 0.536111\n",
      "(Iteration 31 / 200) loss: 0.901382\n",
      "(Epoch 4 / 20) train acc: 0.639000; val_acc: 0.672222\n",
      "(Iteration 41 / 200) loss: 0.854671\n",
      "(Epoch 5 / 20) train acc: 0.751000; val_acc: 0.691667\n",
      "(Iteration 51 / 200) loss: 0.645575\n",
      "(Epoch 6 / 20) train acc: 0.845000; val_acc: 0.847222\n",
      "(Iteration 61 / 200) loss: 0.461043\n",
      "(Epoch 7 / 20) train acc: 0.881000; val_acc: 0.841667\n",
      "(Iteration 71 / 200) loss: 0.517602\n",
      "(Epoch 8 / 20) train acc: 0.914000; val_acc: 0.911111\n",
      "(Iteration 81 / 200) loss: 0.301816\n",
      "(Epoch 9 / 20) train acc: 0.938000; val_acc: 0.919444\n",
      "(Iteration 91 / 200) loss: 0.127982\n",
      "(Epoch 10 / 20) train acc: 0.957000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.081034\n",
      "(Epoch 11 / 20) train acc: 0.972000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.130988\n",
      "(Epoch 12 / 20) train acc: 0.963000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 0.134617\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.073090\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.070225\n",
      "(Epoch 15 / 20) train acc: 0.983000; val_acc: 0.944444\n",
      "(Iteration 151 / 200) loss: 0.030458\n",
      "(Epoch 16 / 20) train acc: 0.964000; val_acc: 0.952778\n",
      "(Iteration 161 / 200) loss: 0.052060\n",
      "(Epoch 17 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.020309\n",
      "(Epoch 18 / 20) train acc: 0.976000; val_acc: 0.941667\n",
      "(Iteration 181 / 200) loss: 0.163680\n",
      "(Epoch 19 / 20) train acc: 0.972000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.085023\n",
      "(Epoch 20 / 20) train acc: 0.965000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.173000; val_acc: 0.188889\n",
      "(Iteration 11 / 200) loss: 2.213305\n",
      "(Epoch 2 / 20) train acc: 0.192000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 2.012017\n",
      "(Epoch 3 / 20) train acc: 0.206000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 1.923417\n",
      "(Epoch 4 / 20) train acc: 0.225000; val_acc: 0.205556\n",
      "(Iteration 41 / 200) loss: 1.870536\n",
      "(Epoch 5 / 20) train acc: 0.201000; val_acc: 0.233333\n",
      "(Iteration 51 / 200) loss: 1.817984\n",
      "(Epoch 6 / 20) train acc: 0.296000; val_acc: 0.277778\n",
      "(Iteration 61 / 200) loss: 1.689573\n",
      "(Epoch 7 / 20) train acc: 0.417000; val_acc: 0.422222\n",
      "(Iteration 71 / 200) loss: 1.508797\n",
      "(Epoch 8 / 20) train acc: 0.461000; val_acc: 0.500000\n",
      "(Iteration 81 / 200) loss: 1.312482\n",
      "(Epoch 9 / 20) train acc: 0.500000; val_acc: 0.563889\n",
      "(Iteration 91 / 200) loss: 1.128039\n",
      "(Epoch 10 / 20) train acc: 0.529000; val_acc: 0.591667\n",
      "(Iteration 101 / 200) loss: 0.944451\n",
      "(Epoch 11 / 20) train acc: 0.716000; val_acc: 0.719444\n",
      "(Iteration 111 / 200) loss: 0.755218\n",
      "(Epoch 12 / 20) train acc: 0.814000; val_acc: 0.813889\n",
      "(Iteration 121 / 200) loss: 0.562038\n",
      "(Epoch 13 / 20) train acc: 0.847000; val_acc: 0.808333\n",
      "(Iteration 131 / 200) loss: 0.391610\n",
      "(Epoch 14 / 20) train acc: 0.842000; val_acc: 0.827778\n",
      "(Iteration 141 / 200) loss: 0.314423\n",
      "(Epoch 15 / 20) train acc: 0.945000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 0.216301\n",
      "(Epoch 16 / 20) train acc: 0.974000; val_acc: 0.927778\n",
      "(Iteration 161 / 200) loss: 0.118490\n",
      "(Epoch 17 / 20) train acc: 0.946000; val_acc: 0.933333\n",
      "(Iteration 171 / 200) loss: 0.126632\n",
      "(Epoch 18 / 20) train acc: 0.938000; val_acc: 0.922222\n",
      "(Iteration 181 / 200) loss: 0.117874\n",
      "(Epoch 19 / 20) train acc: 0.965000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 0.066799\n",
      "(Epoch 20 / 20) train acc: 0.956000; val_acc: 0.925000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.290315\n",
      "(Epoch 2 / 20) train acc: 0.221000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 1.922990\n",
      "(Epoch 3 / 20) train acc: 0.302000; val_acc: 0.311111\n",
      "(Iteration 31 / 200) loss: 1.628339\n",
      "(Epoch 4 / 20) train acc: 0.384000; val_acc: 0.380556\n",
      "(Iteration 41 / 200) loss: 1.605331\n",
      "(Epoch 5 / 20) train acc: 0.504000; val_acc: 0.469444\n",
      "(Iteration 51 / 200) loss: 1.271787\n",
      "(Epoch 6 / 20) train acc: 0.653000; val_acc: 0.605556\n",
      "(Iteration 61 / 200) loss: 0.917006\n",
      "(Epoch 7 / 20) train acc: 0.638000; val_acc: 0.613889\n",
      "(Iteration 71 / 200) loss: 0.769948\n",
      "(Epoch 8 / 20) train acc: 0.733000; val_acc: 0.688889\n",
      "(Iteration 81 / 200) loss: 0.685976\n",
      "(Epoch 9 / 20) train acc: 0.822000; val_acc: 0.780556\n",
      "(Iteration 91 / 200) loss: 0.574222\n",
      "(Epoch 10 / 20) train acc: 0.864000; val_acc: 0.833333\n",
      "(Iteration 101 / 200) loss: 0.531948\n",
      "(Epoch 11 / 20) train acc: 0.878000; val_acc: 0.825000\n",
      "(Iteration 111 / 200) loss: 0.327009\n",
      "(Epoch 12 / 20) train acc: 0.926000; val_acc: 0.877778\n",
      "(Iteration 121 / 200) loss: 0.144030\n",
      "(Epoch 13 / 20) train acc: 0.928000; val_acc: 0.891667\n",
      "(Iteration 131 / 200) loss: 0.349641\n",
      "(Epoch 14 / 20) train acc: 0.946000; val_acc: 0.888889\n",
      "(Iteration 141 / 200) loss: 0.130831\n",
      "(Epoch 15 / 20) train acc: 0.904000; val_acc: 0.877778\n",
      "(Iteration 151 / 200) loss: 0.231072\n",
      "(Epoch 16 / 20) train acc: 0.953000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 0.161123\n",
      "(Epoch 17 / 20) train acc: 0.947000; val_acc: 0.905556\n",
      "(Iteration 171 / 200) loss: 0.103503\n",
      "(Epoch 18 / 20) train acc: 0.970000; val_acc: 0.919444\n",
      "(Iteration 181 / 200) loss: 0.085303\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.908333\n",
      "(Iteration 191 / 200) loss: 0.093344\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.941667\n",
      "(Iteration 1 / 200) loss: 26395.096639\n",
      "(Epoch 0 / 20) train acc: 0.163000; val_acc: 0.150000\n",
      "(Epoch 1 / 20) train acc: 0.580000; val_acc: 0.591667\n",
      "(Iteration 11 / 200) loss: 2992.985245\n",
      "(Epoch 2 / 20) train acc: 0.805000; val_acc: 0.750000\n",
      "(Iteration 21 / 200) loss: 324.829463\n",
      "(Epoch 3 / 20) train acc: 0.892000; val_acc: 0.836111\n",
      "(Iteration 31 / 200) loss: 277.258768\n",
      "(Epoch 4 / 20) train acc: 0.903000; val_acc: 0.852778\n",
      "(Iteration 41 / 200) loss: 186.822943\n",
      "(Epoch 5 / 20) train acc: 0.940000; val_acc: 0.894444\n",
      "(Iteration 51 / 200) loss: 305.536387\n",
      "(Epoch 6 / 20) train acc: 0.927000; val_acc: 0.852778\n",
      "(Iteration 61 / 200) loss: 237.120964\n",
      "(Epoch 7 / 20) train acc: 0.960000; val_acc: 0.894444\n",
      "(Iteration 71 / 200) loss: 203.516781\n",
      "(Epoch 8 / 20) train acc: 0.965000; val_acc: 0.894444\n",
      "(Iteration 81 / 200) loss: 23.335772\n",
      "(Epoch 9 / 20) train acc: 0.985000; val_acc: 0.888889\n",
      "(Iteration 91 / 200) loss: 113.624767\n",
      "(Epoch 10 / 20) train acc: 0.978000; val_acc: 0.891667\n",
      "(Iteration 101 / 200) loss: 17.026652\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.902778\n",
      "(Iteration 111 / 200) loss: 1.235742\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.911111\n",
      "(Iteration 121 / 200) loss: 33.251961\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.905556\n",
      "(Iteration 131 / 200) loss: 1.720485\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.908333\n",
      "(Iteration 141 / 200) loss: 1.229295\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.916667\n",
      "(Iteration 151 / 200) loss: 1.227804\n",
      "(Epoch 16 / 20) train acc: 0.990000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 1.226526\n",
      "(Epoch 17 / 20) train acc: 0.979000; val_acc: 0.908333\n",
      "(Iteration 171 / 200) loss: 34.188684\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.908333\n",
      "(Iteration 181 / 200) loss: 1.224527\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.908333\n",
      "(Iteration 191 / 200) loss: 1.223736\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.908333\n",
      "(Iteration 1 / 200) loss: 3.035355\n",
      "(Epoch 0 / 20) train acc: 0.355000; val_acc: 0.286111\n",
      "(Epoch 1 / 20) train acc: 0.865000; val_acc: 0.852778\n",
      "(Iteration 11 / 200) loss: 0.400904\n",
      "(Epoch 2 / 20) train acc: 0.924000; val_acc: 0.891667\n",
      "(Iteration 21 / 200) loss: 0.260411\n",
      "(Epoch 3 / 20) train acc: 0.954000; val_acc: 0.936111\n",
      "(Iteration 31 / 200) loss: 0.077840\n",
      "(Epoch 4 / 20) train acc: 0.976000; val_acc: 0.961111\n",
      "(Iteration 41 / 200) loss: 0.134399\n",
      "(Epoch 5 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 51 / 200) loss: 0.070315\n",
      "(Epoch 6 / 20) train acc: 0.955000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.058837\n",
      "(Epoch 7 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.018432\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.085004\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.038744\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.041748\n",
      "(Epoch 11 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.026000\n",
      "(Epoch 12 / 20) train acc: 0.967000; val_acc: 0.950000\n",
      "(Iteration 121 / 200) loss: 0.046913\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.020167\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.983333\n",
      "(Iteration 141 / 200) loss: 0.050746\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.024340\n",
      "(Epoch 16 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.036440\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.019785\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.021845\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.023561\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302724\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.333000; val_acc: 0.288889\n",
      "(Iteration 11 / 200) loss: 1.808225\n",
      "(Epoch 2 / 20) train acc: 0.552000; val_acc: 0.555556\n",
      "(Iteration 21 / 200) loss: 1.044962\n",
      "(Epoch 3 / 20) train acc: 0.671000; val_acc: 0.641667\n",
      "(Iteration 31 / 200) loss: 0.718335\n",
      "(Epoch 4 / 20) train acc: 0.848000; val_acc: 0.811111\n",
      "(Iteration 41 / 200) loss: 0.443239\n",
      "(Epoch 5 / 20) train acc: 0.904000; val_acc: 0.875000\n",
      "(Iteration 51 / 200) loss: 0.332167\n",
      "(Epoch 6 / 20) train acc: 0.927000; val_acc: 0.902778\n",
      "(Iteration 61 / 200) loss: 0.194454\n",
      "(Epoch 7 / 20) train acc: 0.923000; val_acc: 0.927778\n",
      "(Iteration 71 / 200) loss: 0.106790\n",
      "(Epoch 8 / 20) train acc: 0.957000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.149475\n",
      "(Epoch 9 / 20) train acc: 0.958000; val_acc: 0.933333\n",
      "(Iteration 91 / 200) loss: 0.331656\n",
      "(Epoch 10 / 20) train acc: 0.957000; val_acc: 0.930556\n",
      "(Iteration 101 / 200) loss: 0.142640\n",
      "(Epoch 11 / 20) train acc: 0.959000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.158261\n",
      "(Epoch 12 / 20) train acc: 0.972000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.058235\n",
      "(Epoch 13 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.073926\n",
      "(Epoch 14 / 20) train acc: 0.960000; val_acc: 0.944444\n",
      "(Iteration 141 / 200) loss: 0.188738\n",
      "(Epoch 15 / 20) train acc: 0.982000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.045888\n",
      "(Epoch 16 / 20) train acc: 0.990000; val_acc: 0.950000\n",
      "(Iteration 161 / 200) loss: 0.056001\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.018113\n",
      "(Epoch 18 / 20) train acc: 0.987000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.029577\n",
      "(Epoch 19 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.023699\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.206000; val_acc: 0.183333\n",
      "(Iteration 11 / 200) loss: 1.840667\n",
      "(Epoch 2 / 20) train acc: 0.561000; val_acc: 0.533333\n",
      "(Iteration 21 / 200) loss: 1.156303\n",
      "(Epoch 3 / 20) train acc: 0.667000; val_acc: 0.675000\n",
      "(Iteration 31 / 200) loss: 0.772336\n",
      "(Epoch 4 / 20) train acc: 0.775000; val_acc: 0.752778\n",
      "(Iteration 41 / 200) loss: 0.500693\n",
      "(Epoch 5 / 20) train acc: 0.882000; val_acc: 0.880556\n",
      "(Iteration 51 / 200) loss: 0.421985\n",
      "(Epoch 6 / 20) train acc: 0.912000; val_acc: 0.894444\n",
      "(Iteration 61 / 200) loss: 0.154663\n",
      "(Epoch 7 / 20) train acc: 0.923000; val_acc: 0.908333\n",
      "(Iteration 71 / 200) loss: 0.399108\n",
      "(Epoch 8 / 20) train acc: 0.948000; val_acc: 0.911111\n",
      "(Iteration 81 / 200) loss: 0.190858\n",
      "(Epoch 9 / 20) train acc: 0.957000; val_acc: 0.936111\n",
      "(Iteration 91 / 200) loss: 0.084046\n",
      "(Epoch 10 / 20) train acc: 0.968000; val_acc: 0.941667\n",
      "(Iteration 101 / 200) loss: 0.109850\n",
      "(Epoch 11 / 20) train acc: 0.971000; val_acc: 0.933333\n",
      "(Iteration 111 / 200) loss: 0.163033\n",
      "(Epoch 12 / 20) train acc: 0.940000; val_acc: 0.905556\n",
      "(Iteration 121 / 200) loss: 0.099887\n",
      "(Epoch 13 / 20) train acc: 0.982000; val_acc: 0.916667\n",
      "(Iteration 131 / 200) loss: 0.025566\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.958333\n",
      "(Iteration 141 / 200) loss: 0.022851\n",
      "(Epoch 15 / 20) train acc: 0.972000; val_acc: 0.925000\n",
      "(Iteration 151 / 200) loss: 0.110238\n",
      "(Epoch 16 / 20) train acc: 0.971000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 0.077285\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.078853\n",
      "(Epoch 18 / 20) train acc: 0.970000; val_acc: 0.919444\n",
      "(Iteration 181 / 200) loss: 0.105707\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.033996\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.223000; val_acc: 0.177778\n",
      "(Iteration 11 / 200) loss: 2.056704\n",
      "(Epoch 2 / 20) train acc: 0.279000; val_acc: 0.227778\n",
      "(Iteration 21 / 200) loss: 1.984551\n",
      "(Epoch 3 / 20) train acc: 0.225000; val_acc: 0.222222\n",
      "(Iteration 31 / 200) loss: 1.666299\n",
      "(Epoch 4 / 20) train acc: 0.221000; val_acc: 0.227778\n",
      "(Iteration 41 / 200) loss: 1.703216\n",
      "(Epoch 5 / 20) train acc: 0.299000; val_acc: 0.294444\n",
      "(Iteration 51 / 200) loss: 1.579137\n",
      "(Epoch 6 / 20) train acc: 0.486000; val_acc: 0.491667\n",
      "(Iteration 61 / 200) loss: 1.043807\n",
      "(Epoch 7 / 20) train acc: 0.736000; val_acc: 0.727778\n",
      "(Iteration 71 / 200) loss: 0.724426\n",
      "(Epoch 8 / 20) train acc: 0.801000; val_acc: 0.788889\n",
      "(Iteration 81 / 200) loss: 0.485089\n",
      "(Epoch 9 / 20) train acc: 0.861000; val_acc: 0.838889\n",
      "(Iteration 91 / 200) loss: 0.523141\n",
      "(Epoch 10 / 20) train acc: 0.883000; val_acc: 0.861111\n",
      "(Iteration 101 / 200) loss: 0.463138\n",
      "(Epoch 11 / 20) train acc: 0.902000; val_acc: 0.875000\n",
      "(Iteration 111 / 200) loss: 0.307061\n",
      "(Epoch 12 / 20) train acc: 0.915000; val_acc: 0.905556\n",
      "(Iteration 121 / 200) loss: 0.243768\n",
      "(Epoch 13 / 20) train acc: 0.937000; val_acc: 0.894444\n",
      "(Iteration 131 / 200) loss: 0.134173\n",
      "(Epoch 14 / 20) train acc: 0.949000; val_acc: 0.919444\n",
      "(Iteration 141 / 200) loss: 0.208890\n",
      "(Epoch 15 / 20) train acc: 0.961000; val_acc: 0.927778\n",
      "(Iteration 151 / 200) loss: 0.171591\n",
      "(Epoch 16 / 20) train acc: 0.964000; val_acc: 0.941667\n",
      "(Iteration 161 / 200) loss: 0.105736\n",
      "(Epoch 17 / 20) train acc: 0.965000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.076476\n",
      "(Epoch 18 / 20) train acc: 0.969000; val_acc: 0.950000\n",
      "(Iteration 181 / 200) loss: 0.080017\n",
      "(Epoch 19 / 20) train acc: 0.960000; val_acc: 0.916667\n",
      "(Iteration 191 / 200) loss: 0.087184\n",
      "(Epoch 20 / 20) train acc: 0.977000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.188000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 2.161701\n",
      "(Epoch 2 / 20) train acc: 0.226000; val_acc: 0.180556\n",
      "(Iteration 21 / 200) loss: 1.909019\n",
      "(Epoch 3 / 20) train acc: 0.212000; val_acc: 0.183333\n",
      "(Iteration 31 / 200) loss: 1.997033\n",
      "(Epoch 4 / 20) train acc: 0.219000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 1.985906\n",
      "(Epoch 5 / 20) train acc: 0.299000; val_acc: 0.316667\n",
      "(Iteration 51 / 200) loss: 1.659759\n",
      "(Epoch 6 / 20) train acc: 0.356000; val_acc: 0.363889\n",
      "(Iteration 61 / 200) loss: 1.626979\n",
      "(Epoch 7 / 20) train acc: 0.380000; val_acc: 0.402778\n",
      "(Iteration 71 / 200) loss: 1.497945\n",
      "(Epoch 8 / 20) train acc: 0.426000; val_acc: 0.483333\n",
      "(Iteration 81 / 200) loss: 1.584876\n",
      "(Epoch 9 / 20) train acc: 0.591000; val_acc: 0.561111\n",
      "(Iteration 91 / 200) loss: 1.126635\n",
      "(Epoch 10 / 20) train acc: 0.674000; val_acc: 0.666667\n",
      "(Iteration 101 / 200) loss: 0.859693\n",
      "(Epoch 11 / 20) train acc: 0.774000; val_acc: 0.711111\n",
      "(Iteration 111 / 200) loss: 0.874126\n",
      "(Epoch 12 / 20) train acc: 0.775000; val_acc: 0.758333\n",
      "(Iteration 121 / 200) loss: 0.521580\n",
      "(Epoch 13 / 20) train acc: 0.852000; val_acc: 0.783333\n",
      "(Iteration 131 / 200) loss: 0.391123\n",
      "(Epoch 14 / 20) train acc: 0.865000; val_acc: 0.822222\n",
      "(Iteration 141 / 200) loss: 0.475542\n",
      "(Epoch 15 / 20) train acc: 0.858000; val_acc: 0.841667\n",
      "(Iteration 151 / 200) loss: 0.318372\n",
      "(Epoch 16 / 20) train acc: 0.871000; val_acc: 0.841667\n",
      "(Iteration 161 / 200) loss: 0.431387\n",
      "(Epoch 17 / 20) train acc: 0.960000; val_acc: 0.919444\n",
      "(Iteration 171 / 200) loss: 0.088407\n",
      "(Epoch 18 / 20) train acc: 0.943000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 0.164353\n",
      "(Epoch 19 / 20) train acc: 0.972000; val_acc: 0.941667\n",
      "(Iteration 191 / 200) loss: 0.114916\n",
      "(Epoch 20 / 20) train acc: 0.967000; val_acc: 0.938889\n",
      "(Iteration 1 / 200) loss: 33725.248760\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.529000; val_acc: 0.472222\n",
      "(Iteration 11 / 200) loss: 2868.221552\n",
      "(Epoch 2 / 20) train acc: 0.787000; val_acc: 0.813889\n",
      "(Iteration 21 / 200) loss: 466.179963\n",
      "(Epoch 3 / 20) train acc: 0.865000; val_acc: 0.841667\n",
      "(Iteration 31 / 200) loss: 739.793459\n",
      "(Epoch 4 / 20) train acc: 0.929000; val_acc: 0.883333\n",
      "(Iteration 41 / 200) loss: 143.294249\n",
      "(Epoch 5 / 20) train acc: 0.923000; val_acc: 0.902778\n",
      "(Iteration 51 / 200) loss: 175.493012\n",
      "(Epoch 6 / 20) train acc: 0.965000; val_acc: 0.902778\n",
      "(Iteration 61 / 200) loss: 201.807948\n",
      "(Epoch 7 / 20) train acc: 0.969000; val_acc: 0.911111\n",
      "(Iteration 71 / 200) loss: 71.484638\n",
      "(Epoch 8 / 20) train acc: 0.968000; val_acc: 0.913889\n",
      "(Iteration 81 / 200) loss: 189.701522\n",
      "(Epoch 9 / 20) train acc: 0.983000; val_acc: 0.933333\n",
      "(Iteration 91 / 200) loss: 58.640911\n",
      "(Epoch 10 / 20) train acc: 0.982000; val_acc: 0.922222\n",
      "(Iteration 101 / 200) loss: 37.061442\n",
      "(Epoch 11 / 20) train acc: 0.996000; val_acc: 0.927778\n",
      "(Iteration 111 / 200) loss: 2.675730\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.925000\n",
      "(Iteration 121 / 200) loss: 1.171216\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.925000\n",
      "(Iteration 131 / 200) loss: 1.168034\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.938889\n",
      "(Iteration 141 / 200) loss: 9.882041\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.927778\n",
      "(Iteration 151 / 200) loss: 5.485181\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 1.161357\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.916667\n",
      "(Iteration 171 / 200) loss: 1.159769\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.922222\n",
      "(Iteration 181 / 200) loss: 4.881661\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.922222\n",
      "(Iteration 191 / 200) loss: 6.420576\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.930556\n",
      "(Iteration 1 / 200) loss: 4.068883\n",
      "(Epoch 0 / 20) train acc: 0.178000; val_acc: 0.191667\n",
      "(Epoch 1 / 20) train acc: 0.812000; val_acc: 0.791667\n",
      "(Iteration 11 / 200) loss: 0.562431\n",
      "(Epoch 2 / 20) train acc: 0.937000; val_acc: 0.886111\n",
      "(Iteration 21 / 200) loss: 0.253478\n",
      "(Epoch 3 / 20) train acc: 0.920000; val_acc: 0.891667\n",
      "(Iteration 31 / 200) loss: 0.157974\n",
      "(Epoch 4 / 20) train acc: 0.951000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.187274\n",
      "(Epoch 5 / 20) train acc: 0.972000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.055874\n",
      "(Epoch 6 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 61 / 200) loss: 0.217960\n",
      "(Epoch 7 / 20) train acc: 0.965000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.076742\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 81 / 200) loss: 0.032249\n",
      "(Epoch 9 / 20) train acc: 0.962000; val_acc: 0.930556\n",
      "(Iteration 91 / 200) loss: 0.111584\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.025590\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.030283\n",
      "(Epoch 12 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.015631\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.102785\n",
      "(Epoch 14 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.033935\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.027250\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.027631\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.024534\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.014620\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.018698\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.302678\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 2.070036\n",
      "(Epoch 2 / 20) train acc: 0.570000; val_acc: 0.583333\n",
      "(Iteration 21 / 200) loss: 1.075170\n",
      "(Epoch 3 / 20) train acc: 0.673000; val_acc: 0.661111\n",
      "(Iteration 31 / 200) loss: 0.852631\n",
      "(Epoch 4 / 20) train acc: 0.719000; val_acc: 0.727778\n",
      "(Iteration 41 / 200) loss: 0.738904\n",
      "(Epoch 5 / 20) train acc: 0.862000; val_acc: 0.822222\n",
      "(Iteration 51 / 200) loss: 0.437628\n",
      "(Epoch 6 / 20) train acc: 0.934000; val_acc: 0.916667\n",
      "(Iteration 61 / 200) loss: 0.232703\n",
      "(Epoch 7 / 20) train acc: 0.961000; val_acc: 0.938889\n",
      "(Iteration 71 / 200) loss: 0.089379\n",
      "(Epoch 8 / 20) train acc: 0.952000; val_acc: 0.933333\n",
      "(Iteration 81 / 200) loss: 0.137026\n",
      "(Epoch 9 / 20) train acc: 0.982000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.067881\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.105177\n",
      "(Epoch 11 / 20) train acc: 0.965000; val_acc: 0.952778\n",
      "(Iteration 111 / 200) loss: 0.069898\n",
      "(Epoch 12 / 20) train acc: 0.916000; val_acc: 0.913889\n",
      "(Iteration 121 / 200) loss: 0.297377\n",
      "(Epoch 13 / 20) train acc: 0.970000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.065028\n",
      "(Epoch 14 / 20) train acc: 0.986000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.045038\n",
      "(Epoch 15 / 20) train acc: 0.979000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.060763\n",
      "(Epoch 16 / 20) train acc: 0.977000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.159731\n",
      "(Epoch 17 / 20) train acc: 0.980000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.047656\n",
      "(Epoch 18 / 20) train acc: 0.984000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.054492\n",
      "(Epoch 19 / 20) train acc: 0.985000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.011184\n",
      "(Epoch 20 / 20) train acc: 0.977000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.365000; val_acc: 0.352778\n",
      "(Iteration 11 / 200) loss: 1.625010\n",
      "(Epoch 2 / 20) train acc: 0.435000; val_acc: 0.397222\n",
      "(Iteration 21 / 200) loss: 1.376932\n",
      "(Epoch 3 / 20) train acc: 0.617000; val_acc: 0.588889\n",
      "(Iteration 31 / 200) loss: 0.955403\n",
      "(Epoch 4 / 20) train acc: 0.617000; val_acc: 0.569444\n",
      "(Iteration 41 / 200) loss: 1.033224\n",
      "(Epoch 5 / 20) train acc: 0.808000; val_acc: 0.763889\n",
      "(Iteration 51 / 200) loss: 0.509521\n",
      "(Epoch 6 / 20) train acc: 0.903000; val_acc: 0.852778\n",
      "(Iteration 61 / 200) loss: 0.332370\n",
      "(Epoch 7 / 20) train acc: 0.920000; val_acc: 0.902778\n",
      "(Iteration 71 / 200) loss: 0.107453\n",
      "(Epoch 8 / 20) train acc: 0.962000; val_acc: 0.922222\n",
      "(Iteration 81 / 200) loss: 0.180740\n",
      "(Epoch 9 / 20) train acc: 0.952000; val_acc: 0.919444\n",
      "(Iteration 91 / 200) loss: 0.121919\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.122304\n",
      "(Epoch 11 / 20) train acc: 0.966000; val_acc: 0.941667\n",
      "(Iteration 111 / 200) loss: 0.037680\n",
      "(Epoch 12 / 20) train acc: 0.951000; val_acc: 0.930556\n",
      "(Iteration 121 / 200) loss: 0.287138\n",
      "(Epoch 13 / 20) train acc: 0.968000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.023929\n",
      "(Epoch 14 / 20) train acc: 0.983000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.024436\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.074477\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.018315\n",
      "(Epoch 17 / 20) train acc: 0.978000; val_acc: 0.927778\n",
      "(Iteration 171 / 200) loss: 0.062333\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.048654\n",
      "(Epoch 19 / 20) train acc: 0.981000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 0.014318\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.175000; val_acc: 0.186111\n",
      "(Iteration 11 / 200) loss: 2.109783\n",
      "(Epoch 2 / 20) train acc: 0.283000; val_acc: 0.241667\n",
      "(Iteration 21 / 200) loss: 1.969624\n",
      "(Epoch 3 / 20) train acc: 0.266000; val_acc: 0.258333\n",
      "(Iteration 31 / 200) loss: 1.859646\n",
      "(Epoch 4 / 20) train acc: 0.347000; val_acc: 0.300000\n",
      "(Iteration 41 / 200) loss: 1.674891\n",
      "(Epoch 5 / 20) train acc: 0.384000; val_acc: 0.313889\n",
      "(Iteration 51 / 200) loss: 1.704135\n",
      "(Epoch 6 / 20) train acc: 0.408000; val_acc: 0.397222\n",
      "(Iteration 61 / 200) loss: 1.379934\n",
      "(Epoch 7 / 20) train acc: 0.531000; val_acc: 0.488889\n",
      "(Iteration 71 / 200) loss: 1.352793\n",
      "(Epoch 8 / 20) train acc: 0.574000; val_acc: 0.547222\n",
      "(Iteration 81 / 200) loss: 1.022600\n",
      "(Epoch 9 / 20) train acc: 0.661000; val_acc: 0.625000\n",
      "(Iteration 91 / 200) loss: 0.919221\n",
      "(Epoch 10 / 20) train acc: 0.817000; val_acc: 0.761111\n",
      "(Iteration 101 / 200) loss: 0.753978\n",
      "(Epoch 11 / 20) train acc: 0.828000; val_acc: 0.811111\n",
      "(Iteration 111 / 200) loss: 0.589306\n",
      "(Epoch 12 / 20) train acc: 0.901000; val_acc: 0.861111\n",
      "(Iteration 121 / 200) loss: 0.349730\n",
      "(Epoch 13 / 20) train acc: 0.907000; val_acc: 0.897222\n",
      "(Iteration 131 / 200) loss: 0.284395\n",
      "(Epoch 14 / 20) train acc: 0.962000; val_acc: 0.897222\n",
      "(Iteration 141 / 200) loss: 0.190705\n",
      "(Epoch 15 / 20) train acc: 0.956000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 0.272599\n",
      "(Epoch 16 / 20) train acc: 0.964000; val_acc: 0.927778\n",
      "(Iteration 161 / 200) loss: 0.056238\n",
      "(Epoch 17 / 20) train acc: 0.955000; val_acc: 0.944444\n",
      "(Iteration 171 / 200) loss: 0.110269\n",
      "(Epoch 18 / 20) train acc: 0.977000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.086490\n",
      "(Epoch 19 / 20) train acc: 0.976000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.061948\n",
      "(Epoch 20 / 20) train acc: 0.988000; val_acc: 0.950000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.161000; val_acc: 0.183333\n",
      "(Iteration 11 / 200) loss: 2.178789\n",
      "(Epoch 2 / 20) train acc: 0.238000; val_acc: 0.172222\n",
      "(Iteration 21 / 200) loss: 2.011108\n",
      "(Epoch 3 / 20) train acc: 0.230000; val_acc: 0.258333\n",
      "(Iteration 31 / 200) loss: 1.931303\n",
      "(Epoch 4 / 20) train acc: 0.398000; val_acc: 0.408333\n",
      "(Iteration 41 / 200) loss: 1.629243\n",
      "(Epoch 5 / 20) train acc: 0.530000; val_acc: 0.502778\n",
      "(Iteration 51 / 200) loss: 1.283070\n",
      "(Epoch 6 / 20) train acc: 0.539000; val_acc: 0.494444\n",
      "(Iteration 61 / 200) loss: 1.227791\n",
      "(Epoch 7 / 20) train acc: 0.756000; val_acc: 0.708333\n",
      "(Iteration 71 / 200) loss: 0.797642\n",
      "(Epoch 8 / 20) train acc: 0.736000; val_acc: 0.686111\n",
      "(Iteration 81 / 200) loss: 0.914455\n",
      "(Epoch 9 / 20) train acc: 0.828000; val_acc: 0.847222\n",
      "(Iteration 91 / 200) loss: 0.630068\n",
      "(Epoch 10 / 20) train acc: 0.813000; val_acc: 0.791667\n",
      "(Iteration 101 / 200) loss: 0.505953\n",
      "(Epoch 11 / 20) train acc: 0.878000; val_acc: 0.855556\n",
      "(Iteration 111 / 200) loss: 0.407882\n",
      "(Epoch 12 / 20) train acc: 0.898000; val_acc: 0.858333\n",
      "(Iteration 121 / 200) loss: 0.256962\n",
      "(Epoch 13 / 20) train acc: 0.949000; val_acc: 0.861111\n",
      "(Iteration 131 / 200) loss: 0.139952\n",
      "(Epoch 14 / 20) train acc: 0.964000; val_acc: 0.919444\n",
      "(Iteration 141 / 200) loss: 0.110927\n",
      "(Epoch 15 / 20) train acc: 0.973000; val_acc: 0.911111\n",
      "(Iteration 151 / 200) loss: 0.041512\n",
      "(Epoch 16 / 20) train acc: 0.974000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 0.033443\n",
      "(Epoch 17 / 20) train acc: 0.950000; val_acc: 0.872222\n",
      "(Iteration 171 / 200) loss: 0.121733\n",
      "(Epoch 18 / 20) train acc: 0.968000; val_acc: 0.938889\n",
      "(Iteration 181 / 200) loss: 0.162025\n",
      "(Epoch 19 / 20) train acc: 0.979000; val_acc: 0.933333\n",
      "(Iteration 191 / 200) loss: 0.091019\n",
      "(Epoch 20 / 20) train acc: 0.977000; val_acc: 0.941667\n",
      "(Iteration 1 / 200) loss: 28847.799357\n",
      "(Epoch 0 / 20) train acc: 0.066000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.595000; val_acc: 0.577778\n",
      "(Iteration 11 / 200) loss: 1828.377193\n",
      "(Epoch 2 / 20) train acc: 0.835000; val_acc: 0.775000\n",
      "(Iteration 21 / 200) loss: 808.722920\n",
      "(Epoch 3 / 20) train acc: 0.887000; val_acc: 0.836111\n",
      "(Iteration 31 / 200) loss: 930.172088\n",
      "(Epoch 4 / 20) train acc: 0.905000; val_acc: 0.886111\n",
      "(Iteration 41 / 200) loss: 372.148204\n",
      "(Epoch 5 / 20) train acc: 0.957000; val_acc: 0.905556\n",
      "(Iteration 51 / 200) loss: 66.147294\n",
      "(Epoch 6 / 20) train acc: 0.929000; val_acc: 0.900000\n",
      "(Iteration 61 / 200) loss: 163.256524\n",
      "(Epoch 7 / 20) train acc: 0.959000; val_acc: 0.922222\n",
      "(Iteration 71 / 200) loss: 19.281387\n",
      "(Epoch 8 / 20) train acc: 0.966000; val_acc: 0.913889\n",
      "(Iteration 81 / 200) loss: 65.386627\n",
      "(Epoch 9 / 20) train acc: 0.984000; val_acc: 0.908333\n",
      "(Iteration 91 / 200) loss: 28.003742\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.925000\n",
      "(Iteration 101 / 200) loss: 21.083809\n",
      "(Epoch 11 / 20) train acc: 0.979000; val_acc: 0.908333\n",
      "(Iteration 111 / 200) loss: 84.888701\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 61.213799\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.913889\n",
      "(Iteration 131 / 200) loss: 1.996769\n",
      "(Epoch 14 / 20) train acc: 0.983000; val_acc: 0.919444\n",
      "(Iteration 141 / 200) loss: 6.198393\n",
      "(Epoch 15 / 20) train acc: 0.991000; val_acc: 0.938889\n",
      "(Iteration 151 / 200) loss: 47.820228\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 81.484730\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.933333\n",
      "(Iteration 171 / 200) loss: 48.267179\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 0.111586\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.927778\n",
      "(Iteration 191 / 200) loss: 1.082114\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.947222\n",
      "(Iteration 1 / 200) loss: 3.733635\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.702000; val_acc: 0.677778\n",
      "(Iteration 11 / 200) loss: 0.904220\n",
      "(Epoch 2 / 20) train acc: 0.885000; val_acc: 0.872222\n",
      "(Iteration 21 / 200) loss: 0.404108\n",
      "(Epoch 3 / 20) train acc: 0.934000; val_acc: 0.908333\n",
      "(Iteration 31 / 200) loss: 0.166295\n",
      "(Epoch 4 / 20) train acc: 0.959000; val_acc: 0.958333\n",
      "(Iteration 41 / 200) loss: 0.101482\n",
      "(Epoch 5 / 20) train acc: 0.973000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.229812\n",
      "(Epoch 6 / 20) train acc: 0.980000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.084644\n",
      "(Epoch 7 / 20) train acc: 0.989000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.047995\n",
      "(Epoch 8 / 20) train acc: 0.977000; val_acc: 0.938889\n",
      "(Iteration 81 / 200) loss: 0.012185\n",
      "(Epoch 9 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.012201\n",
      "(Epoch 10 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.006644\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 111 / 200) loss: 0.044520\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.950000\n",
      "(Iteration 121 / 200) loss: 0.020561\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.026927\n",
      "(Epoch 14 / 20) train acc: 0.981000; val_acc: 0.958333\n",
      "(Iteration 141 / 200) loss: 0.031507\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.004927\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.991667\n",
      "(Iteration 161 / 200) loss: 0.004684\n",
      "(Epoch 17 / 20) train acc: 0.984000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.032316\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 181 / 200) loss: 0.003954\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.021012\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302594\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.216000; val_acc: 0.211111\n",
      "(Iteration 11 / 200) loss: 2.162536\n",
      "(Epoch 2 / 20) train acc: 0.480000; val_acc: 0.472222\n",
      "(Iteration 21 / 200) loss: 1.525226\n",
      "(Epoch 3 / 20) train acc: 0.659000; val_acc: 0.600000\n",
      "(Iteration 31 / 200) loss: 0.900380\n",
      "(Epoch 4 / 20) train acc: 0.801000; val_acc: 0.788889\n",
      "(Iteration 41 / 200) loss: 0.509405\n",
      "(Epoch 5 / 20) train acc: 0.809000; val_acc: 0.761111\n",
      "(Iteration 51 / 200) loss: 0.731119\n",
      "(Epoch 6 / 20) train acc: 0.848000; val_acc: 0.822222\n",
      "(Iteration 61 / 200) loss: 0.382761\n",
      "(Epoch 7 / 20) train acc: 0.910000; val_acc: 0.847222\n",
      "(Iteration 71 / 200) loss: 0.163779\n",
      "(Epoch 8 / 20) train acc: 0.914000; val_acc: 0.880556\n",
      "(Iteration 81 / 200) loss: 0.084100\n",
      "(Epoch 9 / 20) train acc: 0.933000; val_acc: 0.888889\n",
      "(Iteration 91 / 200) loss: 0.208632\n",
      "(Epoch 10 / 20) train acc: 0.956000; val_acc: 0.911111\n",
      "(Iteration 101 / 200) loss: 0.087755\n",
      "(Epoch 11 / 20) train acc: 0.956000; val_acc: 0.916667\n",
      "(Iteration 111 / 200) loss: 0.212276\n",
      "(Epoch 12 / 20) train acc: 0.958000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 0.227336\n",
      "(Epoch 13 / 20) train acc: 0.965000; val_acc: 0.911111\n",
      "(Iteration 131 / 200) loss: 0.124370\n",
      "(Epoch 14 / 20) train acc: 0.934000; val_acc: 0.922222\n",
      "(Iteration 141 / 200) loss: 0.357805\n",
      "(Epoch 15 / 20) train acc: 0.960000; val_acc: 0.927778\n",
      "(Iteration 151 / 200) loss: 0.175661\n",
      "(Epoch 16 / 20) train acc: 0.970000; val_acc: 0.941667\n",
      "(Iteration 161 / 200) loss: 0.073454\n",
      "(Epoch 17 / 20) train acc: 0.979000; val_acc: 0.941667\n",
      "(Iteration 171 / 200) loss: 0.106128\n",
      "(Epoch 18 / 20) train acc: 0.982000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.020546\n",
      "(Epoch 19 / 20) train acc: 0.985000; val_acc: 0.933333\n",
      "(Iteration 191 / 200) loss: 0.062584\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.944444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.174000; val_acc: 0.202778\n",
      "(Iteration 11 / 200) loss: 1.957236\n",
      "(Epoch 2 / 20) train acc: 0.261000; val_acc: 0.275000\n",
      "(Iteration 21 / 200) loss: 1.674327\n",
      "(Epoch 3 / 20) train acc: 0.336000; val_acc: 0.338889\n",
      "(Iteration 31 / 200) loss: 1.643926\n",
      "(Epoch 4 / 20) train acc: 0.381000; val_acc: 0.388889\n",
      "(Iteration 41 / 200) loss: 1.439708\n",
      "(Epoch 5 / 20) train acc: 0.586000; val_acc: 0.605556\n",
      "(Iteration 51 / 200) loss: 0.894056\n",
      "(Epoch 6 / 20) train acc: 0.694000; val_acc: 0.691667\n",
      "(Iteration 61 / 200) loss: 0.691716\n",
      "(Epoch 7 / 20) train acc: 0.704000; val_acc: 0.683333\n",
      "(Iteration 71 / 200) loss: 0.754674\n",
      "(Epoch 8 / 20) train acc: 0.716000; val_acc: 0.722222\n",
      "(Iteration 81 / 200) loss: 0.591650\n",
      "(Epoch 9 / 20) train acc: 0.837000; val_acc: 0.833333\n",
      "(Iteration 91 / 200) loss: 0.447016\n",
      "(Epoch 10 / 20) train acc: 0.897000; val_acc: 0.877778\n",
      "(Iteration 101 / 200) loss: 0.325158\n",
      "(Epoch 11 / 20) train acc: 0.791000; val_acc: 0.819444\n",
      "(Iteration 111 / 200) loss: 0.556044\n",
      "(Epoch 12 / 20) train acc: 0.934000; val_acc: 0.886111\n",
      "(Iteration 121 / 200) loss: 0.225662\n",
      "(Epoch 13 / 20) train acc: 0.913000; val_acc: 0.886111\n",
      "(Iteration 131 / 200) loss: 0.183163\n",
      "(Epoch 14 / 20) train acc: 0.956000; val_acc: 0.908333\n",
      "(Iteration 141 / 200) loss: 0.283712\n",
      "(Epoch 15 / 20) train acc: 0.954000; val_acc: 0.933333\n",
      "(Iteration 151 / 200) loss: 0.214325\n",
      "(Epoch 16 / 20) train acc: 0.962000; val_acc: 0.944444\n",
      "(Iteration 161 / 200) loss: 0.060639\n",
      "(Epoch 17 / 20) train acc: 0.968000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 0.065340\n",
      "(Epoch 18 / 20) train acc: 0.977000; val_acc: 0.944444\n",
      "(Iteration 181 / 200) loss: 0.088240\n",
      "(Epoch 19 / 20) train acc: 0.966000; val_acc: 0.947222\n",
      "(Iteration 191 / 200) loss: 0.070823\n",
      "(Epoch 20 / 20) train acc: 0.971000; val_acc: 0.933333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.082000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302350\n",
      "(Epoch 2 / 20) train acc: 0.180000; val_acc: 0.205556\n",
      "(Iteration 21 / 200) loss: 2.082607\n",
      "(Epoch 3 / 20) train acc: 0.221000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 1.883030\n",
      "(Epoch 4 / 20) train acc: 0.253000; val_acc: 0.294444\n",
      "(Iteration 41 / 200) loss: 1.850317\n",
      "(Epoch 5 / 20) train acc: 0.298000; val_acc: 0.258333\n",
      "(Iteration 51 / 200) loss: 1.656489\n",
      "(Epoch 6 / 20) train acc: 0.366000; val_acc: 0.344444\n",
      "(Iteration 61 / 200) loss: 1.692292\n",
      "(Epoch 7 / 20) train acc: 0.424000; val_acc: 0.375000\n",
      "(Iteration 71 / 200) loss: 1.642822\n",
      "(Epoch 8 / 20) train acc: 0.575000; val_acc: 0.547222\n",
      "(Iteration 81 / 200) loss: 1.232643\n",
      "(Epoch 9 / 20) train acc: 0.618000; val_acc: 0.569444\n",
      "(Iteration 91 / 200) loss: 1.068057\n",
      "(Epoch 10 / 20) train acc: 0.656000; val_acc: 0.647222\n",
      "(Iteration 101 / 200) loss: 0.772780\n",
      "(Epoch 11 / 20) train acc: 0.856000; val_acc: 0.819444\n",
      "(Iteration 111 / 200) loss: 0.492456\n",
      "(Epoch 12 / 20) train acc: 0.874000; val_acc: 0.855556\n",
      "(Iteration 121 / 200) loss: 0.305797\n",
      "(Epoch 13 / 20) train acc: 0.898000; val_acc: 0.880556\n",
      "(Iteration 131 / 200) loss: 0.376509\n",
      "(Epoch 14 / 20) train acc: 0.933000; val_acc: 0.883333\n",
      "(Iteration 141 / 200) loss: 0.303425\n",
      "(Epoch 15 / 20) train acc: 0.939000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 0.167814\n",
      "(Epoch 16 / 20) train acc: 0.963000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 0.063905\n",
      "(Epoch 17 / 20) train acc: 0.943000; val_acc: 0.911111\n",
      "(Iteration 171 / 200) loss: 0.215330\n",
      "(Epoch 18 / 20) train acc: 0.973000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.080156\n",
      "(Epoch 19 / 20) train acc: 0.976000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.074490\n",
      "(Epoch 20 / 20) train acc: 0.971000; val_acc: 0.947222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.255692\n",
      "(Epoch 2 / 20) train acc: 0.210000; val_acc: 0.191667\n",
      "(Iteration 21 / 200) loss: 1.885557\n",
      "(Epoch 3 / 20) train acc: 0.282000; val_acc: 0.263889\n",
      "(Iteration 31 / 200) loss: 1.797572\n",
      "(Epoch 4 / 20) train acc: 0.579000; val_acc: 0.572222\n",
      "(Iteration 41 / 200) loss: 1.093373\n",
      "(Epoch 5 / 20) train acc: 0.699000; val_acc: 0.669444\n",
      "(Iteration 51 / 200) loss: 0.986754\n",
      "(Epoch 6 / 20) train acc: 0.774000; val_acc: 0.736111\n",
      "(Iteration 61 / 200) loss: 0.646677\n",
      "(Epoch 7 / 20) train acc: 0.815000; val_acc: 0.766667\n",
      "(Iteration 71 / 200) loss: 0.532583\n",
      "(Epoch 8 / 20) train acc: 0.829000; val_acc: 0.777778\n",
      "(Iteration 81 / 200) loss: 0.389885\n",
      "(Epoch 9 / 20) train acc: 0.826000; val_acc: 0.783333\n",
      "(Iteration 91 / 200) loss: 0.255427\n",
      "(Epoch 10 / 20) train acc: 0.862000; val_acc: 0.852778\n",
      "(Iteration 101 / 200) loss: 0.392182\n",
      "(Epoch 11 / 20) train acc: 0.893000; val_acc: 0.852778\n",
      "(Iteration 111 / 200) loss: 0.190692\n",
      "(Epoch 12 / 20) train acc: 0.965000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 0.072589\n",
      "(Epoch 13 / 20) train acc: 0.941000; val_acc: 0.930556\n",
      "(Iteration 131 / 200) loss: 0.070330\n",
      "(Epoch 14 / 20) train acc: 0.944000; val_acc: 0.913889\n",
      "(Iteration 141 / 200) loss: 0.207339\n",
      "(Epoch 15 / 20) train acc: 0.961000; val_acc: 0.900000\n",
      "(Iteration 151 / 200) loss: 0.184947\n",
      "(Epoch 16 / 20) train acc: 0.963000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 0.155078\n",
      "(Epoch 17 / 20) train acc: 0.980000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.042777\n",
      "(Epoch 18 / 20) train acc: 0.975000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.112339\n",
      "(Epoch 19 / 20) train acc: 0.974000; val_acc: 0.947222\n",
      "(Iteration 191 / 200) loss: 0.081574\n",
      "(Epoch 20 / 20) train acc: 0.982000; val_acc: 0.958333\n",
      "(Iteration 1 / 200) loss: 35497.409583\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.505000; val_acc: 0.475000\n",
      "(Iteration 11 / 200) loss: 3529.498635\n",
      "(Epoch 2 / 20) train acc: 0.714000; val_acc: 0.680556\n",
      "(Iteration 21 / 200) loss: 2290.010743\n",
      "(Epoch 3 / 20) train acc: 0.826000; val_acc: 0.755556\n",
      "(Iteration 31 / 200) loss: 557.103849\n",
      "(Epoch 4 / 20) train acc: 0.883000; val_acc: 0.808333\n",
      "(Iteration 41 / 200) loss: 502.294339\n",
      "(Epoch 5 / 20) train acc: 0.903000; val_acc: 0.869444\n",
      "(Iteration 51 / 200) loss: 412.057045\n",
      "(Epoch 6 / 20) train acc: 0.943000; val_acc: 0.858333\n",
      "(Iteration 61 / 200) loss: 165.967281\n",
      "(Epoch 7 / 20) train acc: 0.936000; val_acc: 0.863889\n",
      "(Iteration 71 / 200) loss: 78.136580\n",
      "(Epoch 8 / 20) train acc: 0.954000; val_acc: 0.894444\n",
      "(Iteration 81 / 200) loss: 12.882216\n",
      "(Epoch 9 / 20) train acc: 0.968000; val_acc: 0.886111\n",
      "(Iteration 91 / 200) loss: 49.752828\n",
      "(Epoch 10 / 20) train acc: 0.977000; val_acc: 0.891667\n",
      "(Iteration 101 / 200) loss: 33.052259\n",
      "(Epoch 11 / 20) train acc: 0.971000; val_acc: 0.877778\n",
      "(Iteration 111 / 200) loss: 7.264769\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.891667\n",
      "(Iteration 121 / 200) loss: 0.949428\n",
      "(Epoch 13 / 20) train acc: 0.991000; val_acc: 0.883333\n",
      "(Iteration 131 / 200) loss: 17.039445\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.894444\n",
      "(Iteration 141 / 200) loss: 113.494319\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.905556\n",
      "(Iteration 151 / 200) loss: 1.642502\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.894444\n",
      "(Iteration 161 / 200) loss: 25.459203\n",
      "(Epoch 17 / 20) train acc: 0.988000; val_acc: 0.886111\n",
      "(Iteration 171 / 200) loss: 4.107285\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.897222\n",
      "(Iteration 181 / 200) loss: 2.168147\n",
      "(Epoch 19 / 20) train acc: 0.977000; val_acc: 0.908333\n",
      "(Iteration 191 / 200) loss: 1.192631\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.911111\n",
      "(Iteration 1 / 200) loss: 3.258517\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.567000; val_acc: 0.577778\n",
      "(Iteration 11 / 200) loss: 1.152649\n",
      "(Epoch 2 / 20) train acc: 0.873000; val_acc: 0.861111\n",
      "(Iteration 21 / 200) loss: 0.396428\n",
      "(Epoch 3 / 20) train acc: 0.918000; val_acc: 0.897222\n",
      "(Iteration 31 / 200) loss: 0.485098\n",
      "(Epoch 4 / 20) train acc: 0.964000; val_acc: 0.961111\n",
      "(Iteration 41 / 200) loss: 0.163760\n",
      "(Epoch 5 / 20) train acc: 0.977000; val_acc: 0.961111\n",
      "(Iteration 51 / 200) loss: 0.086002\n",
      "(Epoch 6 / 20) train acc: 0.967000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.116064\n",
      "(Epoch 7 / 20) train acc: 0.982000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 0.060345\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 0.007210\n",
      "(Epoch 9 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.022337\n",
      "(Epoch 10 / 20) train acc: 0.999000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.004222\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.003952\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.032675\n",
      "(Epoch 13 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 131 / 200) loss: 0.010970\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.003430\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.983333\n",
      "(Iteration 151 / 200) loss: 0.070890\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.019989\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 171 / 200) loss: 0.005283\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 181 / 200) loss: 0.009911\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.994444\n",
      "(Iteration 191 / 200) loss: 0.049765\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302563\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.370000; val_acc: 0.327778\n",
      "(Iteration 11 / 200) loss: 1.822811\n",
      "(Epoch 2 / 20) train acc: 0.532000; val_acc: 0.500000\n",
      "(Iteration 21 / 200) loss: 1.115355\n",
      "(Epoch 3 / 20) train acc: 0.790000; val_acc: 0.750000\n",
      "(Iteration 31 / 200) loss: 0.595408\n",
      "(Epoch 4 / 20) train acc: 0.902000; val_acc: 0.866667\n",
      "(Iteration 41 / 200) loss: 0.443906\n",
      "(Epoch 5 / 20) train acc: 0.915000; val_acc: 0.883333\n",
      "(Iteration 51 / 200) loss: 0.257679\n",
      "(Epoch 6 / 20) train acc: 0.940000; val_acc: 0.933333\n",
      "(Iteration 61 / 200) loss: 0.214732\n",
      "(Epoch 7 / 20) train acc: 0.963000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.109186\n",
      "(Epoch 8 / 20) train acc: 0.950000; val_acc: 0.941667\n",
      "(Iteration 81 / 200) loss: 0.083382\n",
      "(Epoch 9 / 20) train acc: 0.953000; val_acc: 0.922222\n",
      "(Iteration 91 / 200) loss: 0.114749\n",
      "(Epoch 10 / 20) train acc: 0.968000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.088037\n",
      "(Epoch 11 / 20) train acc: 0.970000; val_acc: 0.950000\n",
      "(Iteration 111 / 200) loss: 0.013164\n",
      "(Epoch 12 / 20) train acc: 0.959000; val_acc: 0.938889\n",
      "(Iteration 121 / 200) loss: 0.190814\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.947222\n",
      "(Iteration 131 / 200) loss: 0.039563\n",
      "(Epoch 14 / 20) train acc: 0.978000; val_acc: 0.950000\n",
      "(Iteration 141 / 200) loss: 0.016766\n",
      "(Epoch 15 / 20) train acc: 0.951000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 0.257264\n",
      "(Epoch 16 / 20) train acc: 0.966000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 0.141178\n",
      "(Epoch 17 / 20) train acc: 0.975000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.051643\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.100765\n",
      "(Epoch 19 / 20) train acc: 0.978000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.023904\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 2.199631\n",
      "(Epoch 2 / 20) train acc: 0.308000; val_acc: 0.286111\n",
      "(Iteration 21 / 200) loss: 1.804214\n",
      "(Epoch 3 / 20) train acc: 0.343000; val_acc: 0.316667\n",
      "(Iteration 31 / 200) loss: 1.492473\n",
      "(Epoch 4 / 20) train acc: 0.449000; val_acc: 0.422222\n",
      "(Iteration 41 / 200) loss: 1.445729\n",
      "(Epoch 5 / 20) train acc: 0.549000; val_acc: 0.597222\n",
      "(Iteration 51 / 200) loss: 1.006948\n",
      "(Epoch 6 / 20) train acc: 0.636000; val_acc: 0.636111\n",
      "(Iteration 61 / 200) loss: 0.953977\n",
      "(Epoch 7 / 20) train acc: 0.714000; val_acc: 0.694444\n",
      "(Iteration 71 / 200) loss: 0.683783\n",
      "(Epoch 8 / 20) train acc: 0.730000; val_acc: 0.752778\n",
      "(Iteration 81 / 200) loss: 0.631461\n",
      "(Epoch 9 / 20) train acc: 0.823000; val_acc: 0.775000\n",
      "(Iteration 91 / 200) loss: 0.383608\n",
      "(Epoch 10 / 20) train acc: 0.829000; val_acc: 0.838889\n",
      "(Iteration 101 / 200) loss: 0.361464\n",
      "(Epoch 11 / 20) train acc: 0.849000; val_acc: 0.825000\n",
      "(Iteration 111 / 200) loss: 0.471511\n",
      "(Epoch 12 / 20) train acc: 0.894000; val_acc: 0.861111\n",
      "(Iteration 121 / 200) loss: 0.448993\n",
      "(Epoch 13 / 20) train acc: 0.910000; val_acc: 0.886111\n",
      "(Iteration 131 / 200) loss: 0.273420\n",
      "(Epoch 14 / 20) train acc: 0.891000; val_acc: 0.869444\n",
      "(Iteration 141 / 200) loss: 0.362465\n",
      "(Epoch 15 / 20) train acc: 0.952000; val_acc: 0.900000\n",
      "(Iteration 151 / 200) loss: 0.123908\n",
      "(Epoch 16 / 20) train acc: 0.944000; val_acc: 0.902778\n",
      "(Iteration 161 / 200) loss: 0.214239\n",
      "(Epoch 17 / 20) train acc: 0.963000; val_acc: 0.902778\n",
      "(Iteration 171 / 200) loss: 0.103356\n",
      "(Epoch 18 / 20) train acc: 0.874000; val_acc: 0.844444\n",
      "(Iteration 181 / 200) loss: 0.513776\n",
      "(Epoch 19 / 20) train acc: 0.961000; val_acc: 0.922222\n",
      "(Iteration 191 / 200) loss: 0.105053\n",
      "(Epoch 20 / 20) train acc: 0.966000; val_acc: 0.938889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.121000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 2.259497\n",
      "(Epoch 2 / 20) train acc: 0.232000; val_acc: 0.219444\n",
      "(Iteration 21 / 200) loss: 1.882571\n",
      "(Epoch 3 / 20) train acc: 0.265000; val_acc: 0.269444\n",
      "(Iteration 31 / 200) loss: 1.779929\n",
      "(Epoch 4 / 20) train acc: 0.265000; val_acc: 0.272222\n",
      "(Iteration 41 / 200) loss: 1.639218\n",
      "(Epoch 5 / 20) train acc: 0.467000; val_acc: 0.472222\n",
      "(Iteration 51 / 200) loss: 1.165957\n",
      "(Epoch 6 / 20) train acc: 0.601000; val_acc: 0.638889\n",
      "(Iteration 61 / 200) loss: 0.774992\n",
      "(Epoch 7 / 20) train acc: 0.752000; val_acc: 0.705556\n",
      "(Iteration 71 / 200) loss: 1.147763\n",
      "(Epoch 8 / 20) train acc: 0.808000; val_acc: 0.811111\n",
      "(Iteration 81 / 200) loss: 0.538989\n",
      "(Epoch 9 / 20) train acc: 0.913000; val_acc: 0.872222\n",
      "(Iteration 91 / 200) loss: 0.340951\n",
      "(Epoch 10 / 20) train acc: 0.948000; val_acc: 0.913889\n",
      "(Iteration 101 / 200) loss: 0.290495\n",
      "(Epoch 11 / 20) train acc: 0.929000; val_acc: 0.888889\n",
      "(Iteration 111 / 200) loss: 0.160393\n",
      "(Epoch 12 / 20) train acc: 0.950000; val_acc: 0.936111\n",
      "(Iteration 121 / 200) loss: 0.108132\n",
      "(Epoch 13 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 131 / 200) loss: 0.103238\n",
      "(Epoch 14 / 20) train acc: 0.970000; val_acc: 0.944444\n",
      "(Iteration 141 / 200) loss: 0.098137\n",
      "(Epoch 15 / 20) train acc: 0.961000; val_acc: 0.936111\n",
      "(Iteration 151 / 200) loss: 0.093853\n",
      "(Epoch 16 / 20) train acc: 0.958000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 0.057896\n",
      "(Epoch 17 / 20) train acc: 0.956000; val_acc: 0.925000\n",
      "(Iteration 171 / 200) loss: 0.107432\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.037286\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 191 / 200) loss: 0.067851\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.291867\n",
      "(Epoch 2 / 20) train acc: 0.237000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 2.065500\n",
      "(Epoch 3 / 20) train acc: 0.180000; val_acc: 0.177778\n",
      "(Iteration 31 / 200) loss: 2.041946\n",
      "(Epoch 4 / 20) train acc: 0.229000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 1.977127\n",
      "(Epoch 5 / 20) train acc: 0.288000; val_acc: 0.258333\n",
      "(Iteration 51 / 200) loss: 1.732024\n",
      "(Epoch 6 / 20) train acc: 0.331000; val_acc: 0.275000\n",
      "(Iteration 61 / 200) loss: 1.686019\n",
      "(Epoch 7 / 20) train acc: 0.366000; val_acc: 0.336111\n",
      "(Iteration 71 / 200) loss: 1.497602\n",
      "(Epoch 8 / 20) train acc: 0.368000; val_acc: 0.327778\n",
      "(Iteration 81 / 200) loss: 1.553355\n",
      "(Epoch 9 / 20) train acc: 0.403000; val_acc: 0.375000\n",
      "(Iteration 91 / 200) loss: 1.441158\n",
      "(Epoch 10 / 20) train acc: 0.497000; val_acc: 0.400000\n",
      "(Iteration 101 / 200) loss: 1.526201\n",
      "(Epoch 11 / 20) train acc: 0.472000; val_acc: 0.452778\n",
      "(Iteration 111 / 200) loss: 1.121999\n",
      "(Epoch 12 / 20) train acc: 0.507000; val_acc: 0.497222\n",
      "(Iteration 121 / 200) loss: 1.344955\n",
      "(Epoch 13 / 20) train acc: 0.568000; val_acc: 0.497222\n",
      "(Iteration 131 / 200) loss: 1.243862\n",
      "(Epoch 14 / 20) train acc: 0.535000; val_acc: 0.491667\n",
      "(Iteration 141 / 200) loss: 1.156992\n",
      "(Epoch 15 / 20) train acc: 0.572000; val_acc: 0.505556\n",
      "(Iteration 151 / 200) loss: 1.045409\n",
      "(Epoch 16 / 20) train acc: 0.584000; val_acc: 0.511111\n",
      "(Iteration 161 / 200) loss: 1.163658\n",
      "(Epoch 17 / 20) train acc: 0.586000; val_acc: 0.527778\n",
      "(Iteration 171 / 200) loss: 0.988497\n",
      "(Epoch 18 / 20) train acc: 0.682000; val_acc: 0.569444\n",
      "(Iteration 181 / 200) loss: 0.809174\n",
      "(Epoch 19 / 20) train acc: 0.666000; val_acc: 0.583333\n",
      "(Iteration 191 / 200) loss: 0.685685\n",
      "(Epoch 20 / 20) train acc: 0.796000; val_acc: 0.727778\n",
      "(Iteration 1 / 200) loss: 19576.897752\n",
      "(Epoch 0 / 20) train acc: 0.165000; val_acc: 0.180556\n",
      "(Epoch 1 / 20) train acc: 0.677000; val_acc: 0.616667\n",
      "(Iteration 11 / 200) loss: 1411.383682\n",
      "(Epoch 2 / 20) train acc: 0.839000; val_acc: 0.813889\n",
      "(Iteration 21 / 200) loss: 1022.158104\n",
      "(Epoch 3 / 20) train acc: 0.908000; val_acc: 0.863889\n",
      "(Iteration 31 / 200) loss: 770.030764\n",
      "(Epoch 4 / 20) train acc: 0.938000; val_acc: 0.883333\n",
      "(Iteration 41 / 200) loss: 223.354888\n",
      "(Epoch 5 / 20) train acc: 0.962000; val_acc: 0.900000\n",
      "(Iteration 51 / 200) loss: 76.549400\n",
      "(Epoch 6 / 20) train acc: 0.935000; val_acc: 0.891667\n",
      "(Iteration 61 / 200) loss: 122.085825\n",
      "(Epoch 7 / 20) train acc: 0.959000; val_acc: 0.911111\n",
      "(Iteration 71 / 200) loss: 43.202936\n",
      "(Epoch 8 / 20) train acc: 0.964000; val_acc: 0.922222\n",
      "(Iteration 81 / 200) loss: 78.818113\n",
      "(Epoch 9 / 20) train acc: 0.967000; val_acc: 0.908333\n",
      "(Iteration 91 / 200) loss: 60.734490\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.902778\n",
      "(Iteration 101 / 200) loss: 10.374352\n",
      "(Epoch 11 / 20) train acc: 0.996000; val_acc: 0.922222\n",
      "(Iteration 111 / 200) loss: 0.115257\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 0.114797\n",
      "(Epoch 13 / 20) train acc: 0.984000; val_acc: 0.911111\n",
      "(Iteration 131 / 200) loss: 85.637664\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.925000\n",
      "(Iteration 141 / 200) loss: 0.114069\n",
      "(Epoch 15 / 20) train acc: 0.985000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 24.276909\n",
      "(Epoch 16 / 20) train acc: 0.985000; val_acc: 0.911111\n",
      "(Iteration 161 / 200) loss: 6.876530\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.936111\n",
      "(Iteration 171 / 200) loss: 8.859406\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.919444\n",
      "(Iteration 181 / 200) loss: 5.675921\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.919444\n",
      "(Iteration 191 / 200) loss: 1.768541\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.925000\n",
      "(Iteration 1 / 200) loss: 3.547143\n",
      "(Epoch 0 / 20) train acc: 0.300000; val_acc: 0.250000\n",
      "(Epoch 1 / 20) train acc: 0.769000; val_acc: 0.727778\n",
      "(Iteration 11 / 200) loss: 0.817836\n",
      "(Epoch 2 / 20) train acc: 0.918000; val_acc: 0.872222\n",
      "(Iteration 21 / 200) loss: 0.249703\n",
      "(Epoch 3 / 20) train acc: 0.953000; val_acc: 0.894444\n",
      "(Iteration 31 / 200) loss: 0.083744\n",
      "(Epoch 4 / 20) train acc: 0.965000; val_acc: 0.947222\n",
      "(Iteration 41 / 200) loss: 0.067735\n",
      "(Epoch 5 / 20) train acc: 0.980000; val_acc: 0.961111\n",
      "(Iteration 51 / 200) loss: 0.073913\n",
      "(Epoch 6 / 20) train acc: 0.973000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.077140\n",
      "(Epoch 7 / 20) train acc: 0.974000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.047102\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.927778\n",
      "(Iteration 81 / 200) loss: 0.073317\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.980556\n",
      "(Iteration 91 / 200) loss: 0.019448\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.061718\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.029286\n",
      "(Epoch 12 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.012408\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.045910\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.007845\n",
      "(Epoch 15 / 20) train acc: 0.964000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.077606\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.008482\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.009270\n",
      "(Epoch 18 / 20) train acc: 0.981000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.056777\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.023634\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 2.302593\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.342000; val_acc: 0.363889\n",
      "(Iteration 11 / 200) loss: 1.680549\n",
      "(Epoch 2 / 20) train acc: 0.679000; val_acc: 0.652778\n",
      "(Iteration 21 / 200) loss: 0.830202\n",
      "(Epoch 3 / 20) train acc: 0.765000; val_acc: 0.752778\n",
      "(Iteration 31 / 200) loss: 0.615089\n",
      "(Epoch 4 / 20) train acc: 0.828000; val_acc: 0.811111\n",
      "(Iteration 41 / 200) loss: 0.507665\n",
      "(Epoch 5 / 20) train acc: 0.913000; val_acc: 0.880556\n",
      "(Iteration 51 / 200) loss: 0.315223\n",
      "(Epoch 6 / 20) train acc: 0.944000; val_acc: 0.891667\n",
      "(Iteration 61 / 200) loss: 0.176775\n",
      "(Epoch 7 / 20) train acc: 0.943000; val_acc: 0.927778\n",
      "(Iteration 71 / 200) loss: 0.078185\n",
      "(Epoch 8 / 20) train acc: 0.956000; val_acc: 0.930556\n",
      "(Iteration 81 / 200) loss: 0.051884\n",
      "(Epoch 9 / 20) train acc: 0.967000; val_acc: 0.941667\n",
      "(Iteration 91 / 200) loss: 0.070243\n",
      "(Epoch 10 / 20) train acc: 0.960000; val_acc: 0.947222\n",
      "(Iteration 101 / 200) loss: 0.153157\n",
      "(Epoch 11 / 20) train acc: 0.955000; val_acc: 0.944444\n",
      "(Iteration 111 / 200) loss: 0.234958\n",
      "(Epoch 12 / 20) train acc: 0.935000; val_acc: 0.916667\n",
      "(Iteration 121 / 200) loss: 0.366154\n",
      "(Epoch 13 / 20) train acc: 0.978000; val_acc: 0.950000\n",
      "(Iteration 131 / 200) loss: 0.052934\n",
      "(Epoch 14 / 20) train acc: 0.962000; val_acc: 0.925000\n",
      "(Iteration 141 / 200) loss: 0.133488\n",
      "(Epoch 15 / 20) train acc: 0.978000; val_acc: 0.938889\n",
      "(Iteration 151 / 200) loss: 0.036352\n",
      "(Epoch 16 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 161 / 200) loss: 0.030600\n",
      "(Epoch 17 / 20) train acc: 0.982000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.046515\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.015577\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.021573\n",
      "(Epoch 20 / 20) train acc: 0.989000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.212000; val_acc: 0.213889\n",
      "(Iteration 11 / 200) loss: 2.261335\n",
      "(Epoch 2 / 20) train acc: 0.451000; val_acc: 0.402778\n",
      "(Iteration 21 / 200) loss: 1.429159\n",
      "(Epoch 3 / 20) train acc: 0.476000; val_acc: 0.463889\n",
      "(Iteration 31 / 200) loss: 1.225371\n",
      "(Epoch 4 / 20) train acc: 0.506000; val_acc: 0.505556\n",
      "(Iteration 41 / 200) loss: 1.084354\n",
      "(Epoch 5 / 20) train acc: 0.691000; val_acc: 0.622222\n",
      "(Iteration 51 / 200) loss: 0.949058\n",
      "(Epoch 6 / 20) train acc: 0.763000; val_acc: 0.744444\n",
      "(Iteration 61 / 200) loss: 0.831590\n",
      "(Epoch 7 / 20) train acc: 0.884000; val_acc: 0.877778\n",
      "(Iteration 71 / 200) loss: 0.375528\n",
      "(Epoch 8 / 20) train acc: 0.869000; val_acc: 0.866667\n",
      "(Iteration 81 / 200) loss: 0.508529\n",
      "(Epoch 9 / 20) train acc: 0.938000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 0.255554\n",
      "(Epoch 10 / 20) train acc: 0.948000; val_acc: 0.927778\n",
      "(Iteration 101 / 200) loss: 0.220376\n",
      "(Epoch 11 / 20) train acc: 0.946000; val_acc: 0.911111\n",
      "(Iteration 111 / 200) loss: 0.161773\n",
      "(Epoch 12 / 20) train acc: 0.956000; val_acc: 0.927778\n",
      "(Iteration 121 / 200) loss: 0.133761\n",
      "(Epoch 13 / 20) train acc: 0.964000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 0.158982\n",
      "(Epoch 14 / 20) train acc: 0.980000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.038015\n",
      "(Epoch 15 / 20) train acc: 0.951000; val_acc: 0.911111\n",
      "(Iteration 151 / 200) loss: 0.102158\n",
      "(Epoch 16 / 20) train acc: 0.961000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 0.125915\n",
      "(Epoch 17 / 20) train acc: 0.956000; val_acc: 0.930556\n",
      "(Iteration 171 / 200) loss: 0.165887\n",
      "(Epoch 18 / 20) train acc: 0.977000; val_acc: 0.952778\n",
      "(Iteration 181 / 200) loss: 0.057545\n",
      "(Epoch 19 / 20) train acc: 0.977000; val_acc: 0.947222\n",
      "(Iteration 191 / 200) loss: 0.029470\n",
      "(Epoch 20 / 20) train acc: 0.972000; val_acc: 0.938889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.294724\n",
      "(Epoch 2 / 20) train acc: 0.224000; val_acc: 0.163889\n",
      "(Iteration 21 / 200) loss: 2.079701\n",
      "(Epoch 3 / 20) train acc: 0.211000; val_acc: 0.161111\n",
      "(Iteration 31 / 200) loss: 1.924524\n",
      "(Epoch 4 / 20) train acc: 0.273000; val_acc: 0.236111\n",
      "(Iteration 41 / 200) loss: 2.023261\n",
      "(Epoch 5 / 20) train acc: 0.340000; val_acc: 0.358333\n",
      "(Iteration 51 / 200) loss: 1.681535\n",
      "(Epoch 6 / 20) train acc: 0.462000; val_acc: 0.430556\n",
      "(Iteration 61 / 200) loss: 1.343956\n",
      "(Epoch 7 / 20) train acc: 0.573000; val_acc: 0.566667\n",
      "(Iteration 71 / 200) loss: 1.078794\n",
      "(Epoch 8 / 20) train acc: 0.587000; val_acc: 0.636111\n",
      "(Iteration 81 / 200) loss: 1.090762\n",
      "(Epoch 9 / 20) train acc: 0.662000; val_acc: 0.663889\n",
      "(Iteration 91 / 200) loss: 0.946844\n",
      "(Epoch 10 / 20) train acc: 0.776000; val_acc: 0.786111\n",
      "(Iteration 101 / 200) loss: 0.535391\n",
      "(Epoch 11 / 20) train acc: 0.859000; val_acc: 0.866667\n",
      "(Iteration 111 / 200) loss: 0.402263\n",
      "(Epoch 12 / 20) train acc: 0.872000; val_acc: 0.858333\n",
      "(Iteration 121 / 200) loss: 0.230585\n",
      "(Epoch 13 / 20) train acc: 0.932000; val_acc: 0.894444\n",
      "(Iteration 131 / 200) loss: 0.410993\n",
      "(Epoch 14 / 20) train acc: 0.947000; val_acc: 0.925000\n",
      "(Iteration 141 / 200) loss: 0.164122\n",
      "(Epoch 15 / 20) train acc: 0.954000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 0.075415\n",
      "(Epoch 16 / 20) train acc: 0.950000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 0.116723\n",
      "(Epoch 17 / 20) train acc: 0.959000; val_acc: 0.936111\n",
      "(Iteration 171 / 200) loss: 0.127904\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.950000\n",
      "(Iteration 181 / 200) loss: 0.105544\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.075819\n",
      "(Epoch 20 / 20) train acc: 0.978000; val_acc: 0.941667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.172000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 2.270547\n",
      "(Epoch 2 / 20) train acc: 0.173000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 2.013937\n",
      "(Epoch 3 / 20) train acc: 0.189000; val_acc: 0.169444\n",
      "(Iteration 31 / 200) loss: 1.806720\n",
      "(Epoch 4 / 20) train acc: 0.233000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 1.694932\n",
      "(Epoch 5 / 20) train acc: 0.393000; val_acc: 0.327778\n",
      "(Iteration 51 / 200) loss: 1.509047\n",
      "(Epoch 6 / 20) train acc: 0.527000; val_acc: 0.538889\n",
      "(Iteration 61 / 200) loss: 1.106058\n",
      "(Epoch 7 / 20) train acc: 0.618000; val_acc: 0.611111\n",
      "(Iteration 71 / 200) loss: 0.863517\n",
      "(Epoch 8 / 20) train acc: 0.818000; val_acc: 0.775000\n",
      "(Iteration 81 / 200) loss: 0.525391\n",
      "(Epoch 9 / 20) train acc: 0.891000; val_acc: 0.836111\n",
      "(Iteration 91 / 200) loss: 0.478464\n",
      "(Epoch 10 / 20) train acc: 0.931000; val_acc: 0.886111\n",
      "(Iteration 101 / 200) loss: 0.292032\n",
      "(Epoch 11 / 20) train acc: 0.908000; val_acc: 0.869444\n",
      "(Iteration 111 / 200) loss: 0.239701\n",
      "(Epoch 12 / 20) train acc: 0.915000; val_acc: 0.891667\n",
      "(Iteration 121 / 200) loss: 0.130551\n",
      "(Epoch 13 / 20) train acc: 0.931000; val_acc: 0.900000\n",
      "(Iteration 131 / 200) loss: 0.216993\n",
      "(Epoch 14 / 20) train acc: 0.941000; val_acc: 0.905556\n",
      "(Iteration 141 / 200) loss: 0.202541\n",
      "(Epoch 15 / 20) train acc: 0.943000; val_acc: 0.894444\n",
      "(Iteration 151 / 200) loss: 0.240105\n",
      "(Epoch 16 / 20) train acc: 0.973000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 0.061613\n",
      "(Epoch 17 / 20) train acc: 0.979000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.061573\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.019688\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.071808\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 36161.233525\n",
      "(Epoch 0 / 20) train acc: 0.048000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.078000; val_acc: 0.072222\n",
      "(Iteration 11 / 200) loss: 27235.201753\n",
      "(Epoch 2 / 20) train acc: 0.168000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 18329.645664\n",
      "(Epoch 3 / 20) train acc: 0.208000; val_acc: 0.186111\n",
      "(Iteration 31 / 200) loss: 11512.070427\n",
      "(Epoch 4 / 20) train acc: 0.256000; val_acc: 0.255556\n",
      "(Iteration 41 / 200) loss: 9564.351562\n",
      "(Epoch 5 / 20) train acc: 0.347000; val_acc: 0.333333\n",
      "(Iteration 51 / 200) loss: 6030.693579\n",
      "(Epoch 6 / 20) train acc: 0.378000; val_acc: 0.383333\n",
      "(Iteration 61 / 200) loss: 5713.802805\n",
      "(Epoch 7 / 20) train acc: 0.442000; val_acc: 0.413889\n",
      "(Iteration 71 / 200) loss: 3544.350015\n",
      "(Epoch 8 / 20) train acc: 0.509000; val_acc: 0.461111\n",
      "(Iteration 81 / 200) loss: 3676.239058\n",
      "(Epoch 9 / 20) train acc: 0.529000; val_acc: 0.494444\n",
      "(Iteration 91 / 200) loss: 3356.483044\n",
      "(Epoch 10 / 20) train acc: 0.572000; val_acc: 0.538889\n",
      "(Iteration 101 / 200) loss: 2376.563896\n",
      "(Epoch 11 / 20) train acc: 0.653000; val_acc: 0.563889\n",
      "(Iteration 111 / 200) loss: 2347.696660\n",
      "(Epoch 12 / 20) train acc: 0.650000; val_acc: 0.572222\n",
      "(Iteration 121 / 200) loss: 1872.037607\n",
      "(Epoch 13 / 20) train acc: 0.677000; val_acc: 0.583333\n",
      "(Iteration 131 / 200) loss: 1877.303379\n",
      "(Epoch 14 / 20) train acc: 0.702000; val_acc: 0.613889\n",
      "(Iteration 141 / 200) loss: 1862.515225\n",
      "(Epoch 15 / 20) train acc: 0.728000; val_acc: 0.638889\n",
      "(Iteration 151 / 200) loss: 1738.548687\n",
      "(Epoch 16 / 20) train acc: 0.746000; val_acc: 0.641667\n",
      "(Iteration 161 / 200) loss: 1284.211550\n",
      "(Epoch 17 / 20) train acc: 0.742000; val_acc: 0.658333\n",
      "(Iteration 171 / 200) loss: 917.572927\n",
      "(Epoch 18 / 20) train acc: 0.757000; val_acc: 0.683333\n",
      "(Iteration 181 / 200) loss: 1226.106521\n",
      "(Epoch 19 / 20) train acc: 0.811000; val_acc: 0.677778\n",
      "(Iteration 191 / 200) loss: 698.504321\n",
      "(Epoch 20 / 20) train acc: 0.795000; val_acc: 0.694444\n",
      "(Iteration 1 / 200) loss: 6.396279\n",
      "(Epoch 0 / 20) train acc: 0.042000; val_acc: 0.036111\n",
      "(Epoch 1 / 20) train acc: 0.485000; val_acc: 0.483333\n",
      "(Iteration 11 / 200) loss: 2.656515\n",
      "(Epoch 2 / 20) train acc: 0.801000; val_acc: 0.783333\n",
      "(Iteration 21 / 200) loss: 1.969548\n",
      "(Epoch 3 / 20) train acc: 0.915000; val_acc: 0.844444\n",
      "(Iteration 31 / 200) loss: 1.653719\n",
      "(Epoch 4 / 20) train acc: 0.927000; val_acc: 0.886111\n",
      "(Iteration 41 / 200) loss: 1.479454\n",
      "(Epoch 5 / 20) train acc: 0.936000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 1.272795\n",
      "(Epoch 6 / 20) train acc: 0.963000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 1.170240\n",
      "(Epoch 7 / 20) train acc: 0.974000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 1.059806\n",
      "(Epoch 8 / 20) train acc: 0.987000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 1.018847\n",
      "(Epoch 9 / 20) train acc: 0.975000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.980581\n",
      "(Epoch 10 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.968589\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.983333\n",
      "(Iteration 111 / 200) loss: 0.890918\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.909373\n",
      "(Epoch 13 / 20) train acc: 0.984000; val_acc: 0.977778\n",
      "(Iteration 131 / 200) loss: 0.867163\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.983333\n",
      "(Iteration 141 / 200) loss: 0.841240\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.986111\n",
      "(Iteration 151 / 200) loss: 0.798484\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.754234\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.757528\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.737639\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.704890\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.315730\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.179000; val_acc: 0.133333\n",
      "(Iteration 11 / 200) loss: 2.302810\n",
      "(Epoch 2 / 20) train acc: 0.263000; val_acc: 0.269444\n",
      "(Iteration 21 / 200) loss: 2.168885\n",
      "(Epoch 3 / 20) train acc: 0.200000; val_acc: 0.227778\n",
      "(Iteration 31 / 200) loss: 1.816503\n",
      "(Epoch 4 / 20) train acc: 0.298000; val_acc: 0.275000\n",
      "(Iteration 41 / 200) loss: 1.757676\n",
      "(Epoch 5 / 20) train acc: 0.330000; val_acc: 0.338889\n",
      "(Iteration 51 / 200) loss: 1.422329\n",
      "(Epoch 6 / 20) train acc: 0.351000; val_acc: 0.369444\n",
      "(Iteration 61 / 200) loss: 1.401416\n",
      "(Epoch 7 / 20) train acc: 0.392000; val_acc: 0.383333\n",
      "(Iteration 71 / 200) loss: 1.414244\n",
      "(Epoch 8 / 20) train acc: 0.395000; val_acc: 0.386111\n",
      "(Iteration 81 / 200) loss: 1.257606\n",
      "(Epoch 9 / 20) train acc: 0.418000; val_acc: 0.411111\n",
      "(Iteration 91 / 200) loss: 1.397446\n",
      "(Epoch 10 / 20) train acc: 0.410000; val_acc: 0.416667\n",
      "(Iteration 101 / 200) loss: 1.429319\n",
      "(Epoch 11 / 20) train acc: 0.385000; val_acc: 0.408333\n",
      "(Iteration 111 / 200) loss: 1.389824\n",
      "(Epoch 12 / 20) train acc: 0.432000; val_acc: 0.388889\n",
      "(Iteration 121 / 200) loss: 1.403900\n",
      "(Epoch 13 / 20) train acc: 0.464000; val_acc: 0.436111\n",
      "(Iteration 131 / 200) loss: 1.207752\n",
      "(Epoch 14 / 20) train acc: 0.445000; val_acc: 0.416667\n",
      "(Iteration 141 / 200) loss: 1.215500\n",
      "(Epoch 15 / 20) train acc: 0.450000; val_acc: 0.441667\n",
      "(Iteration 151 / 200) loss: 1.257828\n",
      "(Epoch 16 / 20) train acc: 0.474000; val_acc: 0.441667\n",
      "(Iteration 161 / 200) loss: 1.186111\n",
      "(Epoch 17 / 20) train acc: 0.481000; val_acc: 0.441667\n",
      "(Iteration 171 / 200) loss: 1.266690\n",
      "(Epoch 18 / 20) train acc: 0.486000; val_acc: 0.483333\n",
      "(Iteration 181 / 200) loss: 1.166117\n",
      "(Epoch 19 / 20) train acc: 0.492000; val_acc: 0.458333\n",
      "(Iteration 191 / 200) loss: 1.432896\n",
      "(Epoch 20 / 20) train acc: 0.484000; val_acc: 0.486111\n",
      "(Iteration 1 / 200) loss: 2.302718\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302656\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302557\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302411\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302453\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.301184\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302899\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302435\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.301806\n",
      "(Epoch 9 / 20) train acc: 0.128000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302481\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.305009\n",
      "(Epoch 11 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302333\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.301200\n",
      "(Epoch 13 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.300712\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.298432\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.301544\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.303232\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302094\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.297440\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.304718\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302993\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.301723\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.304693\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.301111\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.300319\n",
      "(Epoch 6 / 20) train acc: 0.140000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301639\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.301597\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.301629\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.300679\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302494\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.306508\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.307370\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.301267\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.304696\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.298131\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.303372\n",
      "(Epoch 17 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.298926\n",
      "(Epoch 18 / 20) train acc: 0.139000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.297505\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302457\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302191\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302281\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.301505\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302954\n",
      "(Epoch 5 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.303178\n",
      "(Epoch 6 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302610\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302688\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.300591\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.299821\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302948\n",
      "(Epoch 11 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.298651\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.301585\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.303828\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.295906\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302710\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.305631\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.262267\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.153025\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.107758\n",
      "(Epoch 20 / 20) train acc: 0.153000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 31125.060693\n",
      "(Epoch 0 / 20) train acc: 0.149000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.135000; val_acc: 0.141667\n",
      "(Iteration 11 / 200) loss: 27759.498679\n",
      "(Epoch 2 / 20) train acc: 0.251000; val_acc: 0.227778\n",
      "(Iteration 21 / 200) loss: 15797.711797\n",
      "(Epoch 3 / 20) train acc: 0.298000; val_acc: 0.275000\n",
      "(Iteration 31 / 200) loss: 11957.042876\n",
      "(Epoch 4 / 20) train acc: 0.336000; val_acc: 0.316667\n",
      "(Iteration 41 / 200) loss: 9244.799429\n",
      "(Epoch 5 / 20) train acc: 0.350000; val_acc: 0.325000\n",
      "(Iteration 51 / 200) loss: 5446.803718\n",
      "(Epoch 6 / 20) train acc: 0.427000; val_acc: 0.408333\n",
      "(Iteration 61 / 200) loss: 4899.608652\n",
      "(Epoch 7 / 20) train acc: 0.479000; val_acc: 0.475000\n",
      "(Iteration 71 / 200) loss: 2697.943447\n",
      "(Epoch 8 / 20) train acc: 0.560000; val_acc: 0.541667\n",
      "(Iteration 81 / 200) loss: 3129.348091\n",
      "(Epoch 9 / 20) train acc: 0.631000; val_acc: 0.575000\n",
      "(Iteration 91 / 200) loss: 1889.786133\n",
      "(Epoch 10 / 20) train acc: 0.667000; val_acc: 0.608333\n",
      "(Iteration 101 / 200) loss: 3190.607251\n",
      "(Epoch 11 / 20) train acc: 0.710000; val_acc: 0.613889\n",
      "(Iteration 111 / 200) loss: 1963.380854\n",
      "(Epoch 12 / 20) train acc: 0.713000; val_acc: 0.641667\n",
      "(Iteration 121 / 200) loss: 2152.114370\n",
      "(Epoch 13 / 20) train acc: 0.737000; val_acc: 0.677778\n",
      "(Iteration 131 / 200) loss: 1250.209448\n",
      "(Epoch 14 / 20) train acc: 0.750000; val_acc: 0.702778\n",
      "(Iteration 141 / 200) loss: 1491.323735\n",
      "(Epoch 15 / 20) train acc: 0.778000; val_acc: 0.708333\n",
      "(Iteration 151 / 200) loss: 940.156321\n",
      "(Epoch 16 / 20) train acc: 0.791000; val_acc: 0.736111\n",
      "(Iteration 161 / 200) loss: 725.958882\n",
      "(Epoch 17 / 20) train acc: 0.814000; val_acc: 0.747222\n",
      "(Iteration 171 / 200) loss: 525.739797\n",
      "(Epoch 18 / 20) train acc: 0.842000; val_acc: 0.755556\n",
      "(Iteration 181 / 200) loss: 1185.057834\n",
      "(Epoch 19 / 20) train acc: 0.818000; val_acc: 0.761111\n",
      "(Iteration 191 / 200) loss: 743.540278\n",
      "(Epoch 20 / 20) train acc: 0.851000; val_acc: 0.775000\n",
      "(Iteration 1 / 200) loss: 5.261660\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.177778\n",
      "(Epoch 1 / 20) train acc: 0.634000; val_acc: 0.622222\n",
      "(Iteration 11 / 200) loss: 2.513486\n",
      "(Epoch 2 / 20) train acc: 0.860000; val_acc: 0.844444\n",
      "(Iteration 21 / 200) loss: 1.882298\n",
      "(Epoch 3 / 20) train acc: 0.913000; val_acc: 0.908333\n",
      "(Iteration 31 / 200) loss: 1.553414\n",
      "(Epoch 4 / 20) train acc: 0.933000; val_acc: 0.927778\n",
      "(Iteration 41 / 200) loss: 1.373937\n",
      "(Epoch 5 / 20) train acc: 0.960000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 1.250385\n",
      "(Epoch 6 / 20) train acc: 0.967000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 1.146514\n",
      "(Epoch 7 / 20) train acc: 0.971000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 1.079181\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.969444\n",
      "(Iteration 81 / 200) loss: 1.052676\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 1.004010\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 101 / 200) loss: 0.949629\n",
      "(Epoch 11 / 20) train acc: 0.982000; val_acc: 0.980556\n",
      "(Iteration 111 / 200) loss: 0.927883\n",
      "(Epoch 12 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.895922\n",
      "(Epoch 13 / 20) train acc: 0.990000; val_acc: 0.977778\n",
      "(Iteration 131 / 200) loss: 0.850370\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.870703\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.817162\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.796849\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.785938\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 181 / 200) loss: 0.751489\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.730183\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.315681\n",
      "(Epoch 0 / 20) train acc: 0.161000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.126000; val_acc: 0.141667\n",
      "(Iteration 11 / 200) loss: 2.303324\n",
      "(Epoch 2 / 20) train acc: 0.239000; val_acc: 0.277778\n",
      "(Iteration 21 / 200) loss: 2.208958\n",
      "(Epoch 3 / 20) train acc: 0.230000; val_acc: 0.258333\n",
      "(Iteration 31 / 200) loss: 1.864816\n",
      "(Epoch 4 / 20) train acc: 0.300000; val_acc: 0.297222\n",
      "(Iteration 41 / 200) loss: 1.666970\n",
      "(Epoch 5 / 20) train acc: 0.308000; val_acc: 0.308333\n",
      "(Iteration 51 / 200) loss: 1.542961\n",
      "(Epoch 6 / 20) train acc: 0.356000; val_acc: 0.366667\n",
      "(Iteration 61 / 200) loss: 1.546002\n",
      "(Epoch 7 / 20) train acc: 0.395000; val_acc: 0.369444\n",
      "(Iteration 71 / 200) loss: 1.488774\n",
      "(Epoch 8 / 20) train acc: 0.454000; val_acc: 0.400000\n",
      "(Iteration 81 / 200) loss: 1.437560\n",
      "(Epoch 9 / 20) train acc: 0.471000; val_acc: 0.469444\n",
      "(Iteration 91 / 200) loss: 1.351190\n",
      "(Epoch 10 / 20) train acc: 0.404000; val_acc: 0.400000\n",
      "(Iteration 101 / 200) loss: 1.409645\n",
      "(Epoch 11 / 20) train acc: 0.439000; val_acc: 0.419444\n",
      "(Iteration 111 / 200) loss: 1.377617\n",
      "(Epoch 12 / 20) train acc: 0.484000; val_acc: 0.444444\n",
      "(Iteration 121 / 200) loss: 1.333325\n",
      "(Epoch 13 / 20) train acc: 0.516000; val_acc: 0.525000\n",
      "(Iteration 131 / 200) loss: 1.214244\n",
      "(Epoch 14 / 20) train acc: 0.556000; val_acc: 0.550000\n",
      "(Iteration 141 / 200) loss: 1.114450\n",
      "(Epoch 15 / 20) train acc: 0.568000; val_acc: 0.586111\n",
      "(Iteration 151 / 200) loss: 1.097593\n",
      "(Epoch 16 / 20) train acc: 0.654000; val_acc: 0.619444\n",
      "(Iteration 161 / 200) loss: 0.932680\n",
      "(Epoch 17 / 20) train acc: 0.676000; val_acc: 0.661111\n",
      "(Iteration 171 / 200) loss: 0.822414\n",
      "(Epoch 18 / 20) train acc: 0.731000; val_acc: 0.722222\n",
      "(Iteration 181 / 200) loss: 0.733422\n",
      "(Epoch 19 / 20) train acc: 0.770000; val_acc: 0.766667\n",
      "(Iteration 191 / 200) loss: 0.910150\n",
      "(Epoch 20 / 20) train acc: 0.765000; val_acc: 0.772222\n",
      "(Iteration 1 / 200) loss: 2.302718\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.303036\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302373\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.301720\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.301387\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302729\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302634\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.303562\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.305797\n",
      "(Epoch 9 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.300976\n",
      "(Epoch 10 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.304833\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.301588\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.305594\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.301387\n",
      "(Epoch 14 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.299262\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.305521\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.307324\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.299210\n",
      "(Epoch 18 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.295875\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.299761\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302338\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.301984\n",
      "(Epoch 3 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302229\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302999\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.301366\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.303328\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.304777\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.303847\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.309827\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.303309\n",
      "(Epoch 11 / 20) train acc: 0.083000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302973\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.301112\n",
      "(Epoch 13 / 20) train acc: 0.083000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.299379\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.306131\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.309541\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301949\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.297139\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303412\n",
      "(Epoch 19 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.304231\n",
      "(Epoch 20 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302372\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302800\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.301393\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.301061\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.300989\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.300795\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.295150\n",
      "(Epoch 8 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.301896\n",
      "(Epoch 9 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302538\n",
      "(Epoch 10 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.298838\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.306245\n",
      "(Epoch 12 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.301649\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302351\n",
      "(Epoch 14 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.296156\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302437\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.305138\n",
      "(Epoch 17 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.298748\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.307132\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302983\n",
      "(Epoch 20 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 30024.432227\n",
      "(Epoch 0 / 20) train acc: 0.042000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 19567.693396\n",
      "(Epoch 2 / 20) train acc: 0.173000; val_acc: 0.172222\n",
      "(Iteration 21 / 200) loss: 16809.113596\n",
      "(Epoch 3 / 20) train acc: 0.206000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 11681.239663\n",
      "(Epoch 4 / 20) train acc: 0.266000; val_acc: 0.225000\n",
      "(Iteration 41 / 200) loss: 9689.063218\n",
      "(Epoch 5 / 20) train acc: 0.300000; val_acc: 0.272222\n",
      "(Iteration 51 / 200) loss: 8028.590146\n",
      "(Epoch 6 / 20) train acc: 0.340000; val_acc: 0.311111\n",
      "(Iteration 61 / 200) loss: 5013.534888\n",
      "(Epoch 7 / 20) train acc: 0.395000; val_acc: 0.344444\n",
      "(Iteration 71 / 200) loss: 4166.715200\n",
      "(Epoch 8 / 20) train acc: 0.469000; val_acc: 0.419444\n",
      "(Iteration 81 / 200) loss: 3409.003477\n",
      "(Epoch 9 / 20) train acc: 0.516000; val_acc: 0.472222\n",
      "(Iteration 91 / 200) loss: 3030.138223\n",
      "(Epoch 10 / 20) train acc: 0.605000; val_acc: 0.505556\n",
      "(Iteration 101 / 200) loss: 2554.481841\n",
      "(Epoch 11 / 20) train acc: 0.620000; val_acc: 0.547222\n",
      "(Iteration 111 / 200) loss: 1707.558469\n",
      "(Epoch 12 / 20) train acc: 0.694000; val_acc: 0.575000\n",
      "(Iteration 121 / 200) loss: 1377.072661\n",
      "(Epoch 13 / 20) train acc: 0.706000; val_acc: 0.600000\n",
      "(Iteration 131 / 200) loss: 1241.752466\n",
      "(Epoch 14 / 20) train acc: 0.740000; val_acc: 0.627778\n",
      "(Iteration 141 / 200) loss: 722.990083\n",
      "(Epoch 15 / 20) train acc: 0.745000; val_acc: 0.641667\n",
      "(Iteration 151 / 200) loss: 1396.101672\n",
      "(Epoch 16 / 20) train acc: 0.755000; val_acc: 0.658333\n",
      "(Iteration 161 / 200) loss: 1034.418735\n",
      "(Epoch 17 / 20) train acc: 0.795000; val_acc: 0.691667\n",
      "(Iteration 171 / 200) loss: 1063.346992\n",
      "(Epoch 18 / 20) train acc: 0.818000; val_acc: 0.716667\n",
      "(Iteration 181 / 200) loss: 668.428284\n",
      "(Epoch 19 / 20) train acc: 0.835000; val_acc: 0.708333\n",
      "(Iteration 191 / 200) loss: 436.526440\n",
      "(Epoch 20 / 20) train acc: 0.814000; val_acc: 0.719444\n",
      "(Iteration 1 / 200) loss: 4.616811\n",
      "(Epoch 0 / 20) train acc: 0.154000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.652000; val_acc: 0.633333\n",
      "(Iteration 11 / 200) loss: 2.537618\n",
      "(Epoch 2 / 20) train acc: 0.857000; val_acc: 0.830556\n",
      "(Iteration 21 / 200) loss: 1.840205\n",
      "(Epoch 3 / 20) train acc: 0.924000; val_acc: 0.875000\n",
      "(Iteration 31 / 200) loss: 1.390747\n",
      "(Epoch 4 / 20) train acc: 0.946000; val_acc: 0.916667\n",
      "(Iteration 41 / 200) loss: 1.327299\n",
      "(Epoch 5 / 20) train acc: 0.959000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 1.226903\n",
      "(Epoch 6 / 20) train acc: 0.961000; val_acc: 0.958333\n",
      "(Iteration 61 / 200) loss: 1.095822\n",
      "(Epoch 7 / 20) train acc: 0.978000; val_acc: 0.961111\n",
      "(Iteration 71 / 200) loss: 1.034793\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.986111\n",
      "(Iteration 81 / 200) loss: 0.986084\n",
      "(Epoch 9 / 20) train acc: 0.984000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.932378\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.883693\n",
      "(Epoch 11 / 20) train acc: 0.989000; val_acc: 0.977778\n",
      "(Iteration 111 / 200) loss: 0.926658\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.830687\n",
      "(Epoch 13 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.827359\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.983333\n",
      "(Iteration 141 / 200) loss: 0.780570\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.760355\n",
      "(Epoch 16 / 20) train acc: 0.987000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.764480\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 171 / 200) loss: 0.714028\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 181 / 200) loss: 0.693670\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.988889\n",
      "(Iteration 191 / 200) loss: 0.679021\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 1 / 200) loss: 2.315771\n",
      "(Epoch 0 / 20) train acc: 0.178000; val_acc: 0.191667\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.303617\n",
      "(Epoch 2 / 20) train acc: 0.196000; val_acc: 0.205556\n",
      "(Iteration 21 / 200) loss: 2.241122\n",
      "(Epoch 3 / 20) train acc: 0.268000; val_acc: 0.294444\n",
      "(Iteration 31 / 200) loss: 1.788124\n",
      "(Epoch 4 / 20) train acc: 0.438000; val_acc: 0.394444\n",
      "(Iteration 41 / 200) loss: 1.557470\n",
      "(Epoch 5 / 20) train acc: 0.506000; val_acc: 0.527778\n",
      "(Iteration 51 / 200) loss: 1.250445\n",
      "(Epoch 6 / 20) train acc: 0.600000; val_acc: 0.597222\n",
      "(Iteration 61 / 200) loss: 1.116617\n",
      "(Epoch 7 / 20) train acc: 0.627000; val_acc: 0.591667\n",
      "(Iteration 71 / 200) loss: 1.062783\n",
      "(Epoch 8 / 20) train acc: 0.710000; val_acc: 0.694444\n",
      "(Iteration 81 / 200) loss: 1.108979\n",
      "(Epoch 9 / 20) train acc: 0.682000; val_acc: 0.697222\n",
      "(Iteration 91 / 200) loss: 0.987613\n",
      "(Epoch 10 / 20) train acc: 0.713000; val_acc: 0.725000\n",
      "(Iteration 101 / 200) loss: 0.846571\n",
      "(Epoch 11 / 20) train acc: 0.702000; val_acc: 0.722222\n",
      "(Iteration 111 / 200) loss: 0.790836\n",
      "(Epoch 12 / 20) train acc: 0.712000; val_acc: 0.725000\n",
      "(Iteration 121 / 200) loss: 0.849657\n",
      "(Epoch 13 / 20) train acc: 0.764000; val_acc: 0.744444\n",
      "(Iteration 131 / 200) loss: 0.805400\n",
      "(Epoch 14 / 20) train acc: 0.719000; val_acc: 0.713889\n",
      "(Iteration 141 / 200) loss: 0.919732\n",
      "(Epoch 15 / 20) train acc: 0.776000; val_acc: 0.752778\n",
      "(Iteration 151 / 200) loss: 0.819279\n",
      "(Epoch 16 / 20) train acc: 0.802000; val_acc: 0.780556\n",
      "(Iteration 161 / 200) loss: 0.685805\n",
      "(Epoch 17 / 20) train acc: 0.752000; val_acc: 0.797222\n",
      "(Iteration 171 / 200) loss: 0.743784\n",
      "(Epoch 18 / 20) train acc: 0.782000; val_acc: 0.769444\n",
      "(Iteration 181 / 200) loss: 0.777021\n",
      "(Epoch 19 / 20) train acc: 0.774000; val_acc: 0.747222\n",
      "(Iteration 191 / 200) loss: 0.658766\n",
      "(Epoch 20 / 20) train acc: 0.778000; val_acc: 0.763889\n",
      "(Iteration 1 / 200) loss: 2.302718\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302591\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.303041\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.300402\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302998\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301224\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.303524\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.300467\n",
      "(Epoch 9 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.299943\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.300547\n",
      "(Epoch 11 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.308661\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.309485\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.301238\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301255\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.308252\n",
      "(Epoch 16 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.305865\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.300889\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302981\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301712\n",
      "(Epoch 20 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.303354\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.303689\n",
      "(Epoch 3 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301956\n",
      "(Epoch 4 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.301429\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302640\n",
      "(Epoch 6 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.303073\n",
      "(Epoch 7 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.303946\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.299773\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.301459\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.300198\n",
      "(Epoch 11 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.306004\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.299588\n",
      "(Epoch 13 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.299539\n",
      "(Epoch 14 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.300342\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302416\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.298575\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.304891\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.303605\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.303484\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302885\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.299380\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.297435\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.301922\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.305460\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.300121\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302819\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.234255\n",
      "(Epoch 9 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.008924\n",
      "(Epoch 10 / 20) train acc: 0.164000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 1.903556\n",
      "(Epoch 11 / 20) train acc: 0.189000; val_acc: 0.136111\n",
      "(Iteration 111 / 200) loss: 1.960082\n",
      "(Epoch 12 / 20) train acc: 0.197000; val_acc: 0.141667\n",
      "(Iteration 121 / 200) loss: 2.037469\n",
      "(Epoch 13 / 20) train acc: 0.216000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 2.110029\n",
      "(Epoch 14 / 20) train acc: 0.202000; val_acc: 0.158333\n",
      "(Iteration 141 / 200) loss: 1.994283\n",
      "(Epoch 15 / 20) train acc: 0.179000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 1.989400\n",
      "(Epoch 16 / 20) train acc: 0.184000; val_acc: 0.147222\n",
      "(Iteration 161 / 200) loss: 1.985206\n",
      "(Epoch 17 / 20) train acc: 0.219000; val_acc: 0.202778\n",
      "(Iteration 171 / 200) loss: 1.998266\n",
      "(Epoch 18 / 20) train acc: 0.201000; val_acc: 0.205556\n",
      "(Iteration 181 / 200) loss: 2.007710\n",
      "(Epoch 19 / 20) train acc: 0.203000; val_acc: 0.200000\n",
      "(Iteration 191 / 200) loss: 2.078653\n",
      "(Epoch 20 / 20) train acc: 0.229000; val_acc: 0.202778\n",
      "(Iteration 1 / 200) loss: 44609.978690\n",
      "(Epoch 0 / 20) train acc: 0.071000; val_acc: 0.055556\n",
      "(Epoch 1 / 20) train acc: 0.046000; val_acc: 0.041667\n",
      "(Iteration 11 / 200) loss: 35695.671548\n",
      "(Epoch 2 / 20) train acc: 0.068000; val_acc: 0.058333\n",
      "(Iteration 21 / 200) loss: 20858.149567\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 13647.826438\n",
      "(Epoch 4 / 20) train acc: 0.137000; val_acc: 0.152778\n",
      "(Iteration 41 / 200) loss: 9379.102135\n",
      "(Epoch 5 / 20) train acc: 0.200000; val_acc: 0.213889\n",
      "(Iteration 51 / 200) loss: 7531.316927\n",
      "(Epoch 6 / 20) train acc: 0.271000; val_acc: 0.258333\n",
      "(Iteration 61 / 200) loss: 5326.682099\n",
      "(Epoch 7 / 20) train acc: 0.390000; val_acc: 0.319444\n",
      "(Iteration 71 / 200) loss: 3482.893140\n",
      "(Epoch 8 / 20) train acc: 0.466000; val_acc: 0.405556\n",
      "(Iteration 81 / 200) loss: 3852.541206\n",
      "(Epoch 9 / 20) train acc: 0.549000; val_acc: 0.455556\n",
      "(Iteration 91 / 200) loss: 2485.223422\n",
      "(Epoch 10 / 20) train acc: 0.586000; val_acc: 0.513889\n",
      "(Iteration 101 / 200) loss: 2627.138137\n",
      "(Epoch 11 / 20) train acc: 0.645000; val_acc: 0.572222\n",
      "(Iteration 111 / 200) loss: 1557.558161\n",
      "(Epoch 12 / 20) train acc: 0.697000; val_acc: 0.613889\n",
      "(Iteration 121 / 200) loss: 1361.072788\n",
      "(Epoch 13 / 20) train acc: 0.699000; val_acc: 0.647222\n",
      "(Iteration 131 / 200) loss: 1057.805525\n",
      "(Epoch 14 / 20) train acc: 0.720000; val_acc: 0.663889\n",
      "(Iteration 141 / 200) loss: 1235.871475\n",
      "(Epoch 15 / 20) train acc: 0.737000; val_acc: 0.697222\n",
      "(Iteration 151 / 200) loss: 1537.611521\n",
      "(Epoch 16 / 20) train acc: 0.765000; val_acc: 0.716667\n",
      "(Iteration 161 / 200) loss: 1033.343297\n",
      "(Epoch 17 / 20) train acc: 0.780000; val_acc: 0.727778\n",
      "(Iteration 171 / 200) loss: 904.705958\n",
      "(Epoch 18 / 20) train acc: 0.783000; val_acc: 0.733333\n",
      "(Iteration 181 / 200) loss: 1105.563968\n",
      "(Epoch 19 / 20) train acc: 0.790000; val_acc: 0.755556\n",
      "(Iteration 191 / 200) loss: 853.096990\n",
      "(Epoch 20 / 20) train acc: 0.806000; val_acc: 0.755556\n",
      "(Iteration 1 / 200) loss: 4.872688\n",
      "(Epoch 0 / 20) train acc: 0.051000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.477000; val_acc: 0.430556\n",
      "(Iteration 11 / 200) loss: 1.725696\n",
      "(Epoch 2 / 20) train acc: 0.810000; val_acc: 0.794444\n",
      "(Iteration 21 / 200) loss: 0.978838\n",
      "(Epoch 3 / 20) train acc: 0.902000; val_acc: 0.902778\n",
      "(Iteration 31 / 200) loss: 0.552920\n",
      "(Epoch 4 / 20) train acc: 0.924000; val_acc: 0.941667\n",
      "(Iteration 41 / 200) loss: 0.435210\n",
      "(Epoch 5 / 20) train acc: 0.937000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 0.269473\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.298529\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.966667\n",
      "(Iteration 71 / 200) loss: 0.254131\n",
      "(Epoch 8 / 20) train acc: 0.968000; val_acc: 0.952778\n",
      "(Iteration 81 / 200) loss: 0.185235\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.164295\n",
      "(Epoch 10 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.138294\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.977778\n",
      "(Iteration 111 / 200) loss: 0.176832\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.182868\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.991667\n",
      "(Iteration 131 / 200) loss: 0.157161\n",
      "(Epoch 14 / 20) train acc: 0.999000; val_acc: 0.986111\n",
      "(Iteration 141 / 200) loss: 0.130680\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 151 / 200) loss: 0.126493\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.991667\n",
      "(Iteration 161 / 200) loss: 0.136994\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.116636\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.988889\n",
      "(Iteration 181 / 200) loss: 0.124196\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 191 / 200) loss: 0.115971\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.988889\n",
      "(Iteration 1 / 200) loss: 2.303925\n",
      "(Epoch 0 / 20) train acc: 0.164000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.233000; val_acc: 0.197222\n",
      "(Iteration 11 / 200) loss: 2.289403\n",
      "(Epoch 2 / 20) train acc: 0.209000; val_acc: 0.202778\n",
      "(Iteration 21 / 200) loss: 2.040191\n",
      "(Epoch 3 / 20) train acc: 0.487000; val_acc: 0.486111\n",
      "(Iteration 31 / 200) loss: 1.623159\n",
      "(Epoch 4 / 20) train acc: 0.565000; val_acc: 0.544444\n",
      "(Iteration 41 / 200) loss: 1.166837\n",
      "(Epoch 5 / 20) train acc: 0.668000; val_acc: 0.700000\n",
      "(Iteration 51 / 200) loss: 0.825391\n",
      "(Epoch 6 / 20) train acc: 0.753000; val_acc: 0.761111\n",
      "(Iteration 61 / 200) loss: 0.683773\n",
      "(Epoch 7 / 20) train acc: 0.783000; val_acc: 0.786111\n",
      "(Iteration 71 / 200) loss: 0.589190\n",
      "(Epoch 8 / 20) train acc: 0.811000; val_acc: 0.819444\n",
      "(Iteration 81 / 200) loss: 0.534801\n",
      "(Epoch 9 / 20) train acc: 0.818000; val_acc: 0.805556\n",
      "(Iteration 91 / 200) loss: 0.442204\n",
      "(Epoch 10 / 20) train acc: 0.855000; val_acc: 0.825000\n",
      "(Iteration 101 / 200) loss: 0.440881\n",
      "(Epoch 11 / 20) train acc: 0.841000; val_acc: 0.847222\n",
      "(Iteration 111 / 200) loss: 0.528344\n",
      "(Epoch 12 / 20) train acc: 0.887000; val_acc: 0.891667\n",
      "(Iteration 121 / 200) loss: 0.328095\n",
      "(Epoch 13 / 20) train acc: 0.870000; val_acc: 0.866667\n",
      "(Iteration 131 / 200) loss: 0.333447\n",
      "(Epoch 14 / 20) train acc: 0.882000; val_acc: 0.880556\n",
      "(Iteration 141 / 200) loss: 0.409087\n",
      "(Epoch 15 / 20) train acc: 0.903000; val_acc: 0.863889\n",
      "(Iteration 151 / 200) loss: 0.418387\n",
      "(Epoch 16 / 20) train acc: 0.914000; val_acc: 0.869444\n",
      "(Iteration 161 / 200) loss: 0.424650\n",
      "(Epoch 17 / 20) train acc: 0.913000; val_acc: 0.880556\n",
      "(Iteration 171 / 200) loss: 0.269338\n",
      "(Epoch 18 / 20) train acc: 0.913000; val_acc: 0.877778\n",
      "(Iteration 181 / 200) loss: 0.270276\n",
      "(Epoch 19 / 20) train acc: 0.916000; val_acc: 0.872222\n",
      "(Iteration 191 / 200) loss: 0.206289\n",
      "(Epoch 20 / 20) train acc: 0.926000; val_acc: 0.883333\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.301955\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302670\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.301987\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.299282\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.257897\n",
      "(Epoch 6 / 20) train acc: 0.118000; val_acc: 0.102778\n",
      "(Iteration 61 / 200) loss: 2.181939\n",
      "(Epoch 7 / 20) train acc: 0.173000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 1.978922\n",
      "(Epoch 8 / 20) train acc: 0.211000; val_acc: 0.186111\n",
      "(Iteration 81 / 200) loss: 1.973015\n",
      "(Epoch 9 / 20) train acc: 0.210000; val_acc: 0.188889\n",
      "(Iteration 91 / 200) loss: 1.943329\n",
      "(Epoch 10 / 20) train acc: 0.245000; val_acc: 0.211111\n",
      "(Iteration 101 / 200) loss: 2.019082\n",
      "(Epoch 11 / 20) train acc: 0.186000; val_acc: 0.158333\n",
      "(Iteration 111 / 200) loss: 2.052540\n",
      "(Epoch 12 / 20) train acc: 0.187000; val_acc: 0.155556\n",
      "(Iteration 121 / 200) loss: 1.915457\n",
      "(Epoch 13 / 20) train acc: 0.217000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 1.966236\n",
      "(Epoch 14 / 20) train acc: 0.196000; val_acc: 0.180556\n",
      "(Iteration 141 / 200) loss: 1.942609\n",
      "(Epoch 15 / 20) train acc: 0.209000; val_acc: 0.186111\n",
      "(Iteration 151 / 200) loss: 1.934821\n",
      "(Epoch 16 / 20) train acc: 0.176000; val_acc: 0.177778\n",
      "(Iteration 161 / 200) loss: 1.938919\n",
      "(Epoch 17 / 20) train acc: 0.237000; val_acc: 0.205556\n",
      "(Iteration 171 / 200) loss: 1.988090\n",
      "(Epoch 18 / 20) train acc: 0.179000; val_acc: 0.177778\n",
      "(Iteration 181 / 200) loss: 1.957133\n",
      "(Epoch 19 / 20) train acc: 0.194000; val_acc: 0.183333\n",
      "(Iteration 191 / 200) loss: 1.843022\n",
      "(Epoch 20 / 20) train acc: 0.195000; val_acc: 0.186111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.303121\n",
      "(Epoch 2 / 20) train acc: 0.085000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302893\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.303439\n",
      "(Epoch 4 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.301975\n",
      "(Epoch 5 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302457\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302178\n",
      "(Epoch 7 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.301018\n",
      "(Epoch 8 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.303474\n",
      "(Epoch 9 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.301709\n",
      "(Epoch 10 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.303210\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.303550\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.299566\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.282394\n",
      "(Epoch 14 / 20) train acc: 0.144000; val_acc: 0.111111\n",
      "(Iteration 141 / 200) loss: 2.161897\n",
      "(Epoch 15 / 20) train acc: 0.185000; val_acc: 0.163889\n",
      "(Iteration 151 / 200) loss: 2.087407\n",
      "(Epoch 16 / 20) train acc: 0.194000; val_acc: 0.180556\n",
      "(Iteration 161 / 200) loss: 2.101054\n",
      "(Epoch 17 / 20) train acc: 0.197000; val_acc: 0.188889\n",
      "(Iteration 171 / 200) loss: 1.978581\n",
      "(Epoch 18 / 20) train acc: 0.202000; val_acc: 0.186111\n",
      "(Iteration 181 / 200) loss: 1.982687\n",
      "(Epoch 19 / 20) train acc: 0.201000; val_acc: 0.183333\n",
      "(Iteration 191 / 200) loss: 1.914039\n",
      "(Epoch 20 / 20) train acc: 0.191000; val_acc: 0.180556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.122000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302799\n",
      "(Epoch 2 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.301781\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302473\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302573\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.299736\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.301116\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.301626\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302494\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.300822\n",
      "(Epoch 10 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.304905\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.300480\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302352\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.306156\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.294328\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302409\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.299076\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.298985\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.300130\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.307108\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 39941.235375\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.048000; val_acc: 0.063889\n",
      "(Iteration 11 / 200) loss: 22550.895106\n",
      "(Epoch 2 / 20) train acc: 0.054000; val_acc: 0.058333\n",
      "(Iteration 21 / 200) loss: 17166.977792\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 14205.032067\n",
      "(Epoch 4 / 20) train acc: 0.149000; val_acc: 0.133333\n",
      "(Iteration 41 / 200) loss: 8429.054917\n",
      "(Epoch 5 / 20) train acc: 0.193000; val_acc: 0.180556\n",
      "(Iteration 51 / 200) loss: 8038.651046\n",
      "(Epoch 6 / 20) train acc: 0.271000; val_acc: 0.244444\n",
      "(Iteration 61 / 200) loss: 6865.096570\n",
      "(Epoch 7 / 20) train acc: 0.332000; val_acc: 0.325000\n",
      "(Iteration 71 / 200) loss: 4784.099549\n",
      "(Epoch 8 / 20) train acc: 0.437000; val_acc: 0.380556\n",
      "(Iteration 81 / 200) loss: 3819.430988\n",
      "(Epoch 9 / 20) train acc: 0.495000; val_acc: 0.452778\n",
      "(Iteration 91 / 200) loss: 2539.025309\n",
      "(Epoch 10 / 20) train acc: 0.545000; val_acc: 0.533333\n",
      "(Iteration 101 / 200) loss: 2745.069198\n",
      "(Epoch 11 / 20) train acc: 0.596000; val_acc: 0.561111\n",
      "(Iteration 111 / 200) loss: 1754.350281\n",
      "(Epoch 12 / 20) train acc: 0.651000; val_acc: 0.588889\n",
      "(Iteration 121 / 200) loss: 1493.135362\n",
      "(Epoch 13 / 20) train acc: 0.662000; val_acc: 0.611111\n",
      "(Iteration 131 / 200) loss: 1132.621227\n",
      "(Epoch 14 / 20) train acc: 0.672000; val_acc: 0.636111\n",
      "(Iteration 141 / 200) loss: 1711.311704\n",
      "(Epoch 15 / 20) train acc: 0.706000; val_acc: 0.663889\n",
      "(Iteration 151 / 200) loss: 933.885706\n",
      "(Epoch 16 / 20) train acc: 0.737000; val_acc: 0.672222\n",
      "(Iteration 161 / 200) loss: 558.377553\n",
      "(Epoch 17 / 20) train acc: 0.749000; val_acc: 0.686111\n",
      "(Iteration 171 / 200) loss: 880.903675\n",
      "(Epoch 18 / 20) train acc: 0.760000; val_acc: 0.700000\n",
      "(Iteration 181 / 200) loss: 464.920979\n",
      "(Epoch 19 / 20) train acc: 0.793000; val_acc: 0.716667\n",
      "(Iteration 191 / 200) loss: 832.743588\n",
      "(Epoch 20 / 20) train acc: 0.787000; val_acc: 0.747222\n",
      "(Iteration 1 / 200) loss: 3.854466\n",
      "(Epoch 0 / 20) train acc: 0.180000; val_acc: 0.213889\n",
      "(Epoch 1 / 20) train acc: 0.583000; val_acc: 0.577778\n",
      "(Iteration 11 / 200) loss: 1.577979\n",
      "(Epoch 2 / 20) train acc: 0.857000; val_acc: 0.822222\n",
      "(Iteration 21 / 200) loss: 0.680972\n",
      "(Epoch 3 / 20) train acc: 0.897000; val_acc: 0.863889\n",
      "(Iteration 31 / 200) loss: 0.565279\n",
      "(Epoch 4 / 20) train acc: 0.936000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 0.316790\n",
      "(Epoch 5 / 20) train acc: 0.954000; val_acc: 0.916667\n",
      "(Iteration 51 / 200) loss: 0.345168\n",
      "(Epoch 6 / 20) train acc: 0.966000; val_acc: 0.933333\n",
      "(Iteration 61 / 200) loss: 0.196883\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.941667\n",
      "(Iteration 71 / 200) loss: 0.292391\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.947222\n",
      "(Iteration 81 / 200) loss: 0.224637\n",
      "(Epoch 9 / 20) train acc: 0.979000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.193718\n",
      "(Epoch 10 / 20) train acc: 0.985000; val_acc: 0.950000\n",
      "(Iteration 101 / 200) loss: 0.194174\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.173999\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.150839\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.127922\n",
      "(Epoch 14 / 20) train acc: 0.999000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.122407\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.119499\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.118363\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.118944\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.110567\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.112752\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.303866\n",
      "(Epoch 0 / 20) train acc: 0.275000; val_acc: 0.277778\n",
      "(Epoch 1 / 20) train acc: 0.211000; val_acc: 0.183333\n",
      "(Iteration 11 / 200) loss: 2.286976\n",
      "(Epoch 2 / 20) train acc: 0.223000; val_acc: 0.200000\n",
      "(Iteration 21 / 200) loss: 2.081792\n",
      "(Epoch 3 / 20) train acc: 0.378000; val_acc: 0.358333\n",
      "(Iteration 31 / 200) loss: 1.524444\n",
      "(Epoch 4 / 20) train acc: 0.680000; val_acc: 0.691667\n",
      "(Iteration 41 / 200) loss: 0.889833\n",
      "(Epoch 5 / 20) train acc: 0.717000; val_acc: 0.736111\n",
      "(Iteration 51 / 200) loss: 0.568520\n",
      "(Epoch 6 / 20) train acc: 0.758000; val_acc: 0.752778\n",
      "(Iteration 61 / 200) loss: 0.792396\n",
      "(Epoch 7 / 20) train acc: 0.785000; val_acc: 0.791667\n",
      "(Iteration 71 / 200) loss: 0.430947\n",
      "(Epoch 8 / 20) train acc: 0.812000; val_acc: 0.808333\n",
      "(Iteration 81 / 200) loss: 0.682424\n",
      "(Epoch 9 / 20) train acc: 0.830000; val_acc: 0.827778\n",
      "(Iteration 91 / 200) loss: 0.632449\n",
      "(Epoch 10 / 20) train acc: 0.838000; val_acc: 0.827778\n",
      "(Iteration 101 / 200) loss: 0.439436\n",
      "(Epoch 11 / 20) train acc: 0.861000; val_acc: 0.838889\n",
      "(Iteration 111 / 200) loss: 0.357110\n",
      "(Epoch 12 / 20) train acc: 0.872000; val_acc: 0.852778\n",
      "(Iteration 121 / 200) loss: 0.434916\n",
      "(Epoch 13 / 20) train acc: 0.897000; val_acc: 0.858333\n",
      "(Iteration 131 / 200) loss: 0.303979\n",
      "(Epoch 14 / 20) train acc: 0.869000; val_acc: 0.819444\n",
      "(Iteration 141 / 200) loss: 0.497891\n",
      "(Epoch 15 / 20) train acc: 0.920000; val_acc: 0.866667\n",
      "(Iteration 151 / 200) loss: 0.388057\n",
      "(Epoch 16 / 20) train acc: 0.903000; val_acc: 0.891667\n",
      "(Iteration 161 / 200) loss: 0.284412\n",
      "(Epoch 17 / 20) train acc: 0.910000; val_acc: 0.877778\n",
      "(Iteration 171 / 200) loss: 0.350447\n",
      "(Epoch 18 / 20) train acc: 0.911000; val_acc: 0.891667\n",
      "(Iteration 181 / 200) loss: 0.315662\n",
      "(Epoch 19 / 20) train acc: 0.927000; val_acc: 0.875000\n",
      "(Iteration 191 / 200) loss: 0.259182\n",
      "(Epoch 20 / 20) train acc: 0.938000; val_acc: 0.897222\n",
      "(Iteration 1 / 200) loss: 2.302598\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302434\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302608\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.303204\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.301586\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.303389\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302408\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302790\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302912\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.299466\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.301929\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.300710\n",
      "(Epoch 12 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.301514\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.300308\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302993\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.300590\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.303980\n",
      "(Epoch 17 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.299263\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.298468\n",
      "(Epoch 19 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.298924\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302306\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.300823\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.249430\n",
      "(Epoch 4 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.159688\n",
      "(Epoch 5 / 20) train acc: 0.190000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 2.089983\n",
      "(Epoch 6 / 20) train acc: 0.191000; val_acc: 0.205556\n",
      "(Iteration 61 / 200) loss: 1.997109\n",
      "(Epoch 7 / 20) train acc: 0.181000; val_acc: 0.205556\n",
      "(Iteration 71 / 200) loss: 2.041208\n",
      "(Epoch 8 / 20) train acc: 0.213000; val_acc: 0.208333\n",
      "(Iteration 81 / 200) loss: 1.973975\n",
      "(Epoch 9 / 20) train acc: 0.192000; val_acc: 0.208333\n",
      "(Iteration 91 / 200) loss: 1.970387\n",
      "(Epoch 10 / 20) train acc: 0.183000; val_acc: 0.205556\n",
      "(Iteration 101 / 200) loss: 2.061368\n",
      "(Epoch 11 / 20) train acc: 0.187000; val_acc: 0.180556\n",
      "(Iteration 111 / 200) loss: 2.086477\n",
      "(Epoch 12 / 20) train acc: 0.218000; val_acc: 0.194444\n",
      "(Iteration 121 / 200) loss: 2.060431\n",
      "(Epoch 13 / 20) train acc: 0.205000; val_acc: 0.183333\n",
      "(Iteration 131 / 200) loss: 2.005493\n",
      "(Epoch 14 / 20) train acc: 0.221000; val_acc: 0.200000\n",
      "(Iteration 141 / 200) loss: 1.847050\n",
      "(Epoch 15 / 20) train acc: 0.210000; val_acc: 0.191667\n",
      "(Iteration 151 / 200) loss: 1.941583\n",
      "(Epoch 16 / 20) train acc: 0.218000; val_acc: 0.197222\n",
      "(Iteration 161 / 200) loss: 1.936576\n",
      "(Epoch 17 / 20) train acc: 0.213000; val_acc: 0.183333\n",
      "(Iteration 171 / 200) loss: 1.986766\n",
      "(Epoch 18 / 20) train acc: 0.205000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 1.874670\n",
      "(Epoch 19 / 20) train acc: 0.227000; val_acc: 0.186111\n",
      "(Iteration 191 / 200) loss: 1.953409\n",
      "(Epoch 20 / 20) train acc: 0.281000; val_acc: 0.280556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.304558\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.301939\n",
      "(Epoch 3 / 20) train acc: 0.133000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.301015\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.279537\n",
      "(Epoch 5 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.130331\n",
      "(Epoch 6 / 20) train acc: 0.205000; val_acc: 0.180556\n",
      "(Iteration 61 / 200) loss: 1.914657\n",
      "(Epoch 7 / 20) train acc: 0.205000; val_acc: 0.177778\n",
      "(Iteration 71 / 200) loss: 2.016976\n",
      "(Epoch 8 / 20) train acc: 0.246000; val_acc: 0.183333\n",
      "(Iteration 81 / 200) loss: 2.024795\n",
      "(Epoch 9 / 20) train acc: 0.221000; val_acc: 0.188889\n",
      "(Iteration 91 / 200) loss: 1.832522\n",
      "(Epoch 10 / 20) train acc: 0.280000; val_acc: 0.213889\n",
      "(Iteration 101 / 200) loss: 1.931752\n",
      "(Epoch 11 / 20) train acc: 0.223000; val_acc: 0.205556\n",
      "(Iteration 111 / 200) loss: 1.927043\n",
      "(Epoch 12 / 20) train acc: 0.260000; val_acc: 0.255556\n",
      "(Iteration 121 / 200) loss: 1.924586\n",
      "(Epoch 13 / 20) train acc: 0.275000; val_acc: 0.233333\n",
      "(Iteration 131 / 200) loss: 1.984805\n",
      "(Epoch 14 / 20) train acc: 0.306000; val_acc: 0.308333\n",
      "(Iteration 141 / 200) loss: 1.838530\n",
      "(Epoch 15 / 20) train acc: 0.359000; val_acc: 0.322222\n",
      "(Iteration 151 / 200) loss: 1.768704\n",
      "(Epoch 16 / 20) train acc: 0.349000; val_acc: 0.338889\n",
      "(Iteration 161 / 200) loss: 1.833674\n",
      "(Epoch 17 / 20) train acc: 0.401000; val_acc: 0.341667\n",
      "(Iteration 171 / 200) loss: 1.771519\n",
      "(Epoch 18 / 20) train acc: 0.395000; val_acc: 0.361111\n",
      "(Iteration 181 / 200) loss: 1.682312\n",
      "(Epoch 19 / 20) train acc: 0.360000; val_acc: 0.338889\n",
      "(Iteration 191 / 200) loss: 1.757491\n",
      "(Epoch 20 / 20) train acc: 0.403000; val_acc: 0.361111\n",
      "(Iteration 1 / 200) loss: 40790.517834\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.133333\n",
      "(Iteration 11 / 200) loss: 23823.496667\n",
      "(Epoch 2 / 20) train acc: 0.139000; val_acc: 0.147222\n",
      "(Iteration 21 / 200) loss: 17878.758723\n",
      "(Epoch 3 / 20) train acc: 0.171000; val_acc: 0.191667\n",
      "(Iteration 31 / 200) loss: 13150.388988\n",
      "(Epoch 4 / 20) train acc: 0.226000; val_acc: 0.244444\n",
      "(Iteration 41 / 200) loss: 8344.485127\n",
      "(Epoch 5 / 20) train acc: 0.335000; val_acc: 0.283333\n",
      "(Iteration 51 / 200) loss: 6739.336891\n",
      "(Epoch 6 / 20) train acc: 0.405000; val_acc: 0.361111\n",
      "(Iteration 61 / 200) loss: 3846.500573\n",
      "(Epoch 7 / 20) train acc: 0.456000; val_acc: 0.408333\n",
      "(Iteration 71 / 200) loss: 3487.734131\n",
      "(Epoch 8 / 20) train acc: 0.518000; val_acc: 0.475000\n",
      "(Iteration 81 / 200) loss: 2156.387682\n",
      "(Epoch 9 / 20) train acc: 0.563000; val_acc: 0.536111\n",
      "(Iteration 91 / 200) loss: 2525.567935\n",
      "(Epoch 10 / 20) train acc: 0.611000; val_acc: 0.586111\n",
      "(Iteration 101 / 200) loss: 1833.685278\n",
      "(Epoch 11 / 20) train acc: 0.700000; val_acc: 0.633333\n",
      "(Iteration 111 / 200) loss: 2097.895807\n",
      "(Epoch 12 / 20) train acc: 0.704000; val_acc: 0.680556\n",
      "(Iteration 121 / 200) loss: 1443.687224\n",
      "(Epoch 13 / 20) train acc: 0.717000; val_acc: 0.688889\n",
      "(Iteration 131 / 200) loss: 1492.253071\n",
      "(Epoch 14 / 20) train acc: 0.739000; val_acc: 0.697222\n",
      "(Iteration 141 / 200) loss: 880.903338\n",
      "(Epoch 15 / 20) train acc: 0.763000; val_acc: 0.716667\n",
      "(Iteration 151 / 200) loss: 630.823431\n",
      "(Epoch 16 / 20) train acc: 0.767000; val_acc: 0.730556\n",
      "(Iteration 161 / 200) loss: 579.321771\n",
      "(Epoch 17 / 20) train acc: 0.809000; val_acc: 0.750000\n",
      "(Iteration 171 / 200) loss: 699.463368\n",
      "(Epoch 18 / 20) train acc: 0.814000; val_acc: 0.775000\n",
      "(Iteration 181 / 200) loss: 646.759536\n",
      "(Epoch 19 / 20) train acc: 0.842000; val_acc: 0.777778\n",
      "(Iteration 191 / 200) loss: 472.638513\n",
      "(Epoch 20 / 20) train acc: 0.844000; val_acc: 0.788889\n",
      "(Iteration 1 / 200) loss: 4.505148\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.505000; val_acc: 0.452778\n",
      "(Iteration 11 / 200) loss: 1.581652\n",
      "(Epoch 2 / 20) train acc: 0.833000; val_acc: 0.777778\n",
      "(Iteration 21 / 200) loss: 0.955557\n",
      "(Epoch 3 / 20) train acc: 0.923000; val_acc: 0.863889\n",
      "(Iteration 31 / 200) loss: 0.658863\n",
      "(Epoch 4 / 20) train acc: 0.944000; val_acc: 0.902778\n",
      "(Iteration 41 / 200) loss: 0.399663\n",
      "(Epoch 5 / 20) train acc: 0.962000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.243920\n",
      "(Epoch 6 / 20) train acc: 0.977000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 0.244550\n",
      "(Epoch 7 / 20) train acc: 0.984000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.225645\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.177863\n",
      "(Epoch 9 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.176030\n",
      "(Epoch 10 / 20) train acc: 0.988000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.222486\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.156792\n",
      "(Epoch 12 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.140355\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.122998\n",
      "(Epoch 14 / 20) train acc: 0.996000; val_acc: 0.986111\n",
      "(Iteration 141 / 200) loss: 0.128082\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.131231\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 161 / 200) loss: 0.119142\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.118086\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.117165\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.115626\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.983333\n",
      "(Iteration 1 / 200) loss: 2.303901\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.343000; val_acc: 0.322222\n",
      "(Iteration 11 / 200) loss: 2.288167\n",
      "(Epoch 2 / 20) train acc: 0.402000; val_acc: 0.400000\n",
      "(Iteration 21 / 200) loss: 2.045758\n",
      "(Epoch 3 / 20) train acc: 0.459000; val_acc: 0.497222\n",
      "(Iteration 31 / 200) loss: 1.241203\n",
      "(Epoch 4 / 20) train acc: 0.645000; val_acc: 0.677778\n",
      "(Iteration 41 / 200) loss: 1.155636\n",
      "(Epoch 5 / 20) train acc: 0.558000; val_acc: 0.588889\n",
      "(Iteration 51 / 200) loss: 0.989165\n",
      "(Epoch 6 / 20) train acc: 0.626000; val_acc: 0.641667\n",
      "(Iteration 61 / 200) loss: 0.958768\n",
      "(Epoch 7 / 20) train acc: 0.663000; val_acc: 0.683333\n",
      "(Iteration 71 / 200) loss: 0.917261\n",
      "(Epoch 8 / 20) train acc: 0.684000; val_acc: 0.663889\n",
      "(Iteration 81 / 200) loss: 0.778267\n",
      "(Epoch 9 / 20) train acc: 0.747000; val_acc: 0.741667\n",
      "(Iteration 91 / 200) loss: 0.763424\n",
      "(Epoch 10 / 20) train acc: 0.761000; val_acc: 0.763889\n",
      "(Iteration 101 / 200) loss: 0.519607\n",
      "(Epoch 11 / 20) train acc: 0.802000; val_acc: 0.788889\n",
      "(Iteration 111 / 200) loss: 0.547256\n",
      "(Epoch 12 / 20) train acc: 0.825000; val_acc: 0.800000\n",
      "(Iteration 121 / 200) loss: 0.492266\n",
      "(Epoch 13 / 20) train acc: 0.847000; val_acc: 0.816667\n",
      "(Iteration 131 / 200) loss: 0.369698\n",
      "(Epoch 14 / 20) train acc: 0.864000; val_acc: 0.847222\n",
      "(Iteration 141 / 200) loss: 0.328860\n",
      "(Epoch 15 / 20) train acc: 0.883000; val_acc: 0.861111\n",
      "(Iteration 151 / 200) loss: 0.376107\n",
      "(Epoch 16 / 20) train acc: 0.896000; val_acc: 0.877778\n",
      "(Iteration 161 / 200) loss: 0.397778\n",
      "(Epoch 17 / 20) train acc: 0.885000; val_acc: 0.869444\n",
      "(Iteration 171 / 200) loss: 0.247563\n",
      "(Epoch 18 / 20) train acc: 0.898000; val_acc: 0.863889\n",
      "(Iteration 181 / 200) loss: 0.348964\n",
      "(Epoch 19 / 20) train acc: 0.931000; val_acc: 0.908333\n",
      "(Iteration 191 / 200) loss: 0.200838\n",
      "(Epoch 20 / 20) train acc: 0.915000; val_acc: 0.911111\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.301773\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303119\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302402\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.303664\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302172\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.301749\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302088\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.303930\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.300300\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.300588\n",
      "(Epoch 11 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.304416\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.300763\n",
      "(Epoch 13 / 20) train acc: 0.083000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.299532\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.300932\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.304508\n",
      "(Epoch 16 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.299578\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.294063\n",
      "(Epoch 18 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.299371\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.312293\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.082000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302711\n",
      "(Epoch 2 / 20) train acc: 0.087000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.303461\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.303244\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.303302\n",
      "(Epoch 5 / 20) train acc: 0.165000; val_acc: 0.161111\n",
      "(Iteration 51 / 200) loss: 2.301965\n",
      "(Epoch 6 / 20) train acc: 0.149000; val_acc: 0.141667\n",
      "(Iteration 61 / 200) loss: 2.299261\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.295337\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.286818\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.270595\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.111111\n",
      "(Iteration 101 / 200) loss: 2.267174\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.253101\n",
      "(Epoch 12 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.238000\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.223384\n",
      "(Epoch 14 / 20) train acc: 0.162000; val_acc: 0.177778\n",
      "(Iteration 141 / 200) loss: 2.226680\n",
      "(Epoch 15 / 20) train acc: 0.196000; val_acc: 0.188889\n",
      "(Iteration 151 / 200) loss: 2.213382\n",
      "(Epoch 16 / 20) train acc: 0.205000; val_acc: 0.188889\n",
      "(Iteration 161 / 200) loss: 2.148635\n",
      "(Epoch 17 / 20) train acc: 0.206000; val_acc: 0.175000\n",
      "(Iteration 171 / 200) loss: 2.181024\n",
      "(Epoch 18 / 20) train acc: 0.188000; val_acc: 0.175000\n",
      "(Iteration 181 / 200) loss: 2.143440\n",
      "(Epoch 19 / 20) train acc: 0.212000; val_acc: 0.175000\n",
      "(Iteration 191 / 200) loss: 2.136811\n",
      "(Epoch 20 / 20) train acc: 0.211000; val_acc: 0.175000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302452\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.301021\n",
      "(Epoch 3 / 20) train acc: 0.085000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.303040\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.294388\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.207035\n",
      "(Epoch 6 / 20) train acc: 0.142000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.004945\n",
      "(Epoch 7 / 20) train acc: 0.195000; val_acc: 0.125000\n",
      "(Iteration 71 / 200) loss: 1.843348\n",
      "(Epoch 8 / 20) train acc: 0.177000; val_acc: 0.158333\n",
      "(Iteration 81 / 200) loss: 2.116849\n",
      "(Epoch 9 / 20) train acc: 0.216000; val_acc: 0.158333\n",
      "(Iteration 91 / 200) loss: 1.991028\n",
      "(Epoch 10 / 20) train acc: 0.210000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 2.024726\n",
      "(Epoch 11 / 20) train acc: 0.226000; val_acc: 0.158333\n",
      "(Iteration 111 / 200) loss: 1.890815\n",
      "(Epoch 12 / 20) train acc: 0.234000; val_acc: 0.161111\n",
      "(Iteration 121 / 200) loss: 1.976567\n",
      "(Epoch 13 / 20) train acc: 0.195000; val_acc: 0.158333\n",
      "(Iteration 131 / 200) loss: 1.941228\n",
      "(Epoch 14 / 20) train acc: 0.220000; val_acc: 0.161111\n",
      "(Iteration 141 / 200) loss: 1.899948\n",
      "(Epoch 15 / 20) train acc: 0.204000; val_acc: 0.158333\n",
      "(Iteration 151 / 200) loss: 1.991773\n",
      "(Epoch 16 / 20) train acc: 0.197000; val_acc: 0.161111\n",
      "(Iteration 161 / 200) loss: 1.930483\n",
      "(Epoch 17 / 20) train acc: 0.206000; val_acc: 0.161111\n",
      "(Iteration 171 / 200) loss: 1.990956\n",
      "(Epoch 18 / 20) train acc: 0.199000; val_acc: 0.158333\n",
      "(Iteration 181 / 200) loss: 1.907224\n",
      "(Epoch 19 / 20) train acc: 0.208000; val_acc: 0.161111\n",
      "(Iteration 191 / 200) loss: 1.912612\n",
      "(Epoch 20 / 20) train acc: 0.198000; val_acc: 0.161111\n",
      "(Iteration 1 / 200) loss: 42644.817578\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 28240.051500\n",
      "(Epoch 2 / 20) train acc: 0.072000; val_acc: 0.111111\n",
      "(Iteration 21 / 200) loss: 22750.926291\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 14929.902585\n",
      "(Epoch 4 / 20) train acc: 0.146000; val_acc: 0.169444\n",
      "(Iteration 41 / 200) loss: 10125.840312\n",
      "(Epoch 5 / 20) train acc: 0.205000; val_acc: 0.236111\n",
      "(Iteration 51 / 200) loss: 8539.040087\n",
      "(Epoch 6 / 20) train acc: 0.292000; val_acc: 0.291667\n",
      "(Iteration 61 / 200) loss: 6266.396220\n",
      "(Epoch 7 / 20) train acc: 0.364000; val_acc: 0.322222\n",
      "(Iteration 71 / 200) loss: 4381.553693\n",
      "(Epoch 8 / 20) train acc: 0.418000; val_acc: 0.394444\n",
      "(Iteration 81 / 200) loss: 3665.794347\n",
      "(Epoch 9 / 20) train acc: 0.468000; val_acc: 0.466667\n",
      "(Iteration 91 / 200) loss: 3823.208783\n",
      "(Epoch 10 / 20) train acc: 0.536000; val_acc: 0.522222\n",
      "(Iteration 101 / 200) loss: 1731.695742\n",
      "(Epoch 11 / 20) train acc: 0.606000; val_acc: 0.558333\n",
      "(Iteration 111 / 200) loss: 1933.658065\n",
      "(Epoch 12 / 20) train acc: 0.626000; val_acc: 0.577778\n",
      "(Iteration 121 / 200) loss: 1714.419819\n",
      "(Epoch 13 / 20) train acc: 0.669000; val_acc: 0.588889\n",
      "(Iteration 131 / 200) loss: 1494.815825\n",
      "(Epoch 14 / 20) train acc: 0.717000; val_acc: 0.616667\n",
      "(Iteration 141 / 200) loss: 769.472605\n",
      "(Epoch 15 / 20) train acc: 0.721000; val_acc: 0.625000\n",
      "(Iteration 151 / 200) loss: 1467.497979\n",
      "(Epoch 16 / 20) train acc: 0.747000; val_acc: 0.641667\n",
      "(Iteration 161 / 200) loss: 1508.401590\n",
      "(Epoch 17 / 20) train acc: 0.755000; val_acc: 0.666667\n",
      "(Iteration 171 / 200) loss: 765.173061\n",
      "(Epoch 18 / 20) train acc: 0.772000; val_acc: 0.672222\n",
      "(Iteration 181 / 200) loss: 963.329738\n",
      "(Epoch 19 / 20) train acc: 0.795000; val_acc: 0.705556\n",
      "(Iteration 191 / 200) loss: 872.357228\n",
      "(Epoch 20 / 20) train acc: 0.831000; val_acc: 0.708333\n",
      "(Iteration 1 / 200) loss: 4.141497\n",
      "(Epoch 0 / 20) train acc: 0.131000; val_acc: 0.144444\n",
      "(Epoch 1 / 20) train acc: 0.560000; val_acc: 0.583333\n",
      "(Iteration 11 / 200) loss: 1.379968\n",
      "(Epoch 2 / 20) train acc: 0.839000; val_acc: 0.850000\n",
      "(Iteration 21 / 200) loss: 0.635314\n",
      "(Epoch 3 / 20) train acc: 0.906000; val_acc: 0.897222\n",
      "(Iteration 31 / 200) loss: 0.356019\n",
      "(Epoch 4 / 20) train acc: 0.935000; val_acc: 0.916667\n",
      "(Iteration 41 / 200) loss: 0.293167\n",
      "(Epoch 5 / 20) train acc: 0.932000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 0.191388\n",
      "(Epoch 6 / 20) train acc: 0.969000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.109526\n",
      "(Epoch 7 / 20) train acc: 0.968000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.117244\n",
      "(Epoch 8 / 20) train acc: 0.978000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.073131\n",
      "(Epoch 9 / 20) train acc: 0.983000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.065775\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.057885\n",
      "(Epoch 11 / 20) train acc: 0.985000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.066075\n",
      "(Epoch 12 / 20) train acc: 0.990000; val_acc: 0.972222\n",
      "(Iteration 121 / 200) loss: 0.046361\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.087429\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.047854\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.988889\n",
      "(Iteration 151 / 200) loss: 0.023673\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.028116\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.032132\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.021345\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.051362\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302738\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.230000; val_acc: 0.225000\n",
      "(Iteration 11 / 200) loss: 2.285855\n",
      "(Epoch 2 / 20) train acc: 0.308000; val_acc: 0.316667\n",
      "(Iteration 21 / 200) loss: 1.993734\n",
      "(Epoch 3 / 20) train acc: 0.523000; val_acc: 0.588889\n",
      "(Iteration 31 / 200) loss: 1.437473\n",
      "(Epoch 4 / 20) train acc: 0.646000; val_acc: 0.663889\n",
      "(Iteration 41 / 200) loss: 1.139472\n",
      "(Epoch 5 / 20) train acc: 0.742000; val_acc: 0.725000\n",
      "(Iteration 51 / 200) loss: 0.820771\n",
      "(Epoch 6 / 20) train acc: 0.773000; val_acc: 0.755556\n",
      "(Iteration 61 / 200) loss: 0.808899\n",
      "(Epoch 7 / 20) train acc: 0.786000; val_acc: 0.783333\n",
      "(Iteration 71 / 200) loss: 0.573868\n",
      "(Epoch 8 / 20) train acc: 0.830000; val_acc: 0.788889\n",
      "(Iteration 81 / 200) loss: 0.581700\n",
      "(Epoch 9 / 20) train acc: 0.847000; val_acc: 0.813889\n",
      "(Iteration 91 / 200) loss: 0.596415\n",
      "(Epoch 10 / 20) train acc: 0.847000; val_acc: 0.833333\n",
      "(Iteration 101 / 200) loss: 0.507451\n",
      "(Epoch 11 / 20) train acc: 0.891000; val_acc: 0.861111\n",
      "(Iteration 111 / 200) loss: 0.402855\n",
      "(Epoch 12 / 20) train acc: 0.868000; val_acc: 0.838889\n",
      "(Iteration 121 / 200) loss: 0.402456\n",
      "(Epoch 13 / 20) train acc: 0.857000; val_acc: 0.875000\n",
      "(Iteration 131 / 200) loss: 0.335053\n",
      "(Epoch 14 / 20) train acc: 0.878000; val_acc: 0.877778\n",
      "(Iteration 141 / 200) loss: 0.409441\n",
      "(Epoch 15 / 20) train acc: 0.909000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 0.217111\n",
      "(Epoch 16 / 20) train acc: 0.902000; val_acc: 0.916667\n",
      "(Iteration 161 / 200) loss: 0.247799\n",
      "(Epoch 17 / 20) train acc: 0.937000; val_acc: 0.916667\n",
      "(Iteration 171 / 200) loss: 0.238495\n",
      "(Epoch 18 / 20) train acc: 0.928000; val_acc: 0.908333\n",
      "(Iteration 181 / 200) loss: 0.154148\n",
      "(Epoch 19 / 20) train acc: 0.944000; val_acc: 0.902778\n",
      "(Iteration 191 / 200) loss: 0.148929\n",
      "(Epoch 20 / 20) train acc: 0.925000; val_acc: 0.902778\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302025\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.255285\n",
      "(Epoch 3 / 20) train acc: 0.133000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 2.082751\n",
      "(Epoch 4 / 20) train acc: 0.185000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 1.992472\n",
      "(Epoch 5 / 20) train acc: 0.177000; val_acc: 0.213889\n",
      "(Iteration 51 / 200) loss: 1.905088\n",
      "(Epoch 6 / 20) train acc: 0.197000; val_acc: 0.230556\n",
      "(Iteration 61 / 200) loss: 1.900024\n",
      "(Epoch 7 / 20) train acc: 0.244000; val_acc: 0.280556\n",
      "(Iteration 71 / 200) loss: 1.718454\n",
      "(Epoch 8 / 20) train acc: 0.362000; val_acc: 0.361111\n",
      "(Iteration 81 / 200) loss: 1.490998\n",
      "(Epoch 9 / 20) train acc: 0.404000; val_acc: 0.402778\n",
      "(Iteration 91 / 200) loss: 1.629223\n",
      "(Epoch 10 / 20) train acc: 0.426000; val_acc: 0.375000\n",
      "(Iteration 101 / 200) loss: 1.495844\n",
      "(Epoch 11 / 20) train acc: 0.405000; val_acc: 0.383333\n",
      "(Iteration 111 / 200) loss: 1.552810\n",
      "(Epoch 12 / 20) train acc: 0.430000; val_acc: 0.402778\n",
      "(Iteration 121 / 200) loss: 1.222689\n",
      "(Epoch 13 / 20) train acc: 0.467000; val_acc: 0.425000\n",
      "(Iteration 131 / 200) loss: 1.396465\n",
      "(Epoch 14 / 20) train acc: 0.459000; val_acc: 0.430556\n",
      "(Iteration 141 / 200) loss: 1.236510\n",
      "(Epoch 15 / 20) train acc: 0.480000; val_acc: 0.444444\n",
      "(Iteration 151 / 200) loss: 1.204140\n",
      "(Epoch 16 / 20) train acc: 0.481000; val_acc: 0.477778\n",
      "(Iteration 161 / 200) loss: 1.270037\n",
      "(Epoch 17 / 20) train acc: 0.464000; val_acc: 0.419444\n",
      "(Iteration 171 / 200) loss: 1.224189\n",
      "(Epoch 18 / 20) train acc: 0.486000; val_acc: 0.461111\n",
      "(Iteration 181 / 200) loss: 1.197586\n",
      "(Epoch 19 / 20) train acc: 0.498000; val_acc: 0.483333\n",
      "(Iteration 191 / 200) loss: 1.292448\n",
      "(Epoch 20 / 20) train acc: 0.517000; val_acc: 0.494444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302197\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.301785\n",
      "(Epoch 3 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301847\n",
      "(Epoch 4 / 20) train acc: 0.125000; val_acc: 0.150000\n",
      "(Iteration 41 / 200) loss: 2.301996\n",
      "(Epoch 5 / 20) train acc: 0.138000; val_acc: 0.158333\n",
      "(Iteration 51 / 200) loss: 2.289069\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.102778\n",
      "(Iteration 61 / 200) loss: 2.223616\n",
      "(Epoch 7 / 20) train acc: 0.127000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.110993\n",
      "(Epoch 8 / 20) train acc: 0.216000; val_acc: 0.191667\n",
      "(Iteration 81 / 200) loss: 2.051344\n",
      "(Epoch 9 / 20) train acc: 0.214000; val_acc: 0.177778\n",
      "(Iteration 91 / 200) loss: 1.999305\n",
      "(Epoch 10 / 20) train acc: 0.242000; val_acc: 0.233333\n",
      "(Iteration 101 / 200) loss: 2.101448\n",
      "(Epoch 11 / 20) train acc: 0.212000; val_acc: 0.191667\n",
      "(Iteration 111 / 200) loss: 1.901549\n",
      "(Epoch 12 / 20) train acc: 0.232000; val_acc: 0.213889\n",
      "(Iteration 121 / 200) loss: 2.094027\n",
      "(Epoch 13 / 20) train acc: 0.250000; val_acc: 0.244444\n",
      "(Iteration 131 / 200) loss: 1.972851\n",
      "(Epoch 14 / 20) train acc: 0.256000; val_acc: 0.250000\n",
      "(Iteration 141 / 200) loss: 2.022383\n",
      "(Epoch 15 / 20) train acc: 0.287000; val_acc: 0.252778\n",
      "(Iteration 151 / 200) loss: 1.868256\n",
      "(Epoch 16 / 20) train acc: 0.275000; val_acc: 0.263889\n",
      "(Iteration 161 / 200) loss: 1.897213\n",
      "(Epoch 17 / 20) train acc: 0.295000; val_acc: 0.280556\n",
      "(Iteration 171 / 200) loss: 1.829093\n",
      "(Epoch 18 / 20) train acc: 0.334000; val_acc: 0.283333\n",
      "(Iteration 181 / 200) loss: 1.870550\n",
      "(Epoch 19 / 20) train acc: 0.331000; val_acc: 0.297222\n",
      "(Iteration 191 / 200) loss: 1.738603\n",
      "(Epoch 20 / 20) train acc: 0.341000; val_acc: 0.300000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302409\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302428\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.300982\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302478\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302537\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302426\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302559\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.299622\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.301354\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.300722\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.301794\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.301089\n",
      "(Epoch 13 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.301804\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.307168\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301595\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.296870\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.300511\n",
      "(Epoch 18 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302507\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.298524\n",
      "(Epoch 20 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 26916.335850\n",
      "(Epoch 0 / 20) train acc: 0.210000; val_acc: 0.216667\n",
      "(Epoch 1 / 20) train acc: 0.182000; val_acc: 0.238889\n",
      "(Iteration 11 / 200) loss: 18699.483253\n",
      "(Epoch 2 / 20) train acc: 0.180000; val_acc: 0.244444\n",
      "(Iteration 21 / 200) loss: 10231.106266\n",
      "(Epoch 3 / 20) train acc: 0.249000; val_acc: 0.308333\n",
      "(Iteration 31 / 200) loss: 8227.052179\n",
      "(Epoch 4 / 20) train acc: 0.361000; val_acc: 0.386111\n",
      "(Iteration 41 / 200) loss: 5155.826698\n",
      "(Epoch 5 / 20) train acc: 0.414000; val_acc: 0.433333\n",
      "(Iteration 51 / 200) loss: 4290.349494\n",
      "(Epoch 6 / 20) train acc: 0.487000; val_acc: 0.505556\n",
      "(Iteration 61 / 200) loss: 2902.287061\n",
      "(Epoch 7 / 20) train acc: 0.578000; val_acc: 0.558333\n",
      "(Iteration 71 / 200) loss: 3013.656320\n",
      "(Epoch 8 / 20) train acc: 0.625000; val_acc: 0.605556\n",
      "(Iteration 81 / 200) loss: 1862.571797\n",
      "(Epoch 9 / 20) train acc: 0.682000; val_acc: 0.647222\n",
      "(Iteration 91 / 200) loss: 1390.852366\n",
      "(Epoch 10 / 20) train acc: 0.715000; val_acc: 0.702778\n",
      "(Iteration 101 / 200) loss: 1546.852890\n",
      "(Epoch 11 / 20) train acc: 0.735000; val_acc: 0.730556\n",
      "(Iteration 111 / 200) loss: 1090.094185\n",
      "(Epoch 12 / 20) train acc: 0.743000; val_acc: 0.741667\n",
      "(Iteration 121 / 200) loss: 659.815214\n",
      "(Epoch 13 / 20) train acc: 0.756000; val_acc: 0.763889\n",
      "(Iteration 131 / 200) loss: 330.093624\n",
      "(Epoch 14 / 20) train acc: 0.779000; val_acc: 0.777778\n",
      "(Iteration 141 / 200) loss: 1383.535615\n",
      "(Epoch 15 / 20) train acc: 0.793000; val_acc: 0.791667\n",
      "(Iteration 151 / 200) loss: 715.562278\n",
      "(Epoch 16 / 20) train acc: 0.818000; val_acc: 0.813889\n",
      "(Iteration 161 / 200) loss: 1003.258986\n",
      "(Epoch 17 / 20) train acc: 0.835000; val_acc: 0.811111\n",
      "(Iteration 171 / 200) loss: 604.981045\n",
      "(Epoch 18 / 20) train acc: 0.843000; val_acc: 0.811111\n",
      "(Iteration 181 / 200) loss: 421.341139\n",
      "(Epoch 19 / 20) train acc: 0.853000; val_acc: 0.819444\n",
      "(Iteration 191 / 200) loss: 614.326947\n",
      "(Epoch 20 / 20) train acc: 0.869000; val_acc: 0.833333\n",
      "(Iteration 1 / 200) loss: 3.708493\n",
      "(Epoch 0 / 20) train acc: 0.073000; val_acc: 0.052778\n",
      "(Epoch 1 / 20) train acc: 0.640000; val_acc: 0.550000\n",
      "(Iteration 11 / 200) loss: 1.438327\n",
      "(Epoch 2 / 20) train acc: 0.863000; val_acc: 0.816667\n",
      "(Iteration 21 / 200) loss: 0.478223\n",
      "(Epoch 3 / 20) train acc: 0.908000; val_acc: 0.888889\n",
      "(Iteration 31 / 200) loss: 0.340873\n",
      "(Epoch 4 / 20) train acc: 0.945000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 0.202066\n",
      "(Epoch 5 / 20) train acc: 0.939000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.122966\n",
      "(Epoch 6 / 20) train acc: 0.961000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.114888\n",
      "(Epoch 7 / 20) train acc: 0.983000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.176588\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.076717\n",
      "(Epoch 9 / 20) train acc: 0.993000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.049862\n",
      "(Epoch 10 / 20) train acc: 0.995000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.056563\n",
      "(Epoch 11 / 20) train acc: 0.997000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.032731\n",
      "(Epoch 12 / 20) train acc: 0.997000; val_acc: 0.983333\n",
      "(Iteration 121 / 200) loss: 0.036198\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.033913\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.023998\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.027594\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 161 / 200) loss: 0.029283\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.023463\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 181 / 200) loss: 0.028264\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.020707\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302715\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.180000; val_acc: 0.172222\n",
      "(Iteration 11 / 200) loss: 2.280602\n",
      "(Epoch 2 / 20) train acc: 0.287000; val_acc: 0.283333\n",
      "(Iteration 21 / 200) loss: 1.999431\n",
      "(Epoch 3 / 20) train acc: 0.474000; val_acc: 0.475000\n",
      "(Iteration 31 / 200) loss: 1.565940\n",
      "(Epoch 4 / 20) train acc: 0.625000; val_acc: 0.655556\n",
      "(Iteration 41 / 200) loss: 1.271699\n",
      "(Epoch 5 / 20) train acc: 0.716000; val_acc: 0.686111\n",
      "(Iteration 51 / 200) loss: 0.769670\n",
      "(Epoch 6 / 20) train acc: 0.727000; val_acc: 0.711111\n",
      "(Iteration 61 / 200) loss: 0.816170\n",
      "(Epoch 7 / 20) train acc: 0.793000; val_acc: 0.777778\n",
      "(Iteration 71 / 200) loss: 0.534878\n",
      "(Epoch 8 / 20) train acc: 0.781000; val_acc: 0.788889\n",
      "(Iteration 81 / 200) loss: 0.568356\n",
      "(Epoch 9 / 20) train acc: 0.787000; val_acc: 0.766667\n",
      "(Iteration 91 / 200) loss: 0.502372\n",
      "(Epoch 10 / 20) train acc: 0.780000; val_acc: 0.788889\n",
      "(Iteration 101 / 200) loss: 0.625829\n",
      "(Epoch 11 / 20) train acc: 0.854000; val_acc: 0.822222\n",
      "(Iteration 111 / 200) loss: 0.353850\n",
      "(Epoch 12 / 20) train acc: 0.846000; val_acc: 0.813889\n",
      "(Iteration 121 / 200) loss: 0.338306\n",
      "(Epoch 13 / 20) train acc: 0.864000; val_acc: 0.822222\n",
      "(Iteration 131 / 200) loss: 0.373443\n",
      "(Epoch 14 / 20) train acc: 0.860000; val_acc: 0.844444\n",
      "(Iteration 141 / 200) loss: 0.282765\n",
      "(Epoch 15 / 20) train acc: 0.881000; val_acc: 0.875000\n",
      "(Iteration 151 / 200) loss: 0.279207\n",
      "(Epoch 16 / 20) train acc: 0.884000; val_acc: 0.872222\n",
      "(Iteration 161 / 200) loss: 0.384554\n",
      "(Epoch 17 / 20) train acc: 0.896000; val_acc: 0.861111\n",
      "(Iteration 171 / 200) loss: 0.328420\n",
      "(Epoch 18 / 20) train acc: 0.925000; val_acc: 0.883333\n",
      "(Iteration 181 / 200) loss: 0.271563\n",
      "(Epoch 19 / 20) train acc: 0.872000; val_acc: 0.866667\n",
      "(Iteration 191 / 200) loss: 0.305346\n",
      "(Epoch 20 / 20) train acc: 0.901000; val_acc: 0.863889\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302225\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.296907\n",
      "(Epoch 3 / 20) train acc: 0.148000; val_acc: 0.138889\n",
      "(Iteration 31 / 200) loss: 2.222750\n",
      "(Epoch 4 / 20) train acc: 0.198000; val_acc: 0.163889\n",
      "(Iteration 41 / 200) loss: 2.137651\n",
      "(Epoch 5 / 20) train acc: 0.200000; val_acc: 0.169444\n",
      "(Iteration 51 / 200) loss: 1.868206\n",
      "(Epoch 6 / 20) train acc: 0.271000; val_acc: 0.213889\n",
      "(Iteration 61 / 200) loss: 1.816268\n",
      "(Epoch 7 / 20) train acc: 0.253000; val_acc: 0.291667\n",
      "(Iteration 71 / 200) loss: 1.628931\n",
      "(Epoch 8 / 20) train acc: 0.277000; val_acc: 0.255556\n",
      "(Iteration 81 / 200) loss: 1.549165\n",
      "(Epoch 9 / 20) train acc: 0.335000; val_acc: 0.300000\n",
      "(Iteration 91 / 200) loss: 1.519396\n",
      "(Epoch 10 / 20) train acc: 0.334000; val_acc: 0.330556\n",
      "(Iteration 101 / 200) loss: 1.608283\n",
      "(Epoch 11 / 20) train acc: 0.366000; val_acc: 0.369444\n",
      "(Iteration 111 / 200) loss: 1.454613\n",
      "(Epoch 12 / 20) train acc: 0.382000; val_acc: 0.386111\n",
      "(Iteration 121 / 200) loss: 1.429971\n",
      "(Epoch 13 / 20) train acc: 0.402000; val_acc: 0.366667\n",
      "(Iteration 131 / 200) loss: 1.367260\n",
      "(Epoch 14 / 20) train acc: 0.416000; val_acc: 0.388889\n",
      "(Iteration 141 / 200) loss: 1.487157\n",
      "(Epoch 15 / 20) train acc: 0.461000; val_acc: 0.411111\n",
      "(Iteration 151 / 200) loss: 1.614190\n",
      "(Epoch 16 / 20) train acc: 0.394000; val_acc: 0.405556\n",
      "(Iteration 161 / 200) loss: 1.466263\n",
      "(Epoch 17 / 20) train acc: 0.491000; val_acc: 0.441667\n",
      "(Iteration 171 / 200) loss: 1.295262\n",
      "(Epoch 18 / 20) train acc: 0.489000; val_acc: 0.458333\n",
      "(Iteration 181 / 200) loss: 1.327553\n",
      "(Epoch 19 / 20) train acc: 0.479000; val_acc: 0.469444\n",
      "(Iteration 191 / 200) loss: 1.252273\n",
      "(Epoch 20 / 20) train acc: 0.487000; val_acc: 0.491667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302154\n",
      "(Epoch 2 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.219098\n",
      "(Epoch 3 / 20) train acc: 0.212000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 2.071206\n",
      "(Epoch 4 / 20) train acc: 0.201000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 1.974342\n",
      "(Epoch 5 / 20) train acc: 0.195000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 2.130944\n",
      "(Epoch 6 / 20) train acc: 0.216000; val_acc: 0.197222\n",
      "(Iteration 61 / 200) loss: 1.935321\n",
      "(Epoch 7 / 20) train acc: 0.211000; val_acc: 0.197222\n",
      "(Iteration 71 / 200) loss: 2.012660\n",
      "(Epoch 8 / 20) train acc: 0.198000; val_acc: 0.202778\n",
      "(Iteration 81 / 200) loss: 1.990725\n",
      "(Epoch 9 / 20) train acc: 0.221000; val_acc: 0.197222\n",
      "(Iteration 91 / 200) loss: 1.941119\n",
      "(Epoch 10 / 20) train acc: 0.198000; val_acc: 0.197222\n",
      "(Iteration 101 / 200) loss: 1.949603\n",
      "(Epoch 11 / 20) train acc: 0.204000; val_acc: 0.200000\n",
      "(Iteration 111 / 200) loss: 1.919460\n",
      "(Epoch 12 / 20) train acc: 0.191000; val_acc: 0.200000\n",
      "(Iteration 121 / 200) loss: 1.854131\n",
      "(Epoch 13 / 20) train acc: 0.214000; val_acc: 0.197222\n",
      "(Iteration 131 / 200) loss: 1.875834\n",
      "(Epoch 14 / 20) train acc: 0.213000; val_acc: 0.186111\n",
      "(Iteration 141 / 200) loss: 1.979573\n",
      "(Epoch 15 / 20) train acc: 0.215000; val_acc: 0.197222\n",
      "(Iteration 151 / 200) loss: 1.949021\n",
      "(Epoch 16 / 20) train acc: 0.217000; val_acc: 0.202778\n",
      "(Iteration 161 / 200) loss: 1.970530\n",
      "(Epoch 17 / 20) train acc: 0.208000; val_acc: 0.200000\n",
      "(Iteration 171 / 200) loss: 1.967923\n",
      "(Epoch 18 / 20) train acc: 0.216000; val_acc: 0.183333\n",
      "(Iteration 181 / 200) loss: 1.907045\n",
      "(Epoch 19 / 20) train acc: 0.222000; val_acc: 0.197222\n",
      "(Iteration 191 / 200) loss: 2.032054\n",
      "(Epoch 20 / 20) train acc: 0.217000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302214\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.300758\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.259220\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.022254\n",
      "(Epoch 5 / 20) train acc: 0.183000; val_acc: 0.163889\n",
      "(Iteration 51 / 200) loss: 1.981425\n",
      "(Epoch 6 / 20) train acc: 0.194000; val_acc: 0.188889\n",
      "(Iteration 61 / 200) loss: 1.978023\n",
      "(Epoch 7 / 20) train acc: 0.210000; val_acc: 0.175000\n",
      "(Iteration 71 / 200) loss: 1.891376\n",
      "(Epoch 8 / 20) train acc: 0.215000; val_acc: 0.158333\n",
      "(Iteration 81 / 200) loss: 1.832118\n",
      "(Epoch 9 / 20) train acc: 0.219000; val_acc: 0.163889\n",
      "(Iteration 91 / 200) loss: 1.969955\n",
      "(Epoch 10 / 20) train acc: 0.211000; val_acc: 0.163889\n",
      "(Iteration 101 / 200) loss: 1.930841\n",
      "(Epoch 11 / 20) train acc: 0.219000; val_acc: 0.161111\n",
      "(Iteration 111 / 200) loss: 1.875517\n",
      "(Epoch 12 / 20) train acc: 0.219000; val_acc: 0.172222\n",
      "(Iteration 121 / 200) loss: 1.942670\n",
      "(Epoch 13 / 20) train acc: 0.209000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 1.916718\n",
      "(Epoch 14 / 20) train acc: 0.204000; val_acc: 0.161111\n",
      "(Iteration 141 / 200) loss: 1.913753\n",
      "(Epoch 15 / 20) train acc: 0.214000; val_acc: 0.163889\n",
      "(Iteration 151 / 200) loss: 1.886982\n",
      "(Epoch 16 / 20) train acc: 0.191000; val_acc: 0.161111\n",
      "(Iteration 161 / 200) loss: 1.866124\n",
      "(Epoch 17 / 20) train acc: 0.228000; val_acc: 0.163889\n",
      "(Iteration 171 / 200) loss: 1.964799\n",
      "(Epoch 18 / 20) train acc: 0.220000; val_acc: 0.163889\n",
      "(Iteration 181 / 200) loss: 1.953809\n",
      "(Epoch 19 / 20) train acc: 0.225000; val_acc: 0.163889\n",
      "(Iteration 191 / 200) loss: 2.008731\n",
      "(Epoch 20 / 20) train acc: 0.194000; val_acc: 0.158333\n",
      "(Iteration 1 / 200) loss: 32453.219436\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 19695.008508\n",
      "(Epoch 2 / 20) train acc: 0.116000; val_acc: 0.144444\n",
      "(Iteration 21 / 200) loss: 16249.431156\n",
      "(Epoch 3 / 20) train acc: 0.173000; val_acc: 0.194444\n",
      "(Iteration 31 / 200) loss: 11314.590460\n",
      "(Epoch 4 / 20) train acc: 0.183000; val_acc: 0.227778\n",
      "(Iteration 41 / 200) loss: 8355.971211\n",
      "(Epoch 5 / 20) train acc: 0.218000; val_acc: 0.252778\n",
      "(Iteration 51 / 200) loss: 6989.874675\n",
      "(Epoch 6 / 20) train acc: 0.268000; val_acc: 0.311111\n",
      "(Iteration 61 / 200) loss: 3423.967174\n",
      "(Epoch 7 / 20) train acc: 0.365000; val_acc: 0.386111\n",
      "(Iteration 71 / 200) loss: 3407.314514\n",
      "(Epoch 8 / 20) train acc: 0.456000; val_acc: 0.450000\n",
      "(Iteration 81 / 200) loss: 2843.698181\n",
      "(Epoch 9 / 20) train acc: 0.508000; val_acc: 0.519444\n",
      "(Iteration 91 / 200) loss: 2181.144274\n",
      "(Epoch 10 / 20) train acc: 0.565000; val_acc: 0.572222\n",
      "(Iteration 101 / 200) loss: 1202.706684\n",
      "(Epoch 11 / 20) train acc: 0.625000; val_acc: 0.627778\n",
      "(Iteration 111 / 200) loss: 1461.191814\n",
      "(Epoch 12 / 20) train acc: 0.696000; val_acc: 0.666667\n",
      "(Iteration 121 / 200) loss: 1176.742852\n",
      "(Epoch 13 / 20) train acc: 0.698000; val_acc: 0.688889\n",
      "(Iteration 131 / 200) loss: 1039.610189\n",
      "(Epoch 14 / 20) train acc: 0.727000; val_acc: 0.722222\n",
      "(Iteration 141 / 200) loss: 1590.574686\n",
      "(Epoch 15 / 20) train acc: 0.750000; val_acc: 0.730556\n",
      "(Iteration 151 / 200) loss: 722.087814\n",
      "(Epoch 16 / 20) train acc: 0.768000; val_acc: 0.755556\n",
      "(Iteration 161 / 200) loss: 886.952034\n",
      "(Epoch 17 / 20) train acc: 0.775000; val_acc: 0.766667\n",
      "(Iteration 171 / 200) loss: 806.092482\n",
      "(Epoch 18 / 20) train acc: 0.815000; val_acc: 0.772222\n",
      "(Iteration 181 / 200) loss: 429.711637\n",
      "(Epoch 19 / 20) train acc: 0.821000; val_acc: 0.780556\n",
      "(Iteration 191 / 200) loss: 467.805744\n",
      "(Epoch 20 / 20) train acc: 0.845000; val_acc: 0.780556\n",
      "(Iteration 1 / 200) loss: 6.348099\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.152778\n",
      "(Epoch 1 / 20) train acc: 0.473000; val_acc: 0.463889\n",
      "(Iteration 11 / 200) loss: 1.416923\n",
      "(Epoch 2 / 20) train acc: 0.845000; val_acc: 0.791667\n",
      "(Iteration 21 / 200) loss: 0.774008\n",
      "(Epoch 3 / 20) train acc: 0.893000; val_acc: 0.855556\n",
      "(Iteration 31 / 200) loss: 0.391542\n",
      "(Epoch 4 / 20) train acc: 0.925000; val_acc: 0.905556\n",
      "(Iteration 41 / 200) loss: 0.333910\n",
      "(Epoch 5 / 20) train acc: 0.944000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.180745\n",
      "(Epoch 6 / 20) train acc: 0.981000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.150973\n",
      "(Epoch 7 / 20) train acc: 0.971000; val_acc: 0.961111\n",
      "(Iteration 71 / 200) loss: 0.153040\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.088571\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.980556\n",
      "(Iteration 91 / 200) loss: 0.055448\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.083753\n",
      "(Epoch 11 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.054028\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.980556\n",
      "(Iteration 121 / 200) loss: 0.047379\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.033007\n",
      "(Epoch 14 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 141 / 200) loss: 0.052607\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.039215\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.029245\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.030276\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.988889\n",
      "(Iteration 181 / 200) loss: 0.018490\n",
      "(Epoch 19 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.037251\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.302759\n",
      "(Epoch 0 / 20) train acc: 0.160000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.161000; val_acc: 0.158333\n",
      "(Iteration 11 / 200) loss: 2.281039\n",
      "(Epoch 2 / 20) train acc: 0.293000; val_acc: 0.280556\n",
      "(Iteration 21 / 200) loss: 2.099367\n",
      "(Epoch 3 / 20) train acc: 0.451000; val_acc: 0.466667\n",
      "(Iteration 31 / 200) loss: 1.545803\n",
      "(Epoch 4 / 20) train acc: 0.634000; val_acc: 0.555556\n",
      "(Iteration 41 / 200) loss: 1.201697\n",
      "(Epoch 5 / 20) train acc: 0.678000; val_acc: 0.672222\n",
      "(Iteration 51 / 200) loss: 1.061676\n",
      "(Epoch 6 / 20) train acc: 0.693000; val_acc: 0.686111\n",
      "(Iteration 61 / 200) loss: 0.665161\n",
      "(Epoch 7 / 20) train acc: 0.790000; val_acc: 0.791667\n",
      "(Iteration 71 / 200) loss: 0.693048\n",
      "(Epoch 8 / 20) train acc: 0.802000; val_acc: 0.797222\n",
      "(Iteration 81 / 200) loss: 0.621227\n",
      "(Epoch 9 / 20) train acc: 0.861000; val_acc: 0.833333\n",
      "(Iteration 91 / 200) loss: 0.571294\n",
      "(Epoch 10 / 20) train acc: 0.847000; val_acc: 0.827778\n",
      "(Iteration 101 / 200) loss: 0.366391\n",
      "(Epoch 11 / 20) train acc: 0.855000; val_acc: 0.847222\n",
      "(Iteration 111 / 200) loss: 0.407762\n",
      "(Epoch 12 / 20) train acc: 0.826000; val_acc: 0.816667\n",
      "(Iteration 121 / 200) loss: 0.219256\n",
      "(Epoch 13 / 20) train acc: 0.865000; val_acc: 0.844444\n",
      "(Iteration 131 / 200) loss: 0.466633\n",
      "(Epoch 14 / 20) train acc: 0.889000; val_acc: 0.875000\n",
      "(Iteration 141 / 200) loss: 0.435978\n",
      "(Epoch 15 / 20) train acc: 0.875000; val_acc: 0.861111\n",
      "(Iteration 151 / 200) loss: 0.334022\n",
      "(Epoch 16 / 20) train acc: 0.876000; val_acc: 0.869444\n",
      "(Iteration 161 / 200) loss: 0.305922\n",
      "(Epoch 17 / 20) train acc: 0.891000; val_acc: 0.869444\n",
      "(Iteration 171 / 200) loss: 0.219671\n",
      "(Epoch 18 / 20) train acc: 0.899000; val_acc: 0.880556\n",
      "(Iteration 181 / 200) loss: 0.227295\n",
      "(Epoch 19 / 20) train acc: 0.929000; val_acc: 0.908333\n",
      "(Iteration 191 / 200) loss: 0.187994\n",
      "(Epoch 20 / 20) train acc: 0.940000; val_acc: 0.897222\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.301651\n",
      "(Epoch 2 / 20) train acc: 0.116000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2.272515\n",
      "(Epoch 3 / 20) train acc: 0.139000; val_acc: 0.105556\n",
      "(Iteration 31 / 200) loss: 2.087710\n",
      "(Epoch 4 / 20) train acc: 0.238000; val_acc: 0.177778\n",
      "(Iteration 41 / 200) loss: 2.022202\n",
      "(Epoch 5 / 20) train acc: 0.217000; val_acc: 0.180556\n",
      "(Iteration 51 / 200) loss: 2.034210\n",
      "(Epoch 6 / 20) train acc: 0.224000; val_acc: 0.205556\n",
      "(Iteration 61 / 200) loss: 1.973465\n",
      "(Epoch 7 / 20) train acc: 0.218000; val_acc: 0.222222\n",
      "(Iteration 71 / 200) loss: 1.772625\n",
      "(Epoch 8 / 20) train acc: 0.268000; val_acc: 0.288889\n",
      "(Iteration 81 / 200) loss: 1.698861\n",
      "(Epoch 9 / 20) train acc: 0.321000; val_acc: 0.319444\n",
      "(Iteration 91 / 200) loss: 1.651677\n",
      "(Epoch 10 / 20) train acc: 0.338000; val_acc: 0.336111\n",
      "(Iteration 101 / 200) loss: 1.610313\n",
      "(Epoch 11 / 20) train acc: 0.343000; val_acc: 0.327778\n",
      "(Iteration 111 / 200) loss: 1.467038\n",
      "(Epoch 12 / 20) train acc: 0.320000; val_acc: 0.305556\n",
      "(Iteration 121 / 200) loss: 1.464967\n",
      "(Epoch 13 / 20) train acc: 0.360000; val_acc: 0.327778\n",
      "(Iteration 131 / 200) loss: 1.545841\n",
      "(Epoch 14 / 20) train acc: 0.411000; val_acc: 0.394444\n",
      "(Iteration 141 / 200) loss: 1.422317\n",
      "(Epoch 15 / 20) train acc: 0.412000; val_acc: 0.402778\n",
      "(Iteration 151 / 200) loss: 1.316066\n",
      "(Epoch 16 / 20) train acc: 0.438000; val_acc: 0.405556\n",
      "(Iteration 161 / 200) loss: 1.359462\n",
      "(Epoch 17 / 20) train acc: 0.461000; val_acc: 0.447222\n",
      "(Iteration 171 / 200) loss: 1.109302\n",
      "(Epoch 18 / 20) train acc: 0.466000; val_acc: 0.483333\n",
      "(Iteration 181 / 200) loss: 1.195209\n",
      "(Epoch 19 / 20) train acc: 0.511000; val_acc: 0.497222\n",
      "(Iteration 191 / 200) loss: 1.135192\n",
      "(Epoch 20 / 20) train acc: 0.568000; val_acc: 0.550000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.303251\n",
      "(Epoch 2 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302078\n",
      "(Epoch 3 / 20) train acc: 0.114000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.301547\n",
      "(Epoch 4 / 20) train acc: 0.085000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 2.299900\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.297392\n",
      "(Epoch 6 / 20) train acc: 0.161000; val_acc: 0.133333\n",
      "(Iteration 61 / 200) loss: 2.292632\n",
      "(Epoch 7 / 20) train acc: 0.174000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 2.287842\n",
      "(Epoch 8 / 20) train acc: 0.199000; val_acc: 0.152778\n",
      "(Iteration 81 / 200) loss: 2.276568\n",
      "(Epoch 9 / 20) train acc: 0.178000; val_acc: 0.147222\n",
      "(Iteration 91 / 200) loss: 2.255468\n",
      "(Epoch 10 / 20) train acc: 0.175000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 2.258020\n",
      "(Epoch 11 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.234125\n",
      "(Epoch 12 / 20) train acc: 0.189000; val_acc: 0.141667\n",
      "(Iteration 121 / 200) loss: 2.215144\n",
      "(Epoch 13 / 20) train acc: 0.222000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 2.198750\n",
      "(Epoch 14 / 20) train acc: 0.216000; val_acc: 0.152778\n",
      "(Iteration 141 / 200) loss: 2.178425\n",
      "(Epoch 15 / 20) train acc: 0.216000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 2.167844\n",
      "(Epoch 16 / 20) train acc: 0.184000; val_acc: 0.152778\n",
      "(Iteration 161 / 200) loss: 2.178112\n",
      "(Epoch 17 / 20) train acc: 0.227000; val_acc: 0.155556\n",
      "(Iteration 171 / 200) loss: 2.152465\n",
      "(Epoch 18 / 20) train acc: 0.191000; val_acc: 0.152778\n",
      "(Iteration 181 / 200) loss: 2.120371\n",
      "(Epoch 19 / 20) train acc: 0.194000; val_acc: 0.144444\n",
      "(Iteration 191 / 200) loss: 2.079531\n",
      "(Epoch 20 / 20) train acc: 0.202000; val_acc: 0.150000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302985\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.301670\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.264962\n",
      "(Epoch 4 / 20) train acc: 0.184000; val_acc: 0.186111\n",
      "(Iteration 41 / 200) loss: 2.128160\n",
      "(Epoch 5 / 20) train acc: 0.194000; val_acc: 0.197222\n",
      "(Iteration 51 / 200) loss: 2.013638\n",
      "(Epoch 6 / 20) train acc: 0.213000; val_acc: 0.213889\n",
      "(Iteration 61 / 200) loss: 1.980636\n",
      "(Epoch 7 / 20) train acc: 0.230000; val_acc: 0.194444\n",
      "(Iteration 71 / 200) loss: 1.991654\n",
      "(Epoch 8 / 20) train acc: 0.162000; val_acc: 0.211111\n",
      "(Iteration 81 / 200) loss: 2.005239\n",
      "(Epoch 9 / 20) train acc: 0.173000; val_acc: 0.216667\n",
      "(Iteration 91 / 200) loss: 1.948171\n",
      "(Epoch 10 / 20) train acc: 0.201000; val_acc: 0.213889\n",
      "(Iteration 101 / 200) loss: 2.019110\n",
      "(Epoch 11 / 20) train acc: 0.226000; val_acc: 0.194444\n",
      "(Iteration 111 / 200) loss: 1.938913\n",
      "(Epoch 12 / 20) train acc: 0.225000; val_acc: 0.200000\n",
      "(Iteration 121 / 200) loss: 2.063215\n",
      "(Epoch 13 / 20) train acc: 0.203000; val_acc: 0.211111\n",
      "(Iteration 131 / 200) loss: 1.870548\n",
      "(Epoch 14 / 20) train acc: 0.200000; val_acc: 0.197222\n",
      "(Iteration 141 / 200) loss: 2.019862\n",
      "(Epoch 15 / 20) train acc: 0.202000; val_acc: 0.202778\n",
      "(Iteration 151 / 200) loss: 2.011086\n",
      "(Epoch 16 / 20) train acc: 0.219000; val_acc: 0.197222\n",
      "(Iteration 161 / 200) loss: 1.920245\n",
      "(Epoch 17 / 20) train acc: 0.194000; val_acc: 0.213889\n",
      "(Iteration 171 / 200) loss: 1.895040\n",
      "(Epoch 18 / 20) train acc: 0.223000; val_acc: 0.197222\n",
      "(Iteration 181 / 200) loss: 1.793724\n",
      "(Epoch 19 / 20) train acc: 0.225000; val_acc: 0.216667\n",
      "(Iteration 191 / 200) loss: 1.809951\n",
      "(Epoch 20 / 20) train acc: 0.185000; val_acc: 0.152778\n",
      "(Iteration 1 / 200) loss: 38229.061538\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 27627.863699\n",
      "(Epoch 2 / 20) train acc: 0.141000; val_acc: 0.152778\n",
      "(Iteration 21 / 200) loss: 16770.748464\n",
      "(Epoch 3 / 20) train acc: 0.179000; val_acc: 0.175000\n",
      "(Iteration 31 / 200) loss: 11017.890755\n",
      "(Epoch 4 / 20) train acc: 0.192000; val_acc: 0.161111\n",
      "(Iteration 41 / 200) loss: 6250.649310\n",
      "(Epoch 5 / 20) train acc: 0.274000; val_acc: 0.233333\n",
      "(Iteration 51 / 200) loss: 6292.484756\n",
      "(Epoch 6 / 20) train acc: 0.343000; val_acc: 0.330556\n",
      "(Iteration 61 / 200) loss: 4622.495215\n",
      "(Epoch 7 / 20) train acc: 0.443000; val_acc: 0.427778\n",
      "(Iteration 71 / 200) loss: 3494.818179\n",
      "(Epoch 8 / 20) train acc: 0.502000; val_acc: 0.502778\n",
      "(Iteration 81 / 200) loss: 2226.143650\n",
      "(Epoch 9 / 20) train acc: 0.546000; val_acc: 0.530556\n",
      "(Iteration 91 / 200) loss: 2492.780690\n",
      "(Epoch 10 / 20) train acc: 0.599000; val_acc: 0.583333\n",
      "(Iteration 101 / 200) loss: 1239.390233\n",
      "(Epoch 11 / 20) train acc: 0.626000; val_acc: 0.594444\n",
      "(Iteration 111 / 200) loss: 2023.552123\n",
      "(Epoch 12 / 20) train acc: 0.685000; val_acc: 0.613889\n",
      "(Iteration 121 / 200) loss: 1421.534640\n",
      "(Epoch 13 / 20) train acc: 0.676000; val_acc: 0.625000\n",
      "(Iteration 131 / 200) loss: 1020.508255\n",
      "(Epoch 14 / 20) train acc: 0.693000; val_acc: 0.661111\n",
      "(Iteration 141 / 200) loss: 847.104216\n",
      "(Epoch 15 / 20) train acc: 0.730000; val_acc: 0.680556\n",
      "(Iteration 151 / 200) loss: 935.432524\n",
      "(Epoch 16 / 20) train acc: 0.765000; val_acc: 0.697222\n",
      "(Iteration 161 / 200) loss: 624.389038\n",
      "(Epoch 17 / 20) train acc: 0.785000; val_acc: 0.727778\n",
      "(Iteration 171 / 200) loss: 310.387431\n",
      "(Epoch 18 / 20) train acc: 0.811000; val_acc: 0.741667\n",
      "(Iteration 181 / 200) loss: 745.721921\n",
      "(Epoch 19 / 20) train acc: 0.834000; val_acc: 0.750000\n",
      "(Iteration 191 / 200) loss: 469.839773\n",
      "(Epoch 20 / 20) train acc: 0.818000; val_acc: 0.750000\n",
      "(Iteration 1 / 200) loss: 3.412081\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.584000; val_acc: 0.552778\n",
      "(Iteration 11 / 200) loss: 1.379489\n",
      "(Epoch 2 / 20) train acc: 0.872000; val_acc: 0.819444\n",
      "(Iteration 21 / 200) loss: 0.563243\n",
      "(Epoch 3 / 20) train acc: 0.944000; val_acc: 0.894444\n",
      "(Iteration 31 / 200) loss: 0.351642\n",
      "(Epoch 4 / 20) train acc: 0.930000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.170970\n",
      "(Epoch 5 / 20) train acc: 0.945000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 0.112523\n",
      "(Epoch 6 / 20) train acc: 0.978000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.090217\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.141892\n",
      "(Epoch 8 / 20) train acc: 0.984000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.049941\n",
      "(Epoch 9 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 91 / 200) loss: 0.061010\n",
      "(Epoch 10 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.047200\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.088620\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.026832\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 131 / 200) loss: 0.021094\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.015916\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.029032\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.018692\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.007732\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.007193\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.011300\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.302562\n",
      "(Epoch 0 / 20) train acc: 0.139000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.184000; val_acc: 0.230556\n",
      "(Iteration 11 / 200) loss: 2.261297\n",
      "(Epoch 2 / 20) train acc: 0.189000; val_acc: 0.222222\n",
      "(Iteration 21 / 200) loss: 2.002575\n",
      "(Epoch 3 / 20) train acc: 0.428000; val_acc: 0.436111\n",
      "(Iteration 31 / 200) loss: 1.676592\n",
      "(Epoch 4 / 20) train acc: 0.537000; val_acc: 0.519444\n",
      "(Iteration 41 / 200) loss: 1.307263\n",
      "(Epoch 5 / 20) train acc: 0.597000; val_acc: 0.602778\n",
      "(Iteration 51 / 200) loss: 1.125943\n",
      "(Epoch 6 / 20) train acc: 0.624000; val_acc: 0.619444\n",
      "(Iteration 61 / 200) loss: 0.836346\n",
      "(Epoch 7 / 20) train acc: 0.653000; val_acc: 0.622222\n",
      "(Iteration 71 / 200) loss: 0.912404\n",
      "(Epoch 8 / 20) train acc: 0.690000; val_acc: 0.661111\n",
      "(Iteration 81 / 200) loss: 0.805619\n",
      "(Epoch 9 / 20) train acc: 0.665000; val_acc: 0.686111\n",
      "(Iteration 91 / 200) loss: 0.664718\n",
      "(Epoch 10 / 20) train acc: 0.680000; val_acc: 0.633333\n",
      "(Iteration 101 / 200) loss: 0.902440\n",
      "(Epoch 11 / 20) train acc: 0.747000; val_acc: 0.733333\n",
      "(Iteration 111 / 200) loss: 0.558601\n",
      "(Epoch 12 / 20) train acc: 0.769000; val_acc: 0.766667\n",
      "(Iteration 121 / 200) loss: 0.459254\n",
      "(Epoch 13 / 20) train acc: 0.792000; val_acc: 0.769444\n",
      "(Iteration 131 / 200) loss: 0.595980\n",
      "(Epoch 14 / 20) train acc: 0.797000; val_acc: 0.747222\n",
      "(Iteration 141 / 200) loss: 0.465326\n",
      "(Epoch 15 / 20) train acc: 0.801000; val_acc: 0.791667\n",
      "(Iteration 151 / 200) loss: 0.612507\n",
      "(Epoch 16 / 20) train acc: 0.844000; val_acc: 0.800000\n",
      "(Iteration 161 / 200) loss: 0.423881\n",
      "(Epoch 17 / 20) train acc: 0.832000; val_acc: 0.833333\n",
      "(Iteration 171 / 200) loss: 0.429308\n",
      "(Epoch 18 / 20) train acc: 0.855000; val_acc: 0.866667\n",
      "(Iteration 181 / 200) loss: 0.441952\n",
      "(Epoch 19 / 20) train acc: 0.876000; val_acc: 0.847222\n",
      "(Iteration 191 / 200) loss: 0.455249\n",
      "(Epoch 20 / 20) train acc: 0.903000; val_acc: 0.863889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.299628\n",
      "(Epoch 2 / 20) train acc: 0.090000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.225310\n",
      "(Epoch 3 / 20) train acc: 0.175000; val_acc: 0.152778\n",
      "(Iteration 31 / 200) loss: 2.096715\n",
      "(Epoch 4 / 20) train acc: 0.163000; val_acc: 0.150000\n",
      "(Iteration 41 / 200) loss: 2.065018\n",
      "(Epoch 5 / 20) train acc: 0.193000; val_acc: 0.183333\n",
      "(Iteration 51 / 200) loss: 2.014128\n",
      "(Epoch 6 / 20) train acc: 0.200000; val_acc: 0.180556\n",
      "(Iteration 61 / 200) loss: 2.081503\n",
      "(Epoch 7 / 20) train acc: 0.204000; val_acc: 0.183333\n",
      "(Iteration 71 / 200) loss: 2.005523\n",
      "(Epoch 8 / 20) train acc: 0.190000; val_acc: 0.188889\n",
      "(Iteration 81 / 200) loss: 2.031679\n",
      "(Epoch 9 / 20) train acc: 0.204000; val_acc: 0.194444\n",
      "(Iteration 91 / 200) loss: 1.981065\n",
      "(Epoch 10 / 20) train acc: 0.225000; val_acc: 0.191667\n",
      "(Iteration 101 / 200) loss: 1.896405\n",
      "(Epoch 11 / 20) train acc: 0.307000; val_acc: 0.225000\n",
      "(Iteration 111 / 200) loss: 1.922494\n",
      "(Epoch 12 / 20) train acc: 0.289000; val_acc: 0.233333\n",
      "(Iteration 121 / 200) loss: 1.973409\n",
      "(Epoch 13 / 20) train acc: 0.285000; val_acc: 0.277778\n",
      "(Iteration 131 / 200) loss: 1.933175\n",
      "(Epoch 14 / 20) train acc: 0.387000; val_acc: 0.319444\n",
      "(Iteration 141 / 200) loss: 1.826531\n",
      "(Epoch 15 / 20) train acc: 0.358000; val_acc: 0.369444\n",
      "(Iteration 151 / 200) loss: 1.673328\n",
      "(Epoch 16 / 20) train acc: 0.348000; val_acc: 0.350000\n",
      "(Iteration 161 / 200) loss: 1.516021\n",
      "(Epoch 17 / 20) train acc: 0.413000; val_acc: 0.388889\n",
      "(Iteration 171 / 200) loss: 1.431897\n",
      "(Epoch 18 / 20) train acc: 0.417000; val_acc: 0.380556\n",
      "(Iteration 181 / 200) loss: 1.613002\n",
      "(Epoch 19 / 20) train acc: 0.417000; val_acc: 0.366667\n",
      "(Iteration 191 / 200) loss: 1.445355\n",
      "(Epoch 20 / 20) train acc: 0.431000; val_acc: 0.383333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302219\n",
      "(Epoch 2 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.303261\n",
      "(Epoch 3 / 20) train acc: 0.124000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302928\n",
      "(Epoch 4 / 20) train acc: 0.122000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.303839\n",
      "(Epoch 5 / 20) train acc: 0.087000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.302977\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.301754\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.301646\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.301996\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.301409\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 2.301537\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 2.301550\n",
      "(Epoch 12 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 2.303103\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 2.302986\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 141 / 200) loss: 2.301785\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 151 / 200) loss: 2.304345\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 161 / 200) loss: 2.303820\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.303403\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.298456\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.298288\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302344\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302211\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302455\n",
      "(Epoch 4 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302326\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.301957\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.303386\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.303168\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.301762\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302680\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.300967\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.304595\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.300059\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.303568\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.299665\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302143\n",
      "(Epoch 16 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.305525\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.305197\n",
      "(Epoch 18 / 20) train acc: 0.084000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.299535\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.299512\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 69484.317028\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 42668.126628\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 29557.431343\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 20971.574833\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.111111\n",
      "(Iteration 41 / 200) loss: 15101.805839\n",
      "(Epoch 5 / 20) train acc: 0.193000; val_acc: 0.188889\n",
      "(Iteration 51 / 200) loss: 11208.031869\n",
      "(Epoch 6 / 20) train acc: 0.277000; val_acc: 0.266667\n",
      "(Iteration 61 / 200) loss: 8778.249790\n",
      "(Epoch 7 / 20) train acc: 0.328000; val_acc: 0.325000\n",
      "(Iteration 71 / 200) loss: 5499.104592\n",
      "(Epoch 8 / 20) train acc: 0.370000; val_acc: 0.397222\n",
      "(Iteration 81 / 200) loss: 3491.510653\n",
      "(Epoch 9 / 20) train acc: 0.488000; val_acc: 0.455556\n",
      "(Iteration 91 / 200) loss: 4101.949844\n",
      "(Epoch 10 / 20) train acc: 0.515000; val_acc: 0.516667\n",
      "(Iteration 101 / 200) loss: 2098.249975\n",
      "(Epoch 11 / 20) train acc: 0.529000; val_acc: 0.538889\n",
      "(Iteration 111 / 200) loss: 2172.055586\n",
      "(Epoch 12 / 20) train acc: 0.598000; val_acc: 0.569444\n",
      "(Iteration 121 / 200) loss: 3091.222453\n",
      "(Epoch 13 / 20) train acc: 0.632000; val_acc: 0.586111\n",
      "(Iteration 131 / 200) loss: 2562.572607\n",
      "(Epoch 14 / 20) train acc: 0.661000; val_acc: 0.602778\n",
      "(Iteration 141 / 200) loss: 1735.985422\n",
      "(Epoch 15 / 20) train acc: 0.692000; val_acc: 0.641667\n",
      "(Iteration 151 / 200) loss: 1943.776054\n",
      "(Epoch 16 / 20) train acc: 0.683000; val_acc: 0.658333\n",
      "(Iteration 161 / 200) loss: 1420.281065\n",
      "(Epoch 17 / 20) train acc: 0.721000; val_acc: 0.652778\n",
      "(Iteration 171 / 200) loss: 842.786547\n",
      "(Epoch 18 / 20) train acc: 0.730000; val_acc: 0.672222\n",
      "(Iteration 181 / 200) loss: 880.867189\n",
      "(Epoch 19 / 20) train acc: 0.746000; val_acc: 0.686111\n",
      "(Iteration 191 / 200) loss: 866.716739\n",
      "(Epoch 20 / 20) train acc: 0.766000; val_acc: 0.677778\n",
      "(Iteration 1 / 200) loss: 3.258267\n",
      "(Epoch 0 / 20) train acc: 0.163000; val_acc: 0.155556\n",
      "(Epoch 1 / 20) train acc: 0.665000; val_acc: 0.627778\n",
      "(Iteration 11 / 200) loss: 1.152161\n",
      "(Epoch 2 / 20) train acc: 0.866000; val_acc: 0.802778\n",
      "(Iteration 21 / 200) loss: 0.465770\n",
      "(Epoch 3 / 20) train acc: 0.879000; val_acc: 0.855556\n",
      "(Iteration 31 / 200) loss: 0.303918\n",
      "(Epoch 4 / 20) train acc: 0.935000; val_acc: 0.886111\n",
      "(Iteration 41 / 200) loss: 0.241377\n",
      "(Epoch 5 / 20) train acc: 0.959000; val_acc: 0.894444\n",
      "(Iteration 51 / 200) loss: 0.099561\n",
      "(Epoch 6 / 20) train acc: 0.967000; val_acc: 0.927778\n",
      "(Iteration 61 / 200) loss: 0.172570\n",
      "(Epoch 7 / 20) train acc: 0.974000; val_acc: 0.922222\n",
      "(Iteration 71 / 200) loss: 0.114702\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.925000\n",
      "(Iteration 81 / 200) loss: 0.069132\n",
      "(Epoch 9 / 20) train acc: 0.970000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.077248\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.089326\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.066209\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.024489\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.051691\n",
      "(Epoch 14 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.012105\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.031999\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.016080\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.016066\n",
      "(Epoch 18 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.009959\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.028344\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302679\n",
      "(Epoch 0 / 20) train acc: 0.183000; val_acc: 0.180556\n",
      "(Epoch 1 / 20) train acc: 0.283000; val_acc: 0.263889\n",
      "(Iteration 11 / 200) loss: 2.282724\n",
      "(Epoch 2 / 20) train acc: 0.287000; val_acc: 0.261111\n",
      "(Iteration 21 / 200) loss: 2.066275\n",
      "(Epoch 3 / 20) train acc: 0.435000; val_acc: 0.430556\n",
      "(Iteration 31 / 200) loss: 1.714882\n",
      "(Epoch 4 / 20) train acc: 0.550000; val_acc: 0.541667\n",
      "(Iteration 41 / 200) loss: 1.521914\n",
      "(Epoch 5 / 20) train acc: 0.643000; val_acc: 0.647222\n",
      "(Iteration 51 / 200) loss: 1.052428\n",
      "(Epoch 6 / 20) train acc: 0.642000; val_acc: 0.650000\n",
      "(Iteration 61 / 200) loss: 0.996610\n",
      "(Epoch 7 / 20) train acc: 0.725000; val_acc: 0.752778\n",
      "(Iteration 71 / 200) loss: 0.833349\n",
      "(Epoch 8 / 20) train acc: 0.754000; val_acc: 0.750000\n",
      "(Iteration 81 / 200) loss: 0.756072\n",
      "(Epoch 9 / 20) train acc: 0.797000; val_acc: 0.800000\n",
      "(Iteration 91 / 200) loss: 0.464774\n",
      "(Epoch 10 / 20) train acc: 0.794000; val_acc: 0.802778\n",
      "(Iteration 101 / 200) loss: 0.483007\n",
      "(Epoch 11 / 20) train acc: 0.832000; val_acc: 0.850000\n",
      "(Iteration 111 / 200) loss: 0.459581\n",
      "(Epoch 12 / 20) train acc: 0.859000; val_acc: 0.863889\n",
      "(Iteration 121 / 200) loss: 0.449502\n",
      "(Epoch 13 / 20) train acc: 0.894000; val_acc: 0.886111\n",
      "(Iteration 131 / 200) loss: 0.215156\n",
      "(Epoch 14 / 20) train acc: 0.896000; val_acc: 0.875000\n",
      "(Iteration 141 / 200) loss: 0.384553\n",
      "(Epoch 15 / 20) train acc: 0.925000; val_acc: 0.897222\n",
      "(Iteration 151 / 200) loss: 0.220443\n",
      "(Epoch 16 / 20) train acc: 0.918000; val_acc: 0.875000\n",
      "(Iteration 161 / 200) loss: 0.222721\n",
      "(Epoch 17 / 20) train acc: 0.919000; val_acc: 0.883333\n",
      "(Iteration 171 / 200) loss: 0.169993\n",
      "(Epoch 18 / 20) train acc: 0.938000; val_acc: 0.911111\n",
      "(Iteration 181 / 200) loss: 0.227828\n",
      "(Epoch 19 / 20) train acc: 0.937000; val_acc: 0.913889\n",
      "(Iteration 191 / 200) loss: 0.213139\n",
      "(Epoch 20 / 20) train acc: 0.948000; val_acc: 0.913889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.300793\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.224265\n",
      "(Epoch 3 / 20) train acc: 0.209000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 1.899032\n",
      "(Epoch 4 / 20) train acc: 0.179000; val_acc: 0.200000\n",
      "(Iteration 41 / 200) loss: 1.950164\n",
      "(Epoch 5 / 20) train acc: 0.170000; val_acc: 0.219444\n",
      "(Iteration 51 / 200) loss: 2.022929\n",
      "(Epoch 6 / 20) train acc: 0.204000; val_acc: 0.183333\n",
      "(Iteration 61 / 200) loss: 1.883767\n",
      "(Epoch 7 / 20) train acc: 0.206000; val_acc: 0.186111\n",
      "(Iteration 71 / 200) loss: 1.962751\n",
      "(Epoch 8 / 20) train acc: 0.211000; val_acc: 0.202778\n",
      "(Iteration 81 / 200) loss: 1.892416\n",
      "(Epoch 9 / 20) train acc: 0.196000; val_acc: 0.208333\n",
      "(Iteration 91 / 200) loss: 1.672549\n",
      "(Epoch 10 / 20) train acc: 0.363000; val_acc: 0.341667\n",
      "(Iteration 101 / 200) loss: 1.619308\n",
      "(Epoch 11 / 20) train acc: 0.427000; val_acc: 0.419444\n",
      "(Iteration 111 / 200) loss: 1.470783\n",
      "(Epoch 12 / 20) train acc: 0.512000; val_acc: 0.527778\n",
      "(Iteration 121 / 200) loss: 1.127385\n",
      "(Epoch 13 / 20) train acc: 0.591000; val_acc: 0.602778\n",
      "(Iteration 131 / 200) loss: 1.045656\n",
      "(Epoch 14 / 20) train acc: 0.691000; val_acc: 0.716667\n",
      "(Iteration 141 / 200) loss: 0.804881\n",
      "(Epoch 15 / 20) train acc: 0.680000; val_acc: 0.694444\n",
      "(Iteration 151 / 200) loss: 0.764708\n",
      "(Epoch 16 / 20) train acc: 0.737000; val_acc: 0.708333\n",
      "(Iteration 161 / 200) loss: 0.672666\n",
      "(Epoch 17 / 20) train acc: 0.764000; val_acc: 0.738889\n",
      "(Iteration 171 / 200) loss: 0.756237\n",
      "(Epoch 18 / 20) train acc: 0.748000; val_acc: 0.744444\n",
      "(Iteration 181 / 200) loss: 0.734389\n",
      "(Epoch 19 / 20) train acc: 0.747000; val_acc: 0.730556\n",
      "(Iteration 191 / 200) loss: 0.676845\n",
      "(Epoch 20 / 20) train acc: 0.774000; val_acc: 0.730556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.303041\n",
      "(Epoch 2 / 20) train acc: 0.118000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 2.300270\n",
      "(Epoch 3 / 20) train acc: 0.138000; val_acc: 0.166667\n",
      "(Iteration 31 / 200) loss: 2.240962\n",
      "(Epoch 4 / 20) train acc: 0.205000; val_acc: 0.205556\n",
      "(Iteration 41 / 200) loss: 2.010884\n",
      "(Epoch 5 / 20) train acc: 0.194000; val_acc: 0.208333\n",
      "(Iteration 51 / 200) loss: 1.865383\n",
      "(Epoch 6 / 20) train acc: 0.228000; val_acc: 0.200000\n",
      "(Iteration 61 / 200) loss: 1.748949\n",
      "(Epoch 7 / 20) train acc: 0.234000; val_acc: 0.266667\n",
      "(Iteration 71 / 200) loss: 1.717933\n",
      "(Epoch 8 / 20) train acc: 0.285000; val_acc: 0.286111\n",
      "(Iteration 81 / 200) loss: 1.731044\n",
      "(Epoch 9 / 20) train acc: 0.261000; val_acc: 0.283333\n",
      "(Iteration 91 / 200) loss: 1.594010\n",
      "(Epoch 10 / 20) train acc: 0.371000; val_acc: 0.308333\n",
      "(Iteration 101 / 200) loss: 1.566393\n",
      "(Epoch 11 / 20) train acc: 0.337000; val_acc: 0.352778\n",
      "(Iteration 111 / 200) loss: 1.531375\n",
      "(Epoch 12 / 20) train acc: 0.341000; val_acc: 0.333333\n",
      "(Iteration 121 / 200) loss: 1.578551\n",
      "(Epoch 13 / 20) train acc: 0.387000; val_acc: 0.316667\n",
      "(Iteration 131 / 200) loss: 1.499795\n",
      "(Epoch 14 / 20) train acc: 0.346000; val_acc: 0.322222\n",
      "(Iteration 141 / 200) loss: 1.455845\n",
      "(Epoch 15 / 20) train acc: 0.373000; val_acc: 0.336111\n",
      "(Iteration 151 / 200) loss: 1.303602\n",
      "(Epoch 16 / 20) train acc: 0.400000; val_acc: 0.355556\n",
      "(Iteration 161 / 200) loss: 1.295194\n",
      "(Epoch 17 / 20) train acc: 0.367000; val_acc: 0.372222\n",
      "(Iteration 171 / 200) loss: 1.385059\n",
      "(Epoch 18 / 20) train acc: 0.375000; val_acc: 0.341667\n",
      "(Iteration 181 / 200) loss: 1.295046\n",
      "(Epoch 19 / 20) train acc: 0.374000; val_acc: 0.413889\n",
      "(Iteration 191 / 200) loss: 1.300288\n",
      "(Epoch 20 / 20) train acc: 0.389000; val_acc: 0.369444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302364\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303082\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.272302\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.084075\n",
      "(Epoch 5 / 20) train acc: 0.190000; val_acc: 0.155556\n",
      "(Iteration 51 / 200) loss: 2.077288\n",
      "(Epoch 6 / 20) train acc: 0.216000; val_acc: 0.161111\n",
      "(Iteration 61 / 200) loss: 2.059029\n",
      "(Epoch 7 / 20) train acc: 0.196000; val_acc: 0.158333\n",
      "(Iteration 71 / 200) loss: 2.121817\n",
      "(Epoch 8 / 20) train acc: 0.195000; val_acc: 0.161111\n",
      "(Iteration 81 / 200) loss: 2.039759\n",
      "(Epoch 9 / 20) train acc: 0.214000; val_acc: 0.155556\n",
      "(Iteration 91 / 200) loss: 2.079490\n",
      "(Epoch 10 / 20) train acc: 0.219000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 1.938117\n",
      "(Epoch 11 / 20) train acc: 0.214000; val_acc: 0.161111\n",
      "(Iteration 111 / 200) loss: 1.963780\n",
      "(Epoch 12 / 20) train acc: 0.221000; val_acc: 0.158333\n",
      "(Iteration 121 / 200) loss: 1.986870\n",
      "(Epoch 13 / 20) train acc: 0.208000; val_acc: 0.166667\n",
      "(Iteration 131 / 200) loss: 1.895562\n",
      "(Epoch 14 / 20) train acc: 0.214000; val_acc: 0.161111\n",
      "(Iteration 141 / 200) loss: 1.948847\n",
      "(Epoch 15 / 20) train acc: 0.237000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 1.913155\n",
      "(Epoch 16 / 20) train acc: 0.225000; val_acc: 0.163889\n",
      "(Iteration 161 / 200) loss: 1.888311\n",
      "(Epoch 17 / 20) train acc: 0.208000; val_acc: 0.158333\n",
      "(Iteration 171 / 200) loss: 1.992939\n",
      "(Epoch 18 / 20) train acc: 0.192000; val_acc: 0.163889\n",
      "(Iteration 181 / 200) loss: 1.955810\n",
      "(Epoch 19 / 20) train acc: 0.258000; val_acc: 0.213889\n",
      "(Iteration 191 / 200) loss: 1.926443\n",
      "(Epoch 20 / 20) train acc: 0.286000; val_acc: 0.244444\n",
      "(Iteration 1 / 200) loss: 27094.478999\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 16057.208577\n",
      "(Epoch 2 / 20) train acc: 0.132000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 8984.773878\n",
      "(Epoch 3 / 20) train acc: 0.179000; val_acc: 0.155556\n",
      "(Iteration 31 / 200) loss: 7839.261700\n",
      "(Epoch 4 / 20) train acc: 0.249000; val_acc: 0.216667\n",
      "(Iteration 41 / 200) loss: 5565.879531\n",
      "(Epoch 5 / 20) train acc: 0.294000; val_acc: 0.277778\n",
      "(Iteration 51 / 200) loss: 3693.300511\n",
      "(Epoch 6 / 20) train acc: 0.411000; val_acc: 0.397222\n",
      "(Iteration 61 / 200) loss: 3299.493069\n",
      "(Epoch 7 / 20) train acc: 0.522000; val_acc: 0.447222\n",
      "(Iteration 71 / 200) loss: 3212.412825\n",
      "(Epoch 8 / 20) train acc: 0.593000; val_acc: 0.508333\n",
      "(Iteration 81 / 200) loss: 2675.095090\n",
      "(Epoch 9 / 20) train acc: 0.597000; val_acc: 0.555556\n",
      "(Iteration 91 / 200) loss: 1422.656895\n",
      "(Epoch 10 / 20) train acc: 0.660000; val_acc: 0.583333\n",
      "(Iteration 101 / 200) loss: 1654.281673\n",
      "(Epoch 11 / 20) train acc: 0.688000; val_acc: 0.619444\n",
      "(Iteration 111 / 200) loss: 1390.678175\n",
      "(Epoch 12 / 20) train acc: 0.698000; val_acc: 0.641667\n",
      "(Iteration 121 / 200) loss: 849.515229\n",
      "(Epoch 13 / 20) train acc: 0.765000; val_acc: 0.655556\n",
      "(Iteration 131 / 200) loss: 1001.749243\n",
      "(Epoch 14 / 20) train acc: 0.770000; val_acc: 0.666667\n",
      "(Iteration 141 / 200) loss: 1073.418343\n",
      "(Epoch 15 / 20) train acc: 0.780000; val_acc: 0.683333\n",
      "(Iteration 151 / 200) loss: 716.438621\n",
      "(Epoch 16 / 20) train acc: 0.769000; val_acc: 0.688889\n",
      "(Iteration 161 / 200) loss: 508.111639\n",
      "(Epoch 17 / 20) train acc: 0.805000; val_acc: 0.705556\n",
      "(Iteration 171 / 200) loss: 486.995362\n",
      "(Epoch 18 / 20) train acc: 0.820000; val_acc: 0.719444\n",
      "(Iteration 181 / 200) loss: 746.859167\n",
      "(Epoch 19 / 20) train acc: 0.838000; val_acc: 0.730556\n",
      "(Iteration 191 / 200) loss: 401.963599\n",
      "(Epoch 20 / 20) train acc: 0.847000; val_acc: 0.741667\n",
      "(Iteration 1 / 200) loss: 3.343690\n",
      "(Epoch 0 / 20) train acc: 0.170000; val_acc: 0.202778\n",
      "(Epoch 1 / 20) train acc: 0.642000; val_acc: 0.641667\n",
      "(Iteration 11 / 200) loss: 1.212930\n",
      "(Epoch 2 / 20) train acc: 0.870000; val_acc: 0.841667\n",
      "(Iteration 21 / 200) loss: 0.628929\n",
      "(Epoch 3 / 20) train acc: 0.927000; val_acc: 0.891667\n",
      "(Iteration 31 / 200) loss: 0.383060\n",
      "(Epoch 4 / 20) train acc: 0.942000; val_acc: 0.905556\n",
      "(Iteration 41 / 200) loss: 0.276475\n",
      "(Epoch 5 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 51 / 200) loss: 0.187693\n",
      "(Epoch 6 / 20) train acc: 0.976000; val_acc: 0.936111\n",
      "(Iteration 61 / 200) loss: 0.123645\n",
      "(Epoch 7 / 20) train acc: 0.971000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.102419\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.129240\n",
      "(Epoch 9 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 91 / 200) loss: 0.068645\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.023176\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.050883\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 121 / 200) loss: 0.022396\n",
      "(Epoch 13 / 20) train acc: 0.997000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.035702\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 141 / 200) loss: 0.036750\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.024426\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.008404\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.015624\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.007249\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.009389\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.302579\n",
      "(Epoch 0 / 20) train acc: 0.164000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.239000; val_acc: 0.227778\n",
      "(Iteration 11 / 200) loss: 2.284800\n",
      "(Epoch 2 / 20) train acc: 0.369000; val_acc: 0.294444\n",
      "(Iteration 21 / 200) loss: 2.066379\n",
      "(Epoch 3 / 20) train acc: 0.404000; val_acc: 0.380556\n",
      "(Iteration 31 / 200) loss: 1.734458\n",
      "(Epoch 4 / 20) train acc: 0.613000; val_acc: 0.613889\n",
      "(Iteration 41 / 200) loss: 1.275650\n",
      "(Epoch 5 / 20) train acc: 0.670000; val_acc: 0.625000\n",
      "(Iteration 51 / 200) loss: 1.087797\n",
      "(Epoch 6 / 20) train acc: 0.642000; val_acc: 0.647222\n",
      "(Iteration 61 / 200) loss: 0.858739\n",
      "(Epoch 7 / 20) train acc: 0.722000; val_acc: 0.680556\n",
      "(Iteration 71 / 200) loss: 0.780668\n",
      "(Epoch 8 / 20) train acc: 0.740000; val_acc: 0.705556\n",
      "(Iteration 81 / 200) loss: 0.844189\n",
      "(Epoch 9 / 20) train acc: 0.804000; val_acc: 0.750000\n",
      "(Iteration 91 / 200) loss: 0.718991\n",
      "(Epoch 10 / 20) train acc: 0.797000; val_acc: 0.763889\n",
      "(Iteration 101 / 200) loss: 0.609966\n",
      "(Epoch 11 / 20) train acc: 0.800000; val_acc: 0.791667\n",
      "(Iteration 111 / 200) loss: 0.463185\n",
      "(Epoch 12 / 20) train acc: 0.827000; val_acc: 0.827778\n",
      "(Iteration 121 / 200) loss: 0.518086\n",
      "(Epoch 13 / 20) train acc: 0.861000; val_acc: 0.827778\n",
      "(Iteration 131 / 200) loss: 0.412247\n",
      "(Epoch 14 / 20) train acc: 0.866000; val_acc: 0.847222\n",
      "(Iteration 141 / 200) loss: 0.433717\n",
      "(Epoch 15 / 20) train acc: 0.893000; val_acc: 0.863889\n",
      "(Iteration 151 / 200) loss: 0.271465\n",
      "(Epoch 16 / 20) train acc: 0.875000; val_acc: 0.847222\n",
      "(Iteration 161 / 200) loss: 0.352205\n",
      "(Epoch 17 / 20) train acc: 0.898000; val_acc: 0.863889\n",
      "(Iteration 171 / 200) loss: 0.592423\n",
      "(Epoch 18 / 20) train acc: 0.910000; val_acc: 0.861111\n",
      "(Iteration 181 / 200) loss: 0.257615\n",
      "(Epoch 19 / 20) train acc: 0.897000; val_acc: 0.875000\n",
      "(Iteration 191 / 200) loss: 0.298451\n",
      "(Epoch 20 / 20) train acc: 0.891000; val_acc: 0.877778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.301521\n",
      "(Epoch 2 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.190235\n",
      "(Epoch 3 / 20) train acc: 0.193000; val_acc: 0.186111\n",
      "(Iteration 31 / 200) loss: 2.084308\n",
      "(Epoch 4 / 20) train acc: 0.203000; val_acc: 0.188889\n",
      "(Iteration 41 / 200) loss: 2.007744\n",
      "(Epoch 5 / 20) train acc: 0.193000; val_acc: 0.175000\n",
      "(Iteration 51 / 200) loss: 2.025389\n",
      "(Epoch 6 / 20) train acc: 0.202000; val_acc: 0.180556\n",
      "(Iteration 61 / 200) loss: 2.024513\n",
      "(Epoch 7 / 20) train acc: 0.186000; val_acc: 0.186111\n",
      "(Iteration 71 / 200) loss: 1.958588\n",
      "(Epoch 8 / 20) train acc: 0.197000; val_acc: 0.186111\n",
      "(Iteration 81 / 200) loss: 2.020727\n",
      "(Epoch 9 / 20) train acc: 0.199000; val_acc: 0.177778\n",
      "(Iteration 91 / 200) loss: 1.991760\n",
      "(Epoch 10 / 20) train acc: 0.221000; val_acc: 0.188889\n",
      "(Iteration 101 / 200) loss: 1.953744\n",
      "(Epoch 11 / 20) train acc: 0.188000; val_acc: 0.183333\n",
      "(Iteration 111 / 200) loss: 1.873177\n",
      "(Epoch 12 / 20) train acc: 0.193000; val_acc: 0.180556\n",
      "(Iteration 121 / 200) loss: 1.915027\n",
      "(Epoch 13 / 20) train acc: 0.178000; val_acc: 0.197222\n",
      "(Iteration 131 / 200) loss: 1.882265\n",
      "(Epoch 14 / 20) train acc: 0.196000; val_acc: 0.194444\n",
      "(Iteration 141 / 200) loss: 1.849007\n",
      "(Epoch 15 / 20) train acc: 0.218000; val_acc: 0.200000\n",
      "(Iteration 151 / 200) loss: 1.899678\n",
      "(Epoch 16 / 20) train acc: 0.234000; val_acc: 0.208333\n",
      "(Iteration 161 / 200) loss: 1.895826\n",
      "(Epoch 17 / 20) train acc: 0.213000; val_acc: 0.236111\n",
      "(Iteration 171 / 200) loss: 1.884170\n",
      "(Epoch 18 / 20) train acc: 0.250000; val_acc: 0.225000\n",
      "(Iteration 181 / 200) loss: 1.665602\n",
      "(Epoch 19 / 20) train acc: 0.337000; val_acc: 0.277778\n",
      "(Iteration 191 / 200) loss: 1.851338\n",
      "(Epoch 20 / 20) train acc: 0.341000; val_acc: 0.283333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 2.302778\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.295000\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.125000\n",
      "(Iteration 31 / 200) loss: 2.235403\n",
      "(Epoch 4 / 20) train acc: 0.197000; val_acc: 0.211111\n",
      "(Iteration 41 / 200) loss: 2.074988\n",
      "(Epoch 5 / 20) train acc: 0.190000; val_acc: 0.211111\n",
      "(Iteration 51 / 200) loss: 2.086159\n",
      "(Epoch 6 / 20) train acc: 0.187000; val_acc: 0.208333\n",
      "(Iteration 61 / 200) loss: 1.989306\n",
      "(Epoch 7 / 20) train acc: 0.208000; val_acc: 0.211111\n",
      "(Iteration 71 / 200) loss: 2.037769\n",
      "(Epoch 8 / 20) train acc: 0.176000; val_acc: 0.211111\n",
      "(Iteration 81 / 200) loss: 1.989224\n",
      "(Epoch 9 / 20) train acc: 0.203000; val_acc: 0.216667\n",
      "(Iteration 91 / 200) loss: 2.033582\n",
      "(Epoch 10 / 20) train acc: 0.197000; val_acc: 0.211111\n",
      "(Iteration 101 / 200) loss: 1.972393\n",
      "(Epoch 11 / 20) train acc: 0.203000; val_acc: 0.211111\n",
      "(Iteration 111 / 200) loss: 1.904496\n",
      "(Epoch 12 / 20) train acc: 0.215000; val_acc: 0.211111\n",
      "(Iteration 121 / 200) loss: 1.976557\n",
      "(Epoch 13 / 20) train acc: 0.202000; val_acc: 0.219444\n",
      "(Iteration 131 / 200) loss: 1.962389\n",
      "(Epoch 14 / 20) train acc: 0.218000; val_acc: 0.225000\n",
      "(Iteration 141 / 200) loss: 1.898724\n",
      "(Epoch 15 / 20) train acc: 0.307000; val_acc: 0.300000\n",
      "(Iteration 151 / 200) loss: 1.923750\n",
      "(Epoch 16 / 20) train acc: 0.334000; val_acc: 0.288889\n",
      "(Iteration 161 / 200) loss: 1.748454\n",
      "(Epoch 17 / 20) train acc: 0.307000; val_acc: 0.291667\n",
      "(Iteration 171 / 200) loss: 1.788358\n",
      "(Epoch 18 / 20) train acc: 0.324000; val_acc: 0.305556\n",
      "(Iteration 181 / 200) loss: 1.703137\n",
      "(Epoch 19 / 20) train acc: 0.325000; val_acc: 0.300000\n",
      "(Iteration 191 / 200) loss: 1.667392\n",
      "(Epoch 20 / 20) train acc: 0.318000; val_acc: 0.286111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302989\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.301886\n",
      "(Epoch 3 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.301975\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.301640\n",
      "(Epoch 5 / 20) train acc: 0.079000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.301054\n",
      "(Epoch 6 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.304098\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.301520\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.301239\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.300902\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302164\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302802\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302763\n",
      "(Epoch 13 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.300741\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.303157\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.301924\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.299724\n",
      "(Epoch 17 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.299637\n",
      "(Epoch 18 / 20) train acc: 0.122000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.297718\n",
      "(Epoch 19 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.298363\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 35506.788784\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.063889\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.063889\n",
      "(Iteration 11 / 200) loss: 34774.607852\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.069444\n",
      "(Iteration 21 / 200) loss: 36698.839038\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 33861.896223\n",
      "(Epoch 4 / 20) train acc: 0.123000; val_acc: 0.077778\n",
      "(Iteration 41 / 200) loss: 35720.218621\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 30648.861523\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 31545.972236\n",
      "(Epoch 7 / 20) train acc: 0.121000; val_acc: 0.108333\n",
      "(Iteration 71 / 200) loss: 26030.312930\n",
      "(Epoch 8 / 20) train acc: 0.141000; val_acc: 0.113889\n",
      "(Iteration 81 / 200) loss: 27918.896406\n",
      "(Epoch 9 / 20) train acc: 0.141000; val_acc: 0.122222\n",
      "(Iteration 91 / 200) loss: 27104.799941\n",
      "(Epoch 10 / 20) train acc: 0.142000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 27953.376426\n",
      "(Epoch 11 / 20) train acc: 0.137000; val_acc: 0.119444\n",
      "(Iteration 111 / 200) loss: 25550.703198\n",
      "(Epoch 12 / 20) train acc: 0.147000; val_acc: 0.122222\n",
      "(Iteration 121 / 200) loss: 21579.647427\n",
      "(Epoch 13 / 20) train acc: 0.157000; val_acc: 0.127778\n",
      "(Iteration 131 / 200) loss: 22490.042534\n",
      "(Epoch 14 / 20) train acc: 0.147000; val_acc: 0.127778\n",
      "(Iteration 141 / 200) loss: 22654.015474\n",
      "(Epoch 15 / 20) train acc: 0.149000; val_acc: 0.130556\n",
      "(Iteration 151 / 200) loss: 20183.182249\n",
      "(Epoch 16 / 20) train acc: 0.157000; val_acc: 0.136111\n",
      "(Iteration 161 / 200) loss: 16835.961699\n",
      "(Epoch 17 / 20) train acc: 0.143000; val_acc: 0.136111\n",
      "(Iteration 171 / 200) loss: 16873.222769\n",
      "(Epoch 18 / 20) train acc: 0.164000; val_acc: 0.136111\n",
      "(Iteration 181 / 200) loss: 15886.045444\n",
      "(Epoch 19 / 20) train acc: 0.172000; val_acc: 0.133333\n",
      "(Iteration 191 / 200) loss: 18920.120513\n",
      "(Epoch 20 / 20) train acc: 0.156000; val_acc: 0.141667\n",
      "(Iteration 1 / 200) loss: 4.285009\n",
      "(Epoch 0 / 20) train acc: 0.147000; val_acc: 0.150000\n",
      "(Epoch 1 / 20) train acc: 0.208000; val_acc: 0.191667\n",
      "(Iteration 11 / 200) loss: 3.817826\n",
      "(Epoch 2 / 20) train acc: 0.263000; val_acc: 0.233333\n",
      "(Iteration 21 / 200) loss: 3.559240\n",
      "(Epoch 3 / 20) train acc: 0.374000; val_acc: 0.319444\n",
      "(Iteration 31 / 200) loss: 3.230025\n",
      "(Epoch 4 / 20) train acc: 0.413000; val_acc: 0.383333\n",
      "(Iteration 41 / 200) loss: 2.980726\n",
      "(Epoch 5 / 20) train acc: 0.493000; val_acc: 0.441667\n",
      "(Iteration 51 / 200) loss: 3.023513\n",
      "(Epoch 6 / 20) train acc: 0.572000; val_acc: 0.519444\n",
      "(Iteration 61 / 200) loss: 2.796778\n",
      "(Epoch 7 / 20) train acc: 0.617000; val_acc: 0.563889\n",
      "(Iteration 71 / 200) loss: 2.701165\n",
      "(Epoch 8 / 20) train acc: 0.649000; val_acc: 0.594444\n",
      "(Iteration 81 / 200) loss: 2.549303\n",
      "(Epoch 9 / 20) train acc: 0.729000; val_acc: 0.638889\n",
      "(Iteration 91 / 200) loss: 2.329324\n",
      "(Epoch 10 / 20) train acc: 0.766000; val_acc: 0.680556\n",
      "(Iteration 101 / 200) loss: 2.303189\n",
      "(Epoch 11 / 20) train acc: 0.789000; val_acc: 0.711111\n",
      "(Iteration 111 / 200) loss: 2.210130\n",
      "(Epoch 12 / 20) train acc: 0.823000; val_acc: 0.741667\n",
      "(Iteration 121 / 200) loss: 2.209257\n",
      "(Epoch 13 / 20) train acc: 0.861000; val_acc: 0.766667\n",
      "(Iteration 131 / 200) loss: 2.009715\n",
      "(Epoch 14 / 20) train acc: 0.840000; val_acc: 0.783333\n",
      "(Iteration 141 / 200) loss: 1.830791\n",
      "(Epoch 15 / 20) train acc: 0.850000; val_acc: 0.802778\n",
      "(Iteration 151 / 200) loss: 1.955052\n",
      "(Epoch 16 / 20) train acc: 0.867000; val_acc: 0.813889\n",
      "(Iteration 161 / 200) loss: 1.816723\n",
      "(Epoch 17 / 20) train acc: 0.876000; val_acc: 0.825000\n",
      "(Iteration 171 / 200) loss: 1.655821\n",
      "(Epoch 18 / 20) train acc: 0.896000; val_acc: 0.836111\n",
      "(Iteration 181 / 200) loss: 1.695486\n",
      "(Epoch 19 / 20) train acc: 0.901000; val_acc: 0.836111\n",
      "(Iteration 191 / 200) loss: 1.574544\n",
      "(Epoch 20 / 20) train acc: 0.940000; val_acc: 0.830556\n",
      "(Iteration 1 / 200) loss: 2.315705\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.191000; val_acc: 0.166667\n",
      "(Iteration 11 / 200) loss: 2.313636\n",
      "(Epoch 2 / 20) train acc: 0.148000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.311833\n",
      "(Epoch 3 / 20) train acc: 0.234000; val_acc: 0.208333\n",
      "(Iteration 31 / 200) loss: 2.310212\n",
      "(Epoch 4 / 20) train acc: 0.224000; val_acc: 0.230556\n",
      "(Iteration 41 / 200) loss: 2.308482\n",
      "(Epoch 5 / 20) train acc: 0.239000; val_acc: 0.216667\n",
      "(Iteration 51 / 200) loss: 2.306553\n",
      "(Epoch 6 / 20) train acc: 0.251000; val_acc: 0.241667\n",
      "(Iteration 61 / 200) loss: 2.304909\n",
      "(Epoch 7 / 20) train acc: 0.297000; val_acc: 0.263889\n",
      "(Iteration 71 / 200) loss: 2.298804\n",
      "(Epoch 8 / 20) train acc: 0.229000; val_acc: 0.188889\n",
      "(Iteration 81 / 200) loss: 2.283285\n",
      "(Epoch 9 / 20) train acc: 0.151000; val_acc: 0.158333\n",
      "(Iteration 91 / 200) loss: 2.272128\n",
      "(Epoch 10 / 20) train acc: 0.205000; val_acc: 0.194444\n",
      "(Iteration 101 / 200) loss: 2.212216\n",
      "(Epoch 11 / 20) train acc: 0.208000; val_acc: 0.202778\n",
      "(Iteration 111 / 200) loss: 2.137775\n",
      "(Epoch 12 / 20) train acc: 0.215000; val_acc: 0.225000\n",
      "(Iteration 121 / 200) loss: 2.047728\n",
      "(Epoch 13 / 20) train acc: 0.226000; val_acc: 0.211111\n",
      "(Iteration 131 / 200) loss: 1.980048\n",
      "(Epoch 14 / 20) train acc: 0.212000; val_acc: 0.219444\n",
      "(Iteration 141 / 200) loss: 1.986143\n",
      "(Epoch 15 / 20) train acc: 0.280000; val_acc: 0.241667\n",
      "(Iteration 151 / 200) loss: 1.879804\n",
      "(Epoch 16 / 20) train acc: 0.303000; val_acc: 0.266667\n",
      "(Iteration 161 / 200) loss: 1.870576\n",
      "(Epoch 17 / 20) train acc: 0.342000; val_acc: 0.283333\n",
      "(Iteration 171 / 200) loss: 1.746336\n",
      "(Epoch 18 / 20) train acc: 0.356000; val_acc: 0.344444\n",
      "(Iteration 181 / 200) loss: 1.716887\n",
      "(Epoch 19 / 20) train acc: 0.467000; val_acc: 0.433333\n",
      "(Iteration 191 / 200) loss: 1.622647\n",
      "(Epoch 20 / 20) train acc: 0.497000; val_acc: 0.452778\n",
      "(Iteration 1 / 200) loss: 2.302717\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302626\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302573\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302691\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302441\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302315\n",
      "(Epoch 6 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302369\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302736\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302611\n",
      "(Epoch 9 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302814\n",
      "(Epoch 10 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302167\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302660\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302396\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302512\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.301736\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302651\n",
      "(Epoch 16 / 20) train acc: 0.128000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.301408\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302404\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.303104\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.303065\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302536\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302607\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302657\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302616\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302608\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302247\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302838\n",
      "(Epoch 8 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302829\n",
      "(Epoch 9 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302661\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302981\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302318\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302600\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302550\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302597\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302614\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302307\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302645\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302848\n",
      "(Epoch 19 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301608\n",
      "(Epoch 20 / 20) train acc: 0.085000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302672\n",
      "(Epoch 3 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302493\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302588\n",
      "(Epoch 5 / 20) train acc: 0.084000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302509\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302637\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302577\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302704\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302496\n",
      "(Epoch 10 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302604\n",
      "(Epoch 11 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302410\n",
      "(Epoch 12 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302609\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302159\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.303216\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302137\n",
      "(Epoch 16 / 20) train acc: 0.127000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.303013\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302325\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302864\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302818\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 38749.230762\n",
      "(Epoch 0 / 20) train acc: 0.057000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.060000; val_acc: 0.069444\n",
      "(Iteration 11 / 200) loss: 35896.527192\n",
      "(Epoch 2 / 20) train acc: 0.056000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 38110.413823\n",
      "(Epoch 3 / 20) train acc: 0.049000; val_acc: 0.077778\n",
      "(Iteration 31 / 200) loss: 33422.076675\n",
      "(Epoch 4 / 20) train acc: 0.065000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 34994.882573\n",
      "(Epoch 5 / 20) train acc: 0.053000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 32401.773823\n",
      "(Epoch 6 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 33721.230332\n",
      "(Epoch 7 / 20) train acc: 0.054000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 28894.589624\n",
      "(Epoch 8 / 20) train acc: 0.056000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 29388.909111\n",
      "(Epoch 9 / 20) train acc: 0.066000; val_acc: 0.086111\n",
      "(Iteration 91 / 200) loss: 30158.386523\n",
      "(Epoch 10 / 20) train acc: 0.076000; val_acc: 0.086111\n",
      "(Iteration 101 / 200) loss: 26322.209280\n",
      "(Epoch 11 / 20) train acc: 0.057000; val_acc: 0.086111\n",
      "(Iteration 111 / 200) loss: 26678.177222\n",
      "(Epoch 12 / 20) train acc: 0.075000; val_acc: 0.077778\n",
      "(Iteration 121 / 200) loss: 24945.357666\n",
      "(Epoch 13 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 24201.237942\n",
      "(Epoch 14 / 20) train acc: 0.067000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 23906.778530\n",
      "(Epoch 15 / 20) train acc: 0.087000; val_acc: 0.086111\n",
      "(Iteration 151 / 200) loss: 21382.569043\n",
      "(Epoch 16 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 23565.394878\n",
      "(Epoch 17 / 20) train acc: 0.079000; val_acc: 0.086111\n",
      "(Iteration 171 / 200) loss: 20532.107280\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.088889\n",
      "(Iteration 181 / 200) loss: 20333.672507\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 191 / 200) loss: 19100.426978\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 1 / 200) loss: 7.242699\n",
      "(Epoch 0 / 20) train acc: 0.071000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.078000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 6.070428\n",
      "(Epoch 2 / 20) train acc: 0.130000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 4.826624\n",
      "(Epoch 3 / 20) train acc: 0.146000; val_acc: 0.163889\n",
      "(Iteration 31 / 200) loss: 4.527686\n",
      "(Epoch 4 / 20) train acc: 0.260000; val_acc: 0.275000\n",
      "(Iteration 41 / 200) loss: 3.934062\n",
      "(Epoch 5 / 20) train acc: 0.300000; val_acc: 0.308333\n",
      "(Iteration 51 / 200) loss: 3.418570\n",
      "(Epoch 6 / 20) train acc: 0.408000; val_acc: 0.416667\n",
      "(Iteration 61 / 200) loss: 2.934279\n",
      "(Epoch 7 / 20) train acc: 0.500000; val_acc: 0.508333\n",
      "(Iteration 71 / 200) loss: 2.716211\n",
      "(Epoch 8 / 20) train acc: 0.613000; val_acc: 0.588889\n",
      "(Iteration 81 / 200) loss: 2.770420\n",
      "(Epoch 9 / 20) train acc: 0.675000; val_acc: 0.669444\n",
      "(Iteration 91 / 200) loss: 2.557480\n",
      "(Epoch 10 / 20) train acc: 0.740000; val_acc: 0.719444\n",
      "(Iteration 101 / 200) loss: 2.419510\n",
      "(Epoch 11 / 20) train acc: 0.765000; val_acc: 0.750000\n",
      "(Iteration 111 / 200) loss: 2.227108\n",
      "(Epoch 12 / 20) train acc: 0.782000; val_acc: 0.775000\n",
      "(Iteration 121 / 200) loss: 2.359875\n",
      "(Epoch 13 / 20) train acc: 0.834000; val_acc: 0.794444\n",
      "(Iteration 131 / 200) loss: 2.176936\n",
      "(Epoch 14 / 20) train acc: 0.839000; val_acc: 0.825000\n",
      "(Iteration 141 / 200) loss: 1.901850\n",
      "(Epoch 15 / 20) train acc: 0.848000; val_acc: 0.830556\n",
      "(Iteration 151 / 200) loss: 1.914628\n",
      "(Epoch 16 / 20) train acc: 0.857000; val_acc: 0.830556\n",
      "(Iteration 161 / 200) loss: 2.048242\n",
      "(Epoch 17 / 20) train acc: 0.858000; val_acc: 0.838889\n",
      "(Iteration 171 / 200) loss: 1.765453\n",
      "(Epoch 18 / 20) train acc: 0.881000; val_acc: 0.850000\n",
      "(Iteration 181 / 200) loss: 1.798036\n",
      "(Epoch 19 / 20) train acc: 0.876000; val_acc: 0.866667\n",
      "(Iteration 191 / 200) loss: 1.698875\n",
      "(Epoch 20 / 20) train acc: 0.900000; val_acc: 0.872222\n",
      "(Iteration 1 / 200) loss: 2.315812\n",
      "(Epoch 0 / 20) train acc: 0.133000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.313846\n",
      "(Epoch 2 / 20) train acc: 0.225000; val_acc: 0.222222\n",
      "(Iteration 21 / 200) loss: 2.312033\n",
      "(Epoch 3 / 20) train acc: 0.212000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 2.310609\n",
      "(Epoch 4 / 20) train acc: 0.191000; val_acc: 0.186111\n",
      "(Iteration 41 / 200) loss: 2.309283\n",
      "(Epoch 5 / 20) train acc: 0.157000; val_acc: 0.136111\n",
      "(Iteration 51 / 200) loss: 2.307748\n",
      "(Epoch 6 / 20) train acc: 0.243000; val_acc: 0.213889\n",
      "(Iteration 61 / 200) loss: 2.306479\n",
      "(Epoch 7 / 20) train acc: 0.190000; val_acc: 0.186111\n",
      "(Iteration 71 / 200) loss: 2.304474\n",
      "(Epoch 8 / 20) train acc: 0.248000; val_acc: 0.236111\n",
      "(Iteration 81 / 200) loss: 2.300541\n",
      "(Epoch 9 / 20) train acc: 0.253000; val_acc: 0.236111\n",
      "(Iteration 91 / 200) loss: 2.293410\n",
      "(Epoch 10 / 20) train acc: 0.257000; val_acc: 0.230556\n",
      "(Iteration 101 / 200) loss: 2.278548\n",
      "(Epoch 11 / 20) train acc: 0.277000; val_acc: 0.252778\n",
      "(Iteration 111 / 200) loss: 2.238432\n",
      "(Epoch 12 / 20) train acc: 0.273000; val_acc: 0.255556\n",
      "(Iteration 121 / 200) loss: 2.193654\n",
      "(Epoch 13 / 20) train acc: 0.279000; val_acc: 0.258333\n",
      "(Iteration 131 / 200) loss: 2.113693\n",
      "(Epoch 14 / 20) train acc: 0.272000; val_acc: 0.272222\n",
      "(Iteration 141 / 200) loss: 1.930293\n",
      "(Epoch 15 / 20) train acc: 0.305000; val_acc: 0.291667\n",
      "(Iteration 151 / 200) loss: 1.849719\n",
      "(Epoch 16 / 20) train acc: 0.323000; val_acc: 0.288889\n",
      "(Iteration 161 / 200) loss: 1.756581\n",
      "(Epoch 17 / 20) train acc: 0.469000; val_acc: 0.433333\n",
      "(Iteration 171 / 200) loss: 1.606208\n",
      "(Epoch 18 / 20) train acc: 0.521000; val_acc: 0.538889\n",
      "(Iteration 181 / 200) loss: 1.595948\n",
      "(Epoch 19 / 20) train acc: 0.550000; val_acc: 0.566667\n",
      "(Iteration 191 / 200) loss: 1.409171\n",
      "(Epoch 20 / 20) train acc: 0.573000; val_acc: 0.597222\n",
      "(Iteration 1 / 200) loss: 2.302718\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302615\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302615\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302607\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302654\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302521\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302582\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302611\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302573\n",
      "(Epoch 9 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302502\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302479\n",
      "(Epoch 11 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302567\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302385\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302443\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302339\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302589\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302381\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302543\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302389\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302958\n",
      "(Epoch 20 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302542\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302515\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302482\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302596\n",
      "(Epoch 5 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302522\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302912\n",
      "(Epoch 7 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302594\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302405\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302411\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302469\n",
      "(Epoch 11 / 20) train acc: 0.133000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302149\n",
      "(Epoch 12 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302096\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302333\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302540\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302066\n",
      "(Epoch 16 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302381\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302119\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302557\n",
      "(Epoch 19 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302197\n",
      "(Epoch 20 / 20) train acc: 0.084000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302518\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302564\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302560\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302546\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302668\n",
      "(Epoch 6 / 20) train acc: 0.124000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302655\n",
      "(Epoch 7 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302541\n",
      "(Epoch 8 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302624\n",
      "(Epoch 9 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302800\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302298\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302501\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302971\n",
      "(Epoch 13 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302072\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302880\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.303205\n",
      "(Epoch 16 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.302636\n",
      "(Epoch 17 / 20) train acc: 0.119000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.302128\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.302041\n",
      "(Epoch 19 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.302362\n",
      "(Epoch 20 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 55170.163433\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 40550.966311\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.108333\n",
      "(Iteration 21 / 200) loss: 38941.459709\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.113889\n",
      "(Iteration 31 / 200) loss: 44734.136367\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.111111\n",
      "(Iteration 41 / 200) loss: 42253.745742\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.119444\n",
      "(Iteration 51 / 200) loss: 41383.347776\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.130556\n",
      "(Iteration 61 / 200) loss: 42468.900776\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.133333\n",
      "(Iteration 71 / 200) loss: 33882.944014\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.141667\n",
      "(Iteration 81 / 200) loss: 33898.904805\n",
      "(Epoch 9 / 20) train acc: 0.129000; val_acc: 0.136111\n",
      "(Iteration 91 / 200) loss: 29942.593296\n",
      "(Epoch 10 / 20) train acc: 0.129000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 32807.082100\n",
      "(Epoch 11 / 20) train acc: 0.138000; val_acc: 0.152778\n",
      "(Iteration 111 / 200) loss: 31639.516060\n",
      "(Epoch 12 / 20) train acc: 0.143000; val_acc: 0.152778\n",
      "(Iteration 121 / 200) loss: 27573.932693\n",
      "(Epoch 13 / 20) train acc: 0.158000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 29082.689326\n",
      "(Epoch 14 / 20) train acc: 0.179000; val_acc: 0.166667\n",
      "(Iteration 141 / 200) loss: 25476.313782\n",
      "(Epoch 15 / 20) train acc: 0.176000; val_acc: 0.169444\n",
      "(Iteration 151 / 200) loss: 25897.653599\n",
      "(Epoch 16 / 20) train acc: 0.163000; val_acc: 0.177778\n",
      "(Iteration 161 / 200) loss: 25122.596165\n",
      "(Epoch 17 / 20) train acc: 0.185000; val_acc: 0.183333\n",
      "(Iteration 171 / 200) loss: 25069.376970\n",
      "(Epoch 18 / 20) train acc: 0.168000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 24443.373064\n",
      "(Epoch 19 / 20) train acc: 0.195000; val_acc: 0.197222\n",
      "(Iteration 191 / 200) loss: 23767.311802\n",
      "(Epoch 20 / 20) train acc: 0.191000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 6.124940\n",
      "(Epoch 0 / 20) train acc: 0.074000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 4.568791\n",
      "(Epoch 2 / 20) train acc: 0.217000; val_acc: 0.183333\n",
      "(Iteration 21 / 200) loss: 4.350970\n",
      "(Epoch 3 / 20) train acc: 0.258000; val_acc: 0.252778\n",
      "(Iteration 31 / 200) loss: 4.081539\n",
      "(Epoch 4 / 20) train acc: 0.315000; val_acc: 0.286111\n",
      "(Iteration 41 / 200) loss: 3.290328\n",
      "(Epoch 5 / 20) train acc: 0.375000; val_acc: 0.330556\n",
      "(Iteration 51 / 200) loss: 3.304475\n",
      "(Epoch 6 / 20) train acc: 0.431000; val_acc: 0.391667\n",
      "(Iteration 61 / 200) loss: 3.112737\n",
      "(Epoch 7 / 20) train acc: 0.496000; val_acc: 0.436111\n",
      "(Iteration 71 / 200) loss: 2.793988\n",
      "(Epoch 8 / 20) train acc: 0.562000; val_acc: 0.511111\n",
      "(Iteration 81 / 200) loss: 2.597234\n",
      "(Epoch 9 / 20) train acc: 0.636000; val_acc: 0.594444\n",
      "(Iteration 91 / 200) loss: 2.392162\n",
      "(Epoch 10 / 20) train acc: 0.692000; val_acc: 0.650000\n",
      "(Iteration 101 / 200) loss: 2.367257\n",
      "(Epoch 11 / 20) train acc: 0.781000; val_acc: 0.691667\n",
      "(Iteration 111 / 200) loss: 2.287327\n",
      "(Epoch 12 / 20) train acc: 0.779000; val_acc: 0.727778\n",
      "(Iteration 121 / 200) loss: 2.073795\n",
      "(Epoch 13 / 20) train acc: 0.775000; val_acc: 0.766667\n",
      "(Iteration 131 / 200) loss: 1.933348\n",
      "(Epoch 14 / 20) train acc: 0.815000; val_acc: 0.791667\n",
      "(Iteration 141 / 200) loss: 1.878085\n",
      "(Epoch 15 / 20) train acc: 0.840000; val_acc: 0.805556\n",
      "(Iteration 151 / 200) loss: 1.876019\n",
      "(Epoch 16 / 20) train acc: 0.865000; val_acc: 0.825000\n",
      "(Iteration 161 / 200) loss: 1.900974\n",
      "(Epoch 17 / 20) train acc: 0.858000; val_acc: 0.836111\n",
      "(Iteration 171 / 200) loss: 1.860588\n",
      "(Epoch 18 / 20) train acc: 0.853000; val_acc: 0.847222\n",
      "(Iteration 181 / 200) loss: 1.778833\n",
      "(Epoch 19 / 20) train acc: 0.868000; val_acc: 0.872222\n",
      "(Iteration 191 / 200) loss: 1.677245\n",
      "(Epoch 20 / 20) train acc: 0.902000; val_acc: 0.875000\n",
      "(Iteration 1 / 200) loss: 2.315932\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.260000; val_acc: 0.258333\n",
      "(Iteration 11 / 200) loss: 2.313901\n",
      "(Epoch 2 / 20) train acc: 0.116000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.312117\n",
      "(Epoch 3 / 20) train acc: 0.178000; val_acc: 0.144444\n",
      "(Iteration 31 / 200) loss: 2.310543\n",
      "(Epoch 4 / 20) train acc: 0.269000; val_acc: 0.266667\n",
      "(Iteration 41 / 200) loss: 2.309103\n",
      "(Epoch 5 / 20) train acc: 0.305000; val_acc: 0.286111\n",
      "(Iteration 51 / 200) loss: 2.307868\n",
      "(Epoch 6 / 20) train acc: 0.344000; val_acc: 0.316667\n",
      "(Iteration 61 / 200) loss: 2.305502\n",
      "(Epoch 7 / 20) train acc: 0.322000; val_acc: 0.294444\n",
      "(Iteration 71 / 200) loss: 2.302058\n",
      "(Epoch 8 / 20) train acc: 0.327000; val_acc: 0.277778\n",
      "(Iteration 81 / 200) loss: 2.297163\n",
      "(Epoch 9 / 20) train acc: 0.363000; val_acc: 0.313889\n",
      "(Iteration 91 / 200) loss: 2.279404\n",
      "(Epoch 10 / 20) train acc: 0.284000; val_acc: 0.258333\n",
      "(Iteration 101 / 200) loss: 2.245567\n",
      "(Epoch 11 / 20) train acc: 0.339000; val_acc: 0.297222\n",
      "(Iteration 111 / 200) loss: 2.184644\n",
      "(Epoch 12 / 20) train acc: 0.307000; val_acc: 0.272222\n",
      "(Iteration 121 / 200) loss: 2.109483\n",
      "(Epoch 13 / 20) train acc: 0.308000; val_acc: 0.277778\n",
      "(Iteration 131 / 200) loss: 1.930478\n",
      "(Epoch 14 / 20) train acc: 0.322000; val_acc: 0.288889\n",
      "(Iteration 141 / 200) loss: 1.883540\n",
      "(Epoch 15 / 20) train acc: 0.458000; val_acc: 0.391667\n",
      "(Iteration 151 / 200) loss: 1.748768\n",
      "(Epoch 16 / 20) train acc: 0.465000; val_acc: 0.444444\n",
      "(Iteration 161 / 200) loss: 1.619707\n",
      "(Epoch 17 / 20) train acc: 0.479000; val_acc: 0.458333\n",
      "(Iteration 171 / 200) loss: 1.586482\n",
      "(Epoch 18 / 20) train acc: 0.499000; val_acc: 0.483333\n",
      "(Iteration 181 / 200) loss: 1.431392\n",
      "(Epoch 19 / 20) train acc: 0.533000; val_acc: 0.497222\n",
      "(Iteration 191 / 200) loss: 1.316089\n",
      "(Epoch 20 / 20) train acc: 0.510000; val_acc: 0.527778\n",
      "(Iteration 1 / 200) loss: 2.302719\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302602\n",
      "(Epoch 3 / 20) train acc: 0.082000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302547\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302586\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302513\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302658\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302527\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302283\n",
      "(Epoch 9 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302525\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302289\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302229\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302434\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302138\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.301897\n",
      "(Epoch 15 / 20) train acc: 0.137000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302166\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302581\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302309\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.301653\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302956\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302555\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302603\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302833\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302541\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302552\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302766\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302461\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302570\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302445\n",
      "(Epoch 10 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302465\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302448\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302416\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302370\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302267\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302381\n",
      "(Epoch 16 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302800\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302255\n",
      "(Epoch 18 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.301797\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302518\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302611\n",
      "(Epoch 2 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302555\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302700\n",
      "(Epoch 4 / 20) train acc: 0.077000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302483\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302647\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302578\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302478\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302548\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302547\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302399\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.301712\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302781\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302736\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302382\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302618\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302296\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302376\n",
      "(Epoch 18 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302165\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302203\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 46694.472458\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 40122.753494\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.111111\n",
      "(Iteration 21 / 200) loss: 37536.035163\n",
      "(Epoch 3 / 20) train acc: 0.129000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 41990.851895\n",
      "(Epoch 4 / 20) train acc: 0.125000; val_acc: 0.119444\n",
      "(Iteration 41 / 200) loss: 40124.221198\n",
      "(Epoch 5 / 20) train acc: 0.122000; val_acc: 0.127778\n",
      "(Iteration 51 / 200) loss: 38634.573038\n",
      "(Epoch 6 / 20) train acc: 0.134000; val_acc: 0.138889\n",
      "(Iteration 61 / 200) loss: 38564.342418\n",
      "(Epoch 7 / 20) train acc: 0.140000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 34228.904339\n",
      "(Epoch 8 / 20) train acc: 0.138000; val_acc: 0.141667\n",
      "(Iteration 81 / 200) loss: 33583.398791\n",
      "(Epoch 9 / 20) train acc: 0.142000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 31466.985748\n",
      "(Epoch 10 / 20) train acc: 0.145000; val_acc: 0.141667\n",
      "(Iteration 101 / 200) loss: 33405.317750\n",
      "(Epoch 11 / 20) train acc: 0.158000; val_acc: 0.144444\n",
      "(Iteration 111 / 200) loss: 34244.299834\n",
      "(Epoch 12 / 20) train acc: 0.156000; val_acc: 0.141667\n",
      "(Iteration 121 / 200) loss: 27605.811925\n",
      "(Epoch 13 / 20) train acc: 0.160000; val_acc: 0.150000\n",
      "(Iteration 131 / 200) loss: 29852.221547\n",
      "(Epoch 14 / 20) train acc: 0.175000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 28899.308731\n",
      "(Epoch 15 / 20) train acc: 0.177000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 26698.693436\n",
      "(Epoch 16 / 20) train acc: 0.182000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 24035.730697\n",
      "(Epoch 17 / 20) train acc: 0.189000; val_acc: 0.183333\n",
      "(Iteration 171 / 200) loss: 31965.423020\n",
      "(Epoch 18 / 20) train acc: 0.189000; val_acc: 0.188889\n",
      "(Iteration 181 / 200) loss: 26603.302837\n",
      "(Epoch 19 / 20) train acc: 0.197000; val_acc: 0.188889\n",
      "(Iteration 191 / 200) loss: 27549.652676\n",
      "(Epoch 20 / 20) train acc: 0.194000; val_acc: 0.194444\n",
      "(Iteration 1 / 200) loss: 3.769449\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.102778\n",
      "(Iteration 11 / 200) loss: 3.108541\n",
      "(Epoch 2 / 20) train acc: 0.146000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 2.815246\n",
      "(Epoch 3 / 20) train acc: 0.259000; val_acc: 0.197222\n",
      "(Iteration 31 / 200) loss: 2.395478\n",
      "(Epoch 4 / 20) train acc: 0.351000; val_acc: 0.269444\n",
      "(Iteration 41 / 200) loss: 2.075543\n",
      "(Epoch 5 / 20) train acc: 0.429000; val_acc: 0.397222\n",
      "(Iteration 51 / 200) loss: 1.851321\n",
      "(Epoch 6 / 20) train acc: 0.536000; val_acc: 0.475000\n",
      "(Iteration 61 / 200) loss: 1.627734\n",
      "(Epoch 7 / 20) train acc: 0.660000; val_acc: 0.552778\n",
      "(Iteration 71 / 200) loss: 1.570033\n",
      "(Epoch 8 / 20) train acc: 0.712000; val_acc: 0.605556\n",
      "(Iteration 81 / 200) loss: 1.346759\n",
      "(Epoch 9 / 20) train acc: 0.731000; val_acc: 0.650000\n",
      "(Iteration 91 / 200) loss: 1.212264\n",
      "(Epoch 10 / 20) train acc: 0.757000; val_acc: 0.702778\n",
      "(Iteration 101 / 200) loss: 1.084971\n",
      "(Epoch 11 / 20) train acc: 0.798000; val_acc: 0.738889\n",
      "(Iteration 111 / 200) loss: 0.910351\n",
      "(Epoch 12 / 20) train acc: 0.819000; val_acc: 0.763889\n",
      "(Iteration 121 / 200) loss: 0.827415\n",
      "(Epoch 13 / 20) train acc: 0.837000; val_acc: 0.797222\n",
      "(Iteration 131 / 200) loss: 0.799534\n",
      "(Epoch 14 / 20) train acc: 0.856000; val_acc: 0.830556\n",
      "(Iteration 141 / 200) loss: 0.717060\n",
      "(Epoch 15 / 20) train acc: 0.866000; val_acc: 0.847222\n",
      "(Iteration 151 / 200) loss: 0.602002\n",
      "(Epoch 16 / 20) train acc: 0.865000; val_acc: 0.850000\n",
      "(Iteration 161 / 200) loss: 0.666409\n",
      "(Epoch 17 / 20) train acc: 0.895000; val_acc: 0.861111\n",
      "(Iteration 171 / 200) loss: 0.692463\n",
      "(Epoch 18 / 20) train acc: 0.919000; val_acc: 0.866667\n",
      "(Iteration 181 / 200) loss: 0.664027\n",
      "(Epoch 19 / 20) train acc: 0.913000; val_acc: 0.877778\n",
      "(Iteration 191 / 200) loss: 0.619400\n",
      "(Epoch 20 / 20) train acc: 0.933000; val_acc: 0.891667\n",
      "(Iteration 1 / 200) loss: 2.303899\n",
      "(Epoch 0 / 20) train acc: 0.057000; val_acc: 0.063889\n",
      "(Epoch 1 / 20) train acc: 0.495000; val_acc: 0.411111\n",
      "(Iteration 11 / 200) loss: 2.303466\n",
      "(Epoch 2 / 20) train acc: 0.401000; val_acc: 0.336111\n",
      "(Iteration 21 / 200) loss: 2.302602\n",
      "(Epoch 3 / 20) train acc: 0.534000; val_acc: 0.472222\n",
      "(Iteration 31 / 200) loss: 2.301719\n",
      "(Epoch 4 / 20) train acc: 0.472000; val_acc: 0.461111\n",
      "(Iteration 41 / 200) loss: 2.299716\n",
      "(Epoch 5 / 20) train acc: 0.518000; val_acc: 0.461111\n",
      "(Iteration 51 / 200) loss: 2.296684\n",
      "(Epoch 6 / 20) train acc: 0.484000; val_acc: 0.444444\n",
      "(Iteration 61 / 200) loss: 2.287264\n",
      "(Epoch 7 / 20) train acc: 0.555000; val_acc: 0.541667\n",
      "(Iteration 71 / 200) loss: 2.272411\n",
      "(Epoch 8 / 20) train acc: 0.567000; val_acc: 0.533333\n",
      "(Iteration 81 / 200) loss: 2.252056\n",
      "(Epoch 9 / 20) train acc: 0.599000; val_acc: 0.552778\n",
      "(Iteration 91 / 200) loss: 2.198754\n",
      "(Epoch 10 / 20) train acc: 0.504000; val_acc: 0.491667\n",
      "(Iteration 101 / 200) loss: 2.138146\n",
      "(Epoch 11 / 20) train acc: 0.532000; val_acc: 0.525000\n",
      "(Iteration 111 / 200) loss: 2.051470\n",
      "(Epoch 12 / 20) train acc: 0.565000; val_acc: 0.541667\n",
      "(Iteration 121 / 200) loss: 1.930787\n",
      "(Epoch 13 / 20) train acc: 0.522000; val_acc: 0.536111\n",
      "(Iteration 131 / 200) loss: 1.857162\n",
      "(Epoch 14 / 20) train acc: 0.572000; val_acc: 0.563889\n",
      "(Iteration 141 / 200) loss: 1.697857\n",
      "(Epoch 15 / 20) train acc: 0.609000; val_acc: 0.611111\n",
      "(Iteration 151 / 200) loss: 1.528186\n",
      "(Epoch 16 / 20) train acc: 0.589000; val_acc: 0.630556\n",
      "(Iteration 161 / 200) loss: 1.502913\n",
      "(Epoch 17 / 20) train acc: 0.640000; val_acc: 0.655556\n",
      "(Iteration 171 / 200) loss: 1.380043\n",
      "(Epoch 18 / 20) train acc: 0.672000; val_acc: 0.652778\n",
      "(Iteration 181 / 200) loss: 1.320133\n",
      "(Epoch 19 / 20) train acc: 0.680000; val_acc: 0.680556\n",
      "(Iteration 191 / 200) loss: 1.198310\n",
      "(Epoch 20 / 20) train acc: 0.725000; val_acc: 0.716667\n",
      "(Iteration 1 / 200) loss: 2.302598\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302577\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302624\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302522\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302516\n",
      "(Epoch 6 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302661\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302464\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302552\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302585\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302275\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302495\n",
      "(Epoch 12 / 20) train acc: 0.085000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.302363\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.302434\n",
      "(Epoch 14 / 20) train acc: 0.082000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.302411\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302285\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302588\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302828\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302705\n",
      "(Epoch 19 / 20) train acc: 0.085000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302859\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302610\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302547\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302566\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302652\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302347\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302276\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302510\n",
      "(Epoch 8 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302370\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302245\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.301679\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.301357\n",
      "(Epoch 12 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.297952\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.297564\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.263909\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.232457\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.207236\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.163600\n",
      "(Epoch 18 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.199553\n",
      "(Epoch 19 / 20) train acc: 0.165000; val_acc: 0.138889\n",
      "(Iteration 191 / 200) loss: 2.164200\n",
      "(Epoch 20 / 20) train acc: 0.167000; val_acc: 0.163889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 2.302537\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302567\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302524\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302498\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302450\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302478\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302597\n",
      "(Epoch 8 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302618\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302477\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302415\n",
      "(Epoch 11 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302599\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302744\n",
      "(Epoch 13 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302009\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302750\n",
      "(Epoch 15 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.301753\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302616\n",
      "(Epoch 17 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302267\n",
      "(Epoch 18 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302811\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302019\n",
      "(Epoch 20 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 29220.794190\n",
      "(Epoch 0 / 20) train acc: 0.079000; val_acc: 0.069444\n",
      "(Epoch 1 / 20) train acc: 0.076000; val_acc: 0.072222\n",
      "(Iteration 11 / 200) loss: 28618.630722\n",
      "(Epoch 2 / 20) train acc: 0.083000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 27641.773087\n",
      "(Epoch 3 / 20) train acc: 0.084000; val_acc: 0.077778\n",
      "(Iteration 31 / 200) loss: 25098.740630\n",
      "(Epoch 4 / 20) train acc: 0.084000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 23667.290758\n",
      "(Epoch 5 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 25793.793488\n",
      "(Epoch 6 / 20) train acc: 0.095000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 23358.166281\n",
      "(Epoch 7 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 24395.564109\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.094444\n",
      "(Iteration 81 / 200) loss: 22211.076940\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.094444\n",
      "(Iteration 91 / 200) loss: 18839.914826\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 20974.827717\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 19880.203114\n",
      "(Epoch 12 / 20) train acc: 0.101000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 20149.138547\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 15321.448984\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 13759.284459\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 19372.846199\n",
      "(Epoch 16 / 20) train acc: 0.087000; val_acc: 0.102778\n",
      "(Iteration 161 / 200) loss: 15040.626658\n",
      "(Epoch 17 / 20) train acc: 0.093000; val_acc: 0.105556\n",
      "(Iteration 171 / 200) loss: 14836.000885\n",
      "(Epoch 18 / 20) train acc: 0.128000; val_acc: 0.105556\n",
      "(Iteration 181 / 200) loss: 16849.215145\n",
      "(Epoch 19 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 191 / 200) loss: 14477.295697\n",
      "(Epoch 20 / 20) train acc: 0.075000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 3.367582\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.060000; val_acc: 0.038889\n",
      "(Iteration 11 / 200) loss: 2.906689\n",
      "(Epoch 2 / 20) train acc: 0.142000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.481453\n",
      "(Epoch 3 / 20) train acc: 0.179000; val_acc: 0.161111\n",
      "(Iteration 31 / 200) loss: 2.248628\n",
      "(Epoch 4 / 20) train acc: 0.280000; val_acc: 0.252778\n",
      "(Iteration 41 / 200) loss: 2.074338\n",
      "(Epoch 5 / 20) train acc: 0.380000; val_acc: 0.366667\n",
      "(Iteration 51 / 200) loss: 1.857591\n",
      "(Epoch 6 / 20) train acc: 0.454000; val_acc: 0.463889\n",
      "(Iteration 61 / 200) loss: 1.834296\n",
      "(Epoch 7 / 20) train acc: 0.592000; val_acc: 0.536111\n",
      "(Iteration 71 / 200) loss: 1.746479\n",
      "(Epoch 8 / 20) train acc: 0.651000; val_acc: 0.588889\n",
      "(Iteration 81 / 200) loss: 1.539973\n",
      "(Epoch 9 / 20) train acc: 0.691000; val_acc: 0.663889\n",
      "(Iteration 91 / 200) loss: 1.435317\n",
      "(Epoch 10 / 20) train acc: 0.777000; val_acc: 0.705556\n",
      "(Iteration 101 / 200) loss: 1.295852\n",
      "(Epoch 11 / 20) train acc: 0.821000; val_acc: 0.747222\n",
      "(Iteration 111 / 200) loss: 1.245250\n",
      "(Epoch 12 / 20) train acc: 0.823000; val_acc: 0.786111\n",
      "(Iteration 121 / 200) loss: 1.041457\n",
      "(Epoch 13 / 20) train acc: 0.842000; val_acc: 0.802778\n",
      "(Iteration 131 / 200) loss: 0.973790\n",
      "(Epoch 14 / 20) train acc: 0.844000; val_acc: 0.797222\n",
      "(Iteration 141 / 200) loss: 0.938212\n",
      "(Epoch 15 / 20) train acc: 0.886000; val_acc: 0.813889\n",
      "(Iteration 151 / 200) loss: 0.746543\n",
      "(Epoch 16 / 20) train acc: 0.899000; val_acc: 0.838889\n",
      "(Iteration 161 / 200) loss: 0.684121\n",
      "(Epoch 17 / 20) train acc: 0.899000; val_acc: 0.850000\n",
      "(Iteration 171 / 200) loss: 0.587512\n",
      "(Epoch 18 / 20) train acc: 0.920000; val_acc: 0.863889\n",
      "(Iteration 181 / 200) loss: 0.543853\n",
      "(Epoch 19 / 20) train acc: 0.917000; val_acc: 0.869444\n",
      "(Iteration 191 / 200) loss: 0.514345\n",
      "(Epoch 20 / 20) train acc: 0.924000; val_acc: 0.869444\n",
      "(Iteration 1 / 200) loss: 2.303923\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.191000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 2.303420\n",
      "(Epoch 2 / 20) train acc: 0.185000; val_acc: 0.166667\n",
      "(Iteration 21 / 200) loss: 2.303072\n",
      "(Epoch 3 / 20) train acc: 0.321000; val_acc: 0.319444\n",
      "(Iteration 31 / 200) loss: 2.302268\n",
      "(Epoch 4 / 20) train acc: 0.433000; val_acc: 0.433333\n",
      "(Iteration 41 / 200) loss: 2.298565\n",
      "(Epoch 5 / 20) train acc: 0.366000; val_acc: 0.391667\n",
      "(Iteration 51 / 200) loss: 2.296213\n",
      "(Epoch 6 / 20) train acc: 0.377000; val_acc: 0.341667\n",
      "(Iteration 61 / 200) loss: 2.289695\n",
      "(Epoch 7 / 20) train acc: 0.380000; val_acc: 0.355556\n",
      "(Iteration 71 / 200) loss: 2.272835\n",
      "(Epoch 8 / 20) train acc: 0.344000; val_acc: 0.319444\n",
      "(Iteration 81 / 200) loss: 2.243870\n",
      "(Epoch 9 / 20) train acc: 0.257000; val_acc: 0.275000\n",
      "(Iteration 91 / 200) loss: 2.191950\n",
      "(Epoch 10 / 20) train acc: 0.289000; val_acc: 0.261111\n",
      "(Iteration 101 / 200) loss: 2.142052\n",
      "(Epoch 11 / 20) train acc: 0.278000; val_acc: 0.277778\n",
      "(Iteration 111 / 200) loss: 2.004563\n",
      "(Epoch 12 / 20) train acc: 0.339000; val_acc: 0.319444\n",
      "(Iteration 121 / 200) loss: 1.923088\n",
      "(Epoch 13 / 20) train acc: 0.348000; val_acc: 0.333333\n",
      "(Iteration 131 / 200) loss: 1.755710\n",
      "(Epoch 14 / 20) train acc: 0.433000; val_acc: 0.419444\n",
      "(Iteration 141 / 200) loss: 1.677473\n",
      "(Epoch 15 / 20) train acc: 0.552000; val_acc: 0.561111\n",
      "(Iteration 151 / 200) loss: 1.641648\n",
      "(Epoch 16 / 20) train acc: 0.601000; val_acc: 0.622222\n",
      "(Iteration 161 / 200) loss: 1.490549\n",
      "(Epoch 17 / 20) train acc: 0.528000; val_acc: 0.558333\n",
      "(Iteration 171 / 200) loss: 1.351988\n",
      "(Epoch 18 / 20) train acc: 0.583000; val_acc: 0.605556\n",
      "(Iteration 181 / 200) loss: 1.263992\n",
      "(Epoch 19 / 20) train acc: 0.575000; val_acc: 0.594444\n",
      "(Iteration 191 / 200) loss: 1.305366\n",
      "(Epoch 20 / 20) train acc: 0.646000; val_acc: 0.677778\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 2.302597\n",
      "(Epoch 2 / 20) train acc: 0.079000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302553\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302580\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302706\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302570\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302547\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302536\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302500\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302605\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302574\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302608\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302441\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302849\n",
      "(Epoch 15 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302131\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302335\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302441\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302925\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302628\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302627\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302574\n",
      "(Epoch 3 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302742\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302399\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302519\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302330\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302484\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302607\n",
      "(Epoch 9 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302810\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302635\n",
      "(Epoch 11 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302467\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302484\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302765\n",
      "(Epoch 14 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302323\n",
      "(Epoch 15 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.303082\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302867\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302621\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302159\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302121\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302549\n",
      "(Epoch 2 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302471\n",
      "(Epoch 3 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302432\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302571\n",
      "(Epoch 5 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302491\n",
      "(Epoch 6 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302558\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302331\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302403\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302329\n",
      "(Epoch 10 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.301734\n",
      "(Epoch 11 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.301463\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.298801\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.289141\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.260073\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.221748\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.200776\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.200255\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.086111\n",
      "(Iteration 181 / 200) loss: 2.075793\n",
      "(Epoch 19 / 20) train acc: 0.183000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 2.104110\n",
      "(Epoch 20 / 20) train acc: 0.202000; val_acc: 0.188889\n",
      "(Iteration 1 / 200) loss: 26725.515448\n",
      "(Epoch 0 / 20) train acc: 0.073000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.077778\n",
      "(Iteration 11 / 200) loss: 28467.364363\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 25754.701347\n",
      "(Epoch 3 / 20) train acc: 0.077000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 24536.113592\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.094444\n",
      "(Iteration 41 / 200) loss: 23268.508451\n",
      "(Epoch 5 / 20) train acc: 0.088000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 21257.948373\n",
      "(Epoch 6 / 20) train acc: 0.078000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 20354.254519\n",
      "(Epoch 7 / 20) train acc: 0.081000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 19447.930648\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.108333\n",
      "(Iteration 81 / 200) loss: 17459.749288\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.108333\n",
      "(Iteration 91 / 200) loss: 16852.450430\n",
      "(Epoch 10 / 20) train acc: 0.084000; val_acc: 0.108333\n",
      "(Iteration 101 / 200) loss: 13987.519082\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 13371.986491\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.119444\n",
      "(Iteration 121 / 200) loss: 15278.633887\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.127778\n",
      "(Iteration 131 / 200) loss: 15034.970052\n",
      "(Epoch 14 / 20) train acc: 0.127000; val_acc: 0.133333\n",
      "(Iteration 141 / 200) loss: 13769.749964\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.127778\n",
      "(Iteration 151 / 200) loss: 13041.649907\n",
      "(Epoch 16 / 20) train acc: 0.154000; val_acc: 0.138889\n",
      "(Iteration 161 / 200) loss: 12878.789843\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.138889\n",
      "(Iteration 171 / 200) loss: 12986.176032\n",
      "(Epoch 18 / 20) train acc: 0.121000; val_acc: 0.144444\n",
      "(Iteration 181 / 200) loss: 11238.236012\n",
      "(Epoch 19 / 20) train acc: 0.147000; val_acc: 0.138889\n",
      "(Iteration 191 / 200) loss: 10590.356021\n",
      "(Epoch 20 / 20) train acc: 0.150000; val_acc: 0.152778\n",
      "(Iteration 1 / 200) loss: 5.365803\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 3.726478\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.111111\n",
      "(Iteration 21 / 200) loss: 3.273663\n",
      "(Epoch 3 / 20) train acc: 0.146000; val_acc: 0.152778\n",
      "(Iteration 31 / 200) loss: 2.866428\n",
      "(Epoch 4 / 20) train acc: 0.179000; val_acc: 0.191667\n",
      "(Iteration 41 / 200) loss: 2.529128\n",
      "(Epoch 5 / 20) train acc: 0.287000; val_acc: 0.250000\n",
      "(Iteration 51 / 200) loss: 2.319329\n",
      "(Epoch 6 / 20) train acc: 0.376000; val_acc: 0.377778\n",
      "(Iteration 61 / 200) loss: 1.964374\n",
      "(Epoch 7 / 20) train acc: 0.479000; val_acc: 0.483333\n",
      "(Iteration 71 / 200) loss: 1.844995\n",
      "(Epoch 8 / 20) train acc: 0.580000; val_acc: 0.550000\n",
      "(Iteration 81 / 200) loss: 1.566830\n",
      "(Epoch 9 / 20) train acc: 0.655000; val_acc: 0.611111\n",
      "(Iteration 91 / 200) loss: 1.522921\n",
      "(Epoch 10 / 20) train acc: 0.734000; val_acc: 0.669444\n",
      "(Iteration 101 / 200) loss: 1.220278\n",
      "(Epoch 11 / 20) train acc: 0.778000; val_acc: 0.722222\n",
      "(Iteration 111 / 200) loss: 1.203865\n",
      "(Epoch 12 / 20) train acc: 0.828000; val_acc: 0.761111\n",
      "(Iteration 121 / 200) loss: 1.065435\n",
      "(Epoch 13 / 20) train acc: 0.831000; val_acc: 0.780556\n",
      "(Iteration 131 / 200) loss: 0.964771\n",
      "(Epoch 14 / 20) train acc: 0.843000; val_acc: 0.816667\n",
      "(Iteration 141 / 200) loss: 0.811857\n",
      "(Epoch 15 / 20) train acc: 0.882000; val_acc: 0.841667\n",
      "(Iteration 151 / 200) loss: 0.818545\n",
      "(Epoch 16 / 20) train acc: 0.882000; val_acc: 0.836111\n",
      "(Iteration 161 / 200) loss: 0.732780\n",
      "(Epoch 17 / 20) train acc: 0.905000; val_acc: 0.852778\n",
      "(Iteration 171 / 200) loss: 0.731490\n",
      "(Epoch 18 / 20) train acc: 0.915000; val_acc: 0.875000\n",
      "(Iteration 181 / 200) loss: 0.666953\n",
      "(Epoch 19 / 20) train acc: 0.917000; val_acc: 0.880556\n",
      "(Iteration 191 / 200) loss: 0.570967\n",
      "(Epoch 20 / 20) train acc: 0.920000; val_acc: 0.886111\n",
      "(Iteration 1 / 200) loss: 2.303878\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.275000; val_acc: 0.225000\n",
      "(Iteration 11 / 200) loss: 2.303461\n",
      "(Epoch 2 / 20) train acc: 0.443000; val_acc: 0.383333\n",
      "(Iteration 21 / 200) loss: 2.302771\n",
      "(Epoch 3 / 20) train acc: 0.393000; val_acc: 0.369444\n",
      "(Iteration 31 / 200) loss: 2.301567\n",
      "(Epoch 4 / 20) train acc: 0.274000; val_acc: 0.288889\n",
      "(Iteration 41 / 200) loss: 2.299193\n",
      "(Epoch 5 / 20) train acc: 0.373000; val_acc: 0.338889\n",
      "(Iteration 51 / 200) loss: 2.294438\n",
      "(Epoch 6 / 20) train acc: 0.336000; val_acc: 0.338889\n",
      "(Iteration 61 / 200) loss: 2.283928\n",
      "(Epoch 7 / 20) train acc: 0.345000; val_acc: 0.369444\n",
      "(Iteration 71 / 200) loss: 2.263291\n",
      "(Epoch 8 / 20) train acc: 0.333000; val_acc: 0.341667\n",
      "(Iteration 81 / 200) loss: 2.239133\n",
      "(Epoch 9 / 20) train acc: 0.289000; val_acc: 0.297222\n",
      "(Iteration 91 / 200) loss: 2.206683\n",
      "(Epoch 10 / 20) train acc: 0.283000; val_acc: 0.269444\n",
      "(Iteration 101 / 200) loss: 2.082768\n",
      "(Epoch 11 / 20) train acc: 0.252000; val_acc: 0.252778\n",
      "(Iteration 111 / 200) loss: 2.030953\n",
      "(Epoch 12 / 20) train acc: 0.269000; val_acc: 0.241667\n",
      "(Iteration 121 / 200) loss: 1.874601\n",
      "(Epoch 13 / 20) train acc: 0.253000; val_acc: 0.244444\n",
      "(Iteration 131 / 200) loss: 1.826384\n",
      "(Epoch 14 / 20) train acc: 0.323000; val_acc: 0.288889\n",
      "(Iteration 141 / 200) loss: 1.744835\n",
      "(Epoch 15 / 20) train acc: 0.377000; val_acc: 0.375000\n",
      "(Iteration 151 / 200) loss: 1.739734\n",
      "(Epoch 16 / 20) train acc: 0.398000; val_acc: 0.363889\n",
      "(Iteration 161 / 200) loss: 1.695307\n",
      "(Epoch 17 / 20) train acc: 0.379000; val_acc: 0.352778\n",
      "(Iteration 171 / 200) loss: 1.624849\n",
      "(Epoch 18 / 20) train acc: 0.444000; val_acc: 0.413889\n",
      "(Iteration 181 / 200) loss: 1.522604\n",
      "(Epoch 19 / 20) train acc: 0.474000; val_acc: 0.488889\n",
      "(Iteration 191 / 200) loss: 1.569217\n",
      "(Epoch 20 / 20) train acc: 0.493000; val_acc: 0.475000\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302596\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302613\n",
      "(Epoch 3 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302580\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302550\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302580\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302554\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302609\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302635\n",
      "(Epoch 9 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302632\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302550\n",
      "(Epoch 11 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302533\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302800\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302606\n",
      "(Epoch 14 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302346\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302361\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302458\n",
      "(Epoch 17 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302394\n",
      "(Epoch 18 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302481\n",
      "(Epoch 19 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302703\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302483\n",
      "(Epoch 2 / 20) train acc: 0.086000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302661\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302524\n",
      "(Epoch 4 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 2.302652\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.302546\n",
      "(Epoch 6 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 2.302604\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302356\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302400\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302773\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302625\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302805\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302639\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302210\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302508\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302688\n",
      "(Epoch 16 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302383\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302667\n",
      "(Epoch 18 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303497\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302834\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.082000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302522\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302546\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302570\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302392\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302520\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302621\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302439\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302177\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302159\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.301826\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302470\n",
      "(Epoch 12 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302547\n",
      "(Epoch 13 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302596\n",
      "(Epoch 14 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302474\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302541\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302461\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301976\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302449\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302561\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 22521.536287\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.130000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 20815.073423\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 18692.589381\n",
      "(Epoch 3 / 20) train acc: 0.124000; val_acc: 0.102778\n",
      "(Iteration 31 / 200) loss: 19086.617852\n",
      "(Epoch 4 / 20) train acc: 0.134000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 15964.071333\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 15605.524813\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 17579.867047\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 15162.886779\n",
      "(Epoch 8 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 13678.704018\n",
      "(Epoch 9 / 20) train acc: 0.122000; val_acc: 0.088889\n",
      "(Iteration 91 / 200) loss: 13321.932507\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.086111\n",
      "(Iteration 101 / 200) loss: 15197.119746\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 13863.985733\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 13031.890473\n",
      "(Epoch 13 / 20) train acc: 0.121000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 12923.848966\n",
      "(Epoch 14 / 20) train acc: 0.130000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 12342.096212\n",
      "(Epoch 15 / 20) train acc: 0.131000; val_acc: 0.088889\n",
      "(Iteration 151 / 200) loss: 11922.182208\n",
      "(Epoch 16 / 20) train acc: 0.146000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 11278.998205\n",
      "(Epoch 17 / 20) train acc: 0.124000; val_acc: 0.102778\n",
      "(Iteration 171 / 200) loss: 11916.055452\n",
      "(Epoch 18 / 20) train acc: 0.113000; val_acc: 0.113889\n",
      "(Iteration 181 / 200) loss: 11869.663949\n",
      "(Epoch 19 / 20) train acc: 0.149000; val_acc: 0.122222\n",
      "(Iteration 191 / 200) loss: 11739.338699\n",
      "(Epoch 20 / 20) train acc: 0.154000; val_acc: 0.125000\n",
      "(Iteration 1 / 200) loss: 5.780677\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 3.894191\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 3.193290\n",
      "(Epoch 3 / 20) train acc: 0.176000; val_acc: 0.213889\n",
      "(Iteration 31 / 200) loss: 2.635929\n",
      "(Epoch 4 / 20) train acc: 0.227000; val_acc: 0.261111\n",
      "(Iteration 41 / 200) loss: 2.184370\n",
      "(Epoch 5 / 20) train acc: 0.308000; val_acc: 0.297222\n",
      "(Iteration 51 / 200) loss: 2.012920\n",
      "(Epoch 6 / 20) train acc: 0.372000; val_acc: 0.383333\n",
      "(Iteration 61 / 200) loss: 1.931590\n",
      "(Epoch 7 / 20) train acc: 0.426000; val_acc: 0.488889\n",
      "(Iteration 71 / 200) loss: 1.737993\n",
      "(Epoch 8 / 20) train acc: 0.524000; val_acc: 0.569444\n",
      "(Iteration 81 / 200) loss: 1.424555\n",
      "(Epoch 9 / 20) train acc: 0.573000; val_acc: 0.613889\n",
      "(Iteration 91 / 200) loss: 1.329141\n",
      "(Epoch 10 / 20) train acc: 0.677000; val_acc: 0.661111\n",
      "(Iteration 101 / 200) loss: 1.216273\n",
      "(Epoch 11 / 20) train acc: 0.694000; val_acc: 0.694444\n",
      "(Iteration 111 / 200) loss: 1.156418\n",
      "(Epoch 12 / 20) train acc: 0.726000; val_acc: 0.711111\n",
      "(Iteration 121 / 200) loss: 1.102734\n",
      "(Epoch 13 / 20) train acc: 0.779000; val_acc: 0.752778\n",
      "(Iteration 131 / 200) loss: 0.831542\n",
      "(Epoch 14 / 20) train acc: 0.787000; val_acc: 0.780556\n",
      "(Iteration 141 / 200) loss: 0.963826\n",
      "(Epoch 15 / 20) train acc: 0.843000; val_acc: 0.786111\n",
      "(Iteration 151 / 200) loss: 0.721940\n",
      "(Epoch 16 / 20) train acc: 0.833000; val_acc: 0.802778\n",
      "(Iteration 161 / 200) loss: 0.796459\n",
      "(Epoch 17 / 20) train acc: 0.853000; val_acc: 0.833333\n",
      "(Iteration 171 / 200) loss: 0.581025\n",
      "(Epoch 18 / 20) train acc: 0.882000; val_acc: 0.833333\n",
      "(Iteration 181 / 200) loss: 0.593345\n",
      "(Epoch 19 / 20) train acc: 0.892000; val_acc: 0.836111\n",
      "(Iteration 191 / 200) loss: 0.568192\n",
      "(Epoch 20 / 20) train acc: 0.872000; val_acc: 0.850000\n",
      "(Iteration 1 / 200) loss: 2.302727\n",
      "(Epoch 0 / 20) train acc: 0.150000; val_acc: 0.144444\n",
      "(Epoch 1 / 20) train acc: 0.131000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 2.302465\n",
      "(Epoch 2 / 20) train acc: 0.144000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 2.301754\n",
      "(Epoch 3 / 20) train acc: 0.293000; val_acc: 0.244444\n",
      "(Iteration 31 / 200) loss: 2.300779\n",
      "(Epoch 4 / 20) train acc: 0.457000; val_acc: 0.344444\n",
      "(Iteration 41 / 200) loss: 2.298655\n",
      "(Epoch 5 / 20) train acc: 0.419000; val_acc: 0.350000\n",
      "(Iteration 51 / 200) loss: 2.294355\n",
      "(Epoch 6 / 20) train acc: 0.399000; val_acc: 0.316667\n",
      "(Iteration 61 / 200) loss: 2.284938\n",
      "(Epoch 7 / 20) train acc: 0.364000; val_acc: 0.308333\n",
      "(Iteration 71 / 200) loss: 2.269210\n",
      "(Epoch 8 / 20) train acc: 0.486000; val_acc: 0.419444\n",
      "(Iteration 81 / 200) loss: 2.231802\n",
      "(Epoch 9 / 20) train acc: 0.391000; val_acc: 0.344444\n",
      "(Iteration 91 / 200) loss: 2.183791\n",
      "(Epoch 10 / 20) train acc: 0.393000; val_acc: 0.377778\n",
      "(Iteration 101 / 200) loss: 2.125254\n",
      "(Epoch 11 / 20) train acc: 0.371000; val_acc: 0.361111\n",
      "(Iteration 111 / 200) loss: 2.032118\n",
      "(Epoch 12 / 20) train acc: 0.368000; val_acc: 0.341667\n",
      "(Iteration 121 / 200) loss: 1.909441\n",
      "(Epoch 13 / 20) train acc: 0.333000; val_acc: 0.341667\n",
      "(Iteration 131 / 200) loss: 1.869040\n",
      "(Epoch 14 / 20) train acc: 0.407000; val_acc: 0.411111\n",
      "(Iteration 141 / 200) loss: 1.703571\n",
      "(Epoch 15 / 20) train acc: 0.476000; val_acc: 0.461111\n",
      "(Iteration 151 / 200) loss: 1.589135\n",
      "(Epoch 16 / 20) train acc: 0.508000; val_acc: 0.502778\n",
      "(Iteration 161 / 200) loss: 1.551316\n",
      "(Epoch 17 / 20) train acc: 0.576000; val_acc: 0.527778\n",
      "(Iteration 171 / 200) loss: 1.426943\n",
      "(Epoch 18 / 20) train acc: 0.589000; val_acc: 0.544444\n",
      "(Iteration 181 / 200) loss: 1.343186\n",
      "(Epoch 19 / 20) train acc: 0.588000; val_acc: 0.572222\n",
      "(Iteration 191 / 200) loss: 1.245422\n",
      "(Epoch 20 / 20) train acc: 0.606000; val_acc: 0.611111\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302575\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302532\n",
      "(Epoch 3 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302501\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302381\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302159\n",
      "(Epoch 6 / 20) train acc: 0.141000; val_acc: 0.102778\n",
      "(Iteration 61 / 200) loss: 2.299725\n",
      "(Epoch 7 / 20) train acc: 0.145000; val_acc: 0.119444\n",
      "(Iteration 71 / 200) loss: 2.293992\n",
      "(Epoch 8 / 20) train acc: 0.159000; val_acc: 0.155556\n",
      "(Iteration 81 / 200) loss: 2.280203\n",
      "(Epoch 9 / 20) train acc: 0.171000; val_acc: 0.161111\n",
      "(Iteration 91 / 200) loss: 2.243176\n",
      "(Epoch 10 / 20) train acc: 0.167000; val_acc: 0.166667\n",
      "(Iteration 101 / 200) loss: 2.195433\n",
      "(Epoch 11 / 20) train acc: 0.193000; val_acc: 0.172222\n",
      "(Iteration 111 / 200) loss: 2.143066\n",
      "(Epoch 12 / 20) train acc: 0.175000; val_acc: 0.172222\n",
      "(Iteration 121 / 200) loss: 2.048742\n",
      "(Epoch 13 / 20) train acc: 0.192000; val_acc: 0.197222\n",
      "(Iteration 131 / 200) loss: 1.952922\n",
      "(Epoch 14 / 20) train acc: 0.249000; val_acc: 0.202778\n",
      "(Iteration 141 / 200) loss: 1.901047\n",
      "(Epoch 15 / 20) train acc: 0.205000; val_acc: 0.205556\n",
      "(Iteration 151 / 200) loss: 1.849627\n",
      "(Epoch 16 / 20) train acc: 0.200000; val_acc: 0.216667\n",
      "(Iteration 161 / 200) loss: 1.805606\n",
      "(Epoch 17 / 20) train acc: 0.200000; val_acc: 0.211111\n",
      "(Iteration 171 / 200) loss: 1.768005\n",
      "(Epoch 18 / 20) train acc: 0.202000; val_acc: 0.211111\n",
      "(Iteration 181 / 200) loss: 1.795960\n",
      "(Epoch 19 / 20) train acc: 0.211000; val_acc: 0.227778\n",
      "(Iteration 191 / 200) loss: 1.741393\n",
      "(Epoch 20 / 20) train acc: 0.228000; val_acc: 0.216667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302520\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302495\n",
      "(Epoch 3 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302576\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302462\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302619\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302487\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302244\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.301177\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.300151\n",
      "(Epoch 10 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.284242\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.280017\n",
      "(Epoch 12 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.234364\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 2.152501\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 2.056074\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Iteration 151 / 200) loss: 2.073788\n",
      "(Epoch 16 / 20) train acc: 0.127000; val_acc: 0.102778\n",
      "(Iteration 161 / 200) loss: 2.093100\n",
      "(Epoch 17 / 20) train acc: 0.151000; val_acc: 0.122222\n",
      "(Iteration 171 / 200) loss: 2.018108\n",
      "(Epoch 18 / 20) train acc: 0.137000; val_acc: 0.122222\n",
      "(Iteration 181 / 200) loss: 1.920978\n",
      "(Epoch 19 / 20) train acc: 0.177000; val_acc: 0.122222\n",
      "(Iteration 191 / 200) loss: 1.977657\n",
      "(Epoch 20 / 20) train acc: 0.178000; val_acc: 0.180556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302598\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302614\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302620\n",
      "(Epoch 4 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302679\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302646\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302328\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302389\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302232\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302533\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302667\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302603\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302649\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302467\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302573\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302035\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301964\n",
      "(Epoch 17 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302759\n",
      "(Epoch 18 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302502\n",
      "(Epoch 19 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.303113\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 21860.056285\n",
      "(Epoch 0 / 20) train acc: 0.082000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 20032.582150\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.094444\n",
      "(Iteration 21 / 200) loss: 20434.483082\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 18007.857775\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.094444\n",
      "(Iteration 41 / 200) loss: 16288.318728\n",
      "(Epoch 5 / 20) train acc: 0.093000; val_acc: 0.094444\n",
      "(Iteration 51 / 200) loss: 18044.043434\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.102778\n",
      "(Iteration 61 / 200) loss: 16844.456898\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.108333\n",
      "(Iteration 71 / 200) loss: 15157.604123\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.111111\n",
      "(Iteration 81 / 200) loss: 13461.303852\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.111111\n",
      "(Iteration 91 / 200) loss: 14248.039832\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.122222\n",
      "(Iteration 101 / 200) loss: 13110.248311\n",
      "(Epoch 11 / 20) train acc: 0.124000; val_acc: 0.127778\n",
      "(Iteration 111 / 200) loss: 11371.139293\n",
      "(Epoch 12 / 20) train acc: 0.133000; val_acc: 0.138889\n",
      "(Iteration 121 / 200) loss: 9495.355276\n",
      "(Epoch 13 / 20) train acc: 0.136000; val_acc: 0.141667\n",
      "(Iteration 131 / 200) loss: 11053.720008\n",
      "(Epoch 14 / 20) train acc: 0.147000; val_acc: 0.150000\n",
      "(Iteration 141 / 200) loss: 9333.265366\n",
      "(Epoch 15 / 20) train acc: 0.180000; val_acc: 0.169444\n",
      "(Iteration 151 / 200) loss: 9439.091357\n",
      "(Epoch 16 / 20) train acc: 0.144000; val_acc: 0.177778\n",
      "(Iteration 161 / 200) loss: 8728.557353\n",
      "(Epoch 17 / 20) train acc: 0.195000; val_acc: 0.188889\n",
      "(Iteration 171 / 200) loss: 8628.537727\n",
      "(Epoch 18 / 20) train acc: 0.178000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 7727.771853\n",
      "(Epoch 19 / 20) train acc: 0.205000; val_acc: 0.205556\n",
      "(Iteration 191 / 200) loss: 7881.195356\n",
      "(Epoch 20 / 20) train acc: 0.224000; val_acc: 0.216667\n",
      "(Iteration 1 / 200) loss: 5.035166\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 3.470084\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.877399\n",
      "(Epoch 3 / 20) train acc: 0.222000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 2.807092\n",
      "(Epoch 4 / 20) train acc: 0.277000; val_acc: 0.286111\n",
      "(Iteration 41 / 200) loss: 2.180820\n",
      "(Epoch 5 / 20) train acc: 0.324000; val_acc: 0.350000\n",
      "(Iteration 51 / 200) loss: 2.128446\n",
      "(Epoch 6 / 20) train acc: 0.437000; val_acc: 0.413889\n",
      "(Iteration 61 / 200) loss: 1.855825\n",
      "(Epoch 7 / 20) train acc: 0.483000; val_acc: 0.472222\n",
      "(Iteration 71 / 200) loss: 1.564486\n",
      "(Epoch 8 / 20) train acc: 0.545000; val_acc: 0.519444\n",
      "(Iteration 81 / 200) loss: 1.532925\n",
      "(Epoch 9 / 20) train acc: 0.622000; val_acc: 0.586111\n",
      "(Iteration 91 / 200) loss: 1.201797\n",
      "(Epoch 10 / 20) train acc: 0.728000; val_acc: 0.663889\n",
      "(Iteration 101 / 200) loss: 1.178483\n",
      "(Epoch 11 / 20) train acc: 0.782000; val_acc: 0.705556\n",
      "(Iteration 111 / 200) loss: 1.058799\n",
      "(Epoch 12 / 20) train acc: 0.811000; val_acc: 0.755556\n",
      "(Iteration 121 / 200) loss: 0.950137\n",
      "(Epoch 13 / 20) train acc: 0.837000; val_acc: 0.786111\n",
      "(Iteration 131 / 200) loss: 0.821632\n",
      "(Epoch 14 / 20) train acc: 0.828000; val_acc: 0.808333\n",
      "(Iteration 141 / 200) loss: 0.624573\n",
      "(Epoch 15 / 20) train acc: 0.880000; val_acc: 0.825000\n",
      "(Iteration 151 / 200) loss: 0.583533\n",
      "(Epoch 16 / 20) train acc: 0.864000; val_acc: 0.838889\n",
      "(Iteration 161 / 200) loss: 0.619677\n",
      "(Epoch 17 / 20) train acc: 0.897000; val_acc: 0.850000\n",
      "(Iteration 171 / 200) loss: 0.601413\n",
      "(Epoch 18 / 20) train acc: 0.906000; val_acc: 0.855556\n",
      "(Iteration 181 / 200) loss: 0.547901\n",
      "(Epoch 19 / 20) train acc: 0.894000; val_acc: 0.861111\n",
      "(Iteration 191 / 200) loss: 0.369440\n",
      "(Epoch 20 / 20) train acc: 0.906000; val_acc: 0.869444\n",
      "(Iteration 1 / 200) loss: 2.302718\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.189000; val_acc: 0.177778\n",
      "(Iteration 11 / 200) loss: 2.302366\n",
      "(Epoch 2 / 20) train acc: 0.316000; val_acc: 0.286111\n",
      "(Iteration 21 / 200) loss: 2.301550\n",
      "(Epoch 3 / 20) train acc: 0.463000; val_acc: 0.463889\n",
      "(Iteration 31 / 200) loss: 2.300469\n",
      "(Epoch 4 / 20) train acc: 0.505000; val_acc: 0.527778\n",
      "(Iteration 41 / 200) loss: 2.297457\n",
      "(Epoch 5 / 20) train acc: 0.462000; val_acc: 0.458333\n",
      "(Iteration 51 / 200) loss: 2.291289\n",
      "(Epoch 6 / 20) train acc: 0.465000; val_acc: 0.494444\n",
      "(Iteration 61 / 200) loss: 2.282049\n",
      "(Epoch 7 / 20) train acc: 0.480000; val_acc: 0.497222\n",
      "(Iteration 71 / 200) loss: 2.272644\n",
      "(Epoch 8 / 20) train acc: 0.466000; val_acc: 0.486111\n",
      "(Iteration 81 / 200) loss: 2.237781\n",
      "(Epoch 9 / 20) train acc: 0.471000; val_acc: 0.491667\n",
      "(Iteration 91 / 200) loss: 2.199228\n",
      "(Epoch 10 / 20) train acc: 0.432000; val_acc: 0.419444\n",
      "(Iteration 101 / 200) loss: 2.104035\n",
      "(Epoch 11 / 20) train acc: 0.483000; val_acc: 0.472222\n",
      "(Iteration 111 / 200) loss: 2.022600\n",
      "(Epoch 12 / 20) train acc: 0.439000; val_acc: 0.433333\n",
      "(Iteration 121 / 200) loss: 1.882861\n",
      "(Epoch 13 / 20) train acc: 0.433000; val_acc: 0.455556\n",
      "(Iteration 131 / 200) loss: 1.855446\n",
      "(Epoch 14 / 20) train acc: 0.512000; val_acc: 0.533333\n",
      "(Iteration 141 / 200) loss: 1.555942\n",
      "(Epoch 15 / 20) train acc: 0.577000; val_acc: 0.600000\n",
      "(Iteration 151 / 200) loss: 1.586787\n",
      "(Epoch 16 / 20) train acc: 0.657000; val_acc: 0.663889\n",
      "(Iteration 161 / 200) loss: 1.416189\n",
      "(Epoch 17 / 20) train acc: 0.631000; val_acc: 0.669444\n",
      "(Iteration 171 / 200) loss: 1.416430\n",
      "(Epoch 18 / 20) train acc: 0.693000; val_acc: 0.691667\n",
      "(Iteration 181 / 200) loss: 1.276737\n",
      "(Epoch 19 / 20) train acc: 0.694000; val_acc: 0.694444\n",
      "(Iteration 191 / 200) loss: 1.188931\n",
      "(Epoch 20 / 20) train acc: 0.721000; val_acc: 0.702778\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302516\n",
      "(Epoch 2 / 20) train acc: 0.085000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302593\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302625\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302415\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.301250\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.298360\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.290347\n",
      "(Epoch 8 / 20) train acc: 0.132000; val_acc: 0.108333\n",
      "(Iteration 81 / 200) loss: 2.276054\n",
      "(Epoch 9 / 20) train acc: 0.153000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 2.234258\n",
      "(Epoch 10 / 20) train acc: 0.147000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 2.202406\n",
      "(Epoch 11 / 20) train acc: 0.140000; val_acc: 0.161111\n",
      "(Iteration 111 / 200) loss: 2.144152\n",
      "(Epoch 12 / 20) train acc: 0.187000; val_acc: 0.186111\n",
      "(Iteration 121 / 200) loss: 2.061712\n",
      "(Epoch 13 / 20) train acc: 0.176000; val_acc: 0.188889\n",
      "(Iteration 131 / 200) loss: 2.083680\n",
      "(Epoch 14 / 20) train acc: 0.234000; val_acc: 0.200000\n",
      "(Iteration 141 / 200) loss: 1.960713\n",
      "(Epoch 15 / 20) train acc: 0.213000; val_acc: 0.202778\n",
      "(Iteration 151 / 200) loss: 2.082437\n",
      "(Epoch 16 / 20) train acc: 0.200000; val_acc: 0.205556\n",
      "(Iteration 161 / 200) loss: 1.932366\n",
      "(Epoch 17 / 20) train acc: 0.186000; val_acc: 0.205556\n",
      "(Iteration 171 / 200) loss: 1.941804\n",
      "(Epoch 18 / 20) train acc: 0.214000; val_acc: 0.227778\n",
      "(Iteration 181 / 200) loss: 1.942478\n",
      "(Epoch 19 / 20) train acc: 0.216000; val_acc: 0.236111\n",
      "(Iteration 191 / 200) loss: 1.968211\n",
      "(Epoch 20 / 20) train acc: 0.207000; val_acc: 0.241667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.078000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302622\n",
      "(Epoch 2 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302560\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302526\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302736\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302526\n",
      "(Epoch 6 / 20) train acc: 0.083000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.302570\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.302442\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.302691\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.302487\n",
      "(Epoch 10 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302769\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302350\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302404\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302738\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302483\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302318\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302467\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302502\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302349\n",
      "(Epoch 19 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302236\n",
      "(Epoch 20 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302601\n",
      "(Epoch 3 / 20) train acc: 0.081000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302676\n",
      "(Epoch 4 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302520\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302696\n",
      "(Epoch 6 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302483\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302584\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302729\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302459\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302454\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302502\n",
      "(Epoch 12 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302701\n",
      "(Epoch 13 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302637\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302846\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302533\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302751\n",
      "(Epoch 17 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302919\n",
      "(Epoch 18 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302522\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302249\n",
      "(Epoch 20 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 24641.406663\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.115000; val_acc: 0.102778\n",
      "(Iteration 11 / 200) loss: 21672.328783\n",
      "(Epoch 2 / 20) train acc: 0.116000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 21909.770973\n",
      "(Epoch 3 / 20) train acc: 0.136000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 20451.588170\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 20476.086619\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 17672.552570\n",
      "(Epoch 6 / 20) train acc: 0.116000; val_acc: 0.094444\n",
      "(Iteration 61 / 200) loss: 19004.819776\n",
      "(Epoch 7 / 20) train acc: 0.116000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 16981.876985\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 15412.219192\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.088889\n",
      "(Iteration 91 / 200) loss: 14311.786399\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.088889\n",
      "(Iteration 101 / 200) loss: 13688.012357\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 13502.199568\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 14242.231780\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.105556\n",
      "(Iteration 131 / 200) loss: 13093.458993\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.105556\n",
      "(Iteration 141 / 200) loss: 11963.956201\n",
      "(Epoch 15 / 20) train acc: 0.122000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 13181.519663\n",
      "(Epoch 16 / 20) train acc: 0.119000; val_acc: 0.105556\n",
      "(Iteration 161 / 200) loss: 11613.585624\n",
      "(Epoch 17 / 20) train acc: 0.132000; val_acc: 0.108333\n",
      "(Iteration 171 / 200) loss: 10945.107835\n",
      "(Epoch 18 / 20) train acc: 0.135000; val_acc: 0.111111\n",
      "(Iteration 181 / 200) loss: 10994.078796\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.119444\n",
      "(Iteration 191 / 200) loss: 11310.169757\n",
      "(Epoch 20 / 20) train acc: 0.133000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 4.721806\n",
      "(Epoch 0 / 20) train acc: 0.068000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.064000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 4.077814\n",
      "(Epoch 2 / 20) train acc: 0.069000; val_acc: 0.069444\n",
      "(Iteration 21 / 200) loss: 3.107591\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.105556\n",
      "(Iteration 31 / 200) loss: 3.014394\n",
      "(Epoch 4 / 20) train acc: 0.155000; val_acc: 0.177778\n",
      "(Iteration 41 / 200) loss: 2.221625\n",
      "(Epoch 5 / 20) train acc: 0.254000; val_acc: 0.250000\n",
      "(Iteration 51 / 200) loss: 2.016298\n",
      "(Epoch 6 / 20) train acc: 0.405000; val_acc: 0.344444\n",
      "(Iteration 61 / 200) loss: 1.800658\n",
      "(Epoch 7 / 20) train acc: 0.500000; val_acc: 0.472222\n",
      "(Iteration 71 / 200) loss: 1.583912\n",
      "(Epoch 8 / 20) train acc: 0.599000; val_acc: 0.586111\n",
      "(Iteration 81 / 200) loss: 1.413561\n",
      "(Epoch 9 / 20) train acc: 0.675000; val_acc: 0.669444\n",
      "(Iteration 91 / 200) loss: 1.265190\n",
      "(Epoch 10 / 20) train acc: 0.702000; val_acc: 0.725000\n",
      "(Iteration 101 / 200) loss: 1.237117\n",
      "(Epoch 11 / 20) train acc: 0.775000; val_acc: 0.780556\n",
      "(Iteration 111 / 200) loss: 1.042133\n",
      "(Epoch 12 / 20) train acc: 0.774000; val_acc: 0.783333\n",
      "(Iteration 121 / 200) loss: 0.934858\n",
      "(Epoch 13 / 20) train acc: 0.784000; val_acc: 0.813889\n",
      "(Iteration 131 / 200) loss: 0.921087\n",
      "(Epoch 14 / 20) train acc: 0.836000; val_acc: 0.838889\n",
      "(Iteration 141 / 200) loss: 0.765412\n",
      "(Epoch 15 / 20) train acc: 0.845000; val_acc: 0.847222\n",
      "(Iteration 151 / 200) loss: 0.742037\n",
      "(Epoch 16 / 20) train acc: 0.871000; val_acc: 0.847222\n",
      "(Iteration 161 / 200) loss: 0.679864\n",
      "(Epoch 17 / 20) train acc: 0.881000; val_acc: 0.863889\n",
      "(Iteration 171 / 200) loss: 0.549874\n",
      "(Epoch 18 / 20) train acc: 0.874000; val_acc: 0.872222\n",
      "(Iteration 181 / 200) loss: 0.627623\n",
      "(Epoch 19 / 20) train acc: 0.897000; val_acc: 0.875000\n",
      "(Iteration 191 / 200) loss: 0.495379\n",
      "(Epoch 20 / 20) train acc: 0.903000; val_acc: 0.888889\n",
      "(Iteration 1 / 200) loss: 2.302682\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.249000; val_acc: 0.200000\n",
      "(Iteration 11 / 200) loss: 2.302263\n",
      "(Epoch 2 / 20) train acc: 0.269000; val_acc: 0.213889\n",
      "(Iteration 21 / 200) loss: 2.301856\n",
      "(Epoch 3 / 20) train acc: 0.333000; val_acc: 0.291667\n",
      "(Iteration 31 / 200) loss: 2.300734\n",
      "(Epoch 4 / 20) train acc: 0.296000; val_acc: 0.236111\n",
      "(Iteration 41 / 200) loss: 2.297650\n",
      "(Epoch 5 / 20) train acc: 0.408000; val_acc: 0.338889\n",
      "(Iteration 51 / 200) loss: 2.293928\n",
      "(Epoch 6 / 20) train acc: 0.440000; val_acc: 0.383333\n",
      "(Iteration 61 / 200) loss: 2.284965\n",
      "(Epoch 7 / 20) train acc: 0.397000; val_acc: 0.361111\n",
      "(Iteration 71 / 200) loss: 2.269852\n",
      "(Epoch 8 / 20) train acc: 0.367000; val_acc: 0.366667\n",
      "(Iteration 81 / 200) loss: 2.239295\n",
      "(Epoch 9 / 20) train acc: 0.366000; val_acc: 0.347222\n",
      "(Iteration 91 / 200) loss: 2.197809\n",
      "(Epoch 10 / 20) train acc: 0.370000; val_acc: 0.350000\n",
      "(Iteration 101 / 200) loss: 2.114115\n",
      "(Epoch 11 / 20) train acc: 0.316000; val_acc: 0.327778\n",
      "(Iteration 111 / 200) loss: 2.014912\n",
      "(Epoch 12 / 20) train acc: 0.300000; val_acc: 0.300000\n",
      "(Iteration 121 / 200) loss: 1.948244\n",
      "(Epoch 13 / 20) train acc: 0.323000; val_acc: 0.316667\n",
      "(Iteration 131 / 200) loss: 1.852261\n",
      "(Epoch 14 / 20) train acc: 0.357000; val_acc: 0.350000\n",
      "(Iteration 141 / 200) loss: 1.670129\n",
      "(Epoch 15 / 20) train acc: 0.386000; val_acc: 0.413889\n",
      "(Iteration 151 / 200) loss: 1.694768\n",
      "(Epoch 16 / 20) train acc: 0.459000; val_acc: 0.483333\n",
      "(Iteration 161 / 200) loss: 1.638387\n",
      "(Epoch 17 / 20) train acc: 0.504000; val_acc: 0.486111\n",
      "(Iteration 171 / 200) loss: 1.529547\n",
      "(Epoch 18 / 20) train acc: 0.498000; val_acc: 0.480556\n",
      "(Iteration 181 / 200) loss: 1.425941\n",
      "(Epoch 19 / 20) train acc: 0.528000; val_acc: 0.516667\n",
      "(Iteration 191 / 200) loss: 1.476716\n",
      "(Epoch 20 / 20) train acc: 0.548000; val_acc: 0.550000\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302525\n",
      "(Epoch 3 / 20) train acc: 0.139000; val_acc: 0.136111\n",
      "(Iteration 31 / 200) loss: 2.302464\n",
      "(Epoch 4 / 20) train acc: 0.156000; val_acc: 0.177778\n",
      "(Iteration 41 / 200) loss: 2.302504\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.122222\n",
      "(Iteration 51 / 200) loss: 2.301417\n",
      "(Epoch 6 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.298390\n",
      "(Epoch 7 / 20) train acc: 0.178000; val_acc: 0.161111\n",
      "(Iteration 71 / 200) loss: 2.288140\n",
      "(Epoch 8 / 20) train acc: 0.189000; val_acc: 0.175000\n",
      "(Iteration 81 / 200) loss: 2.264018\n",
      "(Epoch 9 / 20) train acc: 0.185000; val_acc: 0.177778\n",
      "(Iteration 91 / 200) loss: 2.228467\n",
      "(Epoch 10 / 20) train acc: 0.198000; val_acc: 0.197222\n",
      "(Iteration 101 / 200) loss: 2.182356\n",
      "(Epoch 11 / 20) train acc: 0.201000; val_acc: 0.197222\n",
      "(Iteration 111 / 200) loss: 2.137740\n",
      "(Epoch 12 / 20) train acc: 0.224000; val_acc: 0.200000\n",
      "(Iteration 121 / 200) loss: 2.086798\n",
      "(Epoch 13 / 20) train acc: 0.209000; val_acc: 0.205556\n",
      "(Iteration 131 / 200) loss: 2.006102\n",
      "(Epoch 14 / 20) train acc: 0.227000; val_acc: 0.227778\n",
      "(Iteration 141 / 200) loss: 1.879839\n",
      "(Epoch 15 / 20) train acc: 0.241000; val_acc: 0.233333\n",
      "(Iteration 151 / 200) loss: 1.833589\n",
      "(Epoch 16 / 20) train acc: 0.235000; val_acc: 0.297222\n",
      "(Iteration 161 / 200) loss: 1.778541\n",
      "(Epoch 17 / 20) train acc: 0.270000; val_acc: 0.297222\n",
      "(Iteration 171 / 200) loss: 1.773521\n",
      "(Epoch 18 / 20) train acc: 0.320000; val_acc: 0.316667\n",
      "(Iteration 181 / 200) loss: 1.686284\n",
      "(Epoch 19 / 20) train acc: 0.303000; val_acc: 0.297222\n",
      "(Iteration 191 / 200) loss: 1.656237\n",
      "(Epoch 20 / 20) train acc: 0.302000; val_acc: 0.319444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302604\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302684\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302588\n",
      "(Epoch 5 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302632\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 2.302588\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302711\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302147\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302602\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302507\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302389\n",
      "(Epoch 12 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302270\n",
      "(Epoch 13 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302713\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302412\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301713\n",
      "(Epoch 16 / 20) train acc: 0.084000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301444\n",
      "(Epoch 17 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.300567\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.298771\n",
      "(Epoch 19 / 20) train acc: 0.082000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.293142\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302531\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302597\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.085000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 2.302597\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.302660\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 2.302416\n",
      "(Epoch 7 / 20) train acc: 0.090000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 2.302460\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302563\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302438\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302250\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302061\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.299218\n",
      "(Epoch 13 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.296624\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.291238\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.277032\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.261939\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.225969\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.237489\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.197648\n",
      "(Epoch 20 / 20) train acc: 0.175000; val_acc: 0.138889\n",
      "(Iteration 1 / 200) loss: 27121.757012\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.129000; val_acc: 0.094444\n",
      "(Iteration 11 / 200) loss: 23617.541981\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 22483.439455\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 22055.736930\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.075000\n",
      "(Iteration 41 / 200) loss: 19988.985655\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.066667\n",
      "(Iteration 51 / 200) loss: 22286.136881\n",
      "(Epoch 6 / 20) train acc: 0.092000; val_acc: 0.072222\n",
      "(Iteration 61 / 200) loss: 20755.774356\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.069444\n",
      "(Iteration 71 / 200) loss: 19432.079332\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.069444\n",
      "(Iteration 81 / 200) loss: 17396.036808\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.075000\n",
      "(Iteration 91 / 200) loss: 16326.466784\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 18666.713011\n",
      "(Epoch 11 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 17768.909237\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Iteration 121 / 200) loss: 16074.327964\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 16146.604191\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 13597.909168\n",
      "(Epoch 15 / 20) train acc: 0.129000; val_acc: 0.088889\n",
      "(Iteration 151 / 200) loss: 13081.939145\n",
      "(Epoch 16 / 20) train acc: 0.131000; val_acc: 0.088889\n",
      "(Iteration 161 / 200) loss: 12456.417873\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.102778\n",
      "(Iteration 171 / 200) loss: 11919.692850\n",
      "(Epoch 18 / 20) train acc: 0.135000; val_acc: 0.108333\n",
      "(Iteration 181 / 200) loss: 13579.259078\n",
      "(Epoch 19 / 20) train acc: 0.138000; val_acc: 0.111111\n",
      "(Iteration 191 / 200) loss: 12819.106556\n",
      "(Epoch 20 / 20) train acc: 0.131000; val_acc: 0.125000\n",
      "(Iteration 1 / 200) loss: 4.350524\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.108333\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 3.093213\n",
      "(Epoch 2 / 20) train acc: 0.131000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 2.385547\n",
      "(Epoch 3 / 20) train acc: 0.204000; val_acc: 0.183333\n",
      "(Iteration 31 / 200) loss: 2.332421\n",
      "(Epoch 4 / 20) train acc: 0.297000; val_acc: 0.266667\n",
      "(Iteration 41 / 200) loss: 1.991275\n",
      "(Epoch 5 / 20) train acc: 0.390000; val_acc: 0.325000\n",
      "(Iteration 51 / 200) loss: 1.811475\n",
      "(Epoch 6 / 20) train acc: 0.453000; val_acc: 0.394444\n",
      "(Iteration 61 / 200) loss: 1.665604\n",
      "(Epoch 7 / 20) train acc: 0.545000; val_acc: 0.458333\n",
      "(Iteration 71 / 200) loss: 1.446423\n",
      "(Epoch 8 / 20) train acc: 0.607000; val_acc: 0.544444\n",
      "(Iteration 81 / 200) loss: 1.273185\n",
      "(Epoch 9 / 20) train acc: 0.691000; val_acc: 0.608333\n",
      "(Iteration 91 / 200) loss: 1.204714\n",
      "(Epoch 10 / 20) train acc: 0.757000; val_acc: 0.672222\n",
      "(Iteration 101 / 200) loss: 1.050196\n",
      "(Epoch 11 / 20) train acc: 0.770000; val_acc: 0.711111\n",
      "(Iteration 111 / 200) loss: 0.989071\n",
      "(Epoch 12 / 20) train acc: 0.813000; val_acc: 0.755556\n",
      "(Iteration 121 / 200) loss: 0.957847\n",
      "(Epoch 13 / 20) train acc: 0.846000; val_acc: 0.777778\n",
      "(Iteration 131 / 200) loss: 0.862346\n",
      "(Epoch 14 / 20) train acc: 0.854000; val_acc: 0.797222\n",
      "(Iteration 141 / 200) loss: 0.754755\n",
      "(Epoch 15 / 20) train acc: 0.877000; val_acc: 0.805556\n",
      "(Iteration 151 / 200) loss: 0.713601\n",
      "(Epoch 16 / 20) train acc: 0.880000; val_acc: 0.811111\n",
      "(Iteration 161 / 200) loss: 0.627303\n",
      "(Epoch 17 / 20) train acc: 0.895000; val_acc: 0.822222\n",
      "(Iteration 171 / 200) loss: 0.691557\n",
      "(Epoch 18 / 20) train acc: 0.911000; val_acc: 0.830556\n",
      "(Iteration 181 / 200) loss: 0.471336\n",
      "(Epoch 19 / 20) train acc: 0.894000; val_acc: 0.838889\n",
      "(Iteration 191 / 200) loss: 0.463327\n",
      "(Epoch 20 / 20) train acc: 0.901000; val_acc: 0.850000\n",
      "(Iteration 1 / 200) loss: 2.302572\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.379000; val_acc: 0.419444\n",
      "(Iteration 11 / 200) loss: 2.302162\n",
      "(Epoch 2 / 20) train acc: 0.523000; val_acc: 0.541667\n",
      "(Iteration 21 / 200) loss: 2.301508\n",
      "(Epoch 3 / 20) train acc: 0.539000; val_acc: 0.527778\n",
      "(Iteration 31 / 200) loss: 2.299729\n",
      "(Epoch 4 / 20) train acc: 0.499000; val_acc: 0.505556\n",
      "(Iteration 41 / 200) loss: 2.296347\n",
      "(Epoch 5 / 20) train acc: 0.498000; val_acc: 0.522222\n",
      "(Iteration 51 / 200) loss: 2.290558\n",
      "(Epoch 6 / 20) train acc: 0.488000; val_acc: 0.505556\n",
      "(Iteration 61 / 200) loss: 2.279081\n",
      "(Epoch 7 / 20) train acc: 0.452000; val_acc: 0.500000\n",
      "(Iteration 71 / 200) loss: 2.258451\n",
      "(Epoch 8 / 20) train acc: 0.403000; val_acc: 0.444444\n",
      "(Iteration 81 / 200) loss: 2.219747\n",
      "(Epoch 9 / 20) train acc: 0.391000; val_acc: 0.425000\n",
      "(Iteration 91 / 200) loss: 2.173783\n",
      "(Epoch 10 / 20) train acc: 0.442000; val_acc: 0.480556\n",
      "(Iteration 101 / 200) loss: 2.081676\n",
      "(Epoch 11 / 20) train acc: 0.489000; val_acc: 0.505556\n",
      "(Iteration 111 / 200) loss: 1.965823\n",
      "(Epoch 12 / 20) train acc: 0.503000; val_acc: 0.522222\n",
      "(Iteration 121 / 200) loss: 1.792780\n",
      "(Epoch 13 / 20) train acc: 0.479000; val_acc: 0.483333\n",
      "(Iteration 131 / 200) loss: 1.679210\n",
      "(Epoch 14 / 20) train acc: 0.515000; val_acc: 0.533333\n",
      "(Iteration 141 / 200) loss: 1.610932\n",
      "(Epoch 15 / 20) train acc: 0.572000; val_acc: 0.550000\n",
      "(Iteration 151 / 200) loss: 1.441216\n",
      "(Epoch 16 / 20) train acc: 0.580000; val_acc: 0.561111\n",
      "(Iteration 161 / 200) loss: 1.302980\n",
      "(Epoch 17 / 20) train acc: 0.532000; val_acc: 0.552778\n",
      "(Iteration 171 / 200) loss: 1.369845\n",
      "(Epoch 18 / 20) train acc: 0.560000; val_acc: 0.569444\n",
      "(Iteration 181 / 200) loss: 1.178989\n",
      "(Epoch 19 / 20) train acc: 0.577000; val_acc: 0.597222\n",
      "(Iteration 191 / 200) loss: 1.064642\n",
      "(Epoch 20 / 20) train acc: 0.619000; val_acc: 0.611111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302632\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302454\n",
      "(Epoch 4 / 20) train acc: 0.123000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302362\n",
      "(Epoch 5 / 20) train acc: 0.186000; val_acc: 0.150000\n",
      "(Iteration 51 / 200) loss: 2.301220\n",
      "(Epoch 6 / 20) train acc: 0.174000; val_acc: 0.147222\n",
      "(Iteration 61 / 200) loss: 2.295811\n",
      "(Epoch 7 / 20) train acc: 0.188000; val_acc: 0.166667\n",
      "(Iteration 71 / 200) loss: 2.282635\n",
      "(Epoch 8 / 20) train acc: 0.163000; val_acc: 0.144444\n",
      "(Iteration 81 / 200) loss: 2.274246\n",
      "(Epoch 9 / 20) train acc: 0.182000; val_acc: 0.169444\n",
      "(Iteration 91 / 200) loss: 2.231243\n",
      "(Epoch 10 / 20) train acc: 0.162000; val_acc: 0.163889\n",
      "(Iteration 101 / 200) loss: 2.168635\n",
      "(Epoch 11 / 20) train acc: 0.205000; val_acc: 0.188889\n",
      "(Iteration 111 / 200) loss: 2.092654\n",
      "(Epoch 12 / 20) train acc: 0.199000; val_acc: 0.202778\n",
      "(Iteration 121 / 200) loss: 1.993445\n",
      "(Epoch 13 / 20) train acc: 0.227000; val_acc: 0.194444\n",
      "(Iteration 131 / 200) loss: 1.920427\n",
      "(Epoch 14 / 20) train acc: 0.237000; val_acc: 0.197222\n",
      "(Iteration 141 / 200) loss: 1.919134\n",
      "(Epoch 15 / 20) train acc: 0.225000; val_acc: 0.222222\n",
      "(Iteration 151 / 200) loss: 1.791158\n",
      "(Epoch 16 / 20) train acc: 0.232000; val_acc: 0.197222\n",
      "(Iteration 161 / 200) loss: 1.836876\n",
      "(Epoch 17 / 20) train acc: 0.247000; val_acc: 0.222222\n",
      "(Iteration 171 / 200) loss: 1.806230\n",
      "(Epoch 18 / 20) train acc: 0.277000; val_acc: 0.250000\n",
      "(Iteration 181 / 200) loss: 1.723108\n",
      "(Epoch 19 / 20) train acc: 0.288000; val_acc: 0.275000\n",
      "(Iteration 191 / 200) loss: 1.717473\n",
      "(Epoch 20 / 20) train acc: 0.295000; val_acc: 0.277778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302580\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302643\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302490\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302501\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302574\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302149\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302781\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302165\n",
      "(Epoch 9 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.299773\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.299323\n",
      "(Epoch 11 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.284328\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.261350\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.259470\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.218918\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.162800\n",
      "(Epoch 16 / 20) train acc: 0.124000; val_acc: 0.091667\n",
      "(Iteration 161 / 200) loss: 2.099559\n",
      "(Epoch 17 / 20) train acc: 0.132000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 2.062307\n",
      "(Epoch 18 / 20) train acc: 0.155000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.036521\n",
      "(Epoch 19 / 20) train acc: 0.169000; val_acc: 0.127778\n",
      "(Iteration 191 / 200) loss: 1.994190\n",
      "(Epoch 20 / 20) train acc: 0.182000; val_acc: 0.136111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302567\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302702\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302550\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302398\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302516\n",
      "(Epoch 6 / 20) train acc: 0.130000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302719\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302599\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302703\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302062\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.301544\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.298416\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.293311\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.279052\n",
      "(Epoch 14 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.263205\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.248508\n",
      "(Epoch 16 / 20) train acc: 0.119000; val_acc: 0.100000\n",
      "(Iteration 161 / 200) loss: 2.200669\n",
      "(Epoch 17 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 2.181084\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.094444\n",
      "(Iteration 181 / 200) loss: 2.069289\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.094444\n",
      "(Iteration 191 / 200) loss: 2.110411\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 1 / 200) loss: 36838.194550\n",
      "(Epoch 0 / 20) train acc: 0.079000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 38779.654503\n",
      "(Epoch 2 / 20) train acc: 0.079000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 35216.049463\n",
      "(Epoch 3 / 20) train acc: 0.083000; val_acc: 0.094444\n",
      "(Iteration 31 / 200) loss: 33301.171924\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 34357.169386\n",
      "(Epoch 5 / 20) train acc: 0.076000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 37164.959348\n",
      "(Epoch 6 / 20) train acc: 0.079000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 32505.956810\n",
      "(Epoch 7 / 20) train acc: 0.074000; val_acc: 0.075000\n",
      "(Iteration 71 / 200) loss: 27304.546773\n",
      "(Epoch 8 / 20) train acc: 0.074000; val_acc: 0.075000\n",
      "(Iteration 81 / 200) loss: 25639.931735\n",
      "(Epoch 9 / 20) train acc: 0.073000; val_acc: 0.077778\n",
      "(Iteration 91 / 200) loss: 28346.121698\n",
      "(Epoch 10 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 27616.174162\n",
      "(Epoch 11 / 20) train acc: 0.073000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 22971.796626\n",
      "(Epoch 12 / 20) train acc: 0.086000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 25020.636590\n",
      "(Epoch 13 / 20) train acc: 0.107000; val_acc: 0.088889\n",
      "(Iteration 131 / 200) loss: 27623.194054\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 141 / 200) loss: 26377.779019\n",
      "(Epoch 15 / 20) train acc: 0.086000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 22349.746483\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 22508.491448\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 22892.051413\n",
      "(Epoch 18 / 20) train acc: 0.122000; val_acc: 0.102778\n",
      "(Iteration 181 / 200) loss: 20047.678878\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 21194.103844\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.877742\n",
      "(Epoch 0 / 20) train acc: 0.122000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.138000; val_acc: 0.147222\n",
      "(Iteration 11 / 200) loss: 2.444779\n",
      "(Epoch 2 / 20) train acc: 0.194000; val_acc: 0.188889\n",
      "(Iteration 21 / 200) loss: 2.168268\n",
      "(Epoch 3 / 20) train acc: 0.299000; val_acc: 0.327778\n",
      "(Iteration 31 / 200) loss: 2.030928\n",
      "(Epoch 4 / 20) train acc: 0.441000; val_acc: 0.436111\n",
      "(Iteration 41 / 200) loss: 1.755265\n",
      "(Epoch 5 / 20) train acc: 0.543000; val_acc: 0.538889\n",
      "(Iteration 51 / 200) loss: 1.472962\n",
      "(Epoch 6 / 20) train acc: 0.590000; val_acc: 0.580556\n",
      "(Iteration 61 / 200) loss: 1.413990\n",
      "(Epoch 7 / 20) train acc: 0.684000; val_acc: 0.633333\n",
      "(Iteration 71 / 200) loss: 1.333524\n",
      "(Epoch 8 / 20) train acc: 0.726000; val_acc: 0.697222\n",
      "(Iteration 81 / 200) loss: 1.041916\n",
      "(Epoch 9 / 20) train acc: 0.758000; val_acc: 0.722222\n",
      "(Iteration 91 / 200) loss: 0.975187\n",
      "(Epoch 10 / 20) train acc: 0.804000; val_acc: 0.741667\n",
      "(Iteration 101 / 200) loss: 0.898746\n",
      "(Epoch 11 / 20) train acc: 0.818000; val_acc: 0.766667\n",
      "(Iteration 111 / 200) loss: 0.753368\n",
      "(Epoch 12 / 20) train acc: 0.844000; val_acc: 0.805556\n",
      "(Iteration 121 / 200) loss: 0.851572\n",
      "(Epoch 13 / 20) train acc: 0.860000; val_acc: 0.825000\n",
      "(Iteration 131 / 200) loss: 0.770723\n",
      "(Epoch 14 / 20) train acc: 0.899000; val_acc: 0.841667\n",
      "(Iteration 141 / 200) loss: 0.762744\n",
      "(Epoch 15 / 20) train acc: 0.895000; val_acc: 0.850000\n",
      "(Iteration 151 / 200) loss: 0.586778\n",
      "(Epoch 16 / 20) train acc: 0.917000; val_acc: 0.861111\n",
      "(Iteration 161 / 200) loss: 0.431282\n",
      "(Epoch 17 / 20) train acc: 0.927000; val_acc: 0.875000\n",
      "(Iteration 171 / 200) loss: 0.454402\n",
      "(Epoch 18 / 20) train acc: 0.916000; val_acc: 0.883333\n",
      "(Iteration 181 / 200) loss: 0.383543\n",
      "(Epoch 19 / 20) train acc: 0.913000; val_acc: 0.883333\n",
      "(Iteration 191 / 200) loss: 0.448762\n",
      "(Epoch 20 / 20) train acc: 0.917000; val_acc: 0.888889\n",
      "(Iteration 1 / 200) loss: 2.302661\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.374000; val_acc: 0.388889\n",
      "(Iteration 11 / 200) loss: 2.302233\n",
      "(Epoch 2 / 20) train acc: 0.536000; val_acc: 0.505556\n",
      "(Iteration 21 / 200) loss: 2.301827\n",
      "(Epoch 3 / 20) train acc: 0.630000; val_acc: 0.583333\n",
      "(Iteration 31 / 200) loss: 2.300382\n",
      "(Epoch 4 / 20) train acc: 0.623000; val_acc: 0.569444\n",
      "(Iteration 41 / 200) loss: 2.298706\n",
      "(Epoch 5 / 20) train acc: 0.671000; val_acc: 0.588889\n",
      "(Iteration 51 / 200) loss: 2.292765\n",
      "(Epoch 6 / 20) train acc: 0.640000; val_acc: 0.577778\n",
      "(Iteration 61 / 200) loss: 2.281781\n",
      "(Epoch 7 / 20) train acc: 0.526000; val_acc: 0.511111\n",
      "(Iteration 71 / 200) loss: 2.271792\n",
      "(Epoch 8 / 20) train acc: 0.504000; val_acc: 0.497222\n",
      "(Iteration 81 / 200) loss: 2.243144\n",
      "(Epoch 9 / 20) train acc: 0.500000; val_acc: 0.491667\n",
      "(Iteration 91 / 200) loss: 2.177013\n",
      "(Epoch 10 / 20) train acc: 0.541000; val_acc: 0.519444\n",
      "(Iteration 101 / 200) loss: 2.106400\n",
      "(Epoch 11 / 20) train acc: 0.483000; val_acc: 0.505556\n",
      "(Iteration 111 / 200) loss: 2.011318\n",
      "(Epoch 12 / 20) train acc: 0.516000; val_acc: 0.522222\n",
      "(Iteration 121 / 200) loss: 1.882016\n",
      "(Epoch 13 / 20) train acc: 0.552000; val_acc: 0.538889\n",
      "(Iteration 131 / 200) loss: 1.705302\n",
      "(Epoch 14 / 20) train acc: 0.543000; val_acc: 0.525000\n",
      "(Iteration 141 / 200) loss: 1.525448\n",
      "(Epoch 15 / 20) train acc: 0.548000; val_acc: 0.566667\n",
      "(Iteration 151 / 200) loss: 1.459197\n",
      "(Epoch 16 / 20) train acc: 0.593000; val_acc: 0.588889\n",
      "(Iteration 161 / 200) loss: 1.317865\n",
      "(Epoch 17 / 20) train acc: 0.598000; val_acc: 0.583333\n",
      "(Iteration 171 / 200) loss: 1.261732\n",
      "(Epoch 18 / 20) train acc: 0.622000; val_acc: 0.630556\n",
      "(Iteration 181 / 200) loss: 1.116718\n",
      "(Epoch 19 / 20) train acc: 0.659000; val_acc: 0.647222\n",
      "(Iteration 191 / 200) loss: 1.177585\n",
      "(Epoch 20 / 20) train acc: 0.656000; val_acc: 0.658333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.169000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302565\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302563\n",
      "(Epoch 3 / 20) train acc: 0.132000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302512\n",
      "(Epoch 4 / 20) train acc: 0.136000; val_acc: 0.125000\n",
      "(Iteration 41 / 200) loss: 2.301881\n",
      "(Epoch 5 / 20) train acc: 0.185000; val_acc: 0.172222\n",
      "(Iteration 51 / 200) loss: 2.300897\n",
      "(Epoch 6 / 20) train acc: 0.187000; val_acc: 0.166667\n",
      "(Iteration 61 / 200) loss: 2.296982\n",
      "(Epoch 7 / 20) train acc: 0.188000; val_acc: 0.172222\n",
      "(Iteration 71 / 200) loss: 2.284807\n",
      "(Epoch 8 / 20) train acc: 0.179000; val_acc: 0.169444\n",
      "(Iteration 81 / 200) loss: 2.253092\n",
      "(Epoch 9 / 20) train acc: 0.200000; val_acc: 0.177778\n",
      "(Iteration 91 / 200) loss: 2.226756\n",
      "(Epoch 10 / 20) train acc: 0.227000; val_acc: 0.208333\n",
      "(Iteration 101 / 200) loss: 2.161065\n",
      "(Epoch 11 / 20) train acc: 0.211000; val_acc: 0.194444\n",
      "(Iteration 111 / 200) loss: 2.105628\n",
      "(Epoch 12 / 20) train acc: 0.200000; val_acc: 0.191667\n",
      "(Iteration 121 / 200) loss: 2.008498\n",
      "(Epoch 13 / 20) train acc: 0.207000; val_acc: 0.194444\n",
      "(Iteration 131 / 200) loss: 1.991729\n",
      "(Epoch 14 / 20) train acc: 0.205000; val_acc: 0.191667\n",
      "(Iteration 141 / 200) loss: 1.891082\n",
      "(Epoch 15 / 20) train acc: 0.244000; val_acc: 0.194444\n",
      "(Iteration 151 / 200) loss: 1.872775\n",
      "(Epoch 16 / 20) train acc: 0.263000; val_acc: 0.200000\n",
      "(Iteration 161 / 200) loss: 1.873147\n",
      "(Epoch 17 / 20) train acc: 0.233000; val_acc: 0.219444\n",
      "(Iteration 171 / 200) loss: 1.806940\n",
      "(Epoch 18 / 20) train acc: 0.253000; val_acc: 0.219444\n",
      "(Iteration 181 / 200) loss: 1.754936\n",
      "(Epoch 19 / 20) train acc: 0.271000; val_acc: 0.236111\n",
      "(Iteration 191 / 200) loss: 1.849816\n",
      "(Epoch 20 / 20) train acc: 0.254000; val_acc: 0.244444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.135000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302604\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302642\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302618\n",
      "(Epoch 4 / 20) train acc: 0.134000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302504\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302601\n",
      "(Epoch 6 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302758\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302510\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302585\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302761\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302395\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302239\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302563\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302571\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302043\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302463\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302175\n",
      "(Epoch 17 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302985\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302330\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302414\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302537\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302727\n",
      "(Epoch 3 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302527\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302404\n",
      "(Epoch 5 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302571\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302679\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302357\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302188\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302629\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302179\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302241\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302393\n",
      "(Epoch 13 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302741\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302305\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.303371\n",
      "(Epoch 16 / 20) train acc: 0.132000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301933\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302626\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303606\n",
      "(Epoch 19 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302423\n",
      "(Epoch 20 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 25219.543924\n",
      "(Epoch 0 / 20) train acc: 0.053000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.071000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 27226.478886\n",
      "(Epoch 2 / 20) train acc: 0.058000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 24854.951355\n",
      "(Epoch 3 / 20) train acc: 0.052000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 23969.893825\n",
      "(Epoch 4 / 20) train acc: 0.063000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 18424.446295\n",
      "(Epoch 5 / 20) train acc: 0.068000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 21022.506266\n",
      "(Epoch 6 / 20) train acc: 0.061000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 18167.801237\n",
      "(Epoch 7 / 20) train acc: 0.071000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 18339.538709\n",
      "(Epoch 8 / 20) train acc: 0.084000; val_acc: 0.102778\n",
      "(Iteration 81 / 200) loss: 18801.661180\n",
      "(Epoch 9 / 20) train acc: 0.072000; val_acc: 0.113889\n",
      "(Iteration 91 / 200) loss: 17755.001153\n",
      "(Epoch 10 / 20) train acc: 0.088000; val_acc: 0.111111\n",
      "(Iteration 101 / 200) loss: 17741.741125\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.113889\n",
      "(Iteration 111 / 200) loss: 15723.788597\n",
      "(Epoch 12 / 20) train acc: 0.084000; val_acc: 0.113889\n",
      "(Iteration 121 / 200) loss: 16005.578569\n",
      "(Epoch 13 / 20) train acc: 0.115000; val_acc: 0.119444\n",
      "(Iteration 131 / 200) loss: 14757.693541\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.125000\n",
      "(Iteration 141 / 200) loss: 15025.701013\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.136111\n",
      "(Iteration 151 / 200) loss: 12922.072236\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.141667\n",
      "(Iteration 161 / 200) loss: 14339.517208\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.150000\n",
      "(Iteration 171 / 200) loss: 13489.288431\n",
      "(Epoch 18 / 20) train acc: 0.143000; val_acc: 0.152778\n",
      "(Iteration 181 / 200) loss: 11151.672154\n",
      "(Epoch 19 / 20) train acc: 0.140000; val_acc: 0.155556\n",
      "(Iteration 191 / 200) loss: 12982.123377\n",
      "(Epoch 20 / 20) train acc: 0.138000; val_acc: 0.163889\n",
      "(Iteration 1 / 200) loss: 3.249264\n",
      "(Epoch 0 / 20) train acc: 0.127000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.130000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 2.806077\n",
      "(Epoch 2 / 20) train acc: 0.197000; val_acc: 0.169444\n",
      "(Iteration 21 / 200) loss: 2.335257\n",
      "(Epoch 3 / 20) train acc: 0.250000; val_acc: 0.236111\n",
      "(Iteration 31 / 200) loss: 2.268932\n",
      "(Epoch 4 / 20) train acc: 0.359000; val_acc: 0.338889\n",
      "(Iteration 41 / 200) loss: 1.869311\n",
      "(Epoch 5 / 20) train acc: 0.475000; val_acc: 0.422222\n",
      "(Iteration 51 / 200) loss: 1.764358\n",
      "(Epoch 6 / 20) train acc: 0.545000; val_acc: 0.491667\n",
      "(Iteration 61 / 200) loss: 1.373403\n",
      "(Epoch 7 / 20) train acc: 0.609000; val_acc: 0.558333\n",
      "(Iteration 71 / 200) loss: 1.376370\n",
      "(Epoch 8 / 20) train acc: 0.677000; val_acc: 0.622222\n",
      "(Iteration 81 / 200) loss: 1.145655\n",
      "(Epoch 9 / 20) train acc: 0.730000; val_acc: 0.666667\n",
      "(Iteration 91 / 200) loss: 1.109564\n",
      "(Epoch 10 / 20) train acc: 0.775000; val_acc: 0.705556\n",
      "(Iteration 101 / 200) loss: 0.836589\n",
      "(Epoch 11 / 20) train acc: 0.801000; val_acc: 0.752778\n",
      "(Iteration 111 / 200) loss: 0.957716\n",
      "(Epoch 12 / 20) train acc: 0.816000; val_acc: 0.786111\n",
      "(Iteration 121 / 200) loss: 0.775999\n",
      "(Epoch 13 / 20) train acc: 0.844000; val_acc: 0.802778\n",
      "(Iteration 131 / 200) loss: 0.680407\n",
      "(Epoch 14 / 20) train acc: 0.868000; val_acc: 0.813889\n",
      "(Iteration 141 / 200) loss: 0.641224\n",
      "(Epoch 15 / 20) train acc: 0.869000; val_acc: 0.838889\n",
      "(Iteration 151 / 200) loss: 0.636354\n",
      "(Epoch 16 / 20) train acc: 0.892000; val_acc: 0.850000\n",
      "(Iteration 161 / 200) loss: 0.478821\n",
      "(Epoch 17 / 20) train acc: 0.890000; val_acc: 0.861111\n",
      "(Iteration 171 / 200) loss: 0.436724\n",
      "(Epoch 18 / 20) train acc: 0.915000; val_acc: 0.877778\n",
      "(Iteration 181 / 200) loss: 0.622101\n",
      "(Epoch 19 / 20) train acc: 0.912000; val_acc: 0.877778\n",
      "(Iteration 191 / 200) loss: 0.441219\n",
      "(Epoch 20 / 20) train acc: 0.887000; val_acc: 0.863889\n",
      "(Iteration 1 / 200) loss: 2.302618\n",
      "(Epoch 0 / 20) train acc: 0.135000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.221000; val_acc: 0.194444\n",
      "(Iteration 11 / 200) loss: 2.302163\n",
      "(Epoch 2 / 20) train acc: 0.223000; val_acc: 0.219444\n",
      "(Iteration 21 / 200) loss: 2.301502\n",
      "(Epoch 3 / 20) train acc: 0.317000; val_acc: 0.261111\n",
      "(Iteration 31 / 200) loss: 2.300590\n",
      "(Epoch 4 / 20) train acc: 0.315000; val_acc: 0.286111\n",
      "(Iteration 41 / 200) loss: 2.298400\n",
      "(Epoch 5 / 20) train acc: 0.433000; val_acc: 0.433333\n",
      "(Iteration 51 / 200) loss: 2.293505\n",
      "(Epoch 6 / 20) train acc: 0.462000; val_acc: 0.450000\n",
      "(Iteration 61 / 200) loss: 2.284300\n",
      "(Epoch 7 / 20) train acc: 0.482000; val_acc: 0.505556\n",
      "(Iteration 71 / 200) loss: 2.264638\n",
      "(Epoch 8 / 20) train acc: 0.443000; val_acc: 0.441667\n",
      "(Iteration 81 / 200) loss: 2.241423\n",
      "(Epoch 9 / 20) train acc: 0.372000; val_acc: 0.388889\n",
      "(Iteration 91 / 200) loss: 2.193328\n",
      "(Epoch 10 / 20) train acc: 0.385000; val_acc: 0.419444\n",
      "(Iteration 101 / 200) loss: 2.119349\n",
      "(Epoch 11 / 20) train acc: 0.342000; val_acc: 0.372222\n",
      "(Iteration 111 / 200) loss: 2.025219\n",
      "(Epoch 12 / 20) train acc: 0.353000; val_acc: 0.369444\n",
      "(Iteration 121 / 200) loss: 1.918256\n",
      "(Epoch 13 / 20) train acc: 0.397000; val_acc: 0.438889\n",
      "(Iteration 131 / 200) loss: 1.795843\n",
      "(Epoch 14 / 20) train acc: 0.478000; val_acc: 0.483333\n",
      "(Iteration 141 / 200) loss: 1.663910\n",
      "(Epoch 15 / 20) train acc: 0.488000; val_acc: 0.497222\n",
      "(Iteration 151 / 200) loss: 1.560105\n",
      "(Epoch 16 / 20) train acc: 0.595000; val_acc: 0.597222\n",
      "(Iteration 161 / 200) loss: 1.371284\n",
      "(Epoch 17 / 20) train acc: 0.588000; val_acc: 0.611111\n",
      "(Iteration 171 / 200) loss: 1.331902\n",
      "(Epoch 18 / 20) train acc: 0.574000; val_acc: 0.638889\n",
      "(Iteration 181 / 200) loss: 1.326644\n",
      "(Epoch 19 / 20) train acc: 0.614000; val_acc: 0.672222\n",
      "(Iteration 191 / 200) loss: 1.196300\n",
      "(Epoch 20 / 20) train acc: 0.719000; val_acc: 0.719444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302515\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302635\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302477\n",
      "(Epoch 4 / 20) train acc: 0.196000; val_acc: 0.155556\n",
      "(Iteration 41 / 200) loss: 2.302308\n",
      "(Epoch 5 / 20) train acc: 0.184000; val_acc: 0.150000\n",
      "(Iteration 51 / 200) loss: 2.301183\n",
      "(Epoch 6 / 20) train acc: 0.132000; val_acc: 0.105556\n",
      "(Iteration 61 / 200) loss: 2.297290\n",
      "(Epoch 7 / 20) train acc: 0.166000; val_acc: 0.105556\n",
      "(Iteration 71 / 200) loss: 2.292350\n",
      "(Epoch 8 / 20) train acc: 0.179000; val_acc: 0.172222\n",
      "(Iteration 81 / 200) loss: 2.276565\n",
      "(Epoch 9 / 20) train acc: 0.168000; val_acc: 0.172222\n",
      "(Iteration 91 / 200) loss: 2.234386\n",
      "(Epoch 10 / 20) train acc: 0.193000; val_acc: 0.180556\n",
      "(Iteration 101 / 200) loss: 2.207176\n",
      "(Epoch 11 / 20) train acc: 0.219000; val_acc: 0.216667\n",
      "(Iteration 111 / 200) loss: 2.117685\n",
      "(Epoch 12 / 20) train acc: 0.237000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 2.010156\n",
      "(Epoch 13 / 20) train acc: 0.245000; val_acc: 0.244444\n",
      "(Iteration 131 / 200) loss: 1.943438\n",
      "(Epoch 14 / 20) train acc: 0.256000; val_acc: 0.269444\n",
      "(Iteration 141 / 200) loss: 1.911297\n",
      "(Epoch 15 / 20) train acc: 0.293000; val_acc: 0.297222\n",
      "(Iteration 151 / 200) loss: 1.853521\n",
      "(Epoch 16 / 20) train acc: 0.264000; val_acc: 0.280556\n",
      "(Iteration 161 / 200) loss: 1.682390\n",
      "(Epoch 17 / 20) train acc: 0.277000; val_acc: 0.275000\n",
      "(Iteration 171 / 200) loss: 1.679112\n",
      "(Epoch 18 / 20) train acc: 0.297000; val_acc: 0.283333\n",
      "(Iteration 181 / 200) loss: 1.658733\n",
      "(Epoch 19 / 20) train acc: 0.332000; val_acc: 0.313889\n",
      "(Iteration 191 / 200) loss: 1.692876\n",
      "(Epoch 20 / 20) train acc: 0.368000; val_acc: 0.319444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302559\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302536\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302470\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302565\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302398\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302660\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302334\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302411\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302360\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302328\n",
      "(Epoch 11 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.301106\n",
      "(Epoch 12 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.301402\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.297179\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.292977\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.285721\n",
      "(Epoch 16 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.262043\n",
      "(Epoch 17 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.244909\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.240518\n",
      "(Epoch 19 / 20) train acc: 0.124000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.207844\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302562\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302515\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302538\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302610\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302242\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302855\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302575\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302709\n",
      "(Epoch 9 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302299\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302673\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302398\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302826\n",
      "(Epoch 13 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302249\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302011\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302046\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302437\n",
      "(Epoch 17 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302078\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302380\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302346\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 49934.622178\n",
      "(Epoch 0 / 20) train acc: 0.081000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 48110.773413\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 50897.780212\n",
      "(Epoch 3 / 20) train acc: 0.074000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 44147.157202\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 48079.839214\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 47137.426274\n",
      "(Epoch 6 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 45142.233335\n",
      "(Epoch 7 / 20) train acc: 0.083000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 44697.780415\n",
      "(Epoch 8 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 43764.842495\n",
      "(Epoch 9 / 20) train acc: 0.069000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 45335.859585\n",
      "(Epoch 10 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 49582.981670\n",
      "(Epoch 11 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 42821.458755\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.113889\n",
      "(Iteration 121 / 200) loss: 41527.858340\n",
      "(Epoch 13 / 20) train acc: 0.084000; val_acc: 0.113889\n",
      "(Iteration 131 / 200) loss: 41424.005435\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.113889\n",
      "(Iteration 141 / 200) loss: 43471.075054\n",
      "(Epoch 15 / 20) train acc: 0.096000; val_acc: 0.111111\n",
      "(Iteration 151 / 200) loss: 42197.302185\n",
      "(Epoch 16 / 20) train acc: 0.095000; val_acc: 0.111111\n",
      "(Iteration 161 / 200) loss: 41434.229312\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.111111\n",
      "(Iteration 171 / 200) loss: 43966.096438\n",
      "(Epoch 18 / 20) train acc: 0.089000; val_acc: 0.111111\n",
      "(Iteration 181 / 200) loss: 45879.633564\n",
      "(Epoch 19 / 20) train acc: 0.080000; val_acc: 0.111111\n",
      "(Iteration 191 / 200) loss: 41331.283193\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.111111\n",
      "(Iteration 1 / 200) loss: 5.021535\n",
      "(Epoch 0 / 20) train acc: 0.132000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.126000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 4.816342\n",
      "(Epoch 2 / 20) train acc: 0.141000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 4.941788\n",
      "(Epoch 3 / 20) train acc: 0.127000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 4.677340\n",
      "(Epoch 4 / 20) train acc: 0.130000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 4.841010\n",
      "(Epoch 5 / 20) train acc: 0.153000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 4.428875\n",
      "(Epoch 6 / 20) train acc: 0.126000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 4.724028\n",
      "(Epoch 7 / 20) train acc: 0.128000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 4.235533\n",
      "(Epoch 8 / 20) train acc: 0.138000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 4.378437\n",
      "(Epoch 9 / 20) train acc: 0.154000; val_acc: 0.102778\n",
      "(Iteration 91 / 200) loss: 4.238903\n",
      "(Epoch 10 / 20) train acc: 0.145000; val_acc: 0.105556\n",
      "(Iteration 101 / 200) loss: 4.316276\n",
      "(Epoch 11 / 20) train acc: 0.134000; val_acc: 0.102778\n",
      "(Iteration 111 / 200) loss: 4.298478\n",
      "(Epoch 12 / 20) train acc: 0.141000; val_acc: 0.105556\n",
      "(Iteration 121 / 200) loss: 4.119448\n",
      "(Epoch 13 / 20) train acc: 0.118000; val_acc: 0.105556\n",
      "(Iteration 131 / 200) loss: 4.490063\n",
      "(Epoch 14 / 20) train acc: 0.149000; val_acc: 0.108333\n",
      "(Iteration 141 / 200) loss: 4.035126\n",
      "(Epoch 15 / 20) train acc: 0.131000; val_acc: 0.113889\n",
      "(Iteration 151 / 200) loss: 4.159815\n",
      "(Epoch 16 / 20) train acc: 0.150000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 4.344493\n",
      "(Epoch 17 / 20) train acc: 0.169000; val_acc: 0.119444\n",
      "(Iteration 171 / 200) loss: 3.825001\n",
      "(Epoch 18 / 20) train acc: 0.172000; val_acc: 0.122222\n",
      "(Iteration 181 / 200) loss: 3.912613\n",
      "(Epoch 19 / 20) train acc: 0.171000; val_acc: 0.130556\n",
      "(Iteration 191 / 200) loss: 3.997734\n",
      "(Epoch 20 / 20) train acc: 0.176000; val_acc: 0.136111\n",
      "(Iteration 1 / 200) loss: 2.315964\n",
      "(Epoch 0 / 20) train acc: 0.136000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.122222\n",
      "(Iteration 11 / 200) loss: 2.315767\n",
      "(Epoch 2 / 20) train acc: 0.161000; val_acc: 0.130556\n",
      "(Iteration 21 / 200) loss: 2.315542\n",
      "(Epoch 3 / 20) train acc: 0.168000; val_acc: 0.166667\n",
      "(Iteration 31 / 200) loss: 2.315313\n",
      "(Epoch 4 / 20) train acc: 0.191000; val_acc: 0.197222\n",
      "(Iteration 41 / 200) loss: 2.315119\n",
      "(Epoch 5 / 20) train acc: 0.197000; val_acc: 0.208333\n",
      "(Iteration 51 / 200) loss: 2.314888\n",
      "(Epoch 6 / 20) train acc: 0.186000; val_acc: 0.205556\n",
      "(Iteration 61 / 200) loss: 2.314720\n",
      "(Epoch 7 / 20) train acc: 0.166000; val_acc: 0.197222\n",
      "(Iteration 71 / 200) loss: 2.314520\n",
      "(Epoch 8 / 20) train acc: 0.183000; val_acc: 0.202778\n",
      "(Iteration 81 / 200) loss: 2.314302\n",
      "(Epoch 9 / 20) train acc: 0.186000; val_acc: 0.208333\n",
      "(Iteration 91 / 200) loss: 2.314094\n",
      "(Epoch 10 / 20) train acc: 0.258000; val_acc: 0.233333\n",
      "(Iteration 101 / 200) loss: 2.313928\n",
      "(Epoch 11 / 20) train acc: 0.255000; val_acc: 0.269444\n",
      "(Iteration 111 / 200) loss: 2.313761\n",
      "(Epoch 12 / 20) train acc: 0.235000; val_acc: 0.244444\n",
      "(Iteration 121 / 200) loss: 2.313532\n",
      "(Epoch 13 / 20) train acc: 0.257000; val_acc: 0.233333\n",
      "(Iteration 131 / 200) loss: 2.313446\n",
      "(Epoch 14 / 20) train acc: 0.258000; val_acc: 0.241667\n",
      "(Iteration 141 / 200) loss: 2.313139\n",
      "(Epoch 15 / 20) train acc: 0.236000; val_acc: 0.241667\n",
      "(Iteration 151 / 200) loss: 2.312969\n",
      "(Epoch 16 / 20) train acc: 0.298000; val_acc: 0.277778\n",
      "(Iteration 161 / 200) loss: 2.312846\n",
      "(Epoch 17 / 20) train acc: 0.258000; val_acc: 0.283333\n",
      "(Iteration 171 / 200) loss: 2.312678\n",
      "(Epoch 18 / 20) train acc: 0.312000; val_acc: 0.288889\n",
      "(Iteration 181 / 200) loss: 2.312514\n",
      "(Epoch 19 / 20) train acc: 0.290000; val_acc: 0.294444\n",
      "(Iteration 191 / 200) loss: 2.312311\n",
      "(Epoch 20 / 20) train acc: 0.301000; val_acc: 0.294444\n",
      "(Iteration 1 / 200) loss: 2.302717\n",
      "(Epoch 0 / 20) train acc: 0.158000; val_acc: 0.166667\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302695\n",
      "(Epoch 2 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302672\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302653\n",
      "(Epoch 4 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302657\n",
      "(Epoch 5 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302652\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302637\n",
      "(Epoch 7 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302626\n",
      "(Epoch 8 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302644\n",
      "(Epoch 9 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302590\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302608\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302632\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302649\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302641\n",
      "(Epoch 14 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302594\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302546\n",
      "(Epoch 16 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302626\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302618\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302537\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302570\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302578\n",
      "(Epoch 3 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302578\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302572\n",
      "(Epoch 7 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302583\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302563\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302577\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302602\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302580\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302526\n",
      "(Epoch 13 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302584\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302599\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302521\n",
      "(Epoch 16 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302598\n",
      "(Epoch 17 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302580\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302570\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302553\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302576\n",
      "(Epoch 2 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302571\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302588\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302596\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302558\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302549\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302581\n",
      "(Epoch 8 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302570\n",
      "(Epoch 9 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302602\n",
      "(Epoch 10 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302557\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302527\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302587\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302609\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302632\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302585\n",
      "(Epoch 16 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302615\n",
      "(Epoch 17 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302556\n",
      "(Epoch 18 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302534\n",
      "(Epoch 19 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302564\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 61495.143940\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.087000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 58338.944509\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 57633.085586\n",
      "(Epoch 3 / 20) train acc: 0.083000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 52172.411802\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Iteration 41 / 200) loss: 61488.133062\n",
      "(Epoch 5 / 20) train acc: 0.089000; val_acc: 0.127778\n",
      "(Iteration 51 / 200) loss: 66450.624319\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 63732.160608\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 60692.331909\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.127778\n",
      "(Iteration 81 / 200) loss: 57089.463203\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.127778\n",
      "(Iteration 91 / 200) loss: 59876.304512\n",
      "(Epoch 10 / 20) train acc: 0.090000; val_acc: 0.127778\n",
      "(Iteration 101 / 200) loss: 59131.785825\n",
      "(Epoch 11 / 20) train acc: 0.085000; val_acc: 0.127778\n",
      "(Iteration 111 / 200) loss: 60546.422148\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Iteration 121 / 200) loss: 53051.353494\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.127778\n",
      "(Iteration 131 / 200) loss: 57102.084846\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.127778\n",
      "(Iteration 141 / 200) loss: 62685.826196\n",
      "(Epoch 15 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Iteration 151 / 200) loss: 55544.702578\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.127778\n",
      "(Iteration 161 / 200) loss: 54250.753940\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.127778\n",
      "(Iteration 171 / 200) loss: 50278.910332\n",
      "(Epoch 18 / 20) train acc: 0.091000; val_acc: 0.127778\n",
      "(Iteration 181 / 200) loss: 58786.761714\n",
      "(Epoch 19 / 20) train acc: 0.089000; val_acc: 0.127778\n",
      "(Iteration 191 / 200) loss: 57673.448081\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 5.533793\n",
      "(Epoch 0 / 20) train acc: 0.042000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.047000; val_acc: 0.063889\n",
      "(Iteration 11 / 200) loss: 5.407004\n",
      "(Epoch 2 / 20) train acc: 0.045000; val_acc: 0.061111\n",
      "(Iteration 21 / 200) loss: 5.190108\n",
      "(Epoch 3 / 20) train acc: 0.041000; val_acc: 0.061111\n",
      "(Iteration 31 / 200) loss: 4.691768\n",
      "(Epoch 4 / 20) train acc: 0.048000; val_acc: 0.061111\n",
      "(Iteration 41 / 200) loss: 5.021760\n",
      "(Epoch 5 / 20) train acc: 0.049000; val_acc: 0.061111\n",
      "(Iteration 51 / 200) loss: 4.920327\n",
      "(Epoch 6 / 20) train acc: 0.049000; val_acc: 0.066667\n",
      "(Iteration 61 / 200) loss: 5.131846\n",
      "(Epoch 7 / 20) train acc: 0.063000; val_acc: 0.072222\n",
      "(Iteration 71 / 200) loss: 4.455300\n",
      "(Epoch 8 / 20) train acc: 0.071000; val_acc: 0.077778\n",
      "(Iteration 81 / 200) loss: 4.641106\n",
      "(Epoch 9 / 20) train acc: 0.060000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 4.577991\n",
      "(Epoch 10 / 20) train acc: 0.066000; val_acc: 0.086111\n",
      "(Iteration 101 / 200) loss: 4.813493\n",
      "(Epoch 11 / 20) train acc: 0.063000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 4.646372\n",
      "(Epoch 12 / 20) train acc: 0.065000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 4.504934\n",
      "(Epoch 13 / 20) train acc: 0.084000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 4.210108\n",
      "(Epoch 14 / 20) train acc: 0.078000; val_acc: 0.091667\n",
      "(Iteration 141 / 200) loss: 4.304675\n",
      "(Epoch 15 / 20) train acc: 0.085000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 4.338105\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 4.232749\n",
      "(Epoch 17 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 4.021749\n",
      "(Epoch 18 / 20) train acc: 0.091000; val_acc: 0.108333\n",
      "(Iteration 181 / 200) loss: 4.048375\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 4.122873\n",
      "(Epoch 20 / 20) train acc: 0.133000; val_acc: 0.122222\n",
      "(Iteration 1 / 200) loss: 2.315739\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 2.315555\n",
      "(Epoch 2 / 20) train acc: 0.166000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 2.315297\n",
      "(Epoch 3 / 20) train acc: 0.195000; val_acc: 0.163889\n",
      "(Iteration 31 / 200) loss: 2.315092\n",
      "(Epoch 4 / 20) train acc: 0.225000; val_acc: 0.200000\n",
      "(Iteration 41 / 200) loss: 2.314872\n",
      "(Epoch 5 / 20) train acc: 0.291000; val_acc: 0.250000\n",
      "(Iteration 51 / 200) loss: 2.314677\n",
      "(Epoch 6 / 20) train acc: 0.287000; val_acc: 0.236111\n",
      "(Iteration 61 / 200) loss: 2.314464\n",
      "(Epoch 7 / 20) train acc: 0.257000; val_acc: 0.197222\n",
      "(Iteration 71 / 200) loss: 2.314274\n",
      "(Epoch 8 / 20) train acc: 0.255000; val_acc: 0.227778\n",
      "(Iteration 81 / 200) loss: 2.314076\n",
      "(Epoch 9 / 20) train acc: 0.282000; val_acc: 0.238889\n",
      "(Iteration 91 / 200) loss: 2.313873\n",
      "(Epoch 10 / 20) train acc: 0.276000; val_acc: 0.236111\n",
      "(Iteration 101 / 200) loss: 2.313703\n",
      "(Epoch 11 / 20) train acc: 0.289000; val_acc: 0.233333\n",
      "(Iteration 111 / 200) loss: 2.313566\n",
      "(Epoch 12 / 20) train acc: 0.279000; val_acc: 0.238889\n",
      "(Iteration 121 / 200) loss: 2.313361\n",
      "(Epoch 13 / 20) train acc: 0.307000; val_acc: 0.227778\n",
      "(Iteration 131 / 200) loss: 2.313197\n",
      "(Epoch 14 / 20) train acc: 0.248000; val_acc: 0.205556\n",
      "(Iteration 141 / 200) loss: 2.313028\n",
      "(Epoch 15 / 20) train acc: 0.236000; val_acc: 0.191667\n",
      "(Iteration 151 / 200) loss: 2.312773\n",
      "(Epoch 16 / 20) train acc: 0.224000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 2.312647\n",
      "(Epoch 17 / 20) train acc: 0.207000; val_acc: 0.155556\n",
      "(Iteration 171 / 200) loss: 2.312414\n",
      "(Epoch 18 / 20) train acc: 0.223000; val_acc: 0.152778\n",
      "(Iteration 181 / 200) loss: 2.312302\n",
      "(Epoch 19 / 20) train acc: 0.174000; val_acc: 0.144444\n",
      "(Iteration 191 / 200) loss: 2.312193\n",
      "(Epoch 20 / 20) train acc: 0.226000; val_acc: 0.161111\n",
      "(Iteration 1 / 200) loss: 2.302718\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302702\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302687\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302666\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302651\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302640\n",
      "(Epoch 6 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302649\n",
      "(Epoch 7 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302648\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302613\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302627\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302620\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302585\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302519\n",
      "(Epoch 13 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302574\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302645\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302564\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302620\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302552\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302527\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302590\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302584\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302595\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302583\n",
      "(Epoch 6 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302569\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302564\n",
      "(Epoch 8 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302572\n",
      "(Epoch 9 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302592\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302572\n",
      "(Epoch 11 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302584\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302534\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302584\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302600\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302573\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302546\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302573\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302578\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302590\n",
      "(Epoch 20 / 20) train acc: 0.083000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302575\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302581\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302573\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302571\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302588\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302578\n",
      "(Epoch 8 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302580\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302565\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302578\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302598\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302583\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302617\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302562\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302609\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302597\n",
      "(Epoch 17 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302559\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302584\n",
      "(Epoch 19 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302584\n",
      "(Epoch 20 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 35239.757751\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.147000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 35591.662314\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 35068.085225\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 33028.538320\n",
      "(Epoch 4 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 33628.658977\n",
      "(Epoch 5 / 20) train acc: 0.129000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 36633.562117\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 36848.780269\n",
      "(Epoch 7 / 20) train acc: 0.131000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 34154.450918\n",
      "(Epoch 8 / 20) train acc: 0.128000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 35908.171541\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 33313.519673\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 35657.135300\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 39638.063462\n",
      "(Epoch 12 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 37711.494106\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 33531.154780\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 32678.565444\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.086111\n",
      "(Iteration 151 / 200) loss: 36629.608623\n",
      "(Epoch 16 / 20) train acc: 0.139000; val_acc: 0.086111\n",
      "(Iteration 161 / 200) loss: 35165.399268\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Iteration 171 / 200) loss: 35140.637427\n",
      "(Epoch 18 / 20) train acc: 0.132000; val_acc: 0.086111\n",
      "(Iteration 181 / 200) loss: 32382.583081\n",
      "(Epoch 19 / 20) train acc: 0.137000; val_acc: 0.086111\n",
      "(Iteration 191 / 200) loss: 28181.851255\n",
      "(Epoch 20 / 20) train acc: 0.119000; val_acc: 0.086111\n",
      "(Iteration 1 / 200) loss: 5.128794\n",
      "(Epoch 0 / 20) train acc: 0.165000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.185000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 5.189966\n",
      "(Epoch 2 / 20) train acc: 0.189000; val_acc: 0.144444\n",
      "(Iteration 21 / 200) loss: 5.073461\n",
      "(Epoch 3 / 20) train acc: 0.202000; val_acc: 0.158333\n",
      "(Iteration 31 / 200) loss: 4.798579\n",
      "(Epoch 4 / 20) train acc: 0.208000; val_acc: 0.169444\n",
      "(Iteration 41 / 200) loss: 4.599820\n",
      "(Epoch 5 / 20) train acc: 0.187000; val_acc: 0.169444\n",
      "(Iteration 51 / 200) loss: 4.638137\n",
      "(Epoch 6 / 20) train acc: 0.226000; val_acc: 0.175000\n",
      "(Iteration 61 / 200) loss: 4.348940\n",
      "(Epoch 7 / 20) train acc: 0.204000; val_acc: 0.183333\n",
      "(Iteration 71 / 200) loss: 4.679374\n",
      "(Epoch 8 / 20) train acc: 0.209000; val_acc: 0.183333\n",
      "(Iteration 81 / 200) loss: 4.510585\n",
      "(Epoch 9 / 20) train acc: 0.188000; val_acc: 0.197222\n",
      "(Iteration 91 / 200) loss: 4.281663\n",
      "(Epoch 10 / 20) train acc: 0.203000; val_acc: 0.202778\n",
      "(Iteration 101 / 200) loss: 4.426056\n",
      "(Epoch 11 / 20) train acc: 0.222000; val_acc: 0.208333\n",
      "(Iteration 111 / 200) loss: 4.167035\n",
      "(Epoch 12 / 20) train acc: 0.248000; val_acc: 0.222222\n",
      "(Iteration 121 / 200) loss: 4.538908\n",
      "(Epoch 13 / 20) train acc: 0.229000; val_acc: 0.225000\n",
      "(Iteration 131 / 200) loss: 4.273459\n",
      "(Epoch 14 / 20) train acc: 0.234000; val_acc: 0.227778\n",
      "(Iteration 141 / 200) loss: 4.389383\n",
      "(Epoch 15 / 20) train acc: 0.242000; val_acc: 0.233333\n",
      "(Iteration 151 / 200) loss: 4.233872\n",
      "(Epoch 16 / 20) train acc: 0.252000; val_acc: 0.238889\n",
      "(Iteration 161 / 200) loss: 4.209221\n",
      "(Epoch 17 / 20) train acc: 0.276000; val_acc: 0.238889\n",
      "(Iteration 171 / 200) loss: 4.291008\n",
      "(Epoch 18 / 20) train acc: 0.253000; val_acc: 0.233333\n",
      "(Iteration 181 / 200) loss: 4.190140\n",
      "(Epoch 19 / 20) train acc: 0.271000; val_acc: 0.238889\n",
      "(Iteration 191 / 200) loss: 4.011841\n",
      "(Epoch 20 / 20) train acc: 0.265000; val_acc: 0.241667\n",
      "(Iteration 1 / 200) loss: 2.315817\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.133333\n",
      "(Iteration 11 / 200) loss: 2.315607\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.315370\n",
      "(Epoch 3 / 20) train acc: 0.129000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 2.315187\n",
      "(Epoch 4 / 20) train acc: 0.131000; val_acc: 0.136111\n",
      "(Iteration 41 / 200) loss: 2.314986\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.105556\n",
      "(Iteration 51 / 200) loss: 2.314767\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.314566\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.077778\n",
      "(Iteration 71 / 200) loss: 2.314356\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.086111\n",
      "(Iteration 81 / 200) loss: 2.314190\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.108333\n",
      "(Iteration 91 / 200) loss: 2.314017\n",
      "(Epoch 10 / 20) train acc: 0.115000; val_acc: 0.105556\n",
      "(Iteration 101 / 200) loss: 2.313806\n",
      "(Epoch 11 / 20) train acc: 0.144000; val_acc: 0.113889\n",
      "(Iteration 111 / 200) loss: 2.313695\n",
      "(Epoch 12 / 20) train acc: 0.138000; val_acc: 0.127778\n",
      "(Iteration 121 / 200) loss: 2.313430\n",
      "(Epoch 13 / 20) train acc: 0.128000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 2.313269\n",
      "(Epoch 14 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.313120\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.312851\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.312779\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.312538\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.312352\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.312177\n",
      "(Epoch 20 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302717\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.130000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302696\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302676\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302664\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302658\n",
      "(Epoch 5 / 20) train acc: 0.085000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302638\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302625\n",
      "(Epoch 7 / 20) train acc: 0.083000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302617\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302604\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302648\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302620\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302592\n",
      "(Epoch 12 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302571\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302601\n",
      "(Epoch 14 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302602\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302564\n",
      "(Epoch 16 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302598\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302566\n",
      "(Epoch 18 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302560\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302620\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302586\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302590\n",
      "(Epoch 4 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302580\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302587\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302584\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302566\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302569\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302593\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302550\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302545\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302572\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302575\n",
      "(Epoch 14 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302586\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302536\n",
      "(Epoch 16 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302617\n",
      "(Epoch 17 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302629\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302558\n",
      "(Epoch 19 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302570\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302589\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302588\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302581\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302578\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302596\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302587\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302576\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302576\n",
      "(Epoch 9 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302576\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302572\n",
      "(Epoch 11 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302604\n",
      "(Epoch 12 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302616\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302594\n",
      "(Epoch 14 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302591\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302594\n",
      "(Epoch 16 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302594\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302578\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302536\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302613\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 30194.610993\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 30620.840573\n",
      "(Epoch 2 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 29404.102731\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 30103.559911\n",
      "(Epoch 4 / 20) train acc: 0.110000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 27027.802094\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 30117.141778\n",
      "(Epoch 6 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 30585.053961\n",
      "(Epoch 7 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 32701.841148\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 28685.728333\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 31344.833018\n",
      "(Epoch 10 / 20) train acc: 0.086000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 30403.187705\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 29085.772393\n",
      "(Epoch 12 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 26419.007081\n",
      "(Epoch 13 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 28320.734271\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 141 / 200) loss: 27787.268960\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.091667\n",
      "(Iteration 151 / 200) loss: 29137.388646\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.088889\n",
      "(Iteration 161 / 200) loss: 30279.883333\n",
      "(Epoch 17 / 20) train acc: 0.079000; val_acc: 0.088889\n",
      "(Iteration 171 / 200) loss: 29056.608018\n",
      "(Epoch 18 / 20) train acc: 0.119000; val_acc: 0.088889\n",
      "(Iteration 181 / 200) loss: 28142.885207\n",
      "(Epoch 19 / 20) train acc: 0.128000; val_acc: 0.088889\n",
      "(Iteration 191 / 200) loss: 31411.902399\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.088889\n",
      "(Iteration 1 / 200) loss: 3.966865\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.077000; val_acc: 0.077778\n",
      "(Iteration 11 / 200) loss: 3.751403\n",
      "(Epoch 2 / 20) train acc: 0.090000; val_acc: 0.077778\n",
      "(Iteration 21 / 200) loss: 3.395103\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 3.647149\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 3.726774\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 3.311824\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 3.493458\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 3.317372\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.088889\n",
      "(Iteration 81 / 200) loss: 3.184592\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 3.111893\n",
      "(Epoch 10 / 20) train acc: 0.130000; val_acc: 0.105556\n",
      "(Iteration 101 / 200) loss: 3.059955\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.105556\n",
      "(Iteration 111 / 200) loss: 3.114816\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.108333\n",
      "(Iteration 121 / 200) loss: 2.859385\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 3.142957\n",
      "(Epoch 14 / 20) train acc: 0.123000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.819797\n",
      "(Epoch 15 / 20) train acc: 0.126000; val_acc: 0.122222\n",
      "(Iteration 151 / 200) loss: 2.670298\n",
      "(Epoch 16 / 20) train acc: 0.159000; val_acc: 0.133333\n",
      "(Iteration 161 / 200) loss: 2.907924\n",
      "(Epoch 17 / 20) train acc: 0.128000; val_acc: 0.141667\n",
      "(Iteration 171 / 200) loss: 2.704688\n",
      "(Epoch 18 / 20) train acc: 0.162000; val_acc: 0.163889\n",
      "(Iteration 181 / 200) loss: 2.790258\n",
      "(Epoch 19 / 20) train acc: 0.149000; val_acc: 0.175000\n",
      "(Iteration 191 / 200) loss: 2.767673\n",
      "(Epoch 20 / 20) train acc: 0.169000; val_acc: 0.175000\n",
      "(Iteration 1 / 200) loss: 2.303938\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.303875\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.303825\n",
      "(Epoch 3 / 20) train acc: 0.124000; val_acc: 0.094444\n",
      "(Iteration 31 / 200) loss: 2.303754\n",
      "(Epoch 4 / 20) train acc: 0.127000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.303728\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.303708\n",
      "(Epoch 6 / 20) train acc: 0.145000; val_acc: 0.113889\n",
      "(Iteration 61 / 200) loss: 2.303620\n",
      "(Epoch 7 / 20) train acc: 0.159000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 2.303606\n",
      "(Epoch 8 / 20) train acc: 0.171000; val_acc: 0.127778\n",
      "(Iteration 81 / 200) loss: 2.303555\n",
      "(Epoch 9 / 20) train acc: 0.183000; val_acc: 0.147222\n",
      "(Iteration 91 / 200) loss: 2.303456\n",
      "(Epoch 10 / 20) train acc: 0.213000; val_acc: 0.183333\n",
      "(Iteration 101 / 200) loss: 2.303366\n",
      "(Epoch 11 / 20) train acc: 0.268000; val_acc: 0.227778\n",
      "(Iteration 111 / 200) loss: 2.303252\n",
      "(Epoch 12 / 20) train acc: 0.268000; val_acc: 0.255556\n",
      "(Iteration 121 / 200) loss: 2.303361\n",
      "(Epoch 13 / 20) train acc: 0.300000; val_acc: 0.269444\n",
      "(Iteration 131 / 200) loss: 2.303053\n",
      "(Epoch 14 / 20) train acc: 0.258000; val_acc: 0.244444\n",
      "(Iteration 141 / 200) loss: 2.303020\n",
      "(Epoch 15 / 20) train acc: 0.253000; val_acc: 0.241667\n",
      "(Iteration 151 / 200) loss: 2.302928\n",
      "(Epoch 16 / 20) train acc: 0.267000; val_acc: 0.238889\n",
      "(Iteration 161 / 200) loss: 2.302908\n",
      "(Epoch 17 / 20) train acc: 0.288000; val_acc: 0.277778\n",
      "(Iteration 171 / 200) loss: 2.302710\n",
      "(Epoch 18 / 20) train acc: 0.306000; val_acc: 0.300000\n",
      "(Iteration 181 / 200) loss: 2.302663\n",
      "(Epoch 19 / 20) train acc: 0.284000; val_acc: 0.266667\n",
      "(Iteration 191 / 200) loss: 2.302359\n",
      "(Epoch 20 / 20) train acc: 0.321000; val_acc: 0.269444\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302593\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302600\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302595\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302595\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302586\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302589\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302576\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302557\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302589\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302568\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302584\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302562\n",
      "(Epoch 13 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302543\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302561\n",
      "(Epoch 15 / 20) train acc: 0.130000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302572\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302559\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302585\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302569\n",
      "(Epoch 19 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302496\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302582\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302587\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302574\n",
      "(Epoch 6 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302569\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302578\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302590\n",
      "(Epoch 9 / 20) train acc: 0.080000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302577\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302579\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302550\n",
      "(Epoch 12 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302605\n",
      "(Epoch 13 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302567\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302563\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302622\n",
      "(Epoch 16 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302543\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302588\n",
      "(Epoch 18 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302579\n",
      "(Epoch 19 / 20) train acc: 0.132000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302525\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302593\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302599\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302599\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302578\n",
      "(Epoch 7 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302601\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302544\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302610\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302640\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302546\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302574\n",
      "(Epoch 13 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302598\n",
      "(Epoch 14 / 20) train acc: 0.131000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302565\n",
      "(Epoch 15 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302543\n",
      "(Epoch 16 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302533\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302543\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302564\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302578\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 39877.811276\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 43655.138481\n",
      "(Epoch 2 / 20) train acc: 0.086000; val_acc: 0.094444\n",
      "(Iteration 21 / 200) loss: 36120.165752\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 41888.680533\n",
      "(Epoch 4 / 20) train acc: 0.088000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 37258.837826\n",
      "(Epoch 5 / 20) train acc: 0.070000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 37957.005120\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 37440.177409\n",
      "(Epoch 7 / 20) train acc: 0.083000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 41337.089699\n",
      "(Epoch 8 / 20) train acc: 0.093000; val_acc: 0.094444\n",
      "(Iteration 81 / 200) loss: 35927.291990\n",
      "(Epoch 9 / 20) train acc: 0.098000; val_acc: 0.094444\n",
      "(Iteration 91 / 200) loss: 40424.201780\n",
      "(Epoch 10 / 20) train acc: 0.092000; val_acc: 0.094444\n",
      "(Iteration 101 / 200) loss: 41420.981570\n",
      "(Epoch 11 / 20) train acc: 0.077000; val_acc: 0.094444\n",
      "(Iteration 111 / 200) loss: 39321.558860\n",
      "(Epoch 12 / 20) train acc: 0.081000; val_acc: 0.094444\n",
      "(Iteration 121 / 200) loss: 39354.896149\n",
      "(Epoch 13 / 20) train acc: 0.077000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 38241.953439\n",
      "(Epoch 14 / 20) train acc: 0.096000; val_acc: 0.094444\n",
      "(Iteration 141 / 200) loss: 40410.625733\n",
      "(Epoch 15 / 20) train acc: 0.090000; val_acc: 0.094444\n",
      "(Iteration 151 / 200) loss: 37712.365528\n",
      "(Epoch 16 / 20) train acc: 0.073000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 42101.995322\n",
      "(Epoch 17 / 20) train acc: 0.090000; val_acc: 0.094444\n",
      "(Iteration 171 / 200) loss: 36995.075113\n",
      "(Epoch 18 / 20) train acc: 0.075000; val_acc: 0.094444\n",
      "(Iteration 181 / 200) loss: 37228.737410\n",
      "(Epoch 19 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 35280.727207\n",
      "(Epoch 20 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 3.262339\n",
      "(Epoch 0 / 20) train acc: 0.152000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.147000; val_acc: 0.136111\n",
      "(Iteration 11 / 200) loss: 3.384629\n",
      "(Epoch 2 / 20) train acc: 0.144000; val_acc: 0.136111\n",
      "(Iteration 21 / 200) loss: 3.173108\n",
      "(Epoch 3 / 20) train acc: 0.169000; val_acc: 0.138889\n",
      "(Iteration 31 / 200) loss: 3.101171\n",
      "(Epoch 4 / 20) train acc: 0.176000; val_acc: 0.141667\n",
      "(Iteration 41 / 200) loss: 3.129763\n",
      "(Epoch 5 / 20) train acc: 0.165000; val_acc: 0.141667\n",
      "(Iteration 51 / 200) loss: 3.234170\n",
      "(Epoch 6 / 20) train acc: 0.161000; val_acc: 0.144444\n",
      "(Iteration 61 / 200) loss: 3.002554\n",
      "(Epoch 7 / 20) train acc: 0.162000; val_acc: 0.152778\n",
      "(Iteration 71 / 200) loss: 2.940386\n",
      "(Epoch 8 / 20) train acc: 0.162000; val_acc: 0.161111\n",
      "(Iteration 81 / 200) loss: 2.997976\n",
      "(Epoch 9 / 20) train acc: 0.157000; val_acc: 0.163889\n",
      "(Iteration 91 / 200) loss: 2.607098\n",
      "(Epoch 10 / 20) train acc: 0.158000; val_acc: 0.163889\n",
      "(Iteration 101 / 200) loss: 3.063691\n",
      "(Epoch 11 / 20) train acc: 0.183000; val_acc: 0.166667\n",
      "(Iteration 111 / 200) loss: 2.668658\n",
      "(Epoch 12 / 20) train acc: 0.168000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.624972\n",
      "(Epoch 13 / 20) train acc: 0.171000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 2.699674\n",
      "(Epoch 14 / 20) train acc: 0.193000; val_acc: 0.188889\n",
      "(Iteration 141 / 200) loss: 2.776315\n",
      "(Epoch 15 / 20) train acc: 0.196000; val_acc: 0.191667\n",
      "(Iteration 151 / 200) loss: 2.640864\n",
      "(Epoch 16 / 20) train acc: 0.208000; val_acc: 0.205556\n",
      "(Iteration 161 / 200) loss: 2.664136\n",
      "(Epoch 17 / 20) train acc: 0.185000; val_acc: 0.211111\n",
      "(Iteration 171 / 200) loss: 2.674856\n",
      "(Epoch 18 / 20) train acc: 0.235000; val_acc: 0.216667\n",
      "(Iteration 181 / 200) loss: 2.763295\n",
      "(Epoch 19 / 20) train acc: 0.216000; val_acc: 0.230556\n",
      "(Iteration 191 / 200) loss: 2.385257\n",
      "(Epoch 20 / 20) train acc: 0.210000; val_acc: 0.233333\n",
      "(Iteration 1 / 200) loss: 2.303897\n",
      "(Epoch 0 / 20) train acc: 0.158000; val_acc: 0.227778\n",
      "(Epoch 1 / 20) train acc: 0.153000; val_acc: 0.208333\n",
      "(Iteration 11 / 200) loss: 2.303861\n",
      "(Epoch 2 / 20) train acc: 0.209000; val_acc: 0.244444\n",
      "(Iteration 21 / 200) loss: 2.303805\n",
      "(Epoch 3 / 20) train acc: 0.274000; val_acc: 0.291667\n",
      "(Iteration 31 / 200) loss: 2.303755\n",
      "(Epoch 4 / 20) train acc: 0.260000; val_acc: 0.283333\n",
      "(Iteration 41 / 200) loss: 2.303719\n",
      "(Epoch 5 / 20) train acc: 0.288000; val_acc: 0.325000\n",
      "(Iteration 51 / 200) loss: 2.303666\n",
      "(Epoch 6 / 20) train acc: 0.318000; val_acc: 0.377778\n",
      "(Iteration 61 / 200) loss: 2.303608\n",
      "(Epoch 7 / 20) train acc: 0.342000; val_acc: 0.372222\n",
      "(Iteration 71 / 200) loss: 2.303591\n",
      "(Epoch 8 / 20) train acc: 0.374000; val_acc: 0.411111\n",
      "(Iteration 81 / 200) loss: 2.303526\n",
      "(Epoch 9 / 20) train acc: 0.422000; val_acc: 0.427778\n",
      "(Iteration 91 / 200) loss: 2.303461\n",
      "(Epoch 10 / 20) train acc: 0.453000; val_acc: 0.455556\n",
      "(Iteration 101 / 200) loss: 2.303362\n",
      "(Epoch 11 / 20) train acc: 0.472000; val_acc: 0.480556\n",
      "(Iteration 111 / 200) loss: 2.303316\n",
      "(Epoch 12 / 20) train acc: 0.454000; val_acc: 0.491667\n",
      "(Iteration 121 / 200) loss: 2.303183\n",
      "(Epoch 13 / 20) train acc: 0.494000; val_acc: 0.502778\n",
      "(Iteration 131 / 200) loss: 2.303129\n",
      "(Epoch 14 / 20) train acc: 0.468000; val_acc: 0.500000\n",
      "(Iteration 141 / 200) loss: 2.303016\n",
      "(Epoch 15 / 20) train acc: 0.511000; val_acc: 0.500000\n",
      "(Iteration 151 / 200) loss: 2.302993\n",
      "(Epoch 16 / 20) train acc: 0.561000; val_acc: 0.538889\n",
      "(Iteration 161 / 200) loss: 2.302784\n",
      "(Epoch 17 / 20) train acc: 0.506000; val_acc: 0.533333\n",
      "(Iteration 171 / 200) loss: 2.302677\n",
      "(Epoch 18 / 20) train acc: 0.563000; val_acc: 0.513889\n",
      "(Iteration 181 / 200) loss: 2.302555\n",
      "(Epoch 19 / 20) train acc: 0.600000; val_acc: 0.558333\n",
      "(Iteration 191 / 200) loss: 2.302434\n",
      "(Epoch 20 / 20) train acc: 0.638000; val_acc: 0.594444\n",
      "(Iteration 1 / 200) loss: 2.302598\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.066667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302600\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302593\n",
      "(Epoch 3 / 20) train acc: 0.141000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302577\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302583\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302575\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302569\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302593\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302599\n",
      "(Epoch 9 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302584\n",
      "(Epoch 10 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302593\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302572\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302596\n",
      "(Epoch 13 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302538\n",
      "(Epoch 14 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302574\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302550\n",
      "(Epoch 16 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302578\n",
      "(Epoch 17 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302609\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302543\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302548\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302584\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302600\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302596\n",
      "(Epoch 6 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302562\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302596\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302589\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302578\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302556\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302596\n",
      "(Epoch 12 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302542\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302617\n",
      "(Epoch 14 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302593\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302557\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302583\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302576\n",
      "(Epoch 18 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302509\n",
      "(Epoch 19 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302618\n",
      "(Epoch 20 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302590\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302584\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302572\n",
      "(Epoch 4 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302605\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302598\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.302595\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.302561\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.302593\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.302595\n",
      "(Epoch 10 / 20) train acc: 0.083000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 2.302582\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302561\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302545\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302611\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302588\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302543\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302583\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302633\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302549\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302553\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 48338.133976\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.076000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 37795.443641\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 42935.538357\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 39028.470585\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 39787.915315\n",
      "(Epoch 5 / 20) train acc: 0.079000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 45993.107543\n",
      "(Epoch 6 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 43109.327272\n",
      "(Epoch 7 / 20) train acc: 0.067000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 39294.389502\n",
      "(Epoch 8 / 20) train acc: 0.079000; val_acc: 0.113889\n",
      "(Iteration 81 / 200) loss: 42434.561732\n",
      "(Epoch 9 / 20) train acc: 0.088000; val_acc: 0.113889\n",
      "(Iteration 91 / 200) loss: 40783.293958\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.113889\n",
      "(Iteration 101 / 200) loss: 35872.348685\n",
      "(Epoch 11 / 20) train acc: 0.084000; val_acc: 0.113889\n",
      "(Iteration 111 / 200) loss: 36633.468412\n",
      "(Epoch 12 / 20) train acc: 0.087000; val_acc: 0.113889\n",
      "(Iteration 121 / 200) loss: 38591.210640\n",
      "(Epoch 13 / 20) train acc: 0.087000; val_acc: 0.113889\n",
      "(Iteration 131 / 200) loss: 37537.717868\n",
      "(Epoch 14 / 20) train acc: 0.089000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 41127.417598\n",
      "(Epoch 15 / 20) train acc: 0.081000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 39032.332329\n",
      "(Epoch 16 / 20) train acc: 0.074000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 42611.684559\n",
      "(Epoch 17 / 20) train acc: 0.084000; val_acc: 0.119444\n",
      "(Iteration 171 / 200) loss: 40958.271787\n",
      "(Epoch 18 / 20) train acc: 0.082000; val_acc: 0.119444\n",
      "(Iteration 181 / 200) loss: 34904.866518\n",
      "(Epoch 19 / 20) train acc: 0.082000; val_acc: 0.119444\n",
      "(Iteration 191 / 200) loss: 40490.193747\n",
      "(Epoch 20 / 20) train acc: 0.086000; val_acc: 0.119444\n",
      "(Iteration 1 / 200) loss: 3.894403\n",
      "(Epoch 0 / 20) train acc: 0.037000; val_acc: 0.038889\n",
      "(Epoch 1 / 20) train acc: 0.020000; val_acc: 0.038889\n",
      "(Iteration 11 / 200) loss: 3.748091\n",
      "(Epoch 2 / 20) train acc: 0.037000; val_acc: 0.038889\n",
      "(Iteration 21 / 200) loss: 3.881474\n",
      "(Epoch 3 / 20) train acc: 0.024000; val_acc: 0.038889\n",
      "(Iteration 31 / 200) loss: 3.455087\n",
      "(Epoch 4 / 20) train acc: 0.024000; val_acc: 0.038889\n",
      "(Iteration 41 / 200) loss: 3.550177\n",
      "(Epoch 5 / 20) train acc: 0.034000; val_acc: 0.038889\n",
      "(Iteration 51 / 200) loss: 3.463751\n",
      "(Epoch 6 / 20) train acc: 0.029000; val_acc: 0.044444\n",
      "(Iteration 61 / 200) loss: 3.770934\n",
      "(Epoch 7 / 20) train acc: 0.024000; val_acc: 0.044444\n",
      "(Iteration 71 / 200) loss: 3.770873\n",
      "(Epoch 8 / 20) train acc: 0.022000; val_acc: 0.047222\n",
      "(Iteration 81 / 200) loss: 3.338206\n",
      "(Epoch 9 / 20) train acc: 0.035000; val_acc: 0.047222\n",
      "(Iteration 91 / 200) loss: 3.281339\n",
      "(Epoch 10 / 20) train acc: 0.042000; val_acc: 0.047222\n",
      "(Iteration 101 / 200) loss: 3.199735\n",
      "(Epoch 11 / 20) train acc: 0.039000; val_acc: 0.047222\n",
      "(Iteration 111 / 200) loss: 3.342615\n",
      "(Epoch 12 / 20) train acc: 0.041000; val_acc: 0.047222\n",
      "(Iteration 121 / 200) loss: 2.999474\n",
      "(Epoch 13 / 20) train acc: 0.044000; val_acc: 0.047222\n",
      "(Iteration 131 / 200) loss: 3.401889\n",
      "(Epoch 14 / 20) train acc: 0.053000; val_acc: 0.044444\n",
      "(Iteration 141 / 200) loss: 2.999293\n",
      "(Epoch 15 / 20) train acc: 0.053000; val_acc: 0.044444\n",
      "(Iteration 151 / 200) loss: 3.036605\n",
      "(Epoch 16 / 20) train acc: 0.060000; val_acc: 0.044444\n",
      "(Iteration 161 / 200) loss: 2.858885\n",
      "(Epoch 17 / 20) train acc: 0.054000; val_acc: 0.047222\n",
      "(Iteration 171 / 200) loss: 3.016012\n",
      "(Epoch 18 / 20) train acc: 0.084000; val_acc: 0.052778\n",
      "(Iteration 181 / 200) loss: 2.893398\n",
      "(Epoch 19 / 20) train acc: 0.058000; val_acc: 0.058333\n",
      "(Iteration 191 / 200) loss: 2.954128\n",
      "(Epoch 20 / 20) train acc: 0.069000; val_acc: 0.063889\n",
      "(Iteration 1 / 200) loss: 2.303885\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.139000; val_acc: 0.172222\n",
      "(Iteration 11 / 200) loss: 2.303826\n",
      "(Epoch 2 / 20) train acc: 0.214000; val_acc: 0.205556\n",
      "(Iteration 21 / 200) loss: 2.303785\n",
      "(Epoch 3 / 20) train acc: 0.253000; val_acc: 0.263889\n",
      "(Iteration 31 / 200) loss: 2.303736\n",
      "(Epoch 4 / 20) train acc: 0.327000; val_acc: 0.333333\n",
      "(Iteration 41 / 200) loss: 2.303682\n",
      "(Epoch 5 / 20) train acc: 0.396000; val_acc: 0.372222\n",
      "(Iteration 51 / 200) loss: 2.303640\n",
      "(Epoch 6 / 20) train acc: 0.420000; val_acc: 0.350000\n",
      "(Iteration 61 / 200) loss: 2.303598\n",
      "(Epoch 7 / 20) train acc: 0.386000; val_acc: 0.325000\n",
      "(Iteration 71 / 200) loss: 2.303445\n",
      "(Epoch 8 / 20) train acc: 0.363000; val_acc: 0.336111\n",
      "(Iteration 81 / 200) loss: 2.303481\n",
      "(Epoch 9 / 20) train acc: 0.334000; val_acc: 0.291667\n",
      "(Iteration 91 / 200) loss: 2.303428\n",
      "(Epoch 10 / 20) train acc: 0.312000; val_acc: 0.280556\n",
      "(Iteration 101 / 200) loss: 2.303281\n",
      "(Epoch 11 / 20) train acc: 0.324000; val_acc: 0.255556\n",
      "(Iteration 111 / 200) loss: 2.303197\n",
      "(Epoch 12 / 20) train acc: 0.367000; val_acc: 0.300000\n",
      "(Iteration 121 / 200) loss: 2.303035\n",
      "(Epoch 13 / 20) train acc: 0.355000; val_acc: 0.336111\n",
      "(Iteration 131 / 200) loss: 2.302986\n",
      "(Epoch 14 / 20) train acc: 0.416000; val_acc: 0.377778\n",
      "(Iteration 141 / 200) loss: 2.302899\n",
      "(Epoch 15 / 20) train acc: 0.435000; val_acc: 0.405556\n",
      "(Iteration 151 / 200) loss: 2.302772\n",
      "(Epoch 16 / 20) train acc: 0.440000; val_acc: 0.375000\n",
      "(Iteration 161 / 200) loss: 2.302468\n",
      "(Epoch 17 / 20) train acc: 0.423000; val_acc: 0.375000\n",
      "(Iteration 171 / 200) loss: 2.302352\n",
      "(Epoch 18 / 20) train acc: 0.418000; val_acc: 0.380556\n",
      "(Iteration 181 / 200) loss: 2.302231\n",
      "(Epoch 19 / 20) train acc: 0.452000; val_acc: 0.408333\n",
      "(Iteration 191 / 200) loss: 2.302012\n",
      "(Epoch 20 / 20) train acc: 0.486000; val_acc: 0.419444\n",
      "(Iteration 1 / 200) loss: 2.302598\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302598\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302590\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302597\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302577\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302588\n",
      "(Epoch 6 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302590\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302602\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302576\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302566\n",
      "(Epoch 10 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302589\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302559\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302578\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302556\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302550\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302551\n",
      "(Epoch 16 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302601\n",
      "(Epoch 17 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302594\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302586\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302582\n",
      "(Epoch 20 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302590\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302574\n",
      "(Epoch 4 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302572\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302578\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302584\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302559\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302605\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302553\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302566\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302574\n",
      "(Epoch 12 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302618\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302573\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302575\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302579\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302589\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302595\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302557\n",
      "(Epoch 19 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302598\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302574\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302594\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302578\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302578\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302591\n",
      "(Epoch 7 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302566\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302553\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302566\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302571\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302601\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302557\n",
      "(Epoch 13 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302584\n",
      "(Epoch 14 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302594\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302588\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302610\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302575\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302579\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302552\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 23811.568443\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.071000; val_acc: 0.075000\n",
      "(Iteration 11 / 200) loss: 23474.840899\n",
      "(Epoch 2 / 20) train acc: 0.081000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 24817.880862\n",
      "(Epoch 3 / 20) train acc: 0.086000; val_acc: 0.075000\n",
      "(Iteration 31 / 200) loss: 20660.539575\n",
      "(Epoch 4 / 20) train acc: 0.077000; val_acc: 0.075000\n",
      "(Iteration 41 / 200) loss: 24914.808288\n",
      "(Epoch 5 / 20) train acc: 0.078000; val_acc: 0.075000\n",
      "(Iteration 51 / 200) loss: 23027.570752\n",
      "(Epoch 6 / 20) train acc: 0.067000; val_acc: 0.075000\n",
      "(Iteration 61 / 200) loss: 22465.238216\n",
      "(Epoch 7 / 20) train acc: 0.083000; val_acc: 0.075000\n",
      "(Iteration 71 / 200) loss: 24499.623179\n",
      "(Epoch 8 / 20) train acc: 0.078000; val_acc: 0.072222\n",
      "(Iteration 81 / 200) loss: 21117.853143\n",
      "(Epoch 9 / 20) train acc: 0.091000; val_acc: 0.072222\n",
      "(Iteration 91 / 200) loss: 26409.365607\n",
      "(Epoch 10 / 20) train acc: 0.080000; val_acc: 0.072222\n",
      "(Iteration 101 / 200) loss: 22473.605571\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.072222\n",
      "(Iteration 111 / 200) loss: 22508.360534\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.072222\n",
      "(Iteration 121 / 200) loss: 25343.185498\n",
      "(Epoch 13 / 20) train acc: 0.089000; val_acc: 0.072222\n",
      "(Iteration 131 / 200) loss: 24178.857961\n",
      "(Epoch 14 / 20) train acc: 0.069000; val_acc: 0.075000\n",
      "(Iteration 141 / 200) loss: 20937.345425\n",
      "(Epoch 15 / 20) train acc: 0.090000; val_acc: 0.075000\n",
      "(Iteration 151 / 200) loss: 23069.472888\n",
      "(Epoch 16 / 20) train acc: 0.060000; val_acc: 0.072222\n",
      "(Iteration 161 / 200) loss: 21439.722852\n",
      "(Epoch 17 / 20) train acc: 0.085000; val_acc: 0.072222\n",
      "(Iteration 171 / 200) loss: 21248.472816\n",
      "(Epoch 18 / 20) train acc: 0.077000; val_acc: 0.072222\n",
      "(Iteration 181 / 200) loss: 22908.220279\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.072222\n",
      "(Iteration 191 / 200) loss: 22893.200243\n",
      "(Epoch 20 / 20) train acc: 0.088000; val_acc: 0.069444\n",
      "(Iteration 1 / 200) loss: 3.657308\n",
      "(Epoch 0 / 20) train acc: 0.043000; val_acc: 0.030556\n",
      "(Epoch 1 / 20) train acc: 0.039000; val_acc: 0.033333\n",
      "(Iteration 11 / 200) loss: 3.681340\n",
      "(Epoch 2 / 20) train acc: 0.037000; val_acc: 0.041667\n",
      "(Iteration 21 / 200) loss: 3.823338\n",
      "(Epoch 3 / 20) train acc: 0.041000; val_acc: 0.047222\n",
      "(Iteration 31 / 200) loss: 3.724385\n",
      "(Epoch 4 / 20) train acc: 0.052000; val_acc: 0.047222\n",
      "(Iteration 41 / 200) loss: 3.639465\n",
      "(Epoch 5 / 20) train acc: 0.045000; val_acc: 0.055556\n",
      "(Iteration 51 / 200) loss: 3.300401\n",
      "(Epoch 6 / 20) train acc: 0.044000; val_acc: 0.055556\n",
      "(Iteration 61 / 200) loss: 3.469911\n",
      "(Epoch 7 / 20) train acc: 0.045000; val_acc: 0.058333\n",
      "(Iteration 71 / 200) loss: 2.968239\n",
      "(Epoch 8 / 20) train acc: 0.052000; val_acc: 0.061111\n",
      "(Iteration 81 / 200) loss: 3.254292\n",
      "(Epoch 9 / 20) train acc: 0.054000; val_acc: 0.063889\n",
      "(Iteration 91 / 200) loss: 2.921332\n",
      "(Epoch 10 / 20) train acc: 0.073000; val_acc: 0.075000\n",
      "(Iteration 101 / 200) loss: 3.332054\n",
      "(Epoch 11 / 20) train acc: 0.083000; val_acc: 0.077778\n",
      "(Iteration 111 / 200) loss: 3.046360\n",
      "(Epoch 12 / 20) train acc: 0.072000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.967455\n",
      "(Epoch 13 / 20) train acc: 0.079000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 3.023256\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 141 / 200) loss: 2.815256\n",
      "(Epoch 15 / 20) train acc: 0.092000; val_acc: 0.102778\n",
      "(Iteration 151 / 200) loss: 2.861930\n",
      "(Epoch 16 / 20) train acc: 0.119000; val_acc: 0.108333\n",
      "(Iteration 161 / 200) loss: 2.685067\n",
      "(Epoch 17 / 20) train acc: 0.126000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.653141\n",
      "(Epoch 18 / 20) train acc: 0.126000; val_acc: 0.127778\n",
      "(Iteration 181 / 200) loss: 2.561596\n",
      "(Epoch 19 / 20) train acc: 0.137000; val_acc: 0.141667\n",
      "(Iteration 191 / 200) loss: 2.570060\n",
      "(Epoch 20 / 20) train acc: 0.131000; val_acc: 0.155556\n",
      "(Iteration 1 / 200) loss: 2.302723\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 2.302662\n",
      "(Epoch 2 / 20) train acc: 0.164000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 2.302648\n",
      "(Epoch 3 / 20) train acc: 0.154000; val_acc: 0.172222\n",
      "(Iteration 31 / 200) loss: 2.302578\n",
      "(Epoch 4 / 20) train acc: 0.206000; val_acc: 0.172222\n",
      "(Iteration 41 / 200) loss: 2.302587\n",
      "(Epoch 5 / 20) train acc: 0.212000; val_acc: 0.202778\n",
      "(Iteration 51 / 200) loss: 2.302511\n",
      "(Epoch 6 / 20) train acc: 0.292000; val_acc: 0.261111\n",
      "(Iteration 61 / 200) loss: 2.302480\n",
      "(Epoch 7 / 20) train acc: 0.341000; val_acc: 0.305556\n",
      "(Iteration 71 / 200) loss: 2.302405\n",
      "(Epoch 8 / 20) train acc: 0.394000; val_acc: 0.363889\n",
      "(Iteration 81 / 200) loss: 2.302351\n",
      "(Epoch 9 / 20) train acc: 0.370000; val_acc: 0.380556\n",
      "(Iteration 91 / 200) loss: 2.302317\n",
      "(Epoch 10 / 20) train acc: 0.438000; val_acc: 0.402778\n",
      "(Iteration 101 / 200) loss: 2.302201\n",
      "(Epoch 11 / 20) train acc: 0.457000; val_acc: 0.427778\n",
      "(Iteration 111 / 200) loss: 2.302117\n",
      "(Epoch 12 / 20) train acc: 0.472000; val_acc: 0.461111\n",
      "(Iteration 121 / 200) loss: 2.302103\n",
      "(Epoch 13 / 20) train acc: 0.531000; val_acc: 0.494444\n",
      "(Iteration 131 / 200) loss: 2.301954\n",
      "(Epoch 14 / 20) train acc: 0.519000; val_acc: 0.513889\n",
      "(Iteration 141 / 200) loss: 2.301850\n",
      "(Epoch 15 / 20) train acc: 0.560000; val_acc: 0.538889\n",
      "(Iteration 151 / 200) loss: 2.301622\n",
      "(Epoch 16 / 20) train acc: 0.569000; val_acc: 0.555556\n",
      "(Iteration 161 / 200) loss: 2.301615\n",
      "(Epoch 17 / 20) train acc: 0.598000; val_acc: 0.572222\n",
      "(Iteration 171 / 200) loss: 2.301508\n",
      "(Epoch 18 / 20) train acc: 0.636000; val_acc: 0.586111\n",
      "(Iteration 181 / 200) loss: 2.301130\n",
      "(Epoch 19 / 20) train acc: 0.632000; val_acc: 0.627778\n",
      "(Iteration 191 / 200) loss: 2.301128\n",
      "(Epoch 20 / 20) train acc: 0.656000; val_acc: 0.650000\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.081000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302582\n",
      "(Epoch 3 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302605\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302602\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302589\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302576\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302568\n",
      "(Epoch 8 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302608\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302597\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302578\n",
      "(Epoch 11 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302622\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302556\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302603\n",
      "(Epoch 14 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302537\n",
      "(Epoch 15 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302516\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302563\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302540\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302523\n",
      "(Epoch 19 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302512\n",
      "(Epoch 20 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302592\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302576\n",
      "(Epoch 5 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302601\n",
      "(Epoch 6 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.302595\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.302610\n",
      "(Epoch 8 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.302562\n",
      "(Epoch 9 / 20) train acc: 0.081000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.302596\n",
      "(Epoch 10 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 2.302597\n",
      "(Epoch 11 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 111 / 200) loss: 2.302579\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302581\n",
      "(Epoch 13 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302610\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302560\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302573\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302589\n",
      "(Epoch 17 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302550\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302549\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302615\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.082000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.090000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 2.302572\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.127778\n",
      "(Iteration 41 / 200) loss: 2.302572\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302596\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302566\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302590\n",
      "(Epoch 9 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302591\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302582\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302571\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302565\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302559\n",
      "(Epoch 14 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302561\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302583\n",
      "(Epoch 16 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302548\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302566\n",
      "(Epoch 18 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302595\n",
      "(Epoch 19 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302542\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 58377.266208\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.066667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.066667\n",
      "(Iteration 11 / 200) loss: 44905.416159\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.066667\n",
      "(Iteration 21 / 200) loss: 55948.171119\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.069444\n",
      "(Iteration 31 / 200) loss: 50619.886080\n",
      "(Epoch 4 / 20) train acc: 0.091000; val_acc: 0.069444\n",
      "(Iteration 41 / 200) loss: 43886.741041\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.069444\n",
      "(Iteration 51 / 200) loss: 49872.066003\n",
      "(Epoch 6 / 20) train acc: 0.082000; val_acc: 0.069444\n",
      "(Iteration 61 / 200) loss: 43234.235965\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.069444\n",
      "(Iteration 71 / 200) loss: 49429.345927\n",
      "(Epoch 8 / 20) train acc: 0.088000; val_acc: 0.069444\n",
      "(Iteration 81 / 200) loss: 44019.650889\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.066667\n",
      "(Iteration 91 / 200) loss: 45206.965851\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.066667\n",
      "(Iteration 101 / 200) loss: 53371.970813\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.066667\n",
      "(Iteration 111 / 200) loss: 48444.240775\n",
      "(Epoch 12 / 20) train acc: 0.099000; val_acc: 0.066667\n",
      "(Iteration 121 / 200) loss: 54492.940737\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.066667\n",
      "(Iteration 131 / 200) loss: 46206.260698\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.066667\n",
      "(Iteration 141 / 200) loss: 40117.558160\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.066667\n",
      "(Iteration 151 / 200) loss: 51877.430622\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.066667\n",
      "(Iteration 161 / 200) loss: 49082.330584\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.069444\n",
      "(Iteration 171 / 200) loss: 41374.118046\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.066667\n",
      "(Iteration 181 / 200) loss: 48735.685508\n",
      "(Epoch 19 / 20) train acc: 0.083000; val_acc: 0.066667\n",
      "(Iteration 191 / 200) loss: 49446.655470\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.069444\n",
      "(Iteration 1 / 200) loss: 3.706536\n",
      "(Epoch 0 / 20) train acc: 0.075000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.075000; val_acc: 0.077778\n",
      "(Iteration 11 / 200) loss: 3.882916\n",
      "(Epoch 2 / 20) train acc: 0.081000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 4.136686\n",
      "(Epoch 3 / 20) train acc: 0.082000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 3.716667\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 3.570515\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 3.734665\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 3.365499\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 3.368712\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 3.471098\n",
      "(Epoch 9 / 20) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 3.309134\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 3.522269\n",
      "(Epoch 11 / 20) train acc: 0.130000; val_acc: 0.102778\n",
      "(Iteration 111 / 200) loss: 3.080721\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.102778\n",
      "(Iteration 121 / 200) loss: 3.079623\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.105556\n",
      "(Iteration 131 / 200) loss: 3.476540\n",
      "(Epoch 14 / 20) train acc: 0.124000; val_acc: 0.108333\n",
      "(Iteration 141 / 200) loss: 2.850997\n",
      "(Epoch 15 / 20) train acc: 0.132000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.897289\n",
      "(Epoch 16 / 20) train acc: 0.126000; val_acc: 0.127778\n",
      "(Iteration 161 / 200) loss: 2.825549\n",
      "(Epoch 17 / 20) train acc: 0.130000; val_acc: 0.130556\n",
      "(Iteration 171 / 200) loss: 2.820509\n",
      "(Epoch 18 / 20) train acc: 0.136000; val_acc: 0.138889\n",
      "(Iteration 181 / 200) loss: 2.556396\n",
      "(Epoch 19 / 20) train acc: 0.118000; val_acc: 0.144444\n",
      "(Iteration 191 / 200) loss: 2.781794\n",
      "(Epoch 20 / 20) train acc: 0.180000; val_acc: 0.144444\n",
      "(Iteration 1 / 200) loss: 2.302672\n",
      "(Epoch 0 / 20) train acc: 0.164000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.205000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 2.302681\n",
      "(Epoch 2 / 20) train acc: 0.214000; val_acc: 0.205556\n",
      "(Iteration 21 / 200) loss: 2.302640\n",
      "(Epoch 3 / 20) train acc: 0.275000; val_acc: 0.222222\n",
      "(Iteration 31 / 200) loss: 2.302642\n",
      "(Epoch 4 / 20) train acc: 0.298000; val_acc: 0.244444\n",
      "(Iteration 41 / 200) loss: 2.302572\n",
      "(Epoch 5 / 20) train acc: 0.340000; val_acc: 0.244444\n",
      "(Iteration 51 / 200) loss: 2.302556\n",
      "(Epoch 6 / 20) train acc: 0.326000; val_acc: 0.255556\n",
      "(Iteration 61 / 200) loss: 2.302521\n",
      "(Epoch 7 / 20) train acc: 0.331000; val_acc: 0.269444\n",
      "(Iteration 71 / 200) loss: 2.302439\n",
      "(Epoch 8 / 20) train acc: 0.352000; val_acc: 0.302778\n",
      "(Iteration 81 / 200) loss: 2.302417\n",
      "(Epoch 9 / 20) train acc: 0.393000; val_acc: 0.325000\n",
      "(Iteration 91 / 200) loss: 2.302396\n",
      "(Epoch 10 / 20) train acc: 0.361000; val_acc: 0.294444\n",
      "(Iteration 101 / 200) loss: 2.302306\n",
      "(Epoch 11 / 20) train acc: 0.351000; val_acc: 0.291667\n",
      "(Iteration 111 / 200) loss: 2.302204\n",
      "(Epoch 12 / 20) train acc: 0.344000; val_acc: 0.286111\n",
      "(Iteration 121 / 200) loss: 2.302145\n",
      "(Epoch 13 / 20) train acc: 0.322000; val_acc: 0.291667\n",
      "(Iteration 131 / 200) loss: 2.302057\n",
      "(Epoch 14 / 20) train acc: 0.365000; val_acc: 0.305556\n",
      "(Iteration 141 / 200) loss: 2.301848\n",
      "(Epoch 15 / 20) train acc: 0.400000; val_acc: 0.313889\n",
      "(Iteration 151 / 200) loss: 2.301574\n",
      "(Epoch 16 / 20) train acc: 0.363000; val_acc: 0.311111\n",
      "(Iteration 161 / 200) loss: 2.301605\n",
      "(Epoch 17 / 20) train acc: 0.335000; val_acc: 0.300000\n",
      "(Iteration 171 / 200) loss: 2.301513\n",
      "(Epoch 18 / 20) train acc: 0.316000; val_acc: 0.283333\n",
      "(Iteration 181 / 200) loss: 2.301532\n",
      "(Epoch 19 / 20) train acc: 0.352000; val_acc: 0.277778\n",
      "(Iteration 191 / 200) loss: 2.300975\n",
      "(Epoch 20 / 20) train acc: 0.373000; val_acc: 0.300000\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302580\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302569\n",
      "(Epoch 5 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302578\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302559\n",
      "(Epoch 7 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302589\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302574\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302565\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302549\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302559\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302558\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302614\n",
      "(Epoch 14 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302551\n",
      "(Epoch 15 / 20) train acc: 0.203000; val_acc: 0.197222\n",
      "(Iteration 151 / 200) loss: 2.302572\n",
      "(Epoch 16 / 20) train acc: 0.213000; val_acc: 0.194444\n",
      "(Iteration 161 / 200) loss: 2.302583\n",
      "(Epoch 17 / 20) train acc: 0.195000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 2.302499\n",
      "(Epoch 18 / 20) train acc: 0.221000; val_acc: 0.158333\n",
      "(Iteration 181 / 200) loss: 2.302572\n",
      "(Epoch 19 / 20) train acc: 0.205000; val_acc: 0.158333\n",
      "(Iteration 191 / 200) loss: 2.302466\n",
      "(Epoch 20 / 20) train acc: 0.191000; val_acc: 0.158333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302556\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302588\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302592\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302582\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302578\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302574\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302574\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302582\n",
      "(Epoch 10 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302601\n",
      "(Epoch 11 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302582\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302573\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302581\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302590\n",
      "(Epoch 15 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302577\n",
      "(Epoch 16 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302519\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302633\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302516\n",
      "(Epoch 19 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302534\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302584\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302581\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302583\n",
      "(Epoch 5 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302572\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302561\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302575\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302609\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302561\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302575\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302591\n",
      "(Epoch 12 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302607\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302605\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302550\n",
      "(Epoch 15 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302576\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302585\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302585\n",
      "(Epoch 18 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302622\n",
      "(Epoch 19 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302590\n",
      "(Epoch 20 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 21795.980477\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 21812.955431\n",
      "(Epoch 2 / 20) train acc: 0.131000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 21837.337893\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 20766.612858\n",
      "(Epoch 4 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 23232.597824\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 21062.295290\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 22669.505255\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 20691.036471\n",
      "(Epoch 8 / 20) train acc: 0.123000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 21346.630187\n",
      "(Epoch 9 / 20) train acc: 0.123000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 20979.830152\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.102778\n",
      "(Iteration 101 / 200) loss: 20203.272618\n",
      "(Epoch 11 / 20) train acc: 0.115000; val_acc: 0.102778\n",
      "(Iteration 111 / 200) loss: 18840.037584\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.102778\n",
      "(Iteration 121 / 200) loss: 20270.403800\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.102778\n",
      "(Iteration 131 / 200) loss: 19700.208766\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.102778\n",
      "(Iteration 141 / 200) loss: 21337.527481\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 21297.997447\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.105556\n",
      "(Iteration 161 / 200) loss: 21944.679912\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.105556\n",
      "(Iteration 171 / 200) loss: 21332.167378\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.105556\n",
      "(Iteration 181 / 200) loss: 20448.752343\n",
      "(Epoch 19 / 20) train acc: 0.120000; val_acc: 0.105556\n",
      "(Iteration 191 / 200) loss: 19210.117310\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.105556\n",
      "(Iteration 1 / 200) loss: 3.492932\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.141667\n",
      "(Iteration 11 / 200) loss: 3.508720\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.144444\n",
      "(Iteration 21 / 200) loss: 3.170458\n",
      "(Epoch 3 / 20) train acc: 0.141000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 3.480999\n",
      "(Epoch 4 / 20) train acc: 0.120000; val_acc: 0.155556\n",
      "(Iteration 41 / 200) loss: 3.464888\n",
      "(Epoch 5 / 20) train acc: 0.141000; val_acc: 0.161111\n",
      "(Iteration 51 / 200) loss: 3.154843\n",
      "(Epoch 6 / 20) train acc: 0.141000; val_acc: 0.158333\n",
      "(Iteration 61 / 200) loss: 3.206128\n",
      "(Epoch 7 / 20) train acc: 0.115000; val_acc: 0.163889\n",
      "(Iteration 71 / 200) loss: 3.044979\n",
      "(Epoch 8 / 20) train acc: 0.137000; val_acc: 0.166667\n",
      "(Iteration 81 / 200) loss: 3.159804\n",
      "(Epoch 9 / 20) train acc: 0.128000; val_acc: 0.169444\n",
      "(Iteration 91 / 200) loss: 2.926363\n",
      "(Epoch 10 / 20) train acc: 0.140000; val_acc: 0.175000\n",
      "(Iteration 101 / 200) loss: 2.932223\n",
      "(Epoch 11 / 20) train acc: 0.172000; val_acc: 0.175000\n",
      "(Iteration 111 / 200) loss: 3.206741\n",
      "(Epoch 12 / 20) train acc: 0.186000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.787795\n",
      "(Epoch 13 / 20) train acc: 0.156000; val_acc: 0.177778\n",
      "(Iteration 131 / 200) loss: 2.829948\n",
      "(Epoch 14 / 20) train acc: 0.160000; val_acc: 0.191667\n",
      "(Iteration 141 / 200) loss: 2.615276\n",
      "(Epoch 15 / 20) train acc: 0.177000; val_acc: 0.202778\n",
      "(Iteration 151 / 200) loss: 3.017589\n",
      "(Epoch 16 / 20) train acc: 0.204000; val_acc: 0.208333\n",
      "(Iteration 161 / 200) loss: 2.483256\n",
      "(Epoch 17 / 20) train acc: 0.183000; val_acc: 0.211111\n",
      "(Iteration 171 / 200) loss: 2.806295\n",
      "(Epoch 18 / 20) train acc: 0.201000; val_acc: 0.219444\n",
      "(Iteration 181 / 200) loss: 2.530562\n",
      "(Epoch 19 / 20) train acc: 0.181000; val_acc: 0.219444\n",
      "(Iteration 191 / 200) loss: 2.639774\n",
      "(Epoch 20 / 20) train acc: 0.214000; val_acc: 0.222222\n",
      "(Iteration 1 / 200) loss: 2.302734\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.130556\n",
      "(Iteration 11 / 200) loss: 2.302711\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 2.302697\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.119444\n",
      "(Iteration 31 / 200) loss: 2.302641\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302614\n",
      "(Epoch 5 / 20) train acc: 0.124000; val_acc: 0.136111\n",
      "(Iteration 51 / 200) loss: 2.302597\n",
      "(Epoch 6 / 20) train acc: 0.143000; val_acc: 0.150000\n",
      "(Iteration 61 / 200) loss: 2.302543\n",
      "(Epoch 7 / 20) train acc: 0.148000; val_acc: 0.172222\n",
      "(Iteration 71 / 200) loss: 2.302530\n",
      "(Epoch 8 / 20) train acc: 0.177000; val_acc: 0.197222\n",
      "(Iteration 81 / 200) loss: 2.302459\n",
      "(Epoch 9 / 20) train acc: 0.231000; val_acc: 0.225000\n",
      "(Iteration 91 / 200) loss: 2.302347\n",
      "(Epoch 10 / 20) train acc: 0.229000; val_acc: 0.250000\n",
      "(Iteration 101 / 200) loss: 2.302241\n",
      "(Epoch 11 / 20) train acc: 0.228000; val_acc: 0.283333\n",
      "(Iteration 111 / 200) loss: 2.302276\n",
      "(Epoch 12 / 20) train acc: 0.304000; val_acc: 0.311111\n",
      "(Iteration 121 / 200) loss: 2.302178\n",
      "(Epoch 13 / 20) train acc: 0.321000; val_acc: 0.355556\n",
      "(Iteration 131 / 200) loss: 2.302170\n",
      "(Epoch 14 / 20) train acc: 0.396000; val_acc: 0.394444\n",
      "(Iteration 141 / 200) loss: 2.301975\n",
      "(Epoch 15 / 20) train acc: 0.475000; val_acc: 0.455556\n",
      "(Iteration 151 / 200) loss: 2.301798\n",
      "(Epoch 16 / 20) train acc: 0.487000; val_acc: 0.477778\n",
      "(Iteration 161 / 200) loss: 2.301795\n",
      "(Epoch 17 / 20) train acc: 0.492000; val_acc: 0.505556\n",
      "(Iteration 171 / 200) loss: 2.301507\n",
      "(Epoch 18 / 20) train acc: 0.522000; val_acc: 0.533333\n",
      "(Iteration 181 / 200) loss: 2.301374\n",
      "(Epoch 19 / 20) train acc: 0.519000; val_acc: 0.530556\n",
      "(Iteration 191 / 200) loss: 2.301154\n",
      "(Epoch 20 / 20) train acc: 0.488000; val_acc: 0.538889\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302582\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302582\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302624\n",
      "(Epoch 6 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302577\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302557\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302597\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302579\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302577\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302543\n",
      "(Epoch 12 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302594\n",
      "(Epoch 13 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302558\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302583\n",
      "(Epoch 15 / 20) train acc: 0.230000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 2.302553\n",
      "(Epoch 16 / 20) train acc: 0.122000; val_acc: 0.113889\n",
      "(Iteration 161 / 200) loss: 2.302549\n",
      "(Epoch 17 / 20) train acc: 0.155000; val_acc: 0.102778\n",
      "(Iteration 171 / 200) loss: 2.302495\n",
      "(Epoch 18 / 20) train acc: 0.150000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.302494\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.094444\n",
      "(Iteration 191 / 200) loss: 2.302382\n",
      "(Epoch 20 / 20) train acc: 0.137000; val_acc: 0.094444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302588\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302573\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302589\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302568\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302568\n",
      "(Epoch 7 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302598\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302588\n",
      "(Epoch 9 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302571\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302593\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302576\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302594\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302577\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302600\n",
      "(Epoch 15 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302589\n",
      "(Epoch 16 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302609\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302572\n",
      "(Epoch 18 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302589\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302588\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302589\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302573\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302585\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302566\n",
      "(Epoch 8 / 20) train acc: 0.084000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302545\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302615\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302551\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302570\n",
      "(Epoch 12 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302629\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302534\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302598\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302496\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302583\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302523\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302539\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302508\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 45879.840714\n",
      "(Epoch 0 / 20) train acc: 0.064000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.068000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 39869.085710\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 50102.740706\n",
      "(Epoch 3 / 20) train acc: 0.070000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 48325.740702\n",
      "(Epoch 4 / 20) train acc: 0.067000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 41937.130699\n",
      "(Epoch 5 / 20) train acc: 0.074000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 42705.245695\n",
      "(Epoch 6 / 20) train acc: 0.079000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 43865.270691\n",
      "(Epoch 7 / 20) train acc: 0.069000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 48362.935688\n",
      "(Epoch 8 / 20) train acc: 0.079000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 48835.235684\n",
      "(Epoch 9 / 20) train acc: 0.060000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 42952.380681\n",
      "(Epoch 10 / 20) train acc: 0.063000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 47404.810677\n",
      "(Epoch 11 / 20) train acc: 0.068000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 38362.640673\n",
      "(Epoch 12 / 20) train acc: 0.070000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 50944.905670\n",
      "(Epoch 13 / 20) train acc: 0.063000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 43958.165666\n",
      "(Epoch 14 / 20) train acc: 0.069000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 46863.705663\n",
      "(Epoch 15 / 20) train acc: 0.055000; val_acc: 0.086111\n",
      "(Iteration 151 / 200) loss: 43136.425659\n",
      "(Epoch 16 / 20) train acc: 0.065000; val_acc: 0.086111\n",
      "(Iteration 161 / 200) loss: 45420.125656\n",
      "(Epoch 17 / 20) train acc: 0.051000; val_acc: 0.086111\n",
      "(Iteration 171 / 200) loss: 42912.040652\n",
      "(Epoch 18 / 20) train acc: 0.069000; val_acc: 0.086111\n",
      "(Iteration 181 / 200) loss: 44571.610649\n",
      "(Epoch 19 / 20) train acc: 0.067000; val_acc: 0.086111\n",
      "(Iteration 191 / 200) loss: 45857.375645\n",
      "(Epoch 20 / 20) train acc: 0.061000; val_acc: 0.086111\n",
      "(Iteration 1 / 200) loss: 2.725155\n",
      "(Epoch 0 / 20) train acc: 0.141000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.872118\n",
      "(Epoch 2 / 20) train acc: 0.147000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2.681533\n",
      "(Epoch 3 / 20) train acc: 0.128000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 2.868131\n",
      "(Epoch 4 / 20) train acc: 0.143000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 2.730694\n",
      "(Epoch 5 / 20) train acc: 0.157000; val_acc: 0.136111\n",
      "(Iteration 51 / 200) loss: 2.565980\n",
      "(Epoch 6 / 20) train acc: 0.159000; val_acc: 0.144444\n",
      "(Iteration 61 / 200) loss: 2.712532\n",
      "(Epoch 7 / 20) train acc: 0.152000; val_acc: 0.147222\n",
      "(Iteration 71 / 200) loss: 2.598747\n",
      "(Epoch 8 / 20) train acc: 0.189000; val_acc: 0.163889\n",
      "(Iteration 81 / 200) loss: 2.363093\n",
      "(Epoch 9 / 20) train acc: 0.201000; val_acc: 0.172222\n",
      "(Iteration 91 / 200) loss: 2.544667\n",
      "(Epoch 10 / 20) train acc: 0.199000; val_acc: 0.177778\n",
      "(Iteration 101 / 200) loss: 2.289657\n",
      "(Epoch 11 / 20) train acc: 0.236000; val_acc: 0.186111\n",
      "(Iteration 111 / 200) loss: 2.342662\n",
      "(Epoch 12 / 20) train acc: 0.211000; val_acc: 0.191667\n",
      "(Iteration 121 / 200) loss: 2.118487\n",
      "(Epoch 13 / 20) train acc: 0.227000; val_acc: 0.197222\n",
      "(Iteration 131 / 200) loss: 2.406868\n",
      "(Epoch 14 / 20) train acc: 0.267000; val_acc: 0.200000\n",
      "(Iteration 141 / 200) loss: 2.211901\n",
      "(Epoch 15 / 20) train acc: 0.247000; val_acc: 0.216667\n",
      "(Iteration 151 / 200) loss: 2.133862\n",
      "(Epoch 16 / 20) train acc: 0.241000; val_acc: 0.227778\n",
      "(Iteration 161 / 200) loss: 2.181015\n",
      "(Epoch 17 / 20) train acc: 0.270000; val_acc: 0.230556\n",
      "(Iteration 171 / 200) loss: 2.151374\n",
      "(Epoch 18 / 20) train acc: 0.288000; val_acc: 0.238889\n",
      "(Iteration 181 / 200) loss: 2.043681\n",
      "(Epoch 19 / 20) train acc: 0.257000; val_acc: 0.252778\n",
      "(Iteration 191 / 200) loss: 2.121302\n",
      "(Epoch 20 / 20) train acc: 0.290000; val_acc: 0.263889\n",
      "(Iteration 1 / 200) loss: 2.302569\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.133000; val_acc: 0.113889\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.134000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302501\n",
      "(Epoch 3 / 20) train acc: 0.157000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 2.302483\n",
      "(Epoch 4 / 20) train acc: 0.155000; val_acc: 0.119444\n",
      "(Iteration 41 / 200) loss: 2.302454\n",
      "(Epoch 5 / 20) train acc: 0.145000; val_acc: 0.119444\n",
      "(Iteration 51 / 200) loss: 2.302435\n",
      "(Epoch 6 / 20) train acc: 0.184000; val_acc: 0.152778\n",
      "(Iteration 61 / 200) loss: 2.302447\n",
      "(Epoch 7 / 20) train acc: 0.285000; val_acc: 0.227778\n",
      "(Iteration 71 / 200) loss: 2.302347\n",
      "(Epoch 8 / 20) train acc: 0.324000; val_acc: 0.294444\n",
      "(Iteration 81 / 200) loss: 2.302334\n",
      "(Epoch 9 / 20) train acc: 0.292000; val_acc: 0.288889\n",
      "(Iteration 91 / 200) loss: 2.302203\n",
      "(Epoch 10 / 20) train acc: 0.306000; val_acc: 0.300000\n",
      "(Iteration 101 / 200) loss: 2.302175\n",
      "(Epoch 11 / 20) train acc: 0.344000; val_acc: 0.352778\n",
      "(Iteration 111 / 200) loss: 2.302121\n",
      "(Epoch 12 / 20) train acc: 0.341000; val_acc: 0.347222\n",
      "(Iteration 121 / 200) loss: 2.301934\n",
      "(Epoch 13 / 20) train acc: 0.392000; val_acc: 0.363889\n",
      "(Iteration 131 / 200) loss: 2.301867\n",
      "(Epoch 14 / 20) train acc: 0.382000; val_acc: 0.372222\n",
      "(Iteration 141 / 200) loss: 2.301773\n",
      "(Epoch 15 / 20) train acc: 0.423000; val_acc: 0.391667\n",
      "(Iteration 151 / 200) loss: 2.301654\n",
      "(Epoch 16 / 20) train acc: 0.391000; val_acc: 0.405556\n",
      "(Iteration 161 / 200) loss: 2.301402\n",
      "(Epoch 17 / 20) train acc: 0.415000; val_acc: 0.427778\n",
      "(Iteration 171 / 200) loss: 2.301417\n",
      "(Epoch 18 / 20) train acc: 0.429000; val_acc: 0.472222\n",
      "(Iteration 181 / 200) loss: 2.301304\n",
      "(Epoch 19 / 20) train acc: 0.469000; val_acc: 0.508333\n",
      "(Iteration 191 / 200) loss: 2.300980\n",
      "(Epoch 20 / 20) train acc: 0.504000; val_acc: 0.508333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.129000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302589\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302582\n",
      "(Epoch 5 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302591\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302590\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302570\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302602\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302566\n",
      "(Epoch 10 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302569\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302577\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302575\n",
      "(Epoch 13 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302521\n",
      "(Epoch 14 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302524\n",
      "(Epoch 15 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302540\n",
      "(Epoch 16 / 20) train acc: 0.133000; val_acc: 0.111111\n",
      "(Iteration 161 / 200) loss: 2.302487\n",
      "(Epoch 17 / 20) train acc: 0.211000; val_acc: 0.141667\n",
      "(Iteration 171 / 200) loss: 2.302487\n",
      "(Epoch 18 / 20) train acc: 0.195000; val_acc: 0.163889\n",
      "(Iteration 181 / 200) loss: 2.302410\n",
      "(Epoch 19 / 20) train acc: 0.211000; val_acc: 0.163889\n",
      "(Iteration 191 / 200) loss: 2.302244\n",
      "(Epoch 20 / 20) train acc: 0.237000; val_acc: 0.161111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302585\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302576\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302576\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302591\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302552\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302567\n",
      "(Epoch 8 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302564\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302567\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302592\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302577\n",
      "(Epoch 12 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302587\n",
      "(Epoch 13 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302583\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302547\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302518\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302531\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302600\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302545\n",
      "(Epoch 19 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302522\n",
      "(Epoch 20 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302580\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302584\n",
      "(Epoch 3 / 20) train acc: 0.084000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302575\n",
      "(Epoch 4 / 20) train acc: 0.081000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302584\n",
      "(Epoch 5 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302597\n",
      "(Epoch 6 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302594\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302603\n",
      "(Epoch 8 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302578\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302576\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302594\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302589\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302581\n",
      "(Epoch 13 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302579\n",
      "(Epoch 14 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302596\n",
      "(Epoch 15 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.302589\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302543\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302567\n",
      "(Epoch 18 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302592\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302583\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 47155.556243\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 51289.161239\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 51032.291235\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 49587.426231\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 47951.461228\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 45740.181224\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 55033.991221\n",
      "(Epoch 7 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 51024.161217\n",
      "(Epoch 8 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 48073.836214\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 47980.016210\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 46388.411207\n",
      "(Epoch 11 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 44789.241203\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 46154.086200\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 41342.263696\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.086111\n",
      "(Iteration 141 / 200) loss: 47833.581193\n",
      "(Epoch 15 / 20) train acc: 0.138000; val_acc: 0.086111\n",
      "(Iteration 151 / 200) loss: 48026.971189\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Iteration 161 / 200) loss: 51392.476186\n",
      "(Epoch 17 / 20) train acc: 0.119000; val_acc: 0.086111\n",
      "(Iteration 171 / 200) loss: 43983.316182\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.086111\n",
      "(Iteration 181 / 200) loss: 42834.261178\n",
      "(Epoch 19 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Iteration 191 / 200) loss: 42953.016175\n",
      "(Epoch 20 / 20) train acc: 0.131000; val_acc: 0.086111\n",
      "(Iteration 1 / 200) loss: 4.184149\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.141000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 4.099067\n",
      "(Epoch 2 / 20) train acc: 0.144000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 3.905123\n",
      "(Epoch 3 / 20) train acc: 0.124000; val_acc: 0.105556\n",
      "(Iteration 31 / 200) loss: 3.136556\n",
      "(Epoch 4 / 20) train acc: 0.132000; val_acc: 0.108333\n",
      "(Iteration 41 / 200) loss: 3.564503\n",
      "(Epoch 5 / 20) train acc: 0.142000; val_acc: 0.105556\n",
      "(Iteration 51 / 200) loss: 3.564877\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.105556\n",
      "(Iteration 61 / 200) loss: 3.365936\n",
      "(Epoch 7 / 20) train acc: 0.145000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 3.377568\n",
      "(Epoch 8 / 20) train acc: 0.157000; val_acc: 0.119444\n",
      "(Iteration 81 / 200) loss: 3.332739\n",
      "(Epoch 9 / 20) train acc: 0.129000; val_acc: 0.130556\n",
      "(Iteration 91 / 200) loss: 3.262901\n",
      "(Epoch 10 / 20) train acc: 0.156000; val_acc: 0.133333\n",
      "(Iteration 101 / 200) loss: 3.218225\n",
      "(Epoch 11 / 20) train acc: 0.147000; val_acc: 0.136111\n",
      "(Iteration 111 / 200) loss: 3.001032\n",
      "(Epoch 12 / 20) train acc: 0.165000; val_acc: 0.144444\n",
      "(Iteration 121 / 200) loss: 3.179493\n",
      "(Epoch 13 / 20) train acc: 0.153000; val_acc: 0.152778\n",
      "(Iteration 131 / 200) loss: 3.121462\n",
      "(Epoch 14 / 20) train acc: 0.157000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 3.079535\n",
      "(Epoch 15 / 20) train acc: 0.147000; val_acc: 0.158333\n",
      "(Iteration 151 / 200) loss: 3.055960\n",
      "(Epoch 16 / 20) train acc: 0.157000; val_acc: 0.161111\n",
      "(Iteration 161 / 200) loss: 2.989338\n",
      "(Epoch 17 / 20) train acc: 0.181000; val_acc: 0.169444\n",
      "(Iteration 171 / 200) loss: 2.909917\n",
      "(Epoch 18 / 20) train acc: 0.153000; val_acc: 0.169444\n",
      "(Iteration 181 / 200) loss: 2.777465\n",
      "(Epoch 19 / 20) train acc: 0.185000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 2.763931\n",
      "(Epoch 20 / 20) train acc: 0.179000; val_acc: 0.175000\n",
      "(Iteration 1 / 200) loss: 2.302577\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.199000; val_acc: 0.177778\n",
      "(Iteration 11 / 200) loss: 2.302569\n",
      "(Epoch 2 / 20) train acc: 0.163000; val_acc: 0.194444\n",
      "(Iteration 21 / 200) loss: 2.302519\n",
      "(Epoch 3 / 20) train acc: 0.201000; val_acc: 0.200000\n",
      "(Iteration 31 / 200) loss: 2.302451\n",
      "(Epoch 4 / 20) train acc: 0.218000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 2.302376\n",
      "(Epoch 5 / 20) train acc: 0.287000; val_acc: 0.241667\n",
      "(Iteration 51 / 200) loss: 2.302383\n",
      "(Epoch 6 / 20) train acc: 0.349000; val_acc: 0.322222\n",
      "(Iteration 61 / 200) loss: 2.302313\n",
      "(Epoch 7 / 20) train acc: 0.383000; val_acc: 0.352778\n",
      "(Iteration 71 / 200) loss: 2.302278\n",
      "(Epoch 8 / 20) train acc: 0.425000; val_acc: 0.408333\n",
      "(Iteration 81 / 200) loss: 2.302163\n",
      "(Epoch 9 / 20) train acc: 0.493000; val_acc: 0.441667\n",
      "(Iteration 91 / 200) loss: 2.302204\n",
      "(Epoch 10 / 20) train acc: 0.519000; val_acc: 0.486111\n",
      "(Iteration 101 / 200) loss: 2.302087\n",
      "(Epoch 11 / 20) train acc: 0.501000; val_acc: 0.502778\n",
      "(Iteration 111 / 200) loss: 2.301914\n",
      "(Epoch 12 / 20) train acc: 0.551000; val_acc: 0.505556\n",
      "(Iteration 121 / 200) loss: 2.301897\n",
      "(Epoch 13 / 20) train acc: 0.568000; val_acc: 0.502778\n",
      "(Iteration 131 / 200) loss: 2.301663\n",
      "(Epoch 14 / 20) train acc: 0.588000; val_acc: 0.536111\n",
      "(Iteration 141 / 200) loss: 2.301533\n",
      "(Epoch 15 / 20) train acc: 0.566000; val_acc: 0.525000\n",
      "(Iteration 151 / 200) loss: 2.301618\n",
      "(Epoch 16 / 20) train acc: 0.583000; val_acc: 0.519444\n",
      "(Iteration 161 / 200) loss: 2.301444\n",
      "(Epoch 17 / 20) train acc: 0.576000; val_acc: 0.530556\n",
      "(Iteration 171 / 200) loss: 2.301163\n",
      "(Epoch 18 / 20) train acc: 0.619000; val_acc: 0.597222\n",
      "(Iteration 181 / 200) loss: 2.301032\n",
      "(Epoch 19 / 20) train acc: 0.613000; val_acc: 0.625000\n",
      "(Iteration 191 / 200) loss: 2.300611\n",
      "(Epoch 20 / 20) train acc: 0.603000; val_acc: 0.613889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.133000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302586\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302582\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302577\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302578\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302588\n",
      "(Epoch 8 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302561\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302583\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302543\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302595\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302600\n",
      "(Epoch 13 / 20) train acc: 0.171000; val_acc: 0.175000\n",
      "(Iteration 131 / 200) loss: 2.302586\n",
      "(Epoch 14 / 20) train acc: 0.193000; val_acc: 0.188889\n",
      "(Iteration 141 / 200) loss: 2.302566\n",
      "(Epoch 15 / 20) train acc: 0.197000; val_acc: 0.194444\n",
      "(Iteration 151 / 200) loss: 2.302531\n",
      "(Epoch 16 / 20) train acc: 0.193000; val_acc: 0.186111\n",
      "(Iteration 161 / 200) loss: 2.302452\n",
      "(Epoch 17 / 20) train acc: 0.197000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 2.302406\n",
      "(Epoch 18 / 20) train acc: 0.191000; val_acc: 0.200000\n",
      "(Iteration 181 / 200) loss: 2.302284\n",
      "(Epoch 19 / 20) train acc: 0.231000; val_acc: 0.194444\n",
      "(Iteration 191 / 200) loss: 2.302114\n",
      "(Epoch 20 / 20) train acc: 0.222000; val_acc: 0.180556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302603\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302615\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302580\n",
      "(Epoch 7 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302564\n",
      "(Epoch 8 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302568\n",
      "(Epoch 9 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302607\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302584\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302573\n",
      "(Epoch 12 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302537\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302561\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302541\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302560\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302557\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302599\n",
      "(Epoch 18 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302543\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302559\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302587\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302579\n",
      "(Epoch 6 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302574\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302588\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302572\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302592\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302577\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302550\n",
      "(Epoch 12 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302571\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302561\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302574\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302555\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302593\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302561\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302587\n",
      "(Epoch 19 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302588\n",
      "(Epoch 20 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 40322.326601\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 40555.066596\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 40245.114092\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.105556\n",
      "(Iteration 31 / 200) loss: 41725.786589\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.105556\n",
      "(Iteration 41 / 200) loss: 41564.431585\n",
      "(Epoch 5 / 20) train acc: 0.081000; val_acc: 0.105556\n",
      "(Iteration 51 / 200) loss: 40930.096582\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.105556\n",
      "(Iteration 61 / 200) loss: 38288.864078\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.105556\n",
      "(Iteration 71 / 200) loss: 36447.769074\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.108333\n",
      "(Iteration 81 / 200) loss: 38487.344071\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.108333\n",
      "(Iteration 91 / 200) loss: 37742.909067\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.108333\n",
      "(Iteration 101 / 200) loss: 39716.744064\n",
      "(Epoch 11 / 20) train acc: 0.091000; val_acc: 0.108333\n",
      "(Iteration 111 / 200) loss: 37618.174060\n",
      "(Epoch 12 / 20) train acc: 0.093000; val_acc: 0.108333\n",
      "(Iteration 121 / 200) loss: 36877.706557\n",
      "(Epoch 13 / 20) train acc: 0.089000; val_acc: 0.108333\n",
      "(Iteration 131 / 200) loss: 41557.211553\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.108333\n",
      "(Iteration 141 / 200) loss: 34647.561550\n",
      "(Epoch 15 / 20) train acc: 0.079000; val_acc: 0.108333\n",
      "(Iteration 151 / 200) loss: 38277.616547\n",
      "(Epoch 16 / 20) train acc: 0.085000; val_acc: 0.108333\n",
      "(Iteration 161 / 200) loss: 38653.751543\n",
      "(Epoch 17 / 20) train acc: 0.091000; val_acc: 0.108333\n",
      "(Iteration 171 / 200) loss: 39260.249040\n",
      "(Epoch 18 / 20) train acc: 0.087000; val_acc: 0.108333\n",
      "(Iteration 181 / 200) loss: 34585.146536\n",
      "(Epoch 19 / 20) train acc: 0.081000; val_acc: 0.108333\n",
      "(Iteration 191 / 200) loss: 39785.909033\n",
      "(Epoch 20 / 20) train acc: 0.088000; val_acc: 0.108333\n",
      "(Iteration 1 / 200) loss: 3.632590\n",
      "(Epoch 0 / 20) train acc: 0.075000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 3.461950\n",
      "(Epoch 2 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 3.435247\n",
      "(Epoch 3 / 20) train acc: 0.113000; val_acc: 0.102778\n",
      "(Iteration 31 / 200) loss: 3.391918\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.108333\n",
      "(Iteration 41 / 200) loss: 3.115845\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.127778\n",
      "(Iteration 51 / 200) loss: 3.341062\n",
      "(Epoch 6 / 20) train acc: 0.126000; val_acc: 0.136111\n",
      "(Iteration 61 / 200) loss: 2.989202\n",
      "(Epoch 7 / 20) train acc: 0.137000; val_acc: 0.138889\n",
      "(Iteration 71 / 200) loss: 3.214974\n",
      "(Epoch 8 / 20) train acc: 0.123000; val_acc: 0.144444\n",
      "(Iteration 81 / 200) loss: 2.945188\n",
      "(Epoch 9 / 20) train acc: 0.137000; val_acc: 0.147222\n",
      "(Iteration 91 / 200) loss: 2.850825\n",
      "(Epoch 10 / 20) train acc: 0.143000; val_acc: 0.152778\n",
      "(Iteration 101 / 200) loss: 2.928600\n",
      "(Epoch 11 / 20) train acc: 0.137000; val_acc: 0.152778\n",
      "(Iteration 111 / 200) loss: 2.863508\n",
      "(Epoch 12 / 20) train acc: 0.128000; val_acc: 0.152778\n",
      "(Iteration 121 / 200) loss: 2.691545\n",
      "(Epoch 13 / 20) train acc: 0.137000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 2.623662\n",
      "(Epoch 14 / 20) train acc: 0.161000; val_acc: 0.169444\n",
      "(Iteration 141 / 200) loss: 2.908081\n",
      "(Epoch 15 / 20) train acc: 0.152000; val_acc: 0.169444\n",
      "(Iteration 151 / 200) loss: 2.637881\n",
      "(Epoch 16 / 20) train acc: 0.125000; val_acc: 0.172222\n",
      "(Iteration 161 / 200) loss: 2.396821\n",
      "(Epoch 17 / 20) train acc: 0.153000; val_acc: 0.175000\n",
      "(Iteration 171 / 200) loss: 2.472628\n",
      "(Epoch 18 / 20) train acc: 0.144000; val_acc: 0.180556\n",
      "(Iteration 181 / 200) loss: 2.496533\n",
      "(Epoch 19 / 20) train acc: 0.170000; val_acc: 0.177778\n",
      "(Iteration 191 / 200) loss: 2.697149\n",
      "(Epoch 20 / 20) train acc: 0.169000; val_acc: 0.183333\n",
      "(Iteration 1 / 200) loss: 2.302621\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.147000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 2.302544\n",
      "(Epoch 2 / 20) train acc: 0.180000; val_acc: 0.186111\n",
      "(Iteration 21 / 200) loss: 2.302539\n",
      "(Epoch 3 / 20) train acc: 0.250000; val_acc: 0.233333\n",
      "(Iteration 31 / 200) loss: 2.302450\n",
      "(Epoch 4 / 20) train acc: 0.308000; val_acc: 0.333333\n",
      "(Iteration 41 / 200) loss: 2.302457\n",
      "(Epoch 5 / 20) train acc: 0.405000; val_acc: 0.394444\n",
      "(Iteration 51 / 200) loss: 2.302375\n",
      "(Epoch 6 / 20) train acc: 0.439000; val_acc: 0.416667\n",
      "(Iteration 61 / 200) loss: 2.302313\n",
      "(Epoch 7 / 20) train acc: 0.450000; val_acc: 0.447222\n",
      "(Iteration 71 / 200) loss: 2.302289\n",
      "(Epoch 8 / 20) train acc: 0.479000; val_acc: 0.469444\n",
      "(Iteration 81 / 200) loss: 2.302250\n",
      "(Epoch 9 / 20) train acc: 0.546000; val_acc: 0.508333\n",
      "(Iteration 91 / 200) loss: 2.302190\n",
      "(Epoch 10 / 20) train acc: 0.497000; val_acc: 0.488889\n",
      "(Iteration 101 / 200) loss: 2.302072\n",
      "(Epoch 11 / 20) train acc: 0.497000; val_acc: 0.488889\n",
      "(Iteration 111 / 200) loss: 2.302003\n",
      "(Epoch 12 / 20) train acc: 0.457000; val_acc: 0.469444\n",
      "(Iteration 121 / 200) loss: 2.302016\n",
      "(Epoch 13 / 20) train acc: 0.502000; val_acc: 0.488889\n",
      "(Iteration 131 / 200) loss: 2.301756\n",
      "(Epoch 14 / 20) train acc: 0.525000; val_acc: 0.527778\n",
      "(Iteration 141 / 200) loss: 2.301800\n",
      "(Epoch 15 / 20) train acc: 0.547000; val_acc: 0.563889\n",
      "(Iteration 151 / 200) loss: 2.301578\n",
      "(Epoch 16 / 20) train acc: 0.527000; val_acc: 0.586111\n",
      "(Iteration 161 / 200) loss: 2.301383\n",
      "(Epoch 17 / 20) train acc: 0.581000; val_acc: 0.605556\n",
      "(Iteration 171 / 200) loss: 2.301304\n",
      "(Epoch 18 / 20) train acc: 0.602000; val_acc: 0.594444\n",
      "(Iteration 181 / 200) loss: 2.301042\n",
      "(Epoch 19 / 20) train acc: 0.614000; val_acc: 0.597222\n",
      "(Iteration 191 / 200) loss: 2.300687\n",
      "(Epoch 20 / 20) train acc: 0.603000; val_acc: 0.600000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302585\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302579\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302585\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302591\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302580\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302585\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302566\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302569\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302586\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302560\n",
      "(Epoch 15 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302522\n",
      "(Epoch 16 / 20) train acc: 0.208000; val_acc: 0.158333\n",
      "(Iteration 161 / 200) loss: 2.302515\n",
      "(Epoch 17 / 20) train acc: 0.193000; val_acc: 0.152778\n",
      "(Iteration 171 / 200) loss: 2.302428\n",
      "(Epoch 18 / 20) train acc: 0.194000; val_acc: 0.147222\n",
      "(Iteration 181 / 200) loss: 2.302267\n",
      "(Epoch 19 / 20) train acc: 0.174000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302264\n",
      "(Epoch 20 / 20) train acc: 0.142000; val_acc: 0.111111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302590\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302582\n",
      "(Epoch 3 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302592\n",
      "(Epoch 4 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302566\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302575\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302582\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302580\n",
      "(Epoch 8 / 20) train acc: 0.088000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302571\n",
      "(Epoch 9 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302579\n",
      "(Epoch 10 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302587\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302604\n",
      "(Epoch 12 / 20) train acc: 0.079000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302577\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302543\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302560\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302583\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302622\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302607\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302607\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302580\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302585\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302584\n",
      "(Epoch 4 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302601\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302602\n",
      "(Epoch 6 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302558\n",
      "(Epoch 7 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302596\n",
      "(Epoch 8 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302603\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302509\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302567\n",
      "(Epoch 11 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302611\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302596\n",
      "(Epoch 13 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302576\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302590\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302563\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302635\n",
      "(Epoch 17 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302565\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302582\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302575\n",
      "(Epoch 20 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 305021.014893\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.440000; val_acc: 0.369444\n",
      "(Iteration 11 / 200) loss: 35985.082590\n",
      "(Epoch 2 / 20) train acc: 0.630000; val_acc: 0.633333\n",
      "(Iteration 21 / 200) loss: 9839.375715\n",
      "(Epoch 3 / 20) train acc: 0.832000; val_acc: 0.811111\n",
      "(Iteration 31 / 200) loss: 3793.066265\n",
      "(Epoch 4 / 20) train acc: 0.900000; val_acc: 0.855556\n",
      "(Iteration 41 / 200) loss: 1263.410679\n",
      "(Epoch 5 / 20) train acc: 0.911000; val_acc: 0.850000\n",
      "(Iteration 51 / 200) loss: 439.604253\n",
      "(Epoch 6 / 20) train acc: 0.931000; val_acc: 0.863889\n",
      "(Iteration 61 / 200) loss: 1491.168042\n",
      "(Epoch 7 / 20) train acc: 0.931000; val_acc: 0.866667\n",
      "(Iteration 71 / 200) loss: 1557.374063\n",
      "(Epoch 8 / 20) train acc: 0.940000; val_acc: 0.861111\n",
      "(Iteration 81 / 200) loss: 259.144675\n",
      "(Epoch 9 / 20) train acc: 0.969000; val_acc: 0.897222\n",
      "(Iteration 91 / 200) loss: 353.151257\n",
      "(Epoch 10 / 20) train acc: 0.975000; val_acc: 0.902778\n",
      "(Iteration 101 / 200) loss: 175.743015\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.894444\n",
      "(Iteration 111 / 200) loss: 220.764319\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.872222\n",
      "(Iteration 121 / 200) loss: 177.908469\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.869444\n",
      "(Iteration 131 / 200) loss: 393.018062\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.888889\n",
      "(Iteration 141 / 200) loss: 160.762866\n",
      "(Epoch 15 / 20) train acc: 0.983000; val_acc: 0.880556\n",
      "(Iteration 151 / 200) loss: 189.875469\n",
      "(Epoch 16 / 20) train acc: 0.977000; val_acc: 0.891667\n",
      "(Iteration 161 / 200) loss: 160.108862\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.894444\n",
      "(Iteration 171 / 200) loss: 159.841416\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.886111\n",
      "(Iteration 181 / 200) loss: 162.552393\n",
      "(Epoch 19 / 20) train acc: 0.991000; val_acc: 0.888889\n",
      "(Iteration 191 / 200) loss: 159.385913\n",
      "(Epoch 20 / 20) train acc: 0.990000; val_acc: 0.891667\n",
      "(Iteration 1 / 200) loss: 4.566154\n",
      "(Epoch 0 / 20) train acc: 0.204000; val_acc: 0.238889\n",
      "(Epoch 1 / 20) train acc: 0.789000; val_acc: 0.730556\n",
      "(Iteration 11 / 200) loss: 2.003925\n",
      "(Epoch 2 / 20) train acc: 0.875000; val_acc: 0.872222\n",
      "(Iteration 21 / 200) loss: 1.599322\n",
      "(Epoch 3 / 20) train acc: 0.925000; val_acc: 0.897222\n",
      "(Iteration 31 / 200) loss: 1.216772\n",
      "(Epoch 4 / 20) train acc: 0.965000; val_acc: 0.930556\n",
      "(Iteration 41 / 200) loss: 1.154241\n",
      "(Epoch 5 / 20) train acc: 0.951000; val_acc: 0.927778\n",
      "(Iteration 51 / 200) loss: 0.790735\n",
      "(Epoch 6 / 20) train acc: 0.976000; val_acc: 0.950000\n",
      "(Iteration 61 / 200) loss: 0.670791\n",
      "(Epoch 7 / 20) train acc: 0.982000; val_acc: 0.961111\n",
      "(Iteration 71 / 200) loss: 0.600372\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 0.525706\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 0.471425\n",
      "(Epoch 10 / 20) train acc: 0.991000; val_acc: 0.983333\n",
      "(Iteration 101 / 200) loss: 0.488704\n",
      "(Epoch 11 / 20) train acc: 0.978000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.437995\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.402026\n",
      "(Epoch 13 / 20) train acc: 0.968000; val_acc: 0.952778\n",
      "(Iteration 131 / 200) loss: 0.375019\n",
      "(Epoch 14 / 20) train acc: 0.984000; val_acc: 0.941667\n",
      "(Iteration 141 / 200) loss: 0.447197\n",
      "(Epoch 15 / 20) train acc: 0.969000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.405452\n",
      "(Epoch 16 / 20) train acc: 0.986000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.328795\n",
      "(Epoch 17 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 171 / 200) loss: 0.293279\n",
      "(Epoch 18 / 20) train acc: 0.966000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.371725\n",
      "(Epoch 19 / 20) train acc: 0.964000; val_acc: 0.963889\n",
      "(Iteration 191 / 200) loss: 0.350082\n",
      "(Epoch 20 / 20) train acc: 0.980000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.320756\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.305261\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303254\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.303390\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302707\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.305050\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.305004\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.304313\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.299071\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.300972\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.304511\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.295074\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304861\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.306131\n",
      "(Epoch 14 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.298895\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.288840\n",
      "(Epoch 16 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.297562\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302430\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.308737\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.299169\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302769\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.306058\n",
      "(Epoch 2 / 20) train acc: 0.088000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302990\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.306793\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.300209\n",
      "(Epoch 5 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.300069\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.305801\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302143\n",
      "(Epoch 8 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.298946\n",
      "(Epoch 9 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.293406\n",
      "(Epoch 10 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.293624\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.306382\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.298580\n",
      "(Epoch 13 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.306288\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.296006\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.314973\n",
      "(Epoch 16 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301423\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.301934\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.303600\n",
      "(Epoch 19 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302914\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.306665\n",
      "(Epoch 2 / 20) train acc: 0.087000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.308784\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.305054\n",
      "(Epoch 4 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 2.292864\n",
      "(Epoch 5 / 20) train acc: 0.080000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.307419\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 2.301755\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 2.306699\n",
      "(Epoch 8 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.296738\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.309186\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.305621\n",
      "(Epoch 11 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.307735\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.321116\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.322428\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.297524\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302344\n",
      "(Epoch 16 / 20) train acc: 0.082000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.305959\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.305692\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302354\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301646\n",
      "(Epoch 20 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.298817\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.295024\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.312616\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.307452\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.296698\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.307369\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.299733\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.294965\n",
      "(Epoch 9 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.310689\n",
      "(Epoch 10 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.304855\n",
      "(Epoch 11 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.309907\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.306112\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302124\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.296551\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.299903\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.308455\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.306294\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.091667\n",
      "(Iteration 181 / 200) loss: 2.298426\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Iteration 191 / 200) loss: 2.307700\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 244233.711143\n",
      "(Epoch 0 / 20) train acc: 0.172000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.566000; val_acc: 0.555556\n",
      "(Iteration 11 / 200) loss: 17947.674722\n",
      "(Epoch 2 / 20) train acc: 0.785000; val_acc: 0.736111\n",
      "(Iteration 21 / 200) loss: 5657.606833\n",
      "(Epoch 3 / 20) train acc: 0.907000; val_acc: 0.836111\n",
      "(Iteration 31 / 200) loss: 2376.963506\n",
      "(Epoch 4 / 20) train acc: 0.910000; val_acc: 0.866667\n",
      "(Iteration 41 / 200) loss: 1514.539482\n",
      "(Epoch 5 / 20) train acc: 0.933000; val_acc: 0.855556\n",
      "(Iteration 51 / 200) loss: 1590.793872\n",
      "(Epoch 6 / 20) train acc: 0.946000; val_acc: 0.877778\n",
      "(Iteration 61 / 200) loss: 439.435239\n",
      "(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.883333\n",
      "(Iteration 71 / 200) loss: 1372.814961\n",
      "(Epoch 8 / 20) train acc: 0.977000; val_acc: 0.888889\n",
      "(Iteration 81 / 200) loss: 271.347002\n",
      "(Epoch 9 / 20) train acc: 0.985000; val_acc: 0.902778\n",
      "(Iteration 91 / 200) loss: 269.162598\n",
      "(Epoch 10 / 20) train acc: 0.976000; val_acc: 0.908333\n",
      "(Iteration 101 / 200) loss: 257.437698\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.894444\n",
      "(Iteration 111 / 200) loss: 154.794148\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 389.444639\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.900000\n",
      "(Iteration 131 / 200) loss: 153.350693\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.905556\n",
      "(Iteration 141 / 200) loss: 205.440327\n",
      "(Epoch 15 / 20) train acc: 0.980000; val_acc: 0.894444\n",
      "(Iteration 151 / 200) loss: 156.177971\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.908333\n",
      "(Iteration 161 / 200) loss: 151.797966\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.911111\n",
      "(Iteration 171 / 200) loss: 151.397742\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.911111\n",
      "(Iteration 181 / 200) loss: 166.477922\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.905556\n",
      "(Iteration 191 / 200) loss: 161.605317\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.897222\n",
      "(Iteration 1 / 200) loss: 4.949920\n",
      "(Epoch 0 / 20) train acc: 0.173000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.746000; val_acc: 0.722222\n",
      "(Iteration 11 / 200) loss: 1.856232\n",
      "(Epoch 2 / 20) train acc: 0.921000; val_acc: 0.908333\n",
      "(Iteration 21 / 200) loss: 1.199138\n",
      "(Epoch 3 / 20) train acc: 0.961000; val_acc: 0.933333\n",
      "(Iteration 31 / 200) loss: 0.949459\n",
      "(Epoch 4 / 20) train acc: 0.949000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.875983\n",
      "(Epoch 5 / 20) train acc: 0.969000; val_acc: 0.955556\n",
      "(Iteration 51 / 200) loss: 0.736066\n",
      "(Epoch 6 / 20) train acc: 0.973000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 0.799974\n",
      "(Epoch 7 / 20) train acc: 0.954000; val_acc: 0.930556\n",
      "(Iteration 71 / 200) loss: 0.574500\n",
      "(Epoch 8 / 20) train acc: 0.964000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.505213\n",
      "(Epoch 9 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 91 / 200) loss: 0.481080\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.458821\n",
      "(Epoch 11 / 20) train acc: 0.988000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.427201\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 121 / 200) loss: 0.428230\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.417338\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.320580\n",
      "(Epoch 15 / 20) train acc: 0.994000; val_acc: 0.983333\n",
      "(Iteration 151 / 200) loss: 0.301362\n",
      "(Epoch 16 / 20) train acc: 0.985000; val_acc: 0.961111\n",
      "(Iteration 161 / 200) loss: 0.292451\n",
      "(Epoch 17 / 20) train acc: 0.991000; val_acc: 0.938889\n",
      "(Iteration 171 / 200) loss: 0.308019\n",
      "(Epoch 18 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.256042\n",
      "(Epoch 19 / 20) train acc: 0.983000; val_acc: 0.969444\n",
      "(Iteration 191 / 200) loss: 0.270100\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.320895\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.305118\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.300127\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.304886\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.298661\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.303341\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.296484\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.304075\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.311970\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.307527\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.305962\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.309174\n",
      "(Epoch 12 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.300544\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.091667\n",
      "(Iteration 131 / 200) loss: 2.296398\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.312192\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.297504\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.301976\n",
      "(Epoch 17 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.297874\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303533\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.293535\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302770\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.315163\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.315480\n",
      "(Epoch 3 / 20) train acc: 0.086000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.296295\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.301893\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.298356\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.307141\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.306757\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.309523\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.294659\n",
      "(Epoch 10 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.308405\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.306518\n",
      "(Epoch 12 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302819\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.291632\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.297891\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.295656\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.315265\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302267\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.297866\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.314420\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.310036\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.305329\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.306162\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.300810\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.296285\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.303014\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.298737\n",
      "(Epoch 8 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.294793\n",
      "(Epoch 9 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.310917\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.306030\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.299482\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.316470\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.307221\n",
      "(Epoch 14 / 20) train acc: 0.142000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.305073\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301904\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.304921\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301154\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302587\n",
      "(Epoch 19 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.297608\n",
      "(Epoch 20 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.306571\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303382\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.303054\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.303181\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.308389\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.312293\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.296574\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.299186\n",
      "(Epoch 9 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.301362\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.303083\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.296856\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.309003\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.292636\n",
      "(Epoch 14 / 20) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.295642\n",
      "(Epoch 15 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.306368\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 161 / 200) loss: 2.301152\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.292887\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302256\n",
      "(Epoch 19 / 20) train acc: 0.129000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.294928\n",
      "(Epoch 20 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 204661.422222\n",
      "(Epoch 0 / 20) train acc: 0.136000; val_acc: 0.136111\n",
      "(Epoch 1 / 20) train acc: 0.593000; val_acc: 0.558333\n",
      "(Iteration 11 / 200) loss: 27296.963623\n",
      "(Epoch 2 / 20) train acc: 0.733000; val_acc: 0.722222\n",
      "(Iteration 21 / 200) loss: 6985.733799\n",
      "(Epoch 3 / 20) train acc: 0.818000; val_acc: 0.786111\n",
      "(Iteration 31 / 200) loss: 3860.599565\n",
      "(Epoch 4 / 20) train acc: 0.878000; val_acc: 0.813889\n",
      "(Iteration 41 / 200) loss: 2496.952844\n",
      "(Epoch 5 / 20) train acc: 0.916000; val_acc: 0.847222\n",
      "(Iteration 51 / 200) loss: 1012.455439\n",
      "(Epoch 6 / 20) train acc: 0.944000; val_acc: 0.872222\n",
      "(Iteration 61 / 200) loss: 805.805679\n",
      "(Epoch 7 / 20) train acc: 0.961000; val_acc: 0.866667\n",
      "(Iteration 71 / 200) loss: 1397.803962\n",
      "(Epoch 8 / 20) train acc: 0.973000; val_acc: 0.869444\n",
      "(Iteration 81 / 200) loss: 162.720762\n",
      "(Epoch 9 / 20) train acc: 0.976000; val_acc: 0.891667\n",
      "(Iteration 91 / 200) loss: 363.261152\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.897222\n",
      "(Iteration 101 / 200) loss: 220.339341\n",
      "(Epoch 11 / 20) train acc: 0.976000; val_acc: 0.888889\n",
      "(Iteration 111 / 200) loss: 376.631338\n",
      "(Epoch 12 / 20) train acc: 0.982000; val_acc: 0.897222\n",
      "(Iteration 121 / 200) loss: 209.124702\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.894444\n",
      "(Iteration 131 / 200) loss: 159.542666\n",
      "(Epoch 14 / 20) train acc: 0.996000; val_acc: 0.897222\n",
      "(Iteration 141 / 200) loss: 159.122651\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.897222\n",
      "(Iteration 151 / 200) loss: 158.745369\n",
      "(Epoch 16 / 20) train acc: 0.989000; val_acc: 0.894444\n",
      "(Iteration 161 / 200) loss: 248.967129\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.900000\n",
      "(Iteration 171 / 200) loss: 158.092246\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.897222\n",
      "(Iteration 181 / 200) loss: 157.809534\n",
      "(Epoch 19 / 20) train acc: 0.998000; val_acc: 0.888889\n",
      "(Iteration 191 / 200) loss: 157.548569\n",
      "(Epoch 20 / 20) train acc: 0.997000; val_acc: 0.891667\n",
      "(Iteration 1 / 200) loss: 4.769812\n",
      "(Epoch 0 / 20) train acc: 0.164000; val_acc: 0.155556\n",
      "(Epoch 1 / 20) train acc: 0.719000; val_acc: 0.730556\n",
      "(Iteration 11 / 200) loss: 2.263351\n",
      "(Epoch 2 / 20) train acc: 0.867000; val_acc: 0.847222\n",
      "(Iteration 21 / 200) loss: 1.847410\n",
      "(Epoch 3 / 20) train acc: 0.958000; val_acc: 0.933333\n",
      "(Iteration 31 / 200) loss: 1.388832\n",
      "(Epoch 4 / 20) train acc: 0.945000; val_acc: 0.916667\n",
      "(Iteration 41 / 200) loss: 1.132148\n",
      "(Epoch 5 / 20) train acc: 0.965000; val_acc: 0.936111\n",
      "(Iteration 51 / 200) loss: 1.054169\n",
      "(Epoch 6 / 20) train acc: 0.983000; val_acc: 0.966667\n",
      "(Iteration 61 / 200) loss: 0.848883\n",
      "(Epoch 7 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.725716\n",
      "(Epoch 8 / 20) train acc: 0.970000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.700603\n",
      "(Epoch 9 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 91 / 200) loss: 0.582958\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.558665\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.526835\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.952778\n",
      "(Iteration 121 / 200) loss: 0.495830\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.462850\n",
      "(Epoch 14 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.479635\n",
      "(Epoch 15 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.397202\n",
      "(Epoch 16 / 20) train acc: 0.988000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.476973\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.406245\n",
      "(Epoch 18 / 20) train acc: 0.980000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.400282\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.330852\n",
      "(Epoch 20 / 20) train acc: 0.984000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.320833\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.308700\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.307462\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.304274\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.308154\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.304317\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.299855\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.305732\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.306181\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.293094\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.298181\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.304548\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.291480\n",
      "(Epoch 13 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.309400\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.291629\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.307009\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.313025\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.299483\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.295594\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.294399\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302769\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302210\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.301778\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.301797\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.297950\n",
      "(Epoch 5 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.292387\n",
      "(Epoch 6 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.301619\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.300714\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.304862\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.298632\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.303586\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.307634\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.308169\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.299877\n",
      "(Epoch 14 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.296222\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.307174\n",
      "(Epoch 16 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.312462\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301635\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.292948\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.306245\n",
      "(Epoch 20 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.304598\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302916\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301894\n",
      "(Epoch 4 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.296117\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.298610\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.309924\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.298348\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302993\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.294288\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302881\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.306061\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.296316\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.303458\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.299148\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.309231\n",
      "(Epoch 16 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.304827\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.305020\n",
      "(Epoch 18 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.303502\n",
      "(Epoch 19 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.293391\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.309847\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.301794\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.299823\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.305169\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.298734\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.299973\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.307288\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.310649\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.313477\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.298052\n",
      "(Epoch 11 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.295963\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302161\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302118\n",
      "(Epoch 14 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.299462\n",
      "(Epoch 15 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.306564\n",
      "(Epoch 16 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.304816\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301582\n",
      "(Epoch 18 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Iteration 181 / 200) loss: 2.299113\n",
      "(Epoch 19 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301216\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 201605.800479\n",
      "(Epoch 0 / 20) train acc: 0.070000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.576000; val_acc: 0.577778\n",
      "(Iteration 11 / 200) loss: 21046.800342\n",
      "(Epoch 2 / 20) train acc: 0.774000; val_acc: 0.755556\n",
      "(Iteration 21 / 200) loss: 9320.327482\n",
      "(Epoch 3 / 20) train acc: 0.892000; val_acc: 0.833333\n",
      "(Iteration 31 / 200) loss: 3651.053829\n",
      "(Epoch 4 / 20) train acc: 0.909000; val_acc: 0.886111\n",
      "(Iteration 41 / 200) loss: 1320.180995\n",
      "(Epoch 5 / 20) train acc: 0.942000; val_acc: 0.905556\n",
      "(Iteration 51 / 200) loss: 1616.005819\n",
      "(Epoch 6 / 20) train acc: 0.946000; val_acc: 0.902778\n",
      "(Iteration 61 / 200) loss: 342.389439\n",
      "(Epoch 7 / 20) train acc: 0.954000; val_acc: 0.905556\n",
      "(Iteration 71 / 200) loss: 667.774198\n",
      "(Epoch 8 / 20) train acc: 0.970000; val_acc: 0.925000\n",
      "(Iteration 81 / 200) loss: 16.561764\n",
      "(Epoch 9 / 20) train acc: 0.977000; val_acc: 0.925000\n",
      "(Iteration 91 / 200) loss: 153.223495\n",
      "(Epoch 10 / 20) train acc: 0.974000; val_acc: 0.919444\n",
      "(Iteration 101 / 200) loss: 256.711553\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.941667\n",
      "(Iteration 111 / 200) loss: 123.051689\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.936111\n",
      "(Iteration 121 / 200) loss: 16.328543\n",
      "(Epoch 13 / 20) train acc: 0.997000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 119.167536\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.930556\n",
      "(Iteration 141 / 200) loss: 16.260320\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.911111\n",
      "(Iteration 151 / 200) loss: 64.027545\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.925000\n",
      "(Iteration 161 / 200) loss: 178.736088\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.919444\n",
      "(Iteration 171 / 200) loss: 16.188480\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.913889\n",
      "(Iteration 181 / 200) loss: 202.900769\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.922222\n",
      "(Iteration 191 / 200) loss: 119.381747\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.911111\n",
      "(Iteration 1 / 200) loss: 3.833251\n",
      "(Epoch 0 / 20) train acc: 0.203000; val_acc: 0.180556\n",
      "(Epoch 1 / 20) train acc: 0.798000; val_acc: 0.816667\n",
      "(Iteration 11 / 200) loss: 0.623708\n",
      "(Epoch 2 / 20) train acc: 0.880000; val_acc: 0.897222\n",
      "(Iteration 21 / 200) loss: 0.497047\n",
      "(Epoch 3 / 20) train acc: 0.939000; val_acc: 0.913889\n",
      "(Iteration 31 / 200) loss: 0.307224\n",
      "(Epoch 4 / 20) train acc: 0.980000; val_acc: 0.958333\n",
      "(Iteration 41 / 200) loss: 0.269182\n",
      "(Epoch 5 / 20) train acc: 0.978000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.203252\n",
      "(Epoch 6 / 20) train acc: 0.975000; val_acc: 0.958333\n",
      "(Iteration 61 / 200) loss: 0.263877\n",
      "(Epoch 7 / 20) train acc: 0.985000; val_acc: 0.966667\n",
      "(Iteration 71 / 200) loss: 0.186751\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.938889\n",
      "(Iteration 81 / 200) loss: 0.162332\n",
      "(Epoch 9 / 20) train acc: 0.988000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.142483\n",
      "(Epoch 10 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.136907\n",
      "(Epoch 11 / 20) train acc: 0.977000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 0.203425\n",
      "(Epoch 12 / 20) train acc: 0.983000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 0.170304\n",
      "(Epoch 13 / 20) train acc: 0.983000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.147322\n",
      "(Epoch 14 / 20) train acc: 0.967000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.299696\n",
      "(Epoch 15 / 20) train acc: 0.984000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.152718\n",
      "(Epoch 16 / 20) train acc: 0.988000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.161892\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.950000\n",
      "(Iteration 171 / 200) loss: 0.150568\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 181 / 200) loss: 0.139444\n",
      "(Epoch 19 / 20) train acc: 0.984000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.226433\n",
      "(Epoch 20 / 20) train acc: 0.979000; val_acc: 0.941667\n",
      "(Iteration 1 / 200) loss: 2.304403\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.258690\n",
      "(Epoch 2 / 20) train acc: 0.148000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 2.101553\n",
      "(Epoch 3 / 20) train acc: 0.220000; val_acc: 0.194444\n",
      "(Iteration 31 / 200) loss: 2.043722\n",
      "(Epoch 4 / 20) train acc: 0.228000; val_acc: 0.194444\n",
      "(Iteration 41 / 200) loss: 2.118790\n",
      "(Epoch 5 / 20) train acc: 0.320000; val_acc: 0.266667\n",
      "(Iteration 51 / 200) loss: 1.648818\n",
      "(Epoch 6 / 20) train acc: 0.294000; val_acc: 0.280556\n",
      "(Iteration 61 / 200) loss: 1.597910\n",
      "(Epoch 7 / 20) train acc: 0.441000; val_acc: 0.400000\n",
      "(Iteration 71 / 200) loss: 1.225458\n",
      "(Epoch 8 / 20) train acc: 0.557000; val_acc: 0.516667\n",
      "(Iteration 81 / 200) loss: 1.862270\n",
      "(Epoch 9 / 20) train acc: 0.594000; val_acc: 0.555556\n",
      "(Iteration 91 / 200) loss: 0.809416\n",
      "(Epoch 10 / 20) train acc: 0.654000; val_acc: 0.627778\n",
      "(Iteration 101 / 200) loss: 0.792208\n",
      "(Epoch 11 / 20) train acc: 0.781000; val_acc: 0.744444\n",
      "(Iteration 111 / 200) loss: 0.692683\n",
      "(Epoch 12 / 20) train acc: 0.834000; val_acc: 0.769444\n",
      "(Iteration 121 / 200) loss: 0.548551\n",
      "(Epoch 13 / 20) train acc: 0.881000; val_acc: 0.883333\n",
      "(Iteration 131 / 200) loss: 0.403196\n",
      "(Epoch 14 / 20) train acc: 0.865000; val_acc: 0.805556\n",
      "(Iteration 141 / 200) loss: 0.486182\n",
      "(Epoch 15 / 20) train acc: 0.938000; val_acc: 0.894444\n",
      "(Iteration 151 / 200) loss: 0.222493\n",
      "(Epoch 16 / 20) train acc: 0.968000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 0.285003\n",
      "(Epoch 17 / 20) train acc: 0.938000; val_acc: 0.880556\n",
      "(Iteration 171 / 200) loss: 0.230783\n",
      "(Epoch 18 / 20) train acc: 0.970000; val_acc: 0.936111\n",
      "(Iteration 181 / 200) loss: 0.178981\n",
      "(Epoch 19 / 20) train acc: 0.956000; val_acc: 0.950000\n",
      "(Iteration 191 / 200) loss: 0.155703\n",
      "(Epoch 20 / 20) train acc: 0.980000; val_acc: 0.933333\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.301047\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.307553\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.298522\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.304548\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.304721\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302977\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.299037\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.300277\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302202\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302608\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.306604\n",
      "(Epoch 12 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.312270\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.297964\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.297856\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.298080\n",
      "(Epoch 16 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.296027\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.304580\n",
      "(Epoch 18 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 181 / 200) loss: 2.293145\n",
      "(Epoch 19 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 191 / 200) loss: 2.313837\n",
      "(Epoch 20 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.299841\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.304433\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.311144\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.310592\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.312788\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.310687\n",
      "(Epoch 7 / 20) train acc: 0.085000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.303147\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.309917\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.301835\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.309266\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.305486\n",
      "(Epoch 12 / 20) train acc: 0.093000; val_acc: 0.086111\n",
      "(Iteration 121 / 200) loss: 2.303599\n",
      "(Epoch 13 / 20) train acc: 0.088000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 2.293716\n",
      "(Epoch 14 / 20) train acc: 0.219000; val_acc: 0.147222\n",
      "(Iteration 141 / 200) loss: 2.259270\n",
      "(Epoch 15 / 20) train acc: 0.171000; val_acc: 0.133333\n",
      "(Iteration 151 / 200) loss: 2.078056\n",
      "(Epoch 16 / 20) train acc: 0.187000; val_acc: 0.155556\n",
      "(Iteration 161 / 200) loss: 2.205108\n",
      "(Epoch 17 / 20) train acc: 0.209000; val_acc: 0.172222\n",
      "(Iteration 171 / 200) loss: 2.139489\n",
      "(Epoch 18 / 20) train acc: 0.238000; val_acc: 0.200000\n",
      "(Iteration 181 / 200) loss: 2.170958\n",
      "(Epoch 19 / 20) train acc: 0.215000; val_acc: 0.197222\n",
      "(Iteration 191 / 200) loss: 2.097225\n",
      "(Epoch 20 / 20) train acc: 0.205000; val_acc: 0.163889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.305006\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302051\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.307278\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.301465\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.307501\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.290124\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.304384\n",
      "(Epoch 8 / 20) train acc: 0.081000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.303642\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.300524\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.304627\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.300330\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.305602\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.313004\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.304535\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.308758\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.306195\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.301822\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.309467\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.291537\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 170811.694243\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.683000; val_acc: 0.663889\n",
      "(Iteration 11 / 200) loss: 13488.314424\n",
      "(Epoch 2 / 20) train acc: 0.826000; val_acc: 0.819444\n",
      "(Iteration 21 / 200) loss: 4611.625425\n",
      "(Epoch 3 / 20) train acc: 0.896000; val_acc: 0.861111\n",
      "(Iteration 31 / 200) loss: 1422.348419\n",
      "(Epoch 4 / 20) train acc: 0.931000; val_acc: 0.877778\n",
      "(Iteration 41 / 200) loss: 2580.659591\n",
      "(Epoch 5 / 20) train acc: 0.940000; val_acc: 0.883333\n",
      "(Iteration 51 / 200) loss: 2246.568643\n",
      "(Epoch 6 / 20) train acc: 0.955000; val_acc: 0.916667\n",
      "(Iteration 61 / 200) loss: 1333.971998\n",
      "(Epoch 7 / 20) train acc: 0.965000; val_acc: 0.900000\n",
      "(Iteration 71 / 200) loss: 659.117395\n",
      "(Epoch 8 / 20) train acc: 0.975000; val_acc: 0.913889\n",
      "(Iteration 81 / 200) loss: 293.573181\n",
      "(Epoch 9 / 20) train acc: 0.974000; val_acc: 0.886111\n",
      "(Iteration 91 / 200) loss: 102.840145\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.938889\n",
      "(Iteration 101 / 200) loss: 347.515141\n",
      "(Epoch 11 / 20) train acc: 0.987000; val_acc: 0.913889\n",
      "(Iteration 111 / 200) loss: 65.733088\n",
      "(Epoch 12 / 20) train acc: 0.989000; val_acc: 0.927778\n",
      "(Iteration 121 / 200) loss: 16.794564\n",
      "(Epoch 13 / 20) train acc: 0.985000; val_acc: 0.919444\n",
      "(Iteration 131 / 200) loss: 240.670271\n",
      "(Epoch 14 / 20) train acc: 0.991000; val_acc: 0.930556\n",
      "(Iteration 141 / 200) loss: 315.816936\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.925000\n",
      "(Iteration 151 / 200) loss: 16.711168\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 310.268277\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 16.675132\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.927778\n",
      "(Iteration 181 / 200) loss: 145.057292\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.930556\n",
      "(Iteration 191 / 200) loss: 16.647417\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.925000\n",
      "(Iteration 1 / 200) loss: 2.681988\n",
      "(Epoch 0 / 20) train acc: 0.284000; val_acc: 0.297222\n",
      "(Epoch 1 / 20) train acc: 0.792000; val_acc: 0.766667\n",
      "(Iteration 11 / 200) loss: 0.743679\n",
      "(Epoch 2 / 20) train acc: 0.909000; val_acc: 0.897222\n",
      "(Iteration 21 / 200) loss: 0.382783\n",
      "(Epoch 3 / 20) train acc: 0.961000; val_acc: 0.955556\n",
      "(Iteration 31 / 200) loss: 0.315210\n",
      "(Epoch 4 / 20) train acc: 0.971000; val_acc: 0.944444\n",
      "(Iteration 41 / 200) loss: 0.283998\n",
      "(Epoch 5 / 20) train acc: 0.974000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.178084\n",
      "(Epoch 6 / 20) train acc: 0.980000; val_acc: 0.977778\n",
      "(Iteration 61 / 200) loss: 0.185550\n",
      "(Epoch 7 / 20) train acc: 0.963000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.244492\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.241408\n",
      "(Epoch 9 / 20) train acc: 0.962000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.438655\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.184067\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.183992\n",
      "(Epoch 12 / 20) train acc: 0.966000; val_acc: 0.955556\n",
      "(Iteration 121 / 200) loss: 0.212327\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.146119\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.133092\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 151 / 200) loss: 0.130229\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.122495\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.114482\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.117193\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.111829\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 1 / 200) loss: 2.304420\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.291000\n",
      "(Epoch 2 / 20) train acc: 0.126000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.157470\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 1.996239\n",
      "(Epoch 4 / 20) train acc: 0.190000; val_acc: 0.180556\n",
      "(Iteration 41 / 200) loss: 1.834773\n",
      "(Epoch 5 / 20) train acc: 0.417000; val_acc: 0.369444\n",
      "(Iteration 51 / 200) loss: 1.710493\n",
      "(Epoch 6 / 20) train acc: 0.372000; val_acc: 0.322222\n",
      "(Iteration 61 / 200) loss: 1.724574\n",
      "(Epoch 7 / 20) train acc: 0.400000; val_acc: 0.377778\n",
      "(Iteration 71 / 200) loss: 1.423327\n",
      "(Epoch 8 / 20) train acc: 0.639000; val_acc: 0.650000\n",
      "(Iteration 81 / 200) loss: 1.091097\n",
      "(Epoch 9 / 20) train acc: 0.633000; val_acc: 0.588889\n",
      "(Iteration 91 / 200) loss: 0.780699\n",
      "(Epoch 10 / 20) train acc: 0.733000; val_acc: 0.708333\n",
      "(Iteration 101 / 200) loss: 0.677819\n",
      "(Epoch 11 / 20) train acc: 0.876000; val_acc: 0.866667\n",
      "(Iteration 111 / 200) loss: 0.310324\n",
      "(Epoch 12 / 20) train acc: 0.911000; val_acc: 0.875000\n",
      "(Iteration 121 / 200) loss: 0.274960\n",
      "(Epoch 13 / 20) train acc: 0.927000; val_acc: 0.911111\n",
      "(Iteration 131 / 200) loss: 0.285371\n",
      "(Epoch 14 / 20) train acc: 0.955000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 0.218761\n",
      "(Epoch 15 / 20) train acc: 0.956000; val_acc: 0.930556\n",
      "(Iteration 151 / 200) loss: 0.356882\n",
      "(Epoch 16 / 20) train acc: 0.953000; val_acc: 0.938889\n",
      "(Iteration 161 / 200) loss: 0.175842\n",
      "(Epoch 17 / 20) train acc: 0.964000; val_acc: 0.941667\n",
      "(Iteration 171 / 200) loss: 0.132313\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 181 / 200) loss: 0.229612\n",
      "(Epoch 19 / 20) train acc: 0.953000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.111708\n",
      "(Epoch 20 / 20) train acc: 0.948000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.300272\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.305581\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.304245\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.297903\n",
      "(Epoch 5 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.300631\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.294805\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.304805\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.312516\n",
      "(Epoch 9 / 20) train acc: 0.128000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.295485\n",
      "(Epoch 10 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.301258\n",
      "(Epoch 11 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302776\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.313936\n",
      "(Epoch 13 / 20) train acc: 0.128000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.307931\n",
      "(Epoch 14 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301290\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.308182\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.304826\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.301733\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.300942\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.308773\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.298843\n",
      "(Epoch 2 / 20) train acc: 0.234000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 2.111655\n",
      "(Epoch 3 / 20) train acc: 0.239000; val_acc: 0.247222\n",
      "(Iteration 31 / 200) loss: 1.837460\n",
      "(Epoch 4 / 20) train acc: 0.208000; val_acc: 0.202778\n",
      "(Iteration 41 / 200) loss: 1.850053\n",
      "(Epoch 5 / 20) train acc: 0.283000; val_acc: 0.277778\n",
      "(Iteration 51 / 200) loss: 1.853745\n",
      "(Epoch 6 / 20) train acc: 0.315000; val_acc: 0.294444\n",
      "(Iteration 61 / 200) loss: 1.876741\n",
      "(Epoch 7 / 20) train acc: 0.322000; val_acc: 0.294444\n",
      "(Iteration 71 / 200) loss: 1.833000\n",
      "(Epoch 8 / 20) train acc: 0.297000; val_acc: 0.286111\n",
      "(Iteration 81 / 200) loss: 1.813453\n",
      "(Epoch 9 / 20) train acc: 0.275000; val_acc: 0.291667\n",
      "(Iteration 91 / 200) loss: 1.633686\n",
      "(Epoch 10 / 20) train acc: 0.393000; val_acc: 0.383333\n",
      "(Iteration 101 / 200) loss: 1.338359\n",
      "(Epoch 11 / 20) train acc: 0.462000; val_acc: 0.450000\n",
      "(Iteration 111 / 200) loss: 1.286251\n",
      "(Epoch 12 / 20) train acc: 0.476000; val_acc: 0.469444\n",
      "(Iteration 121 / 200) loss: 1.198480\n",
      "(Epoch 13 / 20) train acc: 0.488000; val_acc: 0.466667\n",
      "(Iteration 131 / 200) loss: 1.138486\n",
      "(Epoch 14 / 20) train acc: 0.552000; val_acc: 0.497222\n",
      "(Iteration 141 / 200) loss: 1.120064\n",
      "(Epoch 15 / 20) train acc: 0.662000; val_acc: 0.655556\n",
      "(Iteration 151 / 200) loss: 0.958504\n",
      "(Epoch 16 / 20) train acc: 0.672000; val_acc: 0.661111\n",
      "(Iteration 161 / 200) loss: 0.595378\n",
      "(Epoch 17 / 20) train acc: 0.784000; val_acc: 0.747222\n",
      "(Iteration 171 / 200) loss: 0.508763\n",
      "(Epoch 18 / 20) train acc: 0.852000; val_acc: 0.827778\n",
      "(Iteration 181 / 200) loss: 0.452135\n",
      "(Epoch 19 / 20) train acc: 0.760000; val_acc: 0.769444\n",
      "(Iteration 191 / 200) loss: 0.846759\n",
      "(Epoch 20 / 20) train acc: 0.821000; val_acc: 0.775000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.267498\n",
      "(Epoch 2 / 20) train acc: 0.200000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 2.073054\n",
      "(Epoch 3 / 20) train acc: 0.225000; val_acc: 0.163889\n",
      "(Iteration 31 / 200) loss: 2.075604\n",
      "(Epoch 4 / 20) train acc: 0.226000; val_acc: 0.163889\n",
      "(Iteration 41 / 200) loss: 2.053666\n",
      "(Epoch 5 / 20) train acc: 0.254000; val_acc: 0.166667\n",
      "(Iteration 51 / 200) loss: 2.091256\n",
      "(Epoch 6 / 20) train acc: 0.276000; val_acc: 0.250000\n",
      "(Iteration 61 / 200) loss: 2.015223\n",
      "(Epoch 7 / 20) train acc: 0.290000; val_acc: 0.241667\n",
      "(Iteration 71 / 200) loss: 1.801045\n",
      "(Epoch 8 / 20) train acc: 0.313000; val_acc: 0.222222\n",
      "(Iteration 81 / 200) loss: 1.688393\n",
      "(Epoch 9 / 20) train acc: 0.306000; val_acc: 0.302778\n",
      "(Iteration 91 / 200) loss: 1.695145\n",
      "(Epoch 10 / 20) train acc: 0.306000; val_acc: 0.266667\n",
      "(Iteration 101 / 200) loss: 1.762331\n",
      "(Epoch 11 / 20) train acc: 0.382000; val_acc: 0.350000\n",
      "(Iteration 111 / 200) loss: 1.659091\n",
      "(Epoch 12 / 20) train acc: 0.369000; val_acc: 0.316667\n",
      "(Iteration 121 / 200) loss: 1.627599\n",
      "(Epoch 13 / 20) train acc: 0.380000; val_acc: 0.355556\n",
      "(Iteration 131 / 200) loss: 1.459528\n",
      "(Epoch 14 / 20) train acc: 0.540000; val_acc: 0.541667\n",
      "(Iteration 141 / 200) loss: 1.333153\n",
      "(Epoch 15 / 20) train acc: 0.573000; val_acc: 0.550000\n",
      "(Iteration 151 / 200) loss: 1.231220\n",
      "(Epoch 16 / 20) train acc: 0.634000; val_acc: 0.594444\n",
      "(Iteration 161 / 200) loss: 1.067830\n",
      "(Epoch 17 / 20) train acc: 0.613000; val_acc: 0.605556\n",
      "(Iteration 171 / 200) loss: 0.916803\n",
      "(Epoch 18 / 20) train acc: 0.732000; val_acc: 0.694444\n",
      "(Iteration 181 / 200) loss: 0.900716\n",
      "(Epoch 19 / 20) train acc: 0.775000; val_acc: 0.780556\n",
      "(Iteration 191 / 200) loss: 0.625181\n",
      "(Epoch 20 / 20) train acc: 0.865000; val_acc: 0.805556\n",
      "(Iteration 1 / 200) loss: 141603.735393\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.102778\n",
      "(Epoch 1 / 20) train acc: 0.670000; val_acc: 0.627778\n",
      "(Iteration 11 / 200) loss: 11405.629346\n",
      "(Epoch 2 / 20) train acc: 0.809000; val_acc: 0.738889\n",
      "(Iteration 21 / 200) loss: 4292.038653\n",
      "(Epoch 3 / 20) train acc: 0.872000; val_acc: 0.797222\n",
      "(Iteration 31 / 200) loss: 2280.833853\n",
      "(Epoch 4 / 20) train acc: 0.898000; val_acc: 0.833333\n",
      "(Iteration 41 / 200) loss: 2093.501452\n",
      "(Epoch 5 / 20) train acc: 0.953000; val_acc: 0.858333\n",
      "(Iteration 51 / 200) loss: 202.081600\n",
      "(Epoch 6 / 20) train acc: 0.963000; val_acc: 0.877778\n",
      "(Iteration 61 / 200) loss: 518.508811\n",
      "(Epoch 7 / 20) train acc: 0.962000; val_acc: 0.875000\n",
      "(Iteration 71 / 200) loss: 311.671937\n",
      "(Epoch 8 / 20) train acc: 0.962000; val_acc: 0.894444\n",
      "(Iteration 81 / 200) loss: 457.463792\n",
      "(Epoch 9 / 20) train acc: 0.978000; val_acc: 0.888889\n",
      "(Iteration 91 / 200) loss: 109.881789\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.886111\n",
      "(Iteration 101 / 200) loss: 336.455374\n",
      "(Epoch 11 / 20) train acc: 0.971000; val_acc: 0.866667\n",
      "(Iteration 111 / 200) loss: 286.511901\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.891667\n",
      "(Iteration 121 / 200) loss: 150.167039\n",
      "(Epoch 13 / 20) train acc: 0.988000; val_acc: 0.888889\n",
      "(Iteration 131 / 200) loss: 254.709158\n",
      "(Epoch 14 / 20) train acc: 0.989000; val_acc: 0.883333\n",
      "(Iteration 141 / 200) loss: 95.616618\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.888889\n",
      "(Iteration 151 / 200) loss: 33.328964\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.905556\n",
      "(Iteration 161 / 200) loss: 108.872760\n",
      "(Epoch 17 / 20) train acc: 0.994000; val_acc: 0.902778\n",
      "(Iteration 171 / 200) loss: 15.548175\n",
      "(Epoch 18 / 20) train acc: 0.997000; val_acc: 0.925000\n",
      "(Iteration 181 / 200) loss: 57.001025\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.911111\n",
      "(Iteration 191 / 200) loss: 15.509587\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.902778\n",
      "(Iteration 1 / 200) loss: 3.226828\n",
      "(Epoch 0 / 20) train acc: 0.205000; val_acc: 0.188889\n",
      "(Epoch 1 / 20) train acc: 0.826000; val_acc: 0.750000\n",
      "(Iteration 11 / 200) loss: 0.750065\n",
      "(Epoch 2 / 20) train acc: 0.882000; val_acc: 0.836111\n",
      "(Iteration 21 / 200) loss: 0.629815\n",
      "(Epoch 3 / 20) train acc: 0.937000; val_acc: 0.922222\n",
      "(Iteration 31 / 200) loss: 0.336612\n",
      "(Epoch 4 / 20) train acc: 0.969000; val_acc: 0.961111\n",
      "(Iteration 41 / 200) loss: 0.271952\n",
      "(Epoch 5 / 20) train acc: 0.964000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.222304\n",
      "(Epoch 6 / 20) train acc: 0.964000; val_acc: 0.919444\n",
      "(Iteration 61 / 200) loss: 0.186820\n",
      "(Epoch 7 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 71 / 200) loss: 0.307662\n",
      "(Epoch 8 / 20) train acc: 0.984000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.183645\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.144639\n",
      "(Epoch 10 / 20) train acc: 0.993000; val_acc: 0.944444\n",
      "(Iteration 101 / 200) loss: 0.194012\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.140529\n",
      "(Epoch 12 / 20) train acc: 0.979000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.313494\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 131 / 200) loss: 0.137915\n",
      "(Epoch 14 / 20) train acc: 0.994000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.128339\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.141167\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.128089\n",
      "(Epoch 17 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.113992\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.933333\n",
      "(Iteration 181 / 200) loss: 0.125645\n",
      "(Epoch 19 / 20) train acc: 0.976000; val_acc: 0.966667\n",
      "(Iteration 191 / 200) loss: 0.180791\n",
      "(Epoch 20 / 20) train acc: 0.989000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 2.304401\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.165000; val_acc: 0.177778\n",
      "(Iteration 11 / 200) loss: 2.241155\n",
      "(Epoch 2 / 20) train acc: 0.176000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 2.126870\n",
      "(Epoch 3 / 20) train acc: 0.214000; val_acc: 0.213889\n",
      "(Iteration 31 / 200) loss: 2.029198\n",
      "(Epoch 4 / 20) train acc: 0.244000; val_acc: 0.244444\n",
      "(Iteration 41 / 200) loss: 1.789132\n",
      "(Epoch 5 / 20) train acc: 0.354000; val_acc: 0.366667\n",
      "(Iteration 51 / 200) loss: 1.489058\n",
      "(Epoch 6 / 20) train acc: 0.508000; val_acc: 0.505556\n",
      "(Iteration 61 / 200) loss: 1.280138\n",
      "(Epoch 7 / 20) train acc: 0.528000; val_acc: 0.505556\n",
      "(Iteration 71 / 200) loss: 1.034805\n",
      "(Epoch 8 / 20) train acc: 0.717000; val_acc: 0.697222\n",
      "(Iteration 81 / 200) loss: 0.832058\n",
      "(Epoch 9 / 20) train acc: 0.731000; val_acc: 0.700000\n",
      "(Iteration 91 / 200) loss: 0.685538\n",
      "(Epoch 10 / 20) train acc: 0.870000; val_acc: 0.847222\n",
      "(Iteration 101 / 200) loss: 0.576361\n",
      "(Epoch 11 / 20) train acc: 0.858000; val_acc: 0.836111\n",
      "(Iteration 111 / 200) loss: 0.349635\n",
      "(Epoch 12 / 20) train acc: 0.875000; val_acc: 0.852778\n",
      "(Iteration 121 / 200) loss: 0.419548\n",
      "(Epoch 13 / 20) train acc: 0.870000; val_acc: 0.861111\n",
      "(Iteration 131 / 200) loss: 0.468127\n",
      "(Epoch 14 / 20) train acc: 0.946000; val_acc: 0.905556\n",
      "(Iteration 141 / 200) loss: 0.210536\n",
      "(Epoch 15 / 20) train acc: 0.929000; val_acc: 0.894444\n",
      "(Iteration 151 / 200) loss: 0.307343\n",
      "(Epoch 16 / 20) train acc: 0.962000; val_acc: 0.933333\n",
      "(Iteration 161 / 200) loss: 0.287680\n",
      "(Epoch 17 / 20) train acc: 0.971000; val_acc: 0.930556\n",
      "(Iteration 171 / 200) loss: 0.173079\n",
      "(Epoch 18 / 20) train acc: 0.976000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.075250\n",
      "(Epoch 19 / 20) train acc: 0.980000; val_acc: 0.952778\n",
      "(Iteration 191 / 200) loss: 0.087250\n",
      "(Epoch 20 / 20) train acc: 0.978000; val_acc: 0.963889\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.303462\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.248585\n",
      "(Epoch 3 / 20) train acc: 0.240000; val_acc: 0.200000\n",
      "(Iteration 31 / 200) loss: 2.117388\n",
      "(Epoch 4 / 20) train acc: 0.226000; val_acc: 0.186111\n",
      "(Iteration 41 / 200) loss: 1.953141\n",
      "(Epoch 5 / 20) train acc: 0.227000; val_acc: 0.213889\n",
      "(Iteration 51 / 200) loss: 1.835798\n",
      "(Epoch 6 / 20) train acc: 0.323000; val_acc: 0.288889\n",
      "(Iteration 61 / 200) loss: 1.829160\n",
      "(Epoch 7 / 20) train acc: 0.316000; val_acc: 0.302778\n",
      "(Iteration 71 / 200) loss: 1.847192\n",
      "(Epoch 8 / 20) train acc: 0.347000; val_acc: 0.336111\n",
      "(Iteration 81 / 200) loss: 1.707181\n",
      "(Epoch 9 / 20) train acc: 0.478000; val_acc: 0.422222\n",
      "(Iteration 91 / 200) loss: 1.808453\n",
      "(Epoch 10 / 20) train acc: 0.489000; val_acc: 0.450000\n",
      "(Iteration 101 / 200) loss: 1.379724\n",
      "(Epoch 11 / 20) train acc: 0.514000; val_acc: 0.511111\n",
      "(Iteration 111 / 200) loss: 1.206713\n",
      "(Epoch 12 / 20) train acc: 0.622000; val_acc: 0.588889\n",
      "(Iteration 121 / 200) loss: 0.899477\n",
      "(Epoch 13 / 20) train acc: 0.665000; val_acc: 0.655556\n",
      "(Iteration 131 / 200) loss: 1.139743\n",
      "(Epoch 14 / 20) train acc: 0.716000; val_acc: 0.697222\n",
      "(Iteration 141 / 200) loss: 0.844946\n",
      "(Epoch 15 / 20) train acc: 0.795000; val_acc: 0.780556\n",
      "(Iteration 151 / 200) loss: 0.693358\n",
      "(Epoch 16 / 20) train acc: 0.834000; val_acc: 0.811111\n",
      "(Iteration 161 / 200) loss: 0.624292\n",
      "(Epoch 17 / 20) train acc: 0.850000; val_acc: 0.830556\n",
      "(Iteration 171 / 200) loss: 0.630919\n",
      "(Epoch 18 / 20) train acc: 0.922000; val_acc: 0.900000\n",
      "(Iteration 181 / 200) loss: 0.583092\n",
      "(Epoch 19 / 20) train acc: 0.909000; val_acc: 0.888889\n",
      "(Iteration 191 / 200) loss: 0.338354\n",
      "(Epoch 20 / 20) train acc: 0.952000; val_acc: 0.936111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.314405\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.299784\n",
      "(Epoch 3 / 20) train acc: 0.129000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.304319\n",
      "(Epoch 4 / 20) train acc: 0.127000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.301073\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.298498\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.306204\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.304404\n",
      "(Epoch 8 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.305681\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.307710\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.300386\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.301814\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.301834\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.307983\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.305933\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.305073\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.293717\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.294601\n",
      "(Epoch 18 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.298445\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.305390\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.300558\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.301475\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.311739\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.307226\n",
      "(Epoch 5 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.312322\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.307696\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.310139\n",
      "(Epoch 8 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.308435\n",
      "(Epoch 9 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.316367\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.312144\n",
      "(Epoch 11 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.307909\n",
      "(Epoch 12 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.301110\n",
      "(Epoch 13 / 20) train acc: 0.081000; val_acc: 0.086111\n",
      "(Iteration 131 / 200) loss: 2.307976\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.300149\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.311074\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.303606\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.301806\n",
      "(Epoch 18 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.311325\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301512\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 297372.488694\n",
      "(Epoch 0 / 20) train acc: 0.150000; val_acc: 0.161111\n",
      "(Epoch 1 / 20) train acc: 0.463000; val_acc: 0.444444\n",
      "(Iteration 11 / 200) loss: 22412.041266\n",
      "(Epoch 2 / 20) train acc: 0.729000; val_acc: 0.733333\n",
      "(Iteration 21 / 200) loss: 8941.021863\n",
      "(Epoch 3 / 20) train acc: 0.826000; val_acc: 0.777778\n",
      "(Iteration 31 / 200) loss: 3792.191202\n",
      "(Epoch 4 / 20) train acc: 0.896000; val_acc: 0.844444\n",
      "(Iteration 41 / 200) loss: 2128.973041\n",
      "(Epoch 5 / 20) train acc: 0.928000; val_acc: 0.872222\n",
      "(Iteration 51 / 200) loss: 975.081135\n",
      "(Epoch 6 / 20) train acc: 0.948000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 1765.718553\n",
      "(Epoch 7 / 20) train acc: 0.945000; val_acc: 0.872222\n",
      "(Iteration 71 / 200) loss: 410.582253\n",
      "(Epoch 8 / 20) train acc: 0.965000; val_acc: 0.875000\n",
      "(Iteration 81 / 200) loss: 257.570326\n",
      "(Epoch 9 / 20) train acc: 0.968000; val_acc: 0.877778\n",
      "(Iteration 91 / 200) loss: 1.671289\n",
      "(Epoch 10 / 20) train acc: 0.969000; val_acc: 0.886111\n",
      "(Iteration 101 / 200) loss: 134.675614\n",
      "(Epoch 11 / 20) train acc: 0.979000; val_acc: 0.886111\n",
      "(Iteration 111 / 200) loss: 49.822434\n",
      "(Epoch 12 / 20) train acc: 0.982000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 48.059912\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.902778\n",
      "(Iteration 131 / 200) loss: 74.877785\n",
      "(Epoch 14 / 20) train acc: 0.989000; val_acc: 0.891667\n",
      "(Iteration 141 / 200) loss: 3.343595\n",
      "(Epoch 15 / 20) train acc: 0.991000; val_acc: 0.875000\n",
      "(Iteration 151 / 200) loss: 60.801287\n",
      "(Epoch 16 / 20) train acc: 0.991000; val_acc: 0.897222\n",
      "(Iteration 161 / 200) loss: 31.488745\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 1.643871\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.886111\n",
      "(Iteration 181 / 200) loss: 1.642269\n",
      "(Epoch 19 / 20) train acc: 0.996000; val_acc: 0.886111\n",
      "(Iteration 191 / 200) loss: 1.640817\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.894444\n",
      "(Iteration 1 / 200) loss: 3.059445\n",
      "(Epoch 0 / 20) train acc: 0.178000; val_acc: 0.194444\n",
      "(Epoch 1 / 20) train acc: 0.761000; val_acc: 0.738889\n",
      "(Iteration 11 / 200) loss: 0.778025\n",
      "(Epoch 2 / 20) train acc: 0.869000; val_acc: 0.827778\n",
      "(Iteration 21 / 200) loss: 0.575862\n",
      "(Epoch 3 / 20) train acc: 0.944000; val_acc: 0.922222\n",
      "(Iteration 31 / 200) loss: 0.084042\n",
      "(Epoch 4 / 20) train acc: 0.972000; val_acc: 0.950000\n",
      "(Iteration 41 / 200) loss: 0.163304\n",
      "(Epoch 5 / 20) train acc: 0.985000; val_acc: 0.972222\n",
      "(Iteration 51 / 200) loss: 0.112549\n",
      "(Epoch 6 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 61 / 200) loss: 0.051727\n",
      "(Epoch 7 / 20) train acc: 0.972000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.066605\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.972222\n",
      "(Iteration 81 / 200) loss: 0.062985\n",
      "(Epoch 9 / 20) train acc: 0.982000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.112390\n",
      "(Epoch 10 / 20) train acc: 0.990000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.063874\n",
      "(Epoch 11 / 20) train acc: 0.974000; val_acc: 0.947222\n",
      "(Iteration 111 / 200) loss: 0.071910\n",
      "(Epoch 12 / 20) train acc: 0.960000; val_acc: 0.922222\n",
      "(Iteration 121 / 200) loss: 0.307685\n",
      "(Epoch 13 / 20) train acc: 0.957000; val_acc: 0.936111\n",
      "(Iteration 131 / 200) loss: 0.109657\n",
      "(Epoch 14 / 20) train acc: 0.986000; val_acc: 0.925000\n",
      "(Iteration 141 / 200) loss: 0.066177\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 151 / 200) loss: 0.036279\n",
      "(Epoch 16 / 20) train acc: 0.992000; val_acc: 0.947222\n",
      "(Iteration 161 / 200) loss: 0.022942\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.026993\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.955556\n",
      "(Iteration 181 / 200) loss: 0.059490\n",
      "(Epoch 19 / 20) train acc: 0.975000; val_acc: 0.941667\n",
      "(Iteration 191 / 200) loss: 0.083919\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302763\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.187000; val_acc: 0.172222\n",
      "(Iteration 11 / 200) loss: 2.248840\n",
      "(Epoch 2 / 20) train acc: 0.278000; val_acc: 0.225000\n",
      "(Iteration 21 / 200) loss: 2.004473\n",
      "(Epoch 3 / 20) train acc: 0.355000; val_acc: 0.327778\n",
      "(Iteration 31 / 200) loss: 1.506230\n",
      "(Epoch 4 / 20) train acc: 0.393000; val_acc: 0.388889\n",
      "(Iteration 41 / 200) loss: 1.224910\n",
      "(Epoch 5 / 20) train acc: 0.614000; val_acc: 0.605556\n",
      "(Iteration 51 / 200) loss: 1.084307\n",
      "(Epoch 6 / 20) train acc: 0.746000; val_acc: 0.730556\n",
      "(Iteration 61 / 200) loss: 0.634586\n",
      "(Epoch 7 / 20) train acc: 0.782000; val_acc: 0.766667\n",
      "(Iteration 71 / 200) loss: 0.613038\n",
      "(Epoch 8 / 20) train acc: 0.900000; val_acc: 0.897222\n",
      "(Iteration 81 / 200) loss: 0.359763\n",
      "(Epoch 9 / 20) train acc: 0.853000; val_acc: 0.852778\n",
      "(Iteration 91 / 200) loss: 0.311027\n",
      "(Epoch 10 / 20) train acc: 0.921000; val_acc: 0.913889\n",
      "(Iteration 101 / 200) loss: 0.377749\n",
      "(Epoch 11 / 20) train acc: 0.937000; val_acc: 0.888889\n",
      "(Iteration 111 / 200) loss: 0.304533\n",
      "(Epoch 12 / 20) train acc: 0.902000; val_acc: 0.888889\n",
      "(Iteration 121 / 200) loss: 0.215452\n",
      "(Epoch 13 / 20) train acc: 0.966000; val_acc: 0.941667\n",
      "(Iteration 131 / 200) loss: 0.080223\n",
      "(Epoch 14 / 20) train acc: 0.964000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 0.075459\n",
      "(Epoch 15 / 20) train acc: 0.970000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 0.074620\n",
      "(Epoch 16 / 20) train acc: 0.892000; val_acc: 0.850000\n",
      "(Iteration 161 / 200) loss: 0.354266\n",
      "(Epoch 17 / 20) train acc: 0.951000; val_acc: 0.919444\n",
      "(Iteration 171 / 200) loss: 0.134928\n",
      "(Epoch 18 / 20) train acc: 0.974000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 0.114873\n",
      "(Epoch 19 / 20) train acc: 0.966000; val_acc: 0.936111\n",
      "(Iteration 191 / 200) loss: 0.083903\n",
      "(Epoch 20 / 20) train acc: 0.986000; val_acc: 0.952778\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.230124\n",
      "(Epoch 2 / 20) train acc: 0.210000; val_acc: 0.202778\n",
      "(Iteration 21 / 200) loss: 1.905062\n",
      "(Epoch 3 / 20) train acc: 0.224000; val_acc: 0.222222\n",
      "(Iteration 31 / 200) loss: 1.934251\n",
      "(Epoch 4 / 20) train acc: 0.222000; val_acc: 0.205556\n",
      "(Iteration 41 / 200) loss: 1.761725\n",
      "(Epoch 5 / 20) train acc: 0.332000; val_acc: 0.302778\n",
      "(Iteration 51 / 200) loss: 1.708118\n",
      "(Epoch 6 / 20) train acc: 0.346000; val_acc: 0.252778\n",
      "(Iteration 61 / 200) loss: 1.664031\n",
      "(Epoch 7 / 20) train acc: 0.353000; val_acc: 0.283333\n",
      "(Iteration 71 / 200) loss: 1.687820\n",
      "(Epoch 8 / 20) train acc: 0.359000; val_acc: 0.291667\n",
      "(Iteration 81 / 200) loss: 1.875234\n",
      "(Epoch 9 / 20) train acc: 0.376000; val_acc: 0.330556\n",
      "(Iteration 91 / 200) loss: 1.467976\n",
      "(Epoch 10 / 20) train acc: 0.464000; val_acc: 0.450000\n",
      "(Iteration 101 / 200) loss: 1.470788\n",
      "(Epoch 11 / 20) train acc: 0.452000; val_acc: 0.438889\n",
      "(Iteration 111 / 200) loss: 1.313525\n",
      "(Epoch 12 / 20) train acc: 0.551000; val_acc: 0.516667\n",
      "(Iteration 121 / 200) loss: 1.079624\n",
      "(Epoch 13 / 20) train acc: 0.416000; val_acc: 0.391667\n",
      "(Iteration 131 / 200) loss: 1.162848\n",
      "(Epoch 14 / 20) train acc: 0.639000; val_acc: 0.536111\n",
      "(Iteration 141 / 200) loss: 1.101065\n",
      "(Epoch 15 / 20) train acc: 0.683000; val_acc: 0.630556\n",
      "(Iteration 151 / 200) loss: 0.846421\n",
      "(Epoch 16 / 20) train acc: 0.691000; val_acc: 0.655556\n",
      "(Iteration 161 / 200) loss: 0.660432\n",
      "(Epoch 17 / 20) train acc: 0.723000; val_acc: 0.700000\n",
      "(Iteration 171 / 200) loss: 0.543685\n",
      "(Epoch 18 / 20) train acc: 0.861000; val_acc: 0.813889\n",
      "(Iteration 181 / 200) loss: 0.614903\n",
      "(Epoch 19 / 20) train acc: 0.653000; val_acc: 0.633333\n",
      "(Iteration 191 / 200) loss: 0.679228\n",
      "(Epoch 20 / 20) train acc: 0.829000; val_acc: 0.780556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.282600\n",
      "(Epoch 2 / 20) train acc: 0.254000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 2.115015\n",
      "(Epoch 3 / 20) train acc: 0.193000; val_acc: 0.163889\n",
      "(Iteration 31 / 200) loss: 2.072386\n",
      "(Epoch 4 / 20) train acc: 0.212000; val_acc: 0.166667\n",
      "(Iteration 41 / 200) loss: 1.911339\n",
      "(Epoch 5 / 20) train acc: 0.248000; val_acc: 0.202778\n",
      "(Iteration 51 / 200) loss: 1.885254\n",
      "(Epoch 6 / 20) train acc: 0.310000; val_acc: 0.236111\n",
      "(Iteration 61 / 200) loss: 1.933035\n",
      "(Epoch 7 / 20) train acc: 0.302000; val_acc: 0.241667\n",
      "(Iteration 71 / 200) loss: 1.744698\n",
      "(Epoch 8 / 20) train acc: 0.362000; val_acc: 0.313889\n",
      "(Iteration 81 / 200) loss: 1.463002\n",
      "(Epoch 9 / 20) train acc: 0.451000; val_acc: 0.361111\n",
      "(Iteration 91 / 200) loss: 1.341739\n",
      "(Epoch 10 / 20) train acc: 0.453000; val_acc: 0.366667\n",
      "(Iteration 101 / 200) loss: 1.257784\n",
      "(Epoch 11 / 20) train acc: 0.404000; val_acc: 0.355556\n",
      "(Iteration 111 / 200) loss: 1.397653\n",
      "(Epoch 12 / 20) train acc: 0.555000; val_acc: 0.547222\n",
      "(Iteration 121 / 200) loss: 1.360100\n",
      "(Epoch 13 / 20) train acc: 0.657000; val_acc: 0.638889\n",
      "(Iteration 131 / 200) loss: 0.918255\n",
      "(Epoch 14 / 20) train acc: 0.732000; val_acc: 0.730556\n",
      "(Iteration 141 / 200) loss: 0.601186\n",
      "(Epoch 15 / 20) train acc: 0.846000; val_acc: 0.813889\n",
      "(Iteration 151 / 200) loss: 0.439305\n",
      "(Epoch 16 / 20) train acc: 0.881000; val_acc: 0.861111\n",
      "(Iteration 161 / 200) loss: 0.297183\n",
      "(Epoch 17 / 20) train acc: 0.906000; val_acc: 0.880556\n",
      "(Iteration 171 / 200) loss: 0.229256\n",
      "(Epoch 18 / 20) train acc: 0.918000; val_acc: 0.897222\n",
      "(Iteration 181 / 200) loss: 0.247707\n",
      "(Epoch 19 / 20) train acc: 0.959000; val_acc: 0.897222\n",
      "(Iteration 191 / 200) loss: 0.170872\n",
      "(Epoch 20 / 20) train acc: 0.952000; val_acc: 0.922222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.291456\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302978\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.303602\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.301987\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.309377\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.301083\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.307345\n",
      "(Epoch 8 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.301608\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.309737\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.310755\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.287870\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.306246\n",
      "(Epoch 13 / 20) train acc: 0.084000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.311660\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.301403\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302040\n",
      "(Epoch 16 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.293377\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.297268\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.303327\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.299379\n",
      "(Epoch 20 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 222463.738123\n",
      "(Epoch 0 / 20) train acc: 0.168000; val_acc: 0.141667\n",
      "(Epoch 1 / 20) train acc: 0.521000; val_acc: 0.475000\n",
      "(Iteration 11 / 200) loss: 26804.235237\n",
      "(Epoch 2 / 20) train acc: 0.720000; val_acc: 0.691667\n",
      "(Iteration 21 / 200) loss: 8291.247099\n",
      "(Epoch 3 / 20) train acc: 0.824000; val_acc: 0.775000\n",
      "(Iteration 31 / 200) loss: 5666.256228\n",
      "(Epoch 4 / 20) train acc: 0.889000; val_acc: 0.847222\n",
      "(Iteration 41 / 200) loss: 1067.588724\n",
      "(Epoch 5 / 20) train acc: 0.932000; val_acc: 0.880556\n",
      "(Iteration 51 / 200) loss: 1847.170405\n",
      "(Epoch 6 / 20) train acc: 0.946000; val_acc: 0.897222\n",
      "(Iteration 61 / 200) loss: 948.569975\n",
      "(Epoch 7 / 20) train acc: 0.964000; val_acc: 0.916667\n",
      "(Iteration 71 / 200) loss: 515.124849\n",
      "(Epoch 8 / 20) train acc: 0.978000; val_acc: 0.925000\n",
      "(Iteration 81 / 200) loss: 363.216834\n",
      "(Epoch 9 / 20) train acc: 0.979000; val_acc: 0.913889\n",
      "(Iteration 91 / 200) loss: 1.665242\n",
      "(Epoch 10 / 20) train acc: 0.976000; val_acc: 0.911111\n",
      "(Iteration 101 / 200) loss: 370.878260\n",
      "(Epoch 11 / 20) train acc: 0.989000; val_acc: 0.908333\n",
      "(Iteration 111 / 200) loss: 1.655327\n",
      "(Epoch 12 / 20) train acc: 0.988000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 32.861776\n",
      "(Epoch 13 / 20) train acc: 0.990000; val_acc: 0.916667\n",
      "(Iteration 131 / 200) loss: 60.008400\n",
      "(Epoch 14 / 20) train acc: 0.988000; val_acc: 0.900000\n",
      "(Iteration 141 / 200) loss: 1.645737\n",
      "(Epoch 15 / 20) train acc: 0.993000; val_acc: 0.911111\n",
      "(Iteration 151 / 200) loss: 98.759903\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 1.641647\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.927778\n",
      "(Iteration 171 / 200) loss: 1.640029\n",
      "(Epoch 18 / 20) train acc: 0.992000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 67.018461\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 31.793137\n",
      "(Epoch 20 / 20) train acc: 0.987000; val_acc: 0.916667\n",
      "(Iteration 1 / 200) loss: 4.435851\n",
      "(Epoch 0 / 20) train acc: 0.152000; val_acc: 0.175000\n",
      "(Epoch 1 / 20) train acc: 0.653000; val_acc: 0.644444\n",
      "(Iteration 11 / 200) loss: 1.071255\n",
      "(Epoch 2 / 20) train acc: 0.887000; val_acc: 0.880556\n",
      "(Iteration 21 / 200) loss: 0.303905\n",
      "(Epoch 3 / 20) train acc: 0.907000; val_acc: 0.891667\n",
      "(Iteration 31 / 200) loss: 0.352663\n",
      "(Epoch 4 / 20) train acc: 0.942000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.117128\n",
      "(Epoch 5 / 20) train acc: 0.961000; val_acc: 0.944444\n",
      "(Iteration 51 / 200) loss: 0.108357\n",
      "(Epoch 6 / 20) train acc: 0.974000; val_acc: 0.938889\n",
      "(Iteration 61 / 200) loss: 0.075483\n",
      "(Epoch 7 / 20) train acc: 0.985000; val_acc: 0.977778\n",
      "(Iteration 71 / 200) loss: 0.068046\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 0.039135\n",
      "(Epoch 9 / 20) train acc: 0.974000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.093572\n",
      "(Epoch 10 / 20) train acc: 0.989000; val_acc: 0.961111\n",
      "(Iteration 101 / 200) loss: 0.046321\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.030570\n",
      "(Epoch 12 / 20) train acc: 0.980000; val_acc: 0.952778\n",
      "(Iteration 121 / 200) loss: 0.052975\n",
      "(Epoch 13 / 20) train acc: 0.970000; val_acc: 0.944444\n",
      "(Iteration 131 / 200) loss: 0.049502\n",
      "(Epoch 14 / 20) train acc: 0.987000; val_acc: 0.947222\n",
      "(Iteration 141 / 200) loss: 0.094917\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.950000\n",
      "(Iteration 151 / 200) loss: 0.077786\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.936111\n",
      "(Iteration 161 / 200) loss: 0.030698\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.955556\n",
      "(Iteration 171 / 200) loss: 0.027330\n",
      "(Epoch 18 / 20) train acc: 0.991000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.035651\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.958333\n",
      "(Iteration 191 / 200) loss: 0.059370\n",
      "(Epoch 20 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302770\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.300779\n",
      "(Epoch 2 / 20) train acc: 0.225000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 2.063159\n",
      "(Epoch 3 / 20) train acc: 0.226000; val_acc: 0.202778\n",
      "(Iteration 31 / 200) loss: 1.966193\n",
      "(Epoch 4 / 20) train acc: 0.317000; val_acc: 0.272222\n",
      "(Iteration 41 / 200) loss: 1.894516\n",
      "(Epoch 5 / 20) train acc: 0.440000; val_acc: 0.447222\n",
      "(Iteration 51 / 200) loss: 1.480459\n",
      "(Epoch 6 / 20) train acc: 0.502000; val_acc: 0.552778\n",
      "(Iteration 61 / 200) loss: 1.039017\n",
      "(Epoch 7 / 20) train acc: 0.669000; val_acc: 0.725000\n",
      "(Iteration 71 / 200) loss: 0.874914\n",
      "(Epoch 8 / 20) train acc: 0.706000; val_acc: 0.719444\n",
      "(Iteration 81 / 200) loss: 0.783829\n",
      "(Epoch 9 / 20) train acc: 0.754000; val_acc: 0.766667\n",
      "(Iteration 91 / 200) loss: 0.749756\n",
      "(Epoch 10 / 20) train acc: 0.856000; val_acc: 0.855556\n",
      "(Iteration 101 / 200) loss: 0.417164\n",
      "(Epoch 11 / 20) train acc: 0.852000; val_acc: 0.852778\n",
      "(Iteration 111 / 200) loss: 0.609018\n",
      "(Epoch 12 / 20) train acc: 0.837000; val_acc: 0.841667\n",
      "(Iteration 121 / 200) loss: 0.303540\n",
      "(Epoch 13 / 20) train acc: 0.922000; val_acc: 0.888889\n",
      "(Iteration 131 / 200) loss: 0.258046\n",
      "(Epoch 14 / 20) train acc: 0.915000; val_acc: 0.911111\n",
      "(Iteration 141 / 200) loss: 0.151163\n",
      "(Epoch 15 / 20) train acc: 0.927000; val_acc: 0.900000\n",
      "(Iteration 151 / 200) loss: 0.256281\n",
      "(Epoch 16 / 20) train acc: 0.936000; val_acc: 0.930556\n",
      "(Iteration 161 / 200) loss: 0.078446\n",
      "(Epoch 17 / 20) train acc: 0.966000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 0.139114\n",
      "(Epoch 18 / 20) train acc: 0.959000; val_acc: 0.944444\n",
      "(Iteration 181 / 200) loss: 0.076249\n",
      "(Epoch 19 / 20) train acc: 0.957000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.188142\n",
      "(Epoch 20 / 20) train acc: 0.960000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.310124\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.303080\n",
      "(Epoch 3 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.306858\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.308962\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.303945\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302869\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.305631\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.305705\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.299714\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.300857\n",
      "(Epoch 11 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302812\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.298634\n",
      "(Epoch 13 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.298193\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.293834\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.304585\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.305373\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.299313\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.292089\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301244\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.298474\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.165119\n",
      "(Epoch 3 / 20) train acc: 0.210000; val_acc: 0.227778\n",
      "(Iteration 31 / 200) loss: 1.792372\n",
      "(Epoch 4 / 20) train acc: 0.200000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 1.792280\n",
      "(Epoch 5 / 20) train acc: 0.215000; val_acc: 0.213889\n",
      "(Iteration 51 / 200) loss: 1.640577\n",
      "(Epoch 6 / 20) train acc: 0.310000; val_acc: 0.313889\n",
      "(Iteration 61 / 200) loss: 1.589012\n",
      "(Epoch 7 / 20) train acc: 0.386000; val_acc: 0.361111\n",
      "(Iteration 71 / 200) loss: 1.410266\n",
      "(Epoch 8 / 20) train acc: 0.437000; val_acc: 0.416667\n",
      "(Iteration 81 / 200) loss: 1.250561\n",
      "(Epoch 9 / 20) train acc: 0.603000; val_acc: 0.627778\n",
      "(Iteration 91 / 200) loss: 0.832269\n",
      "(Epoch 10 / 20) train acc: 0.513000; val_acc: 0.488889\n",
      "(Iteration 101 / 200) loss: 0.903300\n",
      "(Epoch 11 / 20) train acc: 0.687000; val_acc: 0.716667\n",
      "(Iteration 111 / 200) loss: 0.774617\n",
      "(Epoch 12 / 20) train acc: 0.691000; val_acc: 0.675000\n",
      "(Iteration 121 / 200) loss: 0.639147\n",
      "(Epoch 13 / 20) train acc: 0.709000; val_acc: 0.708333\n",
      "(Iteration 131 / 200) loss: 0.716502\n",
      "(Epoch 14 / 20) train acc: 0.784000; val_acc: 0.777778\n",
      "(Iteration 141 / 200) loss: 0.450895\n",
      "(Epoch 15 / 20) train acc: 0.843000; val_acc: 0.822222\n",
      "(Iteration 151 / 200) loss: 0.508838\n",
      "(Epoch 16 / 20) train acc: 0.847000; val_acc: 0.822222\n",
      "(Iteration 161 / 200) loss: 0.387624\n",
      "(Epoch 17 / 20) train acc: 0.833000; val_acc: 0.800000\n",
      "(Iteration 171 / 200) loss: 0.522563\n",
      "(Epoch 18 / 20) train acc: 0.828000; val_acc: 0.783333\n",
      "(Iteration 181 / 200) loss: 0.469878\n",
      "(Epoch 19 / 20) train acc: 0.860000; val_acc: 0.825000\n",
      "(Iteration 191 / 200) loss: 0.483409\n",
      "(Epoch 20 / 20) train acc: 0.815000; val_acc: 0.797222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.259652\n",
      "(Epoch 2 / 20) train acc: 0.161000; val_acc: 0.186111\n",
      "(Iteration 21 / 200) loss: 2.236558\n",
      "(Epoch 3 / 20) train acc: 0.208000; val_acc: 0.169444\n",
      "(Iteration 31 / 200) loss: 1.982295\n",
      "(Epoch 4 / 20) train acc: 0.304000; val_acc: 0.297222\n",
      "(Iteration 41 / 200) loss: 1.652807\n",
      "(Epoch 5 / 20) train acc: 0.265000; val_acc: 0.238889\n",
      "(Iteration 51 / 200) loss: 1.501550\n",
      "(Epoch 6 / 20) train acc: 0.504000; val_acc: 0.455556\n",
      "(Iteration 61 / 200) loss: 1.491338\n",
      "(Epoch 7 / 20) train acc: 0.555000; val_acc: 0.513889\n",
      "(Iteration 71 / 200) loss: 1.077432\n",
      "(Epoch 8 / 20) train acc: 0.591000; val_acc: 0.544444\n",
      "(Iteration 81 / 200) loss: 1.059048\n",
      "(Epoch 9 / 20) train acc: 0.638000; val_acc: 0.597222\n",
      "(Iteration 91 / 200) loss: 0.843887\n",
      "(Epoch 10 / 20) train acc: 0.676000; val_acc: 0.630556\n",
      "(Iteration 101 / 200) loss: 0.924278\n",
      "(Epoch 11 / 20) train acc: 0.655000; val_acc: 0.650000\n",
      "(Iteration 111 / 200) loss: 0.954548\n",
      "(Epoch 12 / 20) train acc: 0.772000; val_acc: 0.755556\n",
      "(Iteration 121 / 200) loss: 0.603770\n",
      "(Epoch 13 / 20) train acc: 0.815000; val_acc: 0.786111\n",
      "(Iteration 131 / 200) loss: 0.280503\n",
      "(Epoch 14 / 20) train acc: 0.856000; val_acc: 0.830556\n",
      "(Iteration 141 / 200) loss: 0.640392\n",
      "(Epoch 15 / 20) train acc: 0.859000; val_acc: 0.858333\n",
      "(Iteration 151 / 200) loss: 0.477732\n",
      "(Epoch 16 / 20) train acc: 0.903000; val_acc: 0.863889\n",
      "(Iteration 161 / 200) loss: 0.339768\n",
      "(Epoch 17 / 20) train acc: 0.933000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.198332\n",
      "(Epoch 18 / 20) train acc: 0.961000; val_acc: 0.925000\n",
      "(Iteration 181 / 200) loss: 0.141708\n",
      "(Epoch 19 / 20) train acc: 0.963000; val_acc: 0.941667\n",
      "(Iteration 191 / 200) loss: 0.199125\n",
      "(Epoch 20 / 20) train acc: 0.964000; val_acc: 0.947222\n",
      "(Iteration 1 / 200) loss: 171740.153991\n",
      "(Epoch 0 / 20) train acc: 0.145000; val_acc: 0.158333\n",
      "(Epoch 1 / 20) train acc: 0.632000; val_acc: 0.619444\n",
      "(Iteration 11 / 200) loss: 18132.121079\n",
      "(Epoch 2 / 20) train acc: 0.782000; val_acc: 0.758333\n",
      "(Iteration 21 / 200) loss: 4594.211352\n",
      "(Epoch 3 / 20) train acc: 0.837000; val_acc: 0.772222\n",
      "(Iteration 31 / 200) loss: 5814.756688\n",
      "(Epoch 4 / 20) train acc: 0.901000; val_acc: 0.838889\n",
      "(Iteration 41 / 200) loss: 775.364035\n",
      "(Epoch 5 / 20) train acc: 0.929000; val_acc: 0.866667\n",
      "(Iteration 51 / 200) loss: 1267.377220\n",
      "(Epoch 6 / 20) train acc: 0.943000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 390.604087\n",
      "(Epoch 7 / 20) train acc: 0.955000; val_acc: 0.880556\n",
      "(Iteration 71 / 200) loss: 1160.336555\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.886111\n",
      "(Iteration 81 / 200) loss: 120.607910\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.900000\n",
      "(Iteration 91 / 200) loss: 155.751662\n",
      "(Epoch 10 / 20) train acc: 0.981000; val_acc: 0.875000\n",
      "(Iteration 101 / 200) loss: 1.609708\n",
      "(Epoch 11 / 20) train acc: 0.983000; val_acc: 0.886111\n",
      "(Iteration 111 / 200) loss: 1.603693\n",
      "(Epoch 12 / 20) train acc: 0.982000; val_acc: 0.900000\n",
      "(Iteration 121 / 200) loss: 1.598607\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.888889\n",
      "(Iteration 131 / 200) loss: 1.594197\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.900000\n",
      "(Iteration 141 / 200) loss: 67.018194\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.900000\n",
      "(Iteration 151 / 200) loss: 1.587270\n",
      "(Epoch 16 / 20) train acc: 0.993000; val_acc: 0.913889\n",
      "(Iteration 161 / 200) loss: 17.023991\n",
      "(Epoch 17 / 20) train acc: 0.993000; val_acc: 0.913889\n",
      "(Iteration 171 / 200) loss: 1.582398\n",
      "(Epoch 18 / 20) train acc: 0.994000; val_acc: 0.905556\n",
      "(Iteration 181 / 200) loss: 63.129504\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.916667\n",
      "(Iteration 191 / 200) loss: 237.715273\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.922222\n",
      "(Iteration 1 / 200) loss: 4.239405\n",
      "(Epoch 0 / 20) train acc: 0.177000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.822000; val_acc: 0.738889\n",
      "(Iteration 11 / 200) loss: 0.593757\n",
      "(Epoch 2 / 20) train acc: 0.890000; val_acc: 0.883333\n",
      "(Iteration 21 / 200) loss: 0.314870\n",
      "(Epoch 3 / 20) train acc: 0.953000; val_acc: 0.902778\n",
      "(Iteration 31 / 200) loss: 0.111106\n",
      "(Epoch 4 / 20) train acc: 0.944000; val_acc: 0.913889\n",
      "(Iteration 41 / 200) loss: 0.236007\n",
      "(Epoch 5 / 20) train acc: 0.971000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.036340\n",
      "(Epoch 6 / 20) train acc: 0.963000; val_acc: 0.958333\n",
      "(Iteration 61 / 200) loss: 0.139230\n",
      "(Epoch 7 / 20) train acc: 0.963000; val_acc: 0.944444\n",
      "(Iteration 71 / 200) loss: 0.179727\n",
      "(Epoch 8 / 20) train acc: 0.985000; val_acc: 0.955556\n",
      "(Iteration 81 / 200) loss: 0.063436\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.944444\n",
      "(Iteration 91 / 200) loss: 0.105570\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.049447\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.024635\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.983333\n",
      "(Iteration 121 / 200) loss: 0.028232\n",
      "(Epoch 13 / 20) train acc: 0.982000; val_acc: 0.961111\n",
      "(Iteration 131 / 200) loss: 0.030809\n",
      "(Epoch 14 / 20) train acc: 0.979000; val_acc: 0.955556\n",
      "(Iteration 141 / 200) loss: 0.056008\n",
      "(Epoch 15 / 20) train acc: 0.975000; val_acc: 0.947222\n",
      "(Iteration 151 / 200) loss: 0.051248\n",
      "(Epoch 16 / 20) train acc: 0.946000; val_acc: 0.936111\n",
      "(Iteration 161 / 200) loss: 0.271802\n",
      "(Epoch 17 / 20) train acc: 0.988000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.143267\n",
      "(Epoch 18 / 20) train acc: 0.978000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.047821\n",
      "(Epoch 19 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 191 / 200) loss: 0.030714\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.961111\n",
      "(Iteration 1 / 200) loss: 2.302770\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.153000; val_acc: 0.141667\n",
      "(Iteration 11 / 200) loss: 2.142279\n",
      "(Epoch 2 / 20) train acc: 0.129000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.265207\n",
      "(Epoch 3 / 20) train acc: 0.333000; val_acc: 0.302778\n",
      "(Iteration 31 / 200) loss: 1.784467\n",
      "(Epoch 4 / 20) train acc: 0.309000; val_acc: 0.305556\n",
      "(Iteration 41 / 200) loss: 1.810200\n",
      "(Epoch 5 / 20) train acc: 0.325000; val_acc: 0.333333\n",
      "(Iteration 51 / 200) loss: 1.400364\n",
      "(Epoch 6 / 20) train acc: 0.340000; val_acc: 0.322222\n",
      "(Iteration 61 / 200) loss: 1.546491\n",
      "(Epoch 7 / 20) train acc: 0.358000; val_acc: 0.325000\n",
      "(Iteration 71 / 200) loss: 1.327658\n",
      "(Epoch 8 / 20) train acc: 0.379000; val_acc: 0.350000\n",
      "(Iteration 81 / 200) loss: 1.310899\n",
      "(Epoch 9 / 20) train acc: 0.546000; val_acc: 0.541667\n",
      "(Iteration 91 / 200) loss: 1.115751\n",
      "(Epoch 10 / 20) train acc: 0.533000; val_acc: 0.505556\n",
      "(Iteration 101 / 200) loss: 1.044891\n",
      "(Epoch 11 / 20) train acc: 0.583000; val_acc: 0.555556\n",
      "(Iteration 111 / 200) loss: 0.807025\n",
      "(Epoch 12 / 20) train acc: 0.680000; val_acc: 0.625000\n",
      "(Iteration 121 / 200) loss: 0.725531\n",
      "(Epoch 13 / 20) train acc: 0.745000; val_acc: 0.744444\n",
      "(Iteration 131 / 200) loss: 0.668062\n",
      "(Epoch 14 / 20) train acc: 0.806000; val_acc: 0.758333\n",
      "(Iteration 141 / 200) loss: 0.618519\n",
      "(Epoch 15 / 20) train acc: 0.748000; val_acc: 0.736111\n",
      "(Iteration 151 / 200) loss: 0.575688\n",
      "(Epoch 16 / 20) train acc: 0.823000; val_acc: 0.822222\n",
      "(Iteration 161 / 200) loss: 0.372009\n",
      "(Epoch 17 / 20) train acc: 0.853000; val_acc: 0.838889\n",
      "(Iteration 171 / 200) loss: 0.562404\n",
      "(Epoch 18 / 20) train acc: 0.861000; val_acc: 0.830556\n",
      "(Iteration 181 / 200) loss: 0.423979\n",
      "(Epoch 19 / 20) train acc: 0.895000; val_acc: 0.855556\n",
      "(Iteration 191 / 200) loss: 0.436696\n",
      "(Epoch 20 / 20) train acc: 0.894000; val_acc: 0.872222\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.127000; val_acc: 0.122222\n",
      "(Iteration 11 / 200) loss: 2.219420\n",
      "(Epoch 2 / 20) train acc: 0.197000; val_acc: 0.233333\n",
      "(Iteration 21 / 200) loss: 2.060465\n",
      "(Epoch 3 / 20) train acc: 0.193000; val_acc: 0.219444\n",
      "(Iteration 31 / 200) loss: 1.760469\n",
      "(Epoch 4 / 20) train acc: 0.283000; val_acc: 0.319444\n",
      "(Iteration 41 / 200) loss: 1.701200\n",
      "(Epoch 5 / 20) train acc: 0.358000; val_acc: 0.366667\n",
      "(Iteration 51 / 200) loss: 1.501628\n",
      "(Epoch 6 / 20) train acc: 0.374000; val_acc: 0.413889\n",
      "(Iteration 61 / 200) loss: 1.454653\n",
      "(Epoch 7 / 20) train acc: 0.465000; val_acc: 0.447222\n",
      "(Iteration 71 / 200) loss: 1.415613\n",
      "(Epoch 8 / 20) train acc: 0.508000; val_acc: 0.519444\n",
      "(Iteration 81 / 200) loss: 1.180449\n",
      "(Epoch 9 / 20) train acc: 0.612000; val_acc: 0.594444\n",
      "(Iteration 91 / 200) loss: 1.185425\n",
      "(Epoch 10 / 20) train acc: 0.671000; val_acc: 0.688889\n",
      "(Iteration 101 / 200) loss: 0.895208\n",
      "(Epoch 11 / 20) train acc: 0.675000; val_acc: 0.700000\n",
      "(Iteration 111 / 200) loss: 0.835250\n",
      "(Epoch 12 / 20) train acc: 0.722000; val_acc: 0.766667\n",
      "(Iteration 121 / 200) loss: 0.583649\n",
      "(Epoch 13 / 20) train acc: 0.827000; val_acc: 0.830556\n",
      "(Iteration 131 / 200) loss: 0.449639\n",
      "(Epoch 14 / 20) train acc: 0.922000; val_acc: 0.911111\n",
      "(Iteration 141 / 200) loss: 0.347268\n",
      "(Epoch 15 / 20) train acc: 0.948000; val_acc: 0.925000\n",
      "(Iteration 151 / 200) loss: 0.150455\n",
      "(Epoch 16 / 20) train acc: 0.955000; val_acc: 0.897222\n",
      "(Iteration 161 / 200) loss: 0.119712\n",
      "(Epoch 17 / 20) train acc: 0.961000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 0.128406\n",
      "(Epoch 18 / 20) train acc: 0.943000; val_acc: 0.922222\n",
      "(Iteration 181 / 200) loss: 0.103413\n",
      "(Epoch 19 / 20) train acc: 0.948000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.199252\n",
      "(Epoch 20 / 20) train acc: 0.965000; val_acc: 0.927778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.212000; val_acc: 0.161111\n",
      "(Iteration 11 / 200) loss: 2.148219\n",
      "(Epoch 2 / 20) train acc: 0.198000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 2.241615\n",
      "(Epoch 3 / 20) train acc: 0.188000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 2.066330\n",
      "(Epoch 4 / 20) train acc: 0.207000; val_acc: 0.161111\n",
      "(Iteration 41 / 200) loss: 2.042349\n",
      "(Epoch 5 / 20) train acc: 0.207000; val_acc: 0.227778\n",
      "(Iteration 51 / 200) loss: 1.831280\n",
      "(Epoch 6 / 20) train acc: 0.236000; val_acc: 0.258333\n",
      "(Iteration 61 / 200) loss: 1.768208\n",
      "(Epoch 7 / 20) train acc: 0.360000; val_acc: 0.333333\n",
      "(Iteration 71 / 200) loss: 1.241186\n",
      "(Epoch 8 / 20) train acc: 0.595000; val_acc: 0.547222\n",
      "(Iteration 81 / 200) loss: 1.360667\n",
      "(Epoch 9 / 20) train acc: 0.484000; val_acc: 0.469444\n",
      "(Iteration 91 / 200) loss: 1.064269\n",
      "(Epoch 10 / 20) train acc: 0.622000; val_acc: 0.611111\n",
      "(Iteration 101 / 200) loss: 0.980676\n",
      "(Epoch 11 / 20) train acc: 0.650000; val_acc: 0.627778\n",
      "(Iteration 111 / 200) loss: 0.902135\n",
      "(Epoch 12 / 20) train acc: 0.760000; val_acc: 0.725000\n",
      "(Iteration 121 / 200) loss: 0.607597\n",
      "(Epoch 13 / 20) train acc: 0.814000; val_acc: 0.797222\n",
      "(Iteration 131 / 200) loss: 0.532530\n",
      "(Epoch 14 / 20) train acc: 0.817000; val_acc: 0.800000\n",
      "(Iteration 141 / 200) loss: 0.445033\n",
      "(Epoch 15 / 20) train acc: 0.865000; val_acc: 0.838889\n",
      "(Iteration 151 / 200) loss: 0.357044\n",
      "(Epoch 16 / 20) train acc: 0.870000; val_acc: 0.866667\n",
      "(Iteration 161 / 200) loss: 0.567030\n",
      "(Epoch 17 / 20) train acc: 0.914000; val_acc: 0.911111\n",
      "(Iteration 171 / 200) loss: 0.255023\n",
      "(Epoch 18 / 20) train acc: 0.830000; val_acc: 0.841667\n",
      "(Iteration 181 / 200) loss: 0.484893\n",
      "(Epoch 19 / 20) train acc: 0.870000; val_acc: 0.872222\n",
      "(Iteration 191 / 200) loss: 0.229013\n",
      "(Epoch 20 / 20) train acc: 0.937000; val_acc: 0.938889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.102778\n",
      "(Iteration 11 / 200) loss: 2.154208\n",
      "(Epoch 2 / 20) train acc: 0.195000; val_acc: 0.208333\n",
      "(Iteration 21 / 200) loss: 1.990267\n",
      "(Epoch 3 / 20) train acc: 0.226000; val_acc: 0.200000\n",
      "(Iteration 31 / 200) loss: 1.937012\n",
      "(Epoch 4 / 20) train acc: 0.173000; val_acc: 0.180556\n",
      "(Iteration 41 / 200) loss: 2.053945\n",
      "(Epoch 5 / 20) train acc: 0.310000; val_acc: 0.255556\n",
      "(Iteration 51 / 200) loss: 1.861569\n",
      "(Epoch 6 / 20) train acc: 0.222000; val_acc: 0.166667\n",
      "(Iteration 61 / 200) loss: 1.908885\n",
      "(Epoch 7 / 20) train acc: 0.259000; val_acc: 0.188889\n",
      "(Iteration 71 / 200) loss: 1.962817\n",
      "(Epoch 8 / 20) train acc: 0.383000; val_acc: 0.333333\n",
      "(Iteration 81 / 200) loss: 1.699788\n",
      "(Epoch 9 / 20) train acc: 0.405000; val_acc: 0.319444\n",
      "(Iteration 91 / 200) loss: 1.529206\n",
      "(Epoch 10 / 20) train acc: 0.484000; val_acc: 0.419444\n",
      "(Iteration 101 / 200) loss: 1.307495\n",
      "(Epoch 11 / 20) train acc: 0.511000; val_acc: 0.438889\n",
      "(Iteration 111 / 200) loss: 1.327060\n",
      "(Epoch 12 / 20) train acc: 0.498000; val_acc: 0.430556\n",
      "(Iteration 121 / 200) loss: 1.333019\n",
      "(Epoch 13 / 20) train acc: 0.518000; val_acc: 0.436111\n",
      "(Iteration 131 / 200) loss: 1.071259\n",
      "(Epoch 14 / 20) train acc: 0.532000; val_acc: 0.441667\n",
      "(Iteration 141 / 200) loss: 1.110634\n",
      "(Epoch 15 / 20) train acc: 0.557000; val_acc: 0.480556\n",
      "(Iteration 151 / 200) loss: 1.248464\n",
      "(Epoch 16 / 20) train acc: 0.496000; val_acc: 0.455556\n",
      "(Iteration 161 / 200) loss: 1.282532\n",
      "(Epoch 17 / 20) train acc: 0.577000; val_acc: 0.555556\n",
      "(Iteration 171 / 200) loss: 0.967807\n",
      "(Epoch 18 / 20) train acc: 0.665000; val_acc: 0.619444\n",
      "(Iteration 181 / 200) loss: 0.860661\n",
      "(Epoch 19 / 20) train acc: 0.685000; val_acc: 0.625000\n",
      "(Iteration 191 / 200) loss: 0.950649\n",
      "(Epoch 20 / 20) train acc: 0.737000; val_acc: 0.675000\n",
      "(Iteration 1 / 200) loss: 107783.349997\n",
      "(Epoch 0 / 20) train acc: 0.173000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.632000; val_acc: 0.638889\n",
      "(Iteration 11 / 200) loss: 15157.160796\n",
      "(Epoch 2 / 20) train acc: 0.814000; val_acc: 0.755556\n",
      "(Iteration 21 / 200) loss: 3634.940303\n",
      "(Epoch 3 / 20) train acc: 0.903000; val_acc: 0.816667\n",
      "(Iteration 31 / 200) loss: 3098.066486\n",
      "(Epoch 4 / 20) train acc: 0.914000; val_acc: 0.816667\n",
      "(Iteration 41 / 200) loss: 890.275161\n",
      "(Epoch 5 / 20) train acc: 0.948000; val_acc: 0.855556\n",
      "(Iteration 51 / 200) loss: 1180.098745\n",
      "(Epoch 6 / 20) train acc: 0.958000; val_acc: 0.872222\n",
      "(Iteration 61 / 200) loss: 486.378756\n",
      "(Epoch 7 / 20) train acc: 0.955000; val_acc: 0.883333\n",
      "(Iteration 71 / 200) loss: 2327.837568\n",
      "(Epoch 8 / 20) train acc: 0.964000; val_acc: 0.880556\n",
      "(Iteration 81 / 200) loss: 296.133876\n",
      "(Epoch 9 / 20) train acc: 0.968000; val_acc: 0.872222\n",
      "(Iteration 91 / 200) loss: 248.019944\n",
      "(Epoch 10 / 20) train acc: 0.968000; val_acc: 0.858333\n",
      "(Iteration 101 / 200) loss: 93.699518\n",
      "(Epoch 11 / 20) train acc: 0.984000; val_acc: 0.886111\n",
      "(Iteration 111 / 200) loss: 134.916602\n",
      "(Epoch 12 / 20) train acc: 0.981000; val_acc: 0.888889\n",
      "(Iteration 121 / 200) loss: 223.346256\n",
      "(Epoch 13 / 20) train acc: 0.986000; val_acc: 0.902778\n",
      "(Iteration 131 / 200) loss: 12.005742\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.880556\n",
      "(Iteration 141 / 200) loss: 0.156489\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.872222\n",
      "(Iteration 151 / 200) loss: 25.879376\n",
      "(Epoch 16 / 20) train acc: 0.980000; val_acc: 0.875000\n",
      "(Iteration 161 / 200) loss: 68.649905\n",
      "(Epoch 17 / 20) train acc: 0.983000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 176.497110\n",
      "(Epoch 18 / 20) train acc: 0.989000; val_acc: 0.894444\n",
      "(Iteration 181 / 200) loss: 143.277525\n",
      "(Epoch 19 / 20) train acc: 0.993000; val_acc: 0.888889\n",
      "(Iteration 191 / 200) loss: 0.155372\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.894444\n",
      "(Iteration 1 / 200) loss: 2.648416\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.777000; val_acc: 0.744444\n",
      "(Iteration 11 / 200) loss: 0.811212\n",
      "(Epoch 2 / 20) train acc: 0.796000; val_acc: 0.791667\n",
      "(Iteration 21 / 200) loss: 0.673362\n",
      "(Epoch 3 / 20) train acc: 0.892000; val_acc: 0.877778\n",
      "(Iteration 31 / 200) loss: 0.295863\n",
      "(Epoch 4 / 20) train acc: 0.960000; val_acc: 0.936111\n",
      "(Iteration 41 / 200) loss: 0.071941\n",
      "(Epoch 5 / 20) train acc: 0.967000; val_acc: 0.952778\n",
      "(Iteration 51 / 200) loss: 0.065693\n",
      "(Epoch 6 / 20) train acc: 0.982000; val_acc: 0.963889\n",
      "(Iteration 61 / 200) loss: 0.069837\n",
      "(Epoch 7 / 20) train acc: 0.976000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.050372\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.045942\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 0.036918\n",
      "(Epoch 10 / 20) train acc: 0.980000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.076776\n",
      "(Epoch 11 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 111 / 200) loss: 0.083705\n",
      "(Epoch 12 / 20) train acc: 0.972000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.031912\n",
      "(Epoch 13 / 20) train acc: 0.990000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.019137\n",
      "(Epoch 14 / 20) train acc: 0.986000; val_acc: 0.961111\n",
      "(Iteration 141 / 200) loss: 0.007012\n",
      "(Epoch 15 / 20) train acc: 0.990000; val_acc: 0.961111\n",
      "(Iteration 151 / 200) loss: 0.060902\n",
      "(Epoch 16 / 20) train acc: 0.984000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.038953\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.952778\n",
      "(Iteration 171 / 200) loss: 0.042377\n",
      "(Epoch 18 / 20) train acc: 0.985000; val_acc: 0.958333\n",
      "(Iteration 181 / 200) loss: 0.023090\n",
      "(Epoch 19 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 191 / 200) loss: 0.054431\n",
      "(Epoch 20 / 20) train acc: 0.970000; val_acc: 0.936111\n",
      "(Iteration 1 / 200) loss: 2.302606\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.171000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 2.267361\n",
      "(Epoch 2 / 20) train acc: 0.161000; val_acc: 0.161111\n",
      "(Iteration 21 / 200) loss: 1.999716\n",
      "(Epoch 3 / 20) train acc: 0.256000; val_acc: 0.211111\n",
      "(Iteration 31 / 200) loss: 2.022682\n",
      "(Epoch 4 / 20) train acc: 0.208000; val_acc: 0.180556\n",
      "(Iteration 41 / 200) loss: 1.984726\n",
      "(Epoch 5 / 20) train acc: 0.402000; val_acc: 0.352778\n",
      "(Iteration 51 / 200) loss: 1.495117\n",
      "(Epoch 6 / 20) train acc: 0.405000; val_acc: 0.355556\n",
      "(Iteration 61 / 200) loss: 1.451911\n",
      "(Epoch 7 / 20) train acc: 0.493000; val_acc: 0.480556\n",
      "(Iteration 71 / 200) loss: 1.205165\n",
      "(Epoch 8 / 20) train acc: 0.574000; val_acc: 0.522222\n",
      "(Iteration 81 / 200) loss: 0.857269\n",
      "(Epoch 9 / 20) train acc: 0.567000; val_acc: 0.558333\n",
      "(Iteration 91 / 200) loss: 1.021359\n",
      "(Epoch 10 / 20) train acc: 0.585000; val_acc: 0.547222\n",
      "(Iteration 101 / 200) loss: 0.821521\n",
      "(Epoch 11 / 20) train acc: 0.685000; val_acc: 0.619444\n",
      "(Iteration 111 / 200) loss: 0.627236\n",
      "(Epoch 12 / 20) train acc: 0.821000; val_acc: 0.733333\n",
      "(Iteration 121 / 200) loss: 0.475752\n",
      "(Epoch 13 / 20) train acc: 0.874000; val_acc: 0.794444\n",
      "(Iteration 131 / 200) loss: 0.453931\n",
      "(Epoch 14 / 20) train acc: 0.856000; val_acc: 0.805556\n",
      "(Iteration 141 / 200) loss: 0.358933\n",
      "(Epoch 15 / 20) train acc: 0.887000; val_acc: 0.844444\n",
      "(Iteration 151 / 200) loss: 0.226210\n",
      "(Epoch 16 / 20) train acc: 0.901000; val_acc: 0.880556\n",
      "(Iteration 161 / 200) loss: 0.359434\n",
      "(Epoch 17 / 20) train acc: 0.936000; val_acc: 0.905556\n",
      "(Iteration 171 / 200) loss: 0.202271\n",
      "(Epoch 18 / 20) train acc: 0.948000; val_acc: 0.900000\n",
      "(Iteration 181 / 200) loss: 0.119124\n",
      "(Epoch 19 / 20) train acc: 0.961000; val_acc: 0.913889\n",
      "(Iteration 191 / 200) loss: 0.142538\n",
      "(Epoch 20 / 20) train acc: 0.949000; val_acc: 0.894444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.085000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.467711\n",
      "(Epoch 2 / 20) train acc: 0.176000; val_acc: 0.211111\n",
      "(Iteration 21 / 200) loss: 1.958485\n",
      "(Epoch 3 / 20) train acc: 0.182000; val_acc: 0.219444\n",
      "(Iteration 31 / 200) loss: 1.970756\n",
      "(Epoch 4 / 20) train acc: 0.196000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 1.988564\n",
      "(Epoch 5 / 20) train acc: 0.214000; val_acc: 0.219444\n",
      "(Iteration 51 / 200) loss: 1.947304\n",
      "(Epoch 6 / 20) train acc: 0.303000; val_acc: 0.288889\n",
      "(Iteration 61 / 200) loss: 1.938850\n",
      "(Epoch 7 / 20) train acc: 0.210000; val_acc: 0.175000\n",
      "(Iteration 71 / 200) loss: 1.870625\n",
      "(Epoch 8 / 20) train acc: 0.292000; val_acc: 0.313889\n",
      "(Iteration 81 / 200) loss: 1.767411\n",
      "(Epoch 9 / 20) train acc: 0.295000; val_acc: 0.302778\n",
      "(Iteration 91 / 200) loss: 1.560855\n",
      "(Epoch 10 / 20) train acc: 0.343000; val_acc: 0.319444\n",
      "(Iteration 101 / 200) loss: 1.440931\n",
      "(Epoch 11 / 20) train acc: 0.421000; val_acc: 0.411111\n",
      "(Iteration 111 / 200) loss: 1.590040\n",
      "(Epoch 12 / 20) train acc: 0.338000; val_acc: 0.311111\n",
      "(Iteration 121 / 200) loss: 1.411162\n",
      "(Epoch 13 / 20) train acc: 0.523000; val_acc: 0.558333\n",
      "(Iteration 131 / 200) loss: 1.116624\n",
      "(Epoch 14 / 20) train acc: 0.445000; val_acc: 0.500000\n",
      "(Iteration 141 / 200) loss: 1.388848\n",
      "(Epoch 15 / 20) train acc: 0.522000; val_acc: 0.544444\n",
      "(Iteration 151 / 200) loss: 1.134081\n",
      "(Epoch 16 / 20) train acc: 0.527000; val_acc: 0.561111\n",
      "(Iteration 161 / 200) loss: 0.997045\n",
      "(Epoch 17 / 20) train acc: 0.581000; val_acc: 0.616667\n",
      "(Iteration 171 / 200) loss: 0.990366\n",
      "(Epoch 18 / 20) train acc: 0.595000; val_acc: 0.597222\n",
      "(Iteration 181 / 200) loss: 0.829675\n",
      "(Epoch 19 / 20) train acc: 0.610000; val_acc: 0.600000\n",
      "(Iteration 191 / 200) loss: 1.036349\n",
      "(Epoch 20 / 20) train acc: 0.636000; val_acc: 0.630556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.143000; val_acc: 0.161111\n",
      "(Iteration 11 / 200) loss: 2.298281\n",
      "(Epoch 2 / 20) train acc: 0.182000; val_acc: 0.194444\n",
      "(Iteration 21 / 200) loss: 2.059385\n",
      "(Epoch 3 / 20) train acc: 0.186000; val_acc: 0.155556\n",
      "(Iteration 31 / 200) loss: 1.854531\n",
      "(Epoch 4 / 20) train acc: 0.195000; val_acc: 0.222222\n",
      "(Iteration 41 / 200) loss: 1.766417\n",
      "(Epoch 5 / 20) train acc: 0.198000; val_acc: 0.227778\n",
      "(Iteration 51 / 200) loss: 1.722341\n",
      "(Epoch 6 / 20) train acc: 0.245000; val_acc: 0.188889\n",
      "(Iteration 61 / 200) loss: 1.801903\n",
      "(Epoch 7 / 20) train acc: 0.248000; val_acc: 0.213889\n",
      "(Iteration 71 / 200) loss: 1.674072\n",
      "(Epoch 8 / 20) train acc: 0.309000; val_acc: 0.266667\n",
      "(Iteration 81 / 200) loss: 1.551690\n",
      "(Epoch 9 / 20) train acc: 0.374000; val_acc: 0.386111\n",
      "(Iteration 91 / 200) loss: 1.373683\n",
      "(Epoch 10 / 20) train acc: 0.410000; val_acc: 0.391667\n",
      "(Iteration 101 / 200) loss: 1.264764\n",
      "(Epoch 11 / 20) train acc: 0.416000; val_acc: 0.425000\n",
      "(Iteration 111 / 200) loss: 1.169908\n",
      "(Epoch 12 / 20) train acc: 0.585000; val_acc: 0.563889\n",
      "(Iteration 121 / 200) loss: 1.270295\n",
      "(Epoch 13 / 20) train acc: 0.583000; val_acc: 0.608333\n",
      "(Iteration 131 / 200) loss: 1.231359\n",
      "(Epoch 14 / 20) train acc: 0.581000; val_acc: 0.602778\n",
      "(Iteration 141 / 200) loss: 0.918621\n",
      "(Epoch 15 / 20) train acc: 0.688000; val_acc: 0.663889\n",
      "(Iteration 151 / 200) loss: 0.774907\n",
      "(Epoch 16 / 20) train acc: 0.769000; val_acc: 0.727778\n",
      "(Iteration 161 / 200) loss: 0.933264\n",
      "(Epoch 17 / 20) train acc: 0.815000; val_acc: 0.822222\n",
      "(Iteration 171 / 200) loss: 0.462585\n",
      "(Epoch 18 / 20) train acc: 0.849000; val_acc: 0.838889\n",
      "(Iteration 181 / 200) loss: 0.364165\n",
      "(Epoch 19 / 20) train acc: 0.837000; val_acc: 0.833333\n",
      "(Iteration 191 / 200) loss: 0.362381\n",
      "(Epoch 20 / 20) train acc: 0.799000; val_acc: 0.752778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302650\n",
      "(Epoch 2 / 20) train acc: 0.219000; val_acc: 0.158333\n",
      "(Iteration 21 / 200) loss: 2.139643\n",
      "(Epoch 3 / 20) train acc: 0.227000; val_acc: 0.177778\n",
      "(Iteration 31 / 200) loss: 1.987503\n",
      "(Epoch 4 / 20) train acc: 0.211000; val_acc: 0.205556\n",
      "(Iteration 41 / 200) loss: 1.943337\n",
      "(Epoch 5 / 20) train acc: 0.198000; val_acc: 0.225000\n",
      "(Iteration 51 / 200) loss: 1.883000\n",
      "(Epoch 6 / 20) train acc: 0.195000; val_acc: 0.163889\n",
      "(Iteration 61 / 200) loss: 1.788781\n",
      "(Epoch 7 / 20) train acc: 0.195000; val_acc: 0.158333\n",
      "(Iteration 71 / 200) loss: 1.885711\n",
      "(Epoch 8 / 20) train acc: 0.200000; val_acc: 0.175000\n",
      "(Iteration 81 / 200) loss: 1.877557\n",
      "(Epoch 9 / 20) train acc: 0.207000; val_acc: 0.180556\n",
      "(Iteration 91 / 200) loss: 1.824279\n",
      "(Epoch 10 / 20) train acc: 0.266000; val_acc: 0.213889\n",
      "(Iteration 101 / 200) loss: 1.700371\n",
      "(Epoch 11 / 20) train acc: 0.328000; val_acc: 0.263889\n",
      "(Iteration 111 / 200) loss: 1.642898\n",
      "(Epoch 12 / 20) train acc: 0.310000; val_acc: 0.252778\n",
      "(Iteration 121 / 200) loss: 1.689627\n",
      "(Epoch 13 / 20) train acc: 0.285000; val_acc: 0.252778\n",
      "(Iteration 131 / 200) loss: 1.722523\n",
      "(Epoch 14 / 20) train acc: 0.285000; val_acc: 0.250000\n",
      "(Iteration 141 / 200) loss: 1.524876\n",
      "(Epoch 15 / 20) train acc: 0.312000; val_acc: 0.255556\n",
      "(Iteration 151 / 200) loss: 1.659425\n",
      "(Epoch 16 / 20) train acc: 0.338000; val_acc: 0.291667\n",
      "(Iteration 161 / 200) loss: 1.557632\n",
      "(Epoch 17 / 20) train acc: 0.288000; val_acc: 0.275000\n",
      "(Iteration 171 / 200) loss: 1.502397\n",
      "(Epoch 18 / 20) train acc: 0.311000; val_acc: 0.302778\n",
      "(Iteration 181 / 200) loss: 1.660565\n",
      "(Epoch 19 / 20) train acc: 0.324000; val_acc: 0.341667\n",
      "(Iteration 191 / 200) loss: 1.466914\n",
      "(Epoch 20 / 20) train acc: 0.385000; val_acc: 0.386111\n",
      "(Iteration 1 / 200) loss: 157885.799286\n",
      "(Epoch 0 / 20) train acc: 0.062000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.585000; val_acc: 0.516667\n",
      "(Iteration 11 / 200) loss: 9892.272055\n",
      "(Epoch 2 / 20) train acc: 0.788000; val_acc: 0.727778\n",
      "(Iteration 21 / 200) loss: 5603.064877\n",
      "(Epoch 3 / 20) train acc: 0.890000; val_acc: 0.816667\n",
      "(Iteration 31 / 200) loss: 2587.514824\n",
      "(Epoch 4 / 20) train acc: 0.942000; val_acc: 0.841667\n",
      "(Iteration 41 / 200) loss: 1685.687692\n",
      "(Epoch 5 / 20) train acc: 0.945000; val_acc: 0.861111\n",
      "(Iteration 51 / 200) loss: 967.172751\n",
      "(Epoch 6 / 20) train acc: 0.962000; val_acc: 0.861111\n",
      "(Iteration 61 / 200) loss: 326.355804\n",
      "(Epoch 7 / 20) train acc: 0.979000; val_acc: 0.861111\n",
      "(Iteration 71 / 200) loss: 667.075193\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.869444\n",
      "(Iteration 81 / 200) loss: 0.166409\n",
      "(Epoch 9 / 20) train acc: 0.974000; val_acc: 0.866667\n",
      "(Iteration 91 / 200) loss: 0.165786\n",
      "(Epoch 10 / 20) train acc: 0.990000; val_acc: 0.883333\n",
      "(Iteration 101 / 200) loss: 6.586151\n",
      "(Epoch 11 / 20) train acc: 0.981000; val_acc: 0.883333\n",
      "(Iteration 111 / 200) loss: 58.655969\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.888889\n",
      "(Iteration 121 / 200) loss: 0.164415\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.883333\n",
      "(Iteration 131 / 200) loss: 60.113232\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.883333\n",
      "(Iteration 141 / 200) loss: 0.163819\n",
      "(Epoch 15 / 20) train acc: 0.996000; val_acc: 0.883333\n",
      "(Iteration 151 / 200) loss: 98.450657\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.877778\n",
      "(Iteration 161 / 200) loss: 46.982999\n",
      "(Epoch 17 / 20) train acc: 0.995000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.163217\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.888889\n",
      "(Iteration 181 / 200) loss: 0.163073\n",
      "(Epoch 19 / 20) train acc: 0.994000; val_acc: 0.894444\n",
      "(Iteration 191 / 200) loss: 10.581511\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.886111\n",
      "(Iteration 1 / 200) loss: 3.734523\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.811000; val_acc: 0.800000\n",
      "(Iteration 11 / 200) loss: 0.547970\n",
      "(Epoch 2 / 20) train acc: 0.861000; val_acc: 0.861111\n",
      "(Iteration 21 / 200) loss: 0.219811\n",
      "(Epoch 3 / 20) train acc: 0.925000; val_acc: 0.905556\n",
      "(Iteration 31 / 200) loss: 0.195018\n",
      "(Epoch 4 / 20) train acc: 0.947000; val_acc: 0.922222\n",
      "(Iteration 41 / 200) loss: 0.128671\n",
      "(Epoch 5 / 20) train acc: 0.955000; val_acc: 0.919444\n",
      "(Iteration 51 / 200) loss: 0.079165\n",
      "(Epoch 6 / 20) train acc: 0.984000; val_acc: 0.961111\n",
      "(Iteration 61 / 200) loss: 0.094808\n",
      "(Epoch 7 / 20) train acc: 0.983000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.058790\n",
      "(Epoch 8 / 20) train acc: 0.984000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.019595\n",
      "(Epoch 9 / 20) train acc: 0.995000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.023950\n",
      "(Epoch 10 / 20) train acc: 0.985000; val_acc: 0.927778\n",
      "(Iteration 101 / 200) loss: 0.031235\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.012459\n",
      "(Epoch 12 / 20) train acc: 0.960000; val_acc: 0.930556\n",
      "(Iteration 121 / 200) loss: 0.060275\n",
      "(Epoch 13 / 20) train acc: 0.987000; val_acc: 0.947222\n",
      "(Iteration 131 / 200) loss: 0.008490\n",
      "(Epoch 14 / 20) train acc: 0.985000; val_acc: 0.969444\n",
      "(Iteration 141 / 200) loss: 0.029878\n",
      "(Epoch 15 / 20) train acc: 0.989000; val_acc: 0.958333\n",
      "(Iteration 151 / 200) loss: 0.015316\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.966667\n",
      "(Iteration 161 / 200) loss: 0.012383\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.081814\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.015499\n",
      "(Epoch 19 / 20) train acc: 0.984000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.033513\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.302601\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.188000; val_acc: 0.202778\n",
      "(Iteration 11 / 200) loss: 2.034660\n",
      "(Epoch 2 / 20) train acc: 0.184000; val_acc: 0.177778\n",
      "(Iteration 21 / 200) loss: 1.801633\n",
      "(Epoch 3 / 20) train acc: 0.216000; val_acc: 0.177778\n",
      "(Iteration 31 / 200) loss: 1.646961\n",
      "(Epoch 4 / 20) train acc: 0.229000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 1.608517\n",
      "(Epoch 5 / 20) train acc: 0.447000; val_acc: 0.447222\n",
      "(Iteration 51 / 200) loss: 1.362159\n",
      "(Epoch 6 / 20) train acc: 0.671000; val_acc: 0.569444\n",
      "(Iteration 61 / 200) loss: 1.005850\n",
      "(Epoch 7 / 20) train acc: 0.722000; val_acc: 0.730556\n",
      "(Iteration 71 / 200) loss: 0.682767\n",
      "(Epoch 8 / 20) train acc: 0.834000; val_acc: 0.788889\n",
      "(Iteration 81 / 200) loss: 0.407140\n",
      "(Epoch 9 / 20) train acc: 0.819000; val_acc: 0.819444\n",
      "(Iteration 91 / 200) loss: 0.489559\n",
      "(Epoch 10 / 20) train acc: 0.855000; val_acc: 0.844444\n",
      "(Iteration 101 / 200) loss: 0.375285\n",
      "(Epoch 11 / 20) train acc: 0.905000; val_acc: 0.880556\n",
      "(Iteration 111 / 200) loss: 0.225446\n",
      "(Epoch 12 / 20) train acc: 0.910000; val_acc: 0.919444\n",
      "(Iteration 121 / 200) loss: 0.263176\n",
      "(Epoch 13 / 20) train acc: 0.955000; val_acc: 0.927778\n",
      "(Iteration 131 / 200) loss: 0.077550\n",
      "(Epoch 14 / 20) train acc: 0.969000; val_acc: 0.944444\n",
      "(Iteration 141 / 200) loss: 0.060216\n",
      "(Epoch 15 / 20) train acc: 0.975000; val_acc: 0.952778\n",
      "(Iteration 151 / 200) loss: 0.103061\n",
      "(Epoch 16 / 20) train acc: 0.973000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.071612\n",
      "(Epoch 17 / 20) train acc: 0.983000; val_acc: 0.958333\n",
      "(Iteration 171 / 200) loss: 0.064277\n",
      "(Epoch 18 / 20) train acc: 0.979000; val_acc: 0.947222\n",
      "(Iteration 181 / 200) loss: 0.130034\n",
      "(Epoch 19 / 20) train acc: 0.977000; val_acc: 0.944444\n",
      "(Iteration 191 / 200) loss: 0.115536\n",
      "(Epoch 20 / 20) train acc: 0.969000; val_acc: 0.925000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.264198\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.301129\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.305022\n",
      "(Epoch 4 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.297187\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.286817\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.270493\n",
      "(Epoch 7 / 20) train acc: 0.167000; val_acc: 0.158333\n",
      "(Iteration 71 / 200) loss: 2.234622\n",
      "(Epoch 8 / 20) train acc: 0.184000; val_acc: 0.183333\n",
      "(Iteration 81 / 200) loss: 2.139403\n",
      "(Epoch 9 / 20) train acc: 0.207000; val_acc: 0.161111\n",
      "(Iteration 91 / 200) loss: 2.052185\n",
      "(Epoch 10 / 20) train acc: 0.182000; val_acc: 0.163889\n",
      "(Iteration 101 / 200) loss: 1.994677\n",
      "(Epoch 11 / 20) train acc: 0.240000; val_acc: 0.211111\n",
      "(Iteration 111 / 200) loss: 1.921288\n",
      "(Epoch 12 / 20) train acc: 0.290000; val_acc: 0.219444\n",
      "(Iteration 121 / 200) loss: 1.600510\n",
      "(Epoch 13 / 20) train acc: 0.286000; val_acc: 0.230556\n",
      "(Iteration 131 / 200) loss: 1.647344\n",
      "(Epoch 14 / 20) train acc: 0.309000; val_acc: 0.255556\n",
      "(Iteration 141 / 200) loss: 1.586370\n",
      "(Epoch 15 / 20) train acc: 0.258000; val_acc: 0.222222\n",
      "(Iteration 151 / 200) loss: 1.833247\n",
      "(Epoch 16 / 20) train acc: 0.289000; val_acc: 0.233333\n",
      "(Iteration 161 / 200) loss: 1.658944\n",
      "(Epoch 17 / 20) train acc: 0.335000; val_acc: 0.250000\n",
      "(Iteration 171 / 200) loss: 1.676825\n",
      "(Epoch 18 / 20) train acc: 0.323000; val_acc: 0.244444\n",
      "(Iteration 181 / 200) loss: 1.452335\n",
      "(Epoch 19 / 20) train acc: 0.316000; val_acc: 0.247222\n",
      "(Iteration 191 / 200) loss: 1.652410\n",
      "(Epoch 20 / 20) train acc: 0.296000; val_acc: 0.250000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.167000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.246376\n",
      "(Epoch 2 / 20) train acc: 0.231000; val_acc: 0.183333\n",
      "(Iteration 21 / 200) loss: 1.994027\n",
      "(Epoch 3 / 20) train acc: 0.208000; val_acc: 0.194444\n",
      "(Iteration 31 / 200) loss: 1.913591\n",
      "(Epoch 4 / 20) train acc: 0.231000; val_acc: 0.211111\n",
      "(Iteration 41 / 200) loss: 1.825520\n",
      "(Epoch 5 / 20) train acc: 0.203000; val_acc: 0.188889\n",
      "(Iteration 51 / 200) loss: 1.915164\n",
      "(Epoch 6 / 20) train acc: 0.233000; val_acc: 0.211111\n",
      "(Iteration 61 / 200) loss: 1.780538\n",
      "(Epoch 7 / 20) train acc: 0.287000; val_acc: 0.277778\n",
      "(Iteration 71 / 200) loss: 1.679969\n",
      "(Epoch 8 / 20) train acc: 0.302000; val_acc: 0.263889\n",
      "(Iteration 81 / 200) loss: 1.809443\n",
      "(Epoch 9 / 20) train acc: 0.325000; val_acc: 0.294444\n",
      "(Iteration 91 / 200) loss: 1.643129\n",
      "(Epoch 10 / 20) train acc: 0.354000; val_acc: 0.319444\n",
      "(Iteration 101 / 200) loss: 1.446157\n",
      "(Epoch 11 / 20) train acc: 0.454000; val_acc: 0.430556\n",
      "(Iteration 111 / 200) loss: 1.479820\n",
      "(Epoch 12 / 20) train acc: 0.434000; val_acc: 0.425000\n",
      "(Iteration 121 / 200) loss: 1.400762\n",
      "(Epoch 13 / 20) train acc: 0.479000; val_acc: 0.458333\n",
      "(Iteration 131 / 200) loss: 1.165555\n",
      "(Epoch 14 / 20) train acc: 0.504000; val_acc: 0.472222\n",
      "(Iteration 141 / 200) loss: 1.080056\n",
      "(Epoch 15 / 20) train acc: 0.484000; val_acc: 0.444444\n",
      "(Iteration 151 / 200) loss: 1.024895\n",
      "(Epoch 16 / 20) train acc: 0.563000; val_acc: 0.525000\n",
      "(Iteration 161 / 200) loss: 0.928932\n",
      "(Epoch 17 / 20) train acc: 0.616000; val_acc: 0.591667\n",
      "(Iteration 171 / 200) loss: 0.873983\n",
      "(Epoch 18 / 20) train acc: 0.731000; val_acc: 0.647222\n",
      "(Iteration 181 / 200) loss: 0.860842\n",
      "(Epoch 19 / 20) train acc: 0.766000; val_acc: 0.733333\n",
      "(Iteration 191 / 200) loss: 0.641944\n",
      "(Epoch 20 / 20) train acc: 0.750000; val_acc: 0.700000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.297025\n",
      "(Epoch 2 / 20) train acc: 0.211000; val_acc: 0.163889\n",
      "(Iteration 21 / 200) loss: 2.021081\n",
      "(Epoch 3 / 20) train acc: 0.209000; val_acc: 0.166667\n",
      "(Iteration 31 / 200) loss: 2.076712\n",
      "(Epoch 4 / 20) train acc: 0.226000; val_acc: 0.172222\n",
      "(Iteration 41 / 200) loss: 1.868168\n",
      "(Epoch 5 / 20) train acc: 0.287000; val_acc: 0.238889\n",
      "(Iteration 51 / 200) loss: 1.808390\n",
      "(Epoch 6 / 20) train acc: 0.288000; val_acc: 0.238889\n",
      "(Iteration 61 / 200) loss: 1.645056\n",
      "(Epoch 7 / 20) train acc: 0.248000; val_acc: 0.200000\n",
      "(Iteration 71 / 200) loss: 1.919864\n",
      "(Epoch 8 / 20) train acc: 0.323000; val_acc: 0.283333\n",
      "(Iteration 81 / 200) loss: 1.755684\n",
      "(Epoch 9 / 20) train acc: 0.334000; val_acc: 0.300000\n",
      "(Iteration 91 / 200) loss: 1.663962\n",
      "(Epoch 10 / 20) train acc: 0.336000; val_acc: 0.333333\n",
      "(Iteration 101 / 200) loss: 1.501349\n",
      "(Epoch 11 / 20) train acc: 0.394000; val_acc: 0.336111\n",
      "(Iteration 111 / 200) loss: 1.531354\n",
      "(Epoch 12 / 20) train acc: 0.427000; val_acc: 0.372222\n",
      "(Iteration 121 / 200) loss: 1.534528\n",
      "(Epoch 13 / 20) train acc: 0.488000; val_acc: 0.444444\n",
      "(Iteration 131 / 200) loss: 1.406951\n",
      "(Epoch 14 / 20) train acc: 0.472000; val_acc: 0.397222\n",
      "(Iteration 141 / 200) loss: 1.490974\n",
      "(Epoch 15 / 20) train acc: 0.513000; val_acc: 0.458333\n",
      "(Iteration 151 / 200) loss: 1.287308\n",
      "(Epoch 16 / 20) train acc: 0.559000; val_acc: 0.513889\n",
      "(Iteration 161 / 200) loss: 1.015232\n",
      "(Epoch 17 / 20) train acc: 0.589000; val_acc: 0.538889\n",
      "(Iteration 171 / 200) loss: 1.177285\n",
      "(Epoch 18 / 20) train acc: 0.608000; val_acc: 0.530556\n",
      "(Iteration 181 / 200) loss: 0.919098\n",
      "(Epoch 19 / 20) train acc: 0.639000; val_acc: 0.611111\n",
      "(Iteration 191 / 200) loss: 0.672705\n",
      "(Epoch 20 / 20) train acc: 0.701000; val_acc: 0.683333\n",
      "(Iteration 1 / 200) loss: 303674.422239\n",
      "(Epoch 0 / 20) train acc: 0.052000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.511000; val_acc: 0.494444\n",
      "(Iteration 11 / 200) loss: 34778.588297\n",
      "(Epoch 2 / 20) train acc: 0.701000; val_acc: 0.691667\n",
      "(Iteration 21 / 200) loss: 8192.720975\n",
      "(Epoch 3 / 20) train acc: 0.823000; val_acc: 0.763889\n",
      "(Iteration 31 / 200) loss: 2575.270712\n",
      "(Epoch 4 / 20) train acc: 0.883000; val_acc: 0.811111\n",
      "(Iteration 41 / 200) loss: 2651.237724\n",
      "(Epoch 5 / 20) train acc: 0.923000; val_acc: 0.850000\n",
      "(Iteration 51 / 200) loss: 632.672166\n",
      "(Epoch 6 / 20) train acc: 0.953000; val_acc: 0.875000\n",
      "(Iteration 61 / 200) loss: 366.948501\n",
      "(Epoch 7 / 20) train acc: 0.950000; val_acc: 0.888889\n",
      "(Iteration 71 / 200) loss: 79.887697\n",
      "(Epoch 8 / 20) train acc: 0.974000; val_acc: 0.883333\n",
      "(Iteration 81 / 200) loss: 202.484887\n",
      "(Epoch 9 / 20) train acc: 0.976000; val_acc: 0.883333\n",
      "(Iteration 91 / 200) loss: 124.114082\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.894444\n",
      "(Iteration 101 / 200) loss: 208.253549\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.911111\n",
      "(Iteration 111 / 200) loss: 502.605469\n",
      "(Epoch 12 / 20) train acc: 0.986000; val_acc: 0.908333\n",
      "(Iteration 121 / 200) loss: 222.080906\n",
      "(Epoch 13 / 20) train acc: 0.989000; val_acc: 0.905556\n",
      "(Iteration 131 / 200) loss: 59.855182\n",
      "(Epoch 14 / 20) train acc: 0.981000; val_acc: 0.916667\n",
      "(Iteration 141 / 200) loss: 0.166656\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.919444\n",
      "(Iteration 151 / 200) loss: 0.166431\n",
      "(Epoch 16 / 20) train acc: 0.995000; val_acc: 0.905556\n",
      "(Iteration 161 / 200) loss: 135.752680\n",
      "(Epoch 17 / 20) train acc: 0.992000; val_acc: 0.902778\n",
      "(Iteration 171 / 200) loss: 112.833921\n",
      "(Epoch 18 / 20) train acc: 0.993000; val_acc: 0.897222\n",
      "(Iteration 181 / 200) loss: 11.323824\n",
      "(Epoch 19 / 20) train acc: 0.998000; val_acc: 0.902778\n",
      "(Iteration 191 / 200) loss: 25.312345\n",
      "(Epoch 20 / 20) train acc: 0.995000; val_acc: 0.900000\n",
      "(Iteration 1 / 200) loss: 3.396504\n",
      "(Epoch 0 / 20) train acc: 0.187000; val_acc: 0.208333\n",
      "(Epoch 1 / 20) train acc: 0.712000; val_acc: 0.650000\n",
      "(Iteration 11 / 200) loss: 0.879883\n",
      "(Epoch 2 / 20) train acc: 0.878000; val_acc: 0.869444\n",
      "(Iteration 21 / 200) loss: 0.216735\n",
      "(Epoch 3 / 20) train acc: 0.927000; val_acc: 0.888889\n",
      "(Iteration 31 / 200) loss: 0.152391\n",
      "(Epoch 4 / 20) train acc: 0.949000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.182325\n",
      "(Epoch 5 / 20) train acc: 0.969000; val_acc: 0.955556\n",
      "(Iteration 51 / 200) loss: 0.085555\n",
      "(Epoch 6 / 20) train acc: 0.985000; val_acc: 0.975000\n",
      "(Iteration 61 / 200) loss: 0.021589\n",
      "(Epoch 7 / 20) train acc: 0.975000; val_acc: 0.947222\n",
      "(Iteration 71 / 200) loss: 0.236642\n",
      "(Epoch 8 / 20) train acc: 0.946000; val_acc: 0.919444\n",
      "(Iteration 81 / 200) loss: 0.079267\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.021312\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.032175\n",
      "(Epoch 11 / 20) train acc: 0.994000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.007868\n",
      "(Epoch 12 / 20) train acc: 0.976000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.030310\n",
      "(Epoch 13 / 20) train acc: 0.978000; val_acc: 0.958333\n",
      "(Iteration 131 / 200) loss: 0.019524\n",
      "(Epoch 14 / 20) train acc: 0.946000; val_acc: 0.936111\n",
      "(Iteration 141 / 200) loss: 0.170335\n",
      "(Epoch 15 / 20) train acc: 0.948000; val_acc: 0.916667\n",
      "(Iteration 151 / 200) loss: 0.132482\n",
      "(Epoch 16 / 20) train acc: 0.972000; val_acc: 0.922222\n",
      "(Iteration 161 / 200) loss: 0.097222\n",
      "(Epoch 17 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 171 / 200) loss: 0.090413\n",
      "(Epoch 18 / 20) train acc: 0.973000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.050451\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.013610\n",
      "(Epoch 20 / 20) train acc: 0.993000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.085000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.216693\n",
      "(Epoch 2 / 20) train acc: 0.246000; val_acc: 0.225000\n",
      "(Iteration 21 / 200) loss: 1.749408\n",
      "(Epoch 3 / 20) train acc: 0.370000; val_acc: 0.327778\n",
      "(Iteration 31 / 200) loss: 1.602226\n",
      "(Epoch 4 / 20) train acc: 0.423000; val_acc: 0.402778\n",
      "(Iteration 41 / 200) loss: 1.652440\n",
      "(Epoch 5 / 20) train acc: 0.577000; val_acc: 0.577778\n",
      "(Iteration 51 / 200) loss: 0.955367\n",
      "(Epoch 6 / 20) train acc: 0.549000; val_acc: 0.525000\n",
      "(Iteration 61 / 200) loss: 0.996213\n",
      "(Epoch 7 / 20) train acc: 0.646000; val_acc: 0.647222\n",
      "(Iteration 71 / 200) loss: 0.760299\n",
      "(Epoch 8 / 20) train acc: 0.659000; val_acc: 0.677778\n",
      "(Iteration 81 / 200) loss: 0.690454\n",
      "(Epoch 9 / 20) train acc: 0.804000; val_acc: 0.816667\n",
      "(Iteration 91 / 200) loss: 0.484897\n",
      "(Epoch 10 / 20) train acc: 0.831000; val_acc: 0.786111\n",
      "(Iteration 101 / 200) loss: 0.564433\n",
      "(Epoch 11 / 20) train acc: 0.911000; val_acc: 0.841667\n",
      "(Iteration 111 / 200) loss: 0.279313\n",
      "(Epoch 12 / 20) train acc: 0.864000; val_acc: 0.872222\n",
      "(Iteration 121 / 200) loss: 0.591252\n",
      "(Epoch 13 / 20) train acc: 0.875000; val_acc: 0.830556\n",
      "(Iteration 131 / 200) loss: 0.530381\n",
      "(Epoch 14 / 20) train acc: 0.901000; val_acc: 0.866667\n",
      "(Iteration 141 / 200) loss: 0.280620\n",
      "(Epoch 15 / 20) train acc: 0.937000; val_acc: 0.913889\n",
      "(Iteration 151 / 200) loss: 0.312105\n",
      "(Epoch 16 / 20) train acc: 0.965000; val_acc: 0.933333\n",
      "(Iteration 161 / 200) loss: 0.119518\n",
      "(Epoch 17 / 20) train acc: 0.970000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 0.102783\n",
      "(Epoch 18 / 20) train acc: 0.970000; val_acc: 0.930556\n",
      "(Iteration 181 / 200) loss: 0.131113\n",
      "(Epoch 19 / 20) train acc: 0.977000; val_acc: 0.938889\n",
      "(Iteration 191 / 200) loss: 0.131613\n",
      "(Epoch 20 / 20) train acc: 0.983000; val_acc: 0.955556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.299164\n",
      "(Epoch 2 / 20) train acc: 0.176000; val_acc: 0.183333\n",
      "(Iteration 21 / 200) loss: 2.051399\n",
      "(Epoch 3 / 20) train acc: 0.194000; val_acc: 0.183333\n",
      "(Iteration 31 / 200) loss: 1.897546\n",
      "(Epoch 4 / 20) train acc: 0.205000; val_acc: 0.183333\n",
      "(Iteration 41 / 200) loss: 1.750043\n",
      "(Epoch 5 / 20) train acc: 0.253000; val_acc: 0.258333\n",
      "(Iteration 51 / 200) loss: 1.560020\n",
      "(Epoch 6 / 20) train acc: 0.288000; val_acc: 0.275000\n",
      "(Iteration 61 / 200) loss: 1.596097\n",
      "(Epoch 7 / 20) train acc: 0.337000; val_acc: 0.366667\n",
      "(Iteration 71 / 200) loss: 1.512004\n",
      "(Epoch 8 / 20) train acc: 0.463000; val_acc: 0.413889\n",
      "(Iteration 81 / 200) loss: 1.312978\n",
      "(Epoch 9 / 20) train acc: 0.565000; val_acc: 0.566667\n",
      "(Iteration 91 / 200) loss: 1.256162\n",
      "(Epoch 10 / 20) train acc: 0.623000; val_acc: 0.572222\n",
      "(Iteration 101 / 200) loss: 0.852077\n",
      "(Epoch 11 / 20) train acc: 0.722000; val_acc: 0.650000\n",
      "(Iteration 111 / 200) loss: 0.776145\n",
      "(Epoch 12 / 20) train acc: 0.813000; val_acc: 0.752778\n",
      "(Iteration 121 / 200) loss: 0.560558\n",
      "(Epoch 13 / 20) train acc: 0.841000; val_acc: 0.850000\n",
      "(Iteration 131 / 200) loss: 0.389616\n",
      "(Epoch 14 / 20) train acc: 0.886000; val_acc: 0.811111\n",
      "(Iteration 141 / 200) loss: 0.507088\n",
      "(Epoch 15 / 20) train acc: 0.890000; val_acc: 0.861111\n",
      "(Iteration 151 / 200) loss: 0.293450\n",
      "(Epoch 16 / 20) train acc: 0.924000; val_acc: 0.891667\n",
      "(Iteration 161 / 200) loss: 0.238901\n",
      "(Epoch 17 / 20) train acc: 0.903000; val_acc: 0.833333\n",
      "(Iteration 171 / 200) loss: 0.435809\n",
      "(Epoch 18 / 20) train acc: 0.938000; val_acc: 0.908333\n",
      "(Iteration 181 / 200) loss: 0.108807\n",
      "(Epoch 19 / 20) train acc: 0.940000; val_acc: 0.894444\n",
      "(Iteration 191 / 200) loss: 0.235772\n",
      "(Epoch 20 / 20) train acc: 0.956000; val_acc: 0.888889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.303218\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.309520\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.307976\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.307379\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.296846\n",
      "(Epoch 6 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.294450\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.306339\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.301932\n",
      "(Epoch 9 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.301712\n",
      "(Epoch 10 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.297182\n",
      "(Epoch 11 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.305396\n",
      "(Epoch 12 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.297093\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.291179\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.291122\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.293196\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.297600\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.299736\n",
      "(Epoch 18 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.304060\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.310317\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.307100\n",
      "(Epoch 2 / 20) train acc: 0.123000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.309619\n",
      "(Epoch 3 / 20) train acc: 0.122000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.303354\n",
      "(Epoch 4 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.301507\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.298235\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.297135\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.293868\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302033\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.311124\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.305796\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.306061\n",
      "(Epoch 12 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.312332\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.292909\n",
      "(Epoch 14 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.296782\n",
      "(Epoch 15 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.305001\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.310480\n",
      "(Epoch 17 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302477\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.296055\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301440\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 230652.059888\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.063889\n",
      "(Iteration 11 / 200) loss: 105351.304167\n",
      "(Epoch 2 / 20) train acc: 0.148000; val_acc: 0.130556\n",
      "(Iteration 21 / 200) loss: 71570.881943\n",
      "(Epoch 3 / 20) train acc: 0.209000; val_acc: 0.183333\n",
      "(Iteration 31 / 200) loss: 60019.797173\n",
      "(Epoch 4 / 20) train acc: 0.269000; val_acc: 0.266667\n",
      "(Iteration 41 / 200) loss: 34267.103916\n",
      "(Epoch 5 / 20) train acc: 0.355000; val_acc: 0.341667\n",
      "(Iteration 51 / 200) loss: 32362.114170\n",
      "(Epoch 6 / 20) train acc: 0.436000; val_acc: 0.400000\n",
      "(Iteration 61 / 200) loss: 18455.312842\n",
      "(Epoch 7 / 20) train acc: 0.500000; val_acc: 0.466667\n",
      "(Iteration 71 / 200) loss: 24169.644746\n",
      "(Epoch 8 / 20) train acc: 0.559000; val_acc: 0.530556\n",
      "(Iteration 81 / 200) loss: 14997.583538\n",
      "(Epoch 9 / 20) train acc: 0.634000; val_acc: 0.583333\n",
      "(Iteration 91 / 200) loss: 9736.807290\n",
      "(Epoch 10 / 20) train acc: 0.649000; val_acc: 0.611111\n",
      "(Iteration 101 / 200) loss: 12164.004978\n",
      "(Epoch 11 / 20) train acc: 0.664000; val_acc: 0.627778\n",
      "(Iteration 111 / 200) loss: 7505.971729\n",
      "(Epoch 12 / 20) train acc: 0.728000; val_acc: 0.663889\n",
      "(Iteration 121 / 200) loss: 5264.402266\n",
      "(Epoch 13 / 20) train acc: 0.708000; val_acc: 0.688889\n",
      "(Iteration 131 / 200) loss: 6513.057332\n",
      "(Epoch 14 / 20) train acc: 0.756000; val_acc: 0.708333\n",
      "(Iteration 141 / 200) loss: 4995.247188\n",
      "(Epoch 15 / 20) train acc: 0.777000; val_acc: 0.713889\n",
      "(Iteration 151 / 200) loss: 4124.858140\n",
      "(Epoch 16 / 20) train acc: 0.795000; val_acc: 0.716667\n",
      "(Iteration 161 / 200) loss: 5473.617095\n",
      "(Epoch 17 / 20) train acc: 0.771000; val_acc: 0.730556\n",
      "(Iteration 171 / 200) loss: 2569.467568\n",
      "(Epoch 18 / 20) train acc: 0.790000; val_acc: 0.741667\n",
      "(Iteration 181 / 200) loss: 5195.700671\n",
      "(Epoch 19 / 20) train acc: 0.820000; val_acc: 0.747222\n",
      "(Iteration 191 / 200) loss: 1925.812373\n",
      "(Epoch 20 / 20) train acc: 0.830000; val_acc: 0.752778\n",
      "(Iteration 1 / 200) loss: 5.008961\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.678000; val_acc: 0.686111\n",
      "(Iteration 11 / 200) loss: 2.812621\n",
      "(Epoch 2 / 20) train acc: 0.868000; val_acc: 0.838889\n",
      "(Iteration 21 / 200) loss: 2.067003\n",
      "(Epoch 3 / 20) train acc: 0.918000; val_acc: 0.922222\n",
      "(Iteration 31 / 200) loss: 1.826979\n",
      "(Epoch 4 / 20) train acc: 0.952000; val_acc: 0.938889\n",
      "(Iteration 41 / 200) loss: 1.720040\n",
      "(Epoch 5 / 20) train acc: 0.969000; val_acc: 0.961111\n",
      "(Iteration 51 / 200) loss: 1.544854\n",
      "(Epoch 6 / 20) train acc: 0.978000; val_acc: 0.972222\n",
      "(Iteration 61 / 200) loss: 1.433680\n",
      "(Epoch 7 / 20) train acc: 0.974000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 1.373112\n",
      "(Epoch 8 / 20) train acc: 0.990000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 1.308899\n",
      "(Epoch 9 / 20) train acc: 0.981000; val_acc: 0.972222\n",
      "(Iteration 91 / 200) loss: 1.251194\n",
      "(Epoch 10 / 20) train acc: 0.992000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 1.174696\n",
      "(Epoch 11 / 20) train acc: 0.990000; val_acc: 0.966667\n",
      "(Iteration 111 / 200) loss: 1.116685\n",
      "(Epoch 12 / 20) train acc: 0.992000; val_acc: 0.983333\n",
      "(Iteration 121 / 200) loss: 1.121385\n",
      "(Epoch 13 / 20) train acc: 0.992000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 1.033445\n",
      "(Epoch 14 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 1.028155\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.973610\n",
      "(Epoch 16 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 161 / 200) loss: 0.947967\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.907411\n",
      "(Epoch 18 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 181 / 200) loss: 0.871079\n",
      "(Epoch 19 / 20) train acc: 0.988000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.852181\n",
      "(Epoch 20 / 20) train acc: 0.999000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.320664\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.306035\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.303112\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.301952\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.303757\n",
      "(Epoch 5 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302266\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302950\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.303054\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.300993\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.300474\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.301922\n",
      "(Epoch 11 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.300743\n",
      "(Epoch 12 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304128\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302898\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.303987\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.303022\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302247\n",
      "(Epoch 17 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.305660\n",
      "(Epoch 18 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301424\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.296697\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302767\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302567\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302538\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302218\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.303196\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.303337\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.303025\n",
      "(Epoch 7 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.301705\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.300553\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.301191\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.301254\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303547\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.303581\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302113\n",
      "(Epoch 14 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.298460\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.305959\n",
      "(Epoch 16 / 20) train acc: 0.083000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.305872\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.297099\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.303237\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301194\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302216\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302036\n",
      "(Epoch 3 / 20) train acc: 0.131000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302476\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302291\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.300660\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302654\n",
      "(Epoch 7 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.301022\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302111\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.300064\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.307658\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.296589\n",
      "(Epoch 12 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.293877\n",
      "(Epoch 13 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.304710\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.294598\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.298762\n",
      "(Epoch 16 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.293901\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.308541\n",
      "(Epoch 18 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303169\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302051\n",
      "(Epoch 20 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302564\n",
      "(Epoch 3 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302896\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.303111\n",
      "(Epoch 5 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.300179\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301432\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.301039\n",
      "(Epoch 8 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.297794\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.298495\n",
      "(Epoch 10 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.304440\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.300349\n",
      "(Epoch 12 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.311196\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.298197\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.293491\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.297498\n",
      "(Epoch 16 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.308519\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.305285\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303326\n",
      "(Epoch 19 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301608\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 146109.448774\n",
      "(Epoch 0 / 20) train acc: 0.061000; val_acc: 0.061111\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 88902.865684\n",
      "(Epoch 2 / 20) train acc: 0.150000; val_acc: 0.108333\n",
      "(Iteration 21 / 200) loss: 52415.457183\n",
      "(Epoch 3 / 20) train acc: 0.201000; val_acc: 0.188889\n",
      "(Iteration 31 / 200) loss: 37669.393101\n",
      "(Epoch 4 / 20) train acc: 0.314000; val_acc: 0.291667\n",
      "(Iteration 41 / 200) loss: 23480.908369\n",
      "(Epoch 5 / 20) train acc: 0.436000; val_acc: 0.377778\n",
      "(Iteration 51 / 200) loss: 19278.671411\n",
      "(Epoch 6 / 20) train acc: 0.537000; val_acc: 0.450000\n",
      "(Iteration 61 / 200) loss: 14152.910439\n",
      "(Epoch 7 / 20) train acc: 0.590000; val_acc: 0.511111\n",
      "(Iteration 71 / 200) loss: 13262.506929\n",
      "(Epoch 8 / 20) train acc: 0.655000; val_acc: 0.552778\n",
      "(Iteration 81 / 200) loss: 11740.530967\n",
      "(Epoch 9 / 20) train acc: 0.713000; val_acc: 0.588889\n",
      "(Iteration 91 / 200) loss: 8902.926167\n",
      "(Epoch 10 / 20) train acc: 0.696000; val_acc: 0.616667\n",
      "(Iteration 101 / 200) loss: 5732.843381\n",
      "(Epoch 11 / 20) train acc: 0.737000; val_acc: 0.625000\n",
      "(Iteration 111 / 200) loss: 7560.634377\n",
      "(Epoch 12 / 20) train acc: 0.798000; val_acc: 0.655556\n",
      "(Iteration 121 / 200) loss: 3320.805103\n",
      "(Epoch 13 / 20) train acc: 0.789000; val_acc: 0.672222\n",
      "(Iteration 131 / 200) loss: 6018.949771\n",
      "(Epoch 14 / 20) train acc: 0.788000; val_acc: 0.672222\n",
      "(Iteration 141 / 200) loss: 6159.189834\n",
      "(Epoch 15 / 20) train acc: 0.783000; val_acc: 0.672222\n",
      "(Iteration 151 / 200) loss: 4927.929773\n",
      "(Epoch 16 / 20) train acc: 0.810000; val_acc: 0.688889\n",
      "(Iteration 161 / 200) loss: 2524.248584\n",
      "(Epoch 17 / 20) train acc: 0.836000; val_acc: 0.691667\n",
      "(Iteration 171 / 200) loss: 3763.546362\n",
      "(Epoch 18 / 20) train acc: 0.851000; val_acc: 0.716667\n",
      "(Iteration 181 / 200) loss: 3536.776646\n",
      "(Epoch 19 / 20) train acc: 0.847000; val_acc: 0.719444\n",
      "(Iteration 191 / 200) loss: 1242.951406\n",
      "(Epoch 20 / 20) train acc: 0.877000; val_acc: 0.736111\n",
      "(Iteration 1 / 200) loss: 4.824272\n",
      "(Epoch 0 / 20) train acc: 0.149000; val_acc: 0.155556\n",
      "(Epoch 1 / 20) train acc: 0.673000; val_acc: 0.650000\n",
      "(Iteration 11 / 200) loss: 3.105394\n",
      "(Epoch 2 / 20) train acc: 0.871000; val_acc: 0.827778\n",
      "(Iteration 21 / 200) loss: 2.252512\n",
      "(Epoch 3 / 20) train acc: 0.925000; val_acc: 0.877778\n",
      "(Iteration 31 / 200) loss: 1.873820\n",
      "(Epoch 4 / 20) train acc: 0.917000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 1.712269\n",
      "(Epoch 5 / 20) train acc: 0.961000; val_acc: 0.927778\n",
      "(Iteration 51 / 200) loss: 1.557609\n",
      "(Epoch 6 / 20) train acc: 0.970000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 1.468565\n",
      "(Epoch 7 / 20) train acc: 0.976000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 1.333314\n",
      "(Epoch 8 / 20) train acc: 0.994000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 1.283796\n",
      "(Epoch 9 / 20) train acc: 0.982000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 1.234018\n",
      "(Epoch 10 / 20) train acc: 0.984000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 1.185640\n",
      "(Epoch 11 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 1.120129\n",
      "(Epoch 12 / 20) train acc: 0.976000; val_acc: 0.947222\n",
      "(Iteration 121 / 200) loss: 1.136140\n",
      "(Epoch 13 / 20) train acc: 0.993000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 1.066992\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 1.068063\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.974177\n",
      "(Epoch 16 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.954093\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.912354\n",
      "(Epoch 18 / 20) train acc: 0.985000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.893351\n",
      "(Epoch 19 / 20) train acc: 0.992000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.915481\n",
      "(Epoch 20 / 20) train acc: 0.994000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.320723\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.306153\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.303212\n",
      "(Epoch 3 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.301685\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.303468\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.300529\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302166\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.298949\n",
      "(Epoch 8 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.303718\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.300666\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.300643\n",
      "(Epoch 11 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.301295\n",
      "(Epoch 12 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.303698\n",
      "(Epoch 13 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302875\n",
      "(Epoch 14 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302241\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302491\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.303465\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.304409\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.300599\n",
      "(Epoch 19 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.299446\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302767\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302030\n",
      "(Epoch 2 / 20) train acc: 0.090000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302973\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.300579\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.301324\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.301190\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301510\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302784\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.301444\n",
      "(Epoch 9 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.299865\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.299132\n",
      "(Epoch 11 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.299846\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304589\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.300231\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.292278\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.304056\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302270\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301219\n",
      "(Epoch 18 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.298041\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.304895\n",
      "(Epoch 20 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.301815\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302155\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.301613\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.300877\n",
      "(Epoch 5 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302166\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302056\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.303875\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.301592\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.297601\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.299883\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.303381\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302565\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.301043\n",
      "(Epoch 14 / 20) train acc: 0.129000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302916\n",
      "(Epoch 15 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.304306\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.305801\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.303198\n",
      "(Epoch 18 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.306008\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302530\n",
      "(Epoch 20 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302256\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.300791\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302667\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.300930\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.297028\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.305993\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.307885\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.308188\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302943\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.296981\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.293832\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.298449\n",
      "(Epoch 13 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.303316\n",
      "(Epoch 14 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.300437\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.306348\n",
      "(Epoch 16 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.297232\n",
      "(Epoch 17 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.293813\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.308918\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301195\n",
      "(Epoch 20 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 326046.484658\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 125216.343247\n",
      "(Epoch 2 / 20) train acc: 0.185000; val_acc: 0.163889\n",
      "(Iteration 21 / 200) loss: 98634.391318\n",
      "(Epoch 3 / 20) train acc: 0.200000; val_acc: 0.183333\n",
      "(Iteration 31 / 200) loss: 62676.243384\n",
      "(Epoch 4 / 20) train acc: 0.277000; val_acc: 0.211111\n",
      "(Iteration 41 / 200) loss: 50798.213914\n",
      "(Epoch 5 / 20) train acc: 0.302000; val_acc: 0.275000\n",
      "(Iteration 51 / 200) loss: 34072.638525\n",
      "(Epoch 6 / 20) train acc: 0.387000; val_acc: 0.358333\n",
      "(Iteration 61 / 200) loss: 23552.283406\n",
      "(Epoch 7 / 20) train acc: 0.519000; val_acc: 0.441667\n",
      "(Iteration 71 / 200) loss: 27292.331667\n",
      "(Epoch 8 / 20) train acc: 0.550000; val_acc: 0.488889\n",
      "(Iteration 81 / 200) loss: 18597.511074\n",
      "(Epoch 9 / 20) train acc: 0.590000; val_acc: 0.527778\n",
      "(Iteration 91 / 200) loss: 16013.890830\n",
      "(Epoch 10 / 20) train acc: 0.626000; val_acc: 0.569444\n",
      "(Iteration 101 / 200) loss: 11764.217349\n",
      "(Epoch 11 / 20) train acc: 0.655000; val_acc: 0.627778\n",
      "(Iteration 111 / 200) loss: 6070.354341\n",
      "(Epoch 12 / 20) train acc: 0.682000; val_acc: 0.647222\n",
      "(Iteration 121 / 200) loss: 7143.470576\n",
      "(Epoch 13 / 20) train acc: 0.736000; val_acc: 0.666667\n",
      "(Iteration 131 / 200) loss: 8644.584551\n",
      "(Epoch 14 / 20) train acc: 0.756000; val_acc: 0.675000\n",
      "(Iteration 141 / 200) loss: 9779.578521\n",
      "(Epoch 15 / 20) train acc: 0.758000; val_acc: 0.680556\n",
      "(Iteration 151 / 200) loss: 3010.573662\n",
      "(Epoch 16 / 20) train acc: 0.783000; val_acc: 0.711111\n",
      "(Iteration 161 / 200) loss: 6454.284429\n",
      "(Epoch 17 / 20) train acc: 0.808000; val_acc: 0.713889\n",
      "(Iteration 171 / 200) loss: 2646.830786\n",
      "(Epoch 18 / 20) train acc: 0.783000; val_acc: 0.725000\n",
      "(Iteration 181 / 200) loss: 2774.334919\n",
      "(Epoch 19 / 20) train acc: 0.831000; val_acc: 0.730556\n",
      "(Iteration 191 / 200) loss: 3874.608721\n",
      "(Epoch 20 / 20) train acc: 0.822000; val_acc: 0.736111\n",
      "(Iteration 1 / 200) loss: 5.305235\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.130556\n",
      "(Epoch 1 / 20) train acc: 0.645000; val_acc: 0.638889\n",
      "(Iteration 11 / 200) loss: 3.147517\n",
      "(Epoch 2 / 20) train acc: 0.851000; val_acc: 0.805556\n",
      "(Iteration 21 / 200) loss: 2.319417\n",
      "(Epoch 3 / 20) train acc: 0.912000; val_acc: 0.872222\n",
      "(Iteration 31 / 200) loss: 1.770132\n",
      "(Epoch 4 / 20) train acc: 0.942000; val_acc: 0.927778\n",
      "(Iteration 41 / 200) loss: 1.709006\n",
      "(Epoch 5 / 20) train acc: 0.955000; val_acc: 0.941667\n",
      "(Iteration 51 / 200) loss: 1.558697\n",
      "(Epoch 6 / 20) train acc: 0.971000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 1.492834\n",
      "(Epoch 7 / 20) train acc: 0.976000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 1.362400\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 1.337027\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 1.213805\n",
      "(Epoch 10 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 1.165547\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.952778\n",
      "(Iteration 111 / 200) loss: 1.119665\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 121 / 200) loss: 1.100759\n",
      "(Epoch 13 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 1.031390\n",
      "(Epoch 14 / 20) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 1.004820\n",
      "(Epoch 15 / 20) train acc: 0.995000; val_acc: 0.969444\n",
      "(Iteration 151 / 200) loss: 0.990173\n",
      "(Epoch 16 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.942163\n",
      "(Epoch 17 / 20) train acc: 0.999000; val_acc: 0.977778\n",
      "(Iteration 171 / 200) loss: 0.912985\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.881257\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 191 / 200) loss: 0.851995\n",
      "(Epoch 20 / 20) train acc: 0.998000; val_acc: 0.972222\n",
      "(Iteration 1 / 200) loss: 2.320848\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.305522\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.304052\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302785\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.303339\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.301990\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.303415\n",
      "(Epoch 7 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.303136\n",
      "(Epoch 8 / 20) train acc: 0.132000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302953\n",
      "(Epoch 9 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.303512\n",
      "(Epoch 10 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.301353\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303922\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.304186\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302020\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301521\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.296140\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.303624\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.303801\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.298718\n",
      "(Epoch 19 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.297440\n",
      "(Epoch 20 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302767\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302497\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302148\n",
      "(Epoch 3 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302106\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302164\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.299767\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.298574\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.300039\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.291901\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.301919\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302972\n",
      "(Epoch 11 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.301633\n",
      "(Epoch 12 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302967\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.297409\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.292626\n",
      "(Epoch 15 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.299562\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.305939\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.299958\n",
      "(Epoch 18 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.311291\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.305428\n",
      "(Epoch 20 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302686\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302753\n",
      "(Epoch 3 / 20) train acc: 0.130000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302834\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302712\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.303097\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.303670\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.306524\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.301876\n",
      "(Epoch 9 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.304688\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.303582\n",
      "(Epoch 11 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.304345\n",
      "(Epoch 12 / 20) train acc: 0.077000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.298676\n",
      "(Epoch 13 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.298907\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.297366\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.304639\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.291725\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.296545\n",
      "(Epoch 18 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.297710\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.294356\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302573\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.303387\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302247\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302522\n",
      "(Epoch 5 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.299249\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.296520\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.303318\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.303003\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.312725\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.299166\n",
      "(Epoch 11 / 20) train acc: 0.083000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302704\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.303155\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.290722\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.301089\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.304695\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.294202\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.308221\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.305353\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.299406\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 328030.065220\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.144444\n",
      "(Epoch 1 / 20) train acc: 0.192000; val_acc: 0.163889\n",
      "(Iteration 11 / 200) loss: 213533.110094\n",
      "(Epoch 2 / 20) train acc: 0.206000; val_acc: 0.213889\n",
      "(Iteration 21 / 200) loss: 156635.938079\n",
      "(Epoch 3 / 20) train acc: 0.227000; val_acc: 0.241667\n",
      "(Iteration 31 / 200) loss: 127824.772806\n",
      "(Epoch 4 / 20) train acc: 0.260000; val_acc: 0.261111\n",
      "(Iteration 41 / 200) loss: 79822.041715\n",
      "(Epoch 5 / 20) train acc: 0.348000; val_acc: 0.316667\n",
      "(Iteration 51 / 200) loss: 47646.019991\n",
      "(Epoch 6 / 20) train acc: 0.370000; val_acc: 0.383333\n",
      "(Iteration 61 / 200) loss: 43097.750576\n",
      "(Epoch 7 / 20) train acc: 0.447000; val_acc: 0.430556\n",
      "(Iteration 71 / 200) loss: 28043.760729\n",
      "(Epoch 8 / 20) train acc: 0.507000; val_acc: 0.475000\n",
      "(Iteration 81 / 200) loss: 24286.477174\n",
      "(Epoch 9 / 20) train acc: 0.543000; val_acc: 0.530556\n",
      "(Iteration 91 / 200) loss: 16129.067322\n",
      "(Epoch 10 / 20) train acc: 0.605000; val_acc: 0.588889\n",
      "(Iteration 101 / 200) loss: 10700.402227\n",
      "(Epoch 11 / 20) train acc: 0.669000; val_acc: 0.638889\n",
      "(Iteration 111 / 200) loss: 13449.443342\n",
      "(Epoch 12 / 20) train acc: 0.686000; val_acc: 0.669444\n",
      "(Iteration 121 / 200) loss: 11634.672943\n",
      "(Epoch 13 / 20) train acc: 0.710000; val_acc: 0.700000\n",
      "(Iteration 131 / 200) loss: 8854.492969\n",
      "(Epoch 14 / 20) train acc: 0.727000; val_acc: 0.711111\n",
      "(Iteration 141 / 200) loss: 10199.821817\n",
      "(Epoch 15 / 20) train acc: 0.758000; val_acc: 0.750000\n",
      "(Iteration 151 / 200) loss: 5003.256939\n",
      "(Epoch 16 / 20) train acc: 0.794000; val_acc: 0.758333\n",
      "(Iteration 161 / 200) loss: 6334.890702\n",
      "(Epoch 17 / 20) train acc: 0.773000; val_acc: 0.772222\n",
      "(Iteration 171 / 200) loss: 4134.409513\n",
      "(Epoch 18 / 20) train acc: 0.789000; val_acc: 0.763889\n",
      "(Iteration 181 / 200) loss: 5257.789279\n",
      "(Epoch 19 / 20) train acc: 0.808000; val_acc: 0.786111\n",
      "(Iteration 191 / 200) loss: 5593.246077\n",
      "(Epoch 20 / 20) train acc: 0.848000; val_acc: 0.802778\n",
      "(Iteration 1 / 200) loss: 3.207490\n",
      "(Epoch 0 / 20) train acc: 0.141000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.610000; val_acc: 0.555556\n",
      "(Iteration 11 / 200) loss: 1.584541\n",
      "(Epoch 2 / 20) train acc: 0.890000; val_acc: 0.827778\n",
      "(Iteration 21 / 200) loss: 0.801332\n",
      "(Epoch 3 / 20) train acc: 0.911000; val_acc: 0.891667\n",
      "(Iteration 31 / 200) loss: 0.386194\n",
      "(Epoch 4 / 20) train acc: 0.954000; val_acc: 0.944444\n",
      "(Iteration 41 / 200) loss: 0.395911\n",
      "(Epoch 5 / 20) train acc: 0.963000; val_acc: 0.947222\n",
      "(Iteration 51 / 200) loss: 0.312757\n",
      "(Epoch 6 / 20) train acc: 0.976000; val_acc: 0.952778\n",
      "(Iteration 61 / 200) loss: 0.252448\n",
      "(Epoch 7 / 20) train acc: 0.991000; val_acc: 0.969444\n",
      "(Iteration 71 / 200) loss: 0.195802\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.975000\n",
      "(Iteration 81 / 200) loss: 0.195559\n",
      "(Epoch 9 / 20) train acc: 0.996000; val_acc: 0.980556\n",
      "(Iteration 91 / 200) loss: 0.193884\n",
      "(Epoch 10 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 101 / 200) loss: 0.198858\n",
      "(Epoch 11 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.166130\n",
      "(Epoch 12 / 20) train acc: 0.998000; val_acc: 0.983333\n",
      "(Iteration 121 / 200) loss: 0.177699\n",
      "(Epoch 13 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.164504\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 141 / 200) loss: 0.183983\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.185737\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 161 / 200) loss: 0.166335\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.161285\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.151858\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.142119\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.304417\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.303175\n",
      "(Epoch 2 / 20) train acc: 0.178000; val_acc: 0.172222\n",
      "(Iteration 21 / 200) loss: 2.300037\n",
      "(Epoch 3 / 20) train acc: 0.191000; val_acc: 0.183333\n",
      "(Iteration 31 / 200) loss: 2.126798\n",
      "(Epoch 4 / 20) train acc: 0.201000; val_acc: 0.213889\n",
      "(Iteration 41 / 200) loss: 1.841266\n",
      "(Epoch 5 / 20) train acc: 0.246000; val_acc: 0.250000\n",
      "(Iteration 51 / 200) loss: 1.755632\n",
      "(Epoch 6 / 20) train acc: 0.296000; val_acc: 0.269444\n",
      "(Iteration 61 / 200) loss: 1.656116\n",
      "(Epoch 7 / 20) train acc: 0.303000; val_acc: 0.277778\n",
      "(Iteration 71 / 200) loss: 1.651420\n",
      "(Epoch 8 / 20) train acc: 0.341000; val_acc: 0.330556\n",
      "(Iteration 81 / 200) loss: 1.594638\n",
      "(Epoch 9 / 20) train acc: 0.369000; val_acc: 0.330556\n",
      "(Iteration 91 / 200) loss: 1.435301\n",
      "(Epoch 10 / 20) train acc: 0.327000; val_acc: 0.313889\n",
      "(Iteration 101 / 200) loss: 1.545419\n",
      "(Epoch 11 / 20) train acc: 0.407000; val_acc: 0.416667\n",
      "(Iteration 111 / 200) loss: 1.439300\n",
      "(Epoch 12 / 20) train acc: 0.482000; val_acc: 0.461111\n",
      "(Iteration 121 / 200) loss: 1.315946\n",
      "(Epoch 13 / 20) train acc: 0.457000; val_acc: 0.472222\n",
      "(Iteration 131 / 200) loss: 1.254506\n",
      "(Epoch 14 / 20) train acc: 0.496000; val_acc: 0.536111\n",
      "(Iteration 141 / 200) loss: 1.084545\n",
      "(Epoch 15 / 20) train acc: 0.538000; val_acc: 0.577778\n",
      "(Iteration 151 / 200) loss: 1.138702\n",
      "(Epoch 16 / 20) train acc: 0.538000; val_acc: 0.583333\n",
      "(Iteration 161 / 200) loss: 0.929856\n",
      "(Epoch 17 / 20) train acc: 0.592000; val_acc: 0.583333\n",
      "(Iteration 171 / 200) loss: 0.927088\n",
      "(Epoch 18 / 20) train acc: 0.654000; val_acc: 0.661111\n",
      "(Iteration 181 / 200) loss: 1.003072\n",
      "(Epoch 19 / 20) train acc: 0.669000; val_acc: 0.644444\n",
      "(Iteration 191 / 200) loss: 0.931236\n",
      "(Epoch 20 / 20) train acc: 0.624000; val_acc: 0.616667\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.301644\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.304849\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.303506\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.301097\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.305227\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.300933\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.299620\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.299965\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.305245\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.304224\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.306663\n",
      "(Epoch 12 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.301250\n",
      "(Epoch 13 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.303424\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.296763\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301086\n",
      "(Epoch 16 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301269\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.296406\n",
      "(Epoch 18 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.307401\n",
      "(Epoch 19 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.297957\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302567\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302065\n",
      "(Epoch 3 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302891\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302657\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.300138\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.299889\n",
      "(Epoch 7 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.304031\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.296964\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.299105\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.304072\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.298388\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.306046\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.303801\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.297901\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.305311\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.296138\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.297694\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.298385\n",
      "(Epoch 19 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.303659\n",
      "(Epoch 20 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302140\n",
      "(Epoch 2 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.301879\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.300527\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302568\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302686\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301352\n",
      "(Epoch 7 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302717\n",
      "(Epoch 8 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.303590\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.300713\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302498\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302164\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.298747\n",
      "(Epoch 13 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.301304\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.308511\n",
      "(Epoch 15 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.297546\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.299331\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.306416\n",
      "(Epoch 18 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.305290\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.305442\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 194028.992784\n",
      "(Epoch 0 / 20) train acc: 0.134000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.176000; val_acc: 0.161111\n",
      "(Iteration 11 / 200) loss: 109217.037083\n",
      "(Epoch 2 / 20) train acc: 0.217000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 92604.056818\n",
      "(Epoch 3 / 20) train acc: 0.249000; val_acc: 0.216667\n",
      "(Iteration 31 / 200) loss: 56664.445903\n",
      "(Epoch 4 / 20) train acc: 0.281000; val_acc: 0.244444\n",
      "(Iteration 41 / 200) loss: 41214.822576\n",
      "(Epoch 5 / 20) train acc: 0.380000; val_acc: 0.294444\n",
      "(Iteration 51 / 200) loss: 25844.521249\n",
      "(Epoch 6 / 20) train acc: 0.425000; val_acc: 0.383333\n",
      "(Iteration 61 / 200) loss: 25481.740408\n",
      "(Epoch 7 / 20) train acc: 0.518000; val_acc: 0.461111\n",
      "(Iteration 71 / 200) loss: 14257.755312\n",
      "(Epoch 8 / 20) train acc: 0.618000; val_acc: 0.527778\n",
      "(Iteration 81 / 200) loss: 15375.796674\n",
      "(Epoch 9 / 20) train acc: 0.684000; val_acc: 0.580556\n",
      "(Iteration 91 / 200) loss: 13431.950116\n",
      "(Epoch 10 / 20) train acc: 0.672000; val_acc: 0.625000\n",
      "(Iteration 101 / 200) loss: 11974.633526\n",
      "(Epoch 11 / 20) train acc: 0.710000; val_acc: 0.644444\n",
      "(Iteration 111 / 200) loss: 8092.038389\n",
      "(Epoch 12 / 20) train acc: 0.739000; val_acc: 0.669444\n",
      "(Iteration 121 / 200) loss: 7417.042907\n",
      "(Epoch 13 / 20) train acc: 0.772000; val_acc: 0.675000\n",
      "(Iteration 131 / 200) loss: 3846.047044\n",
      "(Epoch 14 / 20) train acc: 0.799000; val_acc: 0.691667\n",
      "(Iteration 141 / 200) loss: 5724.579215\n",
      "(Epoch 15 / 20) train acc: 0.828000; val_acc: 0.702778\n",
      "(Iteration 151 / 200) loss: 5310.182980\n",
      "(Epoch 16 / 20) train acc: 0.812000; val_acc: 0.702778\n",
      "(Iteration 161 / 200) loss: 4428.405288\n",
      "(Epoch 17 / 20) train acc: 0.803000; val_acc: 0.708333\n",
      "(Iteration 171 / 200) loss: 4097.428240\n",
      "(Epoch 18 / 20) train acc: 0.842000; val_acc: 0.733333\n",
      "(Iteration 181 / 200) loss: 3663.797307\n",
      "(Epoch 19 / 20) train acc: 0.845000; val_acc: 0.736111\n",
      "(Iteration 191 / 200) loss: 4462.549231\n",
      "(Epoch 20 / 20) train acc: 0.844000; val_acc: 0.750000\n",
      "(Iteration 1 / 200) loss: 3.102941\n",
      "(Epoch 0 / 20) train acc: 0.205000; val_acc: 0.252778\n",
      "(Epoch 1 / 20) train acc: 0.714000; val_acc: 0.641667\n",
      "(Iteration 11 / 200) loss: 1.484050\n",
      "(Epoch 2 / 20) train acc: 0.878000; val_acc: 0.894444\n",
      "(Iteration 21 / 200) loss: 0.781078\n",
      "(Epoch 3 / 20) train acc: 0.940000; val_acc: 0.922222\n",
      "(Iteration 31 / 200) loss: 0.431168\n",
      "(Epoch 4 / 20) train acc: 0.945000; val_acc: 0.930556\n",
      "(Iteration 41 / 200) loss: 0.282563\n",
      "(Epoch 5 / 20) train acc: 0.965000; val_acc: 0.950000\n",
      "(Iteration 51 / 200) loss: 0.357361\n",
      "(Epoch 6 / 20) train acc: 0.973000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.287718\n",
      "(Epoch 7 / 20) train acc: 0.979000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.257226\n",
      "(Epoch 8 / 20) train acc: 0.981000; val_acc: 0.950000\n",
      "(Iteration 81 / 200) loss: 0.314700\n",
      "(Epoch 9 / 20) train acc: 0.987000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.176687\n",
      "(Epoch 10 / 20) train acc: 0.983000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.212937\n",
      "(Epoch 11 / 20) train acc: 0.994000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.189685\n",
      "(Epoch 12 / 20) train acc: 0.994000; val_acc: 0.980556\n",
      "(Iteration 121 / 200) loss: 0.169534\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.983333\n",
      "(Iteration 131 / 200) loss: 0.163181\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.156791\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 151 / 200) loss: 0.157571\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 161 / 200) loss: 0.158998\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 171 / 200) loss: 0.158406\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.152480\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 191 / 200) loss: 0.149239\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.304400\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302200\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.300333\n",
      "(Epoch 3 / 20) train acc: 0.134000; val_acc: 0.125000\n",
      "(Iteration 31 / 200) loss: 2.183107\n",
      "(Epoch 4 / 20) train acc: 0.234000; val_acc: 0.202778\n",
      "(Iteration 41 / 200) loss: 1.832263\n",
      "(Epoch 5 / 20) train acc: 0.289000; val_acc: 0.313889\n",
      "(Iteration 51 / 200) loss: 1.502769\n",
      "(Epoch 6 / 20) train acc: 0.332000; val_acc: 0.291667\n",
      "(Iteration 61 / 200) loss: 1.589297\n",
      "(Epoch 7 / 20) train acc: 0.400000; val_acc: 0.358333\n",
      "(Iteration 71 / 200) loss: 1.563330\n",
      "(Epoch 8 / 20) train acc: 0.427000; val_acc: 0.380556\n",
      "(Iteration 81 / 200) loss: 1.267037\n",
      "(Epoch 9 / 20) train acc: 0.389000; val_acc: 0.352778\n",
      "(Iteration 91 / 200) loss: 1.445364\n",
      "(Epoch 10 / 20) train acc: 0.410000; val_acc: 0.380556\n",
      "(Iteration 101 / 200) loss: 1.310077\n",
      "(Epoch 11 / 20) train acc: 0.440000; val_acc: 0.386111\n",
      "(Iteration 111 / 200) loss: 1.153987\n",
      "(Epoch 12 / 20) train acc: 0.453000; val_acc: 0.430556\n",
      "(Iteration 121 / 200) loss: 1.240583\n",
      "(Epoch 13 / 20) train acc: 0.481000; val_acc: 0.400000\n",
      "(Iteration 131 / 200) loss: 1.358972\n",
      "(Epoch 14 / 20) train acc: 0.461000; val_acc: 0.422222\n",
      "(Iteration 141 / 200) loss: 1.216630\n",
      "(Epoch 15 / 20) train acc: 0.476000; val_acc: 0.408333\n",
      "(Iteration 151 / 200) loss: 1.206421\n",
      "(Epoch 16 / 20) train acc: 0.462000; val_acc: 0.411111\n",
      "(Iteration 161 / 200) loss: 1.145934\n",
      "(Epoch 17 / 20) train acc: 0.473000; val_acc: 0.444444\n",
      "(Iteration 171 / 200) loss: 1.210734\n",
      "(Epoch 18 / 20) train acc: 0.465000; val_acc: 0.438889\n",
      "(Iteration 181 / 200) loss: 1.235056\n",
      "(Epoch 19 / 20) train acc: 0.511000; val_acc: 0.422222\n",
      "(Iteration 191 / 200) loss: 1.146516\n",
      "(Epoch 20 / 20) train acc: 0.505000; val_acc: 0.441667\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302240\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.303143\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.301901\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.300212\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.303225\n",
      "(Epoch 6 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302311\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.303025\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.300185\n",
      "(Epoch 9 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.301993\n",
      "(Epoch 10 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302456\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303739\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.303898\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.300889\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.300575\n",
      "(Epoch 15 / 20) train acc: 0.083000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.294562\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.300576\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.299959\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302594\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.305273\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302908\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302696\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301885\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302931\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.301293\n",
      "(Epoch 6 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.300184\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.305279\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.300181\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.304675\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.303742\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302007\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.301813\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.301855\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.303097\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.301897\n",
      "(Epoch 16 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301853\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.298123\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.298971\n",
      "(Epoch 19 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301096\n",
      "(Epoch 20 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302207\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302513\n",
      "(Epoch 3 / 20) train acc: 0.089000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302331\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302656\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.300331\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.299702\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.300765\n",
      "(Epoch 8 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.303854\n",
      "(Epoch 9 / 20) train acc: 0.086000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.297910\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.308111\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.300530\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.298739\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.307146\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.305783\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.304064\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 161 / 200) loss: 2.301494\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 2.299961\n",
      "(Epoch 18 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 181 / 200) loss: 2.304323\n",
      "(Epoch 19 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.303403\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 170582.267600\n",
      "(Epoch 0 / 20) train acc: 0.063000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.160000; val_acc: 0.161111\n",
      "(Iteration 11 / 200) loss: 113995.076291\n",
      "(Epoch 2 / 20) train acc: 0.215000; val_acc: 0.216667\n",
      "(Iteration 21 / 200) loss: 69594.337865\n",
      "(Epoch 3 / 20) train acc: 0.269000; val_acc: 0.294444\n",
      "(Iteration 31 / 200) loss: 42931.879346\n",
      "(Epoch 4 / 20) train acc: 0.314000; val_acc: 0.375000\n",
      "(Iteration 41 / 200) loss: 43502.109172\n",
      "(Epoch 5 / 20) train acc: 0.387000; val_acc: 0.433333\n",
      "(Iteration 51 / 200) loss: 36353.889115\n",
      "(Epoch 6 / 20) train acc: 0.511000; val_acc: 0.494444\n",
      "(Iteration 61 / 200) loss: 22142.303345\n",
      "(Epoch 7 / 20) train acc: 0.566000; val_acc: 0.586111\n",
      "(Iteration 71 / 200) loss: 11718.355666\n",
      "(Epoch 8 / 20) train acc: 0.624000; val_acc: 0.619444\n",
      "(Iteration 81 / 200) loss: 11989.687836\n",
      "(Epoch 9 / 20) train acc: 0.648000; val_acc: 0.633333\n",
      "(Iteration 91 / 200) loss: 8105.635040\n",
      "(Epoch 10 / 20) train acc: 0.735000; val_acc: 0.661111\n",
      "(Iteration 101 / 200) loss: 7547.853857\n",
      "(Epoch 11 / 20) train acc: 0.748000; val_acc: 0.686111\n",
      "(Iteration 111 / 200) loss: 5317.179571\n",
      "(Epoch 12 / 20) train acc: 0.747000; val_acc: 0.702778\n",
      "(Iteration 121 / 200) loss: 9000.224193\n",
      "(Epoch 13 / 20) train acc: 0.788000; val_acc: 0.711111\n",
      "(Iteration 131 / 200) loss: 5518.829466\n",
      "(Epoch 14 / 20) train acc: 0.814000; val_acc: 0.722222\n",
      "(Iteration 141 / 200) loss: 4985.435892\n",
      "(Epoch 15 / 20) train acc: 0.809000; val_acc: 0.727778\n",
      "(Iteration 151 / 200) loss: 3574.676264\n",
      "(Epoch 16 / 20) train acc: 0.839000; val_acc: 0.733333\n",
      "(Iteration 161 / 200) loss: 4600.084468\n",
      "(Epoch 17 / 20) train acc: 0.842000; val_acc: 0.752778\n",
      "(Iteration 171 / 200) loss: 4761.037125\n",
      "(Epoch 18 / 20) train acc: 0.847000; val_acc: 0.766667\n",
      "(Iteration 181 / 200) loss: 2964.734369\n",
      "(Epoch 19 / 20) train acc: 0.880000; val_acc: 0.769444\n",
      "(Iteration 191 / 200) loss: 2904.554034\n",
      "(Epoch 20 / 20) train acc: 0.861000; val_acc: 0.755556\n",
      "(Iteration 1 / 200) loss: 3.090220\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.682000; val_acc: 0.652778\n",
      "(Iteration 11 / 200) loss: 1.542247\n",
      "(Epoch 2 / 20) train acc: 0.837000; val_acc: 0.825000\n",
      "(Iteration 21 / 200) loss: 0.856576\n",
      "(Epoch 3 / 20) train acc: 0.904000; val_acc: 0.863889\n",
      "(Iteration 31 / 200) loss: 0.532411\n",
      "(Epoch 4 / 20) train acc: 0.950000; val_acc: 0.911111\n",
      "(Iteration 41 / 200) loss: 0.377084\n",
      "(Epoch 5 / 20) train acc: 0.960000; val_acc: 0.927778\n",
      "(Iteration 51 / 200) loss: 0.274308\n",
      "(Epoch 6 / 20) train acc: 0.976000; val_acc: 0.944444\n",
      "(Iteration 61 / 200) loss: 0.192268\n",
      "(Epoch 7 / 20) train acc: 0.972000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.240839\n",
      "(Epoch 8 / 20) train acc: 0.983000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.188026\n",
      "(Epoch 9 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 91 / 200) loss: 0.215552\n",
      "(Epoch 10 / 20) train acc: 0.988000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.177577\n",
      "(Epoch 11 / 20) train acc: 0.991000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.168591\n",
      "(Epoch 12 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 121 / 200) loss: 0.184673\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.172294\n",
      "(Epoch 14 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.165142\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 151 / 200) loss: 0.149904\n",
      "(Epoch 16 / 20) train acc: 0.999000; val_acc: 0.963889\n",
      "(Iteration 161 / 200) loss: 0.151716\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.145144\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.143408\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 191 / 200) loss: 0.140084\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 1 / 200) loss: 2.304384\n",
      "(Epoch 0 / 20) train acc: 0.148000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.303662\n",
      "(Epoch 2 / 20) train acc: 0.135000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302055\n",
      "(Epoch 3 / 20) train acc: 0.231000; val_acc: 0.200000\n",
      "(Iteration 31 / 200) loss: 2.287017\n",
      "(Epoch 4 / 20) train acc: 0.211000; val_acc: 0.191667\n",
      "(Iteration 41 / 200) loss: 2.000655\n",
      "(Epoch 5 / 20) train acc: 0.242000; val_acc: 0.213889\n",
      "(Iteration 51 / 200) loss: 1.874622\n",
      "(Epoch 6 / 20) train acc: 0.272000; val_acc: 0.244444\n",
      "(Iteration 61 / 200) loss: 1.702373\n",
      "(Epoch 7 / 20) train acc: 0.320000; val_acc: 0.330556\n",
      "(Iteration 71 / 200) loss: 1.752548\n",
      "(Epoch 8 / 20) train acc: 0.354000; val_acc: 0.363889\n",
      "(Iteration 81 / 200) loss: 1.670010\n",
      "(Epoch 9 / 20) train acc: 0.370000; val_acc: 0.325000\n",
      "(Iteration 91 / 200) loss: 1.494167\n",
      "(Epoch 10 / 20) train acc: 0.367000; val_acc: 0.336111\n",
      "(Iteration 101 / 200) loss: 1.545857\n",
      "(Epoch 11 / 20) train acc: 0.367000; val_acc: 0.375000\n",
      "(Iteration 111 / 200) loss: 1.390984\n",
      "(Epoch 12 / 20) train acc: 0.385000; val_acc: 0.372222\n",
      "(Iteration 121 / 200) loss: 1.511158\n",
      "(Epoch 13 / 20) train acc: 0.394000; val_acc: 0.402778\n",
      "(Iteration 131 / 200) loss: 1.566671\n",
      "(Epoch 14 / 20) train acc: 0.399000; val_acc: 0.413889\n",
      "(Iteration 141 / 200) loss: 1.569303\n",
      "(Epoch 15 / 20) train acc: 0.433000; val_acc: 0.433333\n",
      "(Iteration 151 / 200) loss: 1.297685\n",
      "(Epoch 16 / 20) train acc: 0.477000; val_acc: 0.452778\n",
      "(Iteration 161 / 200) loss: 1.347895\n",
      "(Epoch 17 / 20) train acc: 0.485000; val_acc: 0.486111\n",
      "(Iteration 171 / 200) loss: 1.195052\n",
      "(Epoch 18 / 20) train acc: 0.462000; val_acc: 0.458333\n",
      "(Iteration 181 / 200) loss: 1.233885\n",
      "(Epoch 19 / 20) train acc: 0.486000; val_acc: 0.511111\n",
      "(Iteration 191 / 200) loss: 1.386711\n",
      "(Epoch 20 / 20) train acc: 0.486000; val_acc: 0.488889\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302530\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302790\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.301820\n",
      "(Epoch 4 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.299854\n",
      "(Epoch 5 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.301162\n",
      "(Epoch 6 / 20) train acc: 0.121000; val_acc: 0.102778\n",
      "(Iteration 61 / 200) loss: 2.297442\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.202345\n",
      "(Epoch 8 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 1.930860\n",
      "(Epoch 9 / 20) train acc: 0.228000; val_acc: 0.172222\n",
      "(Iteration 91 / 200) loss: 2.004454\n",
      "(Epoch 10 / 20) train acc: 0.228000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 2.093457\n",
      "(Epoch 11 / 20) train acc: 0.228000; val_acc: 0.155556\n",
      "(Iteration 111 / 200) loss: 1.968038\n",
      "(Epoch 12 / 20) train acc: 0.222000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.041199\n",
      "(Epoch 13 / 20) train acc: 0.192000; val_acc: 0.172222\n",
      "(Iteration 131 / 200) loss: 2.113051\n",
      "(Epoch 14 / 20) train acc: 0.213000; val_acc: 0.172222\n",
      "(Iteration 141 / 200) loss: 2.010185\n",
      "(Epoch 15 / 20) train acc: 0.233000; val_acc: 0.158333\n",
      "(Iteration 151 / 200) loss: 2.017503\n",
      "(Epoch 16 / 20) train acc: 0.229000; val_acc: 0.161111\n",
      "(Iteration 161 / 200) loss: 1.997564\n",
      "(Epoch 17 / 20) train acc: 0.206000; val_acc: 0.161111\n",
      "(Iteration 171 / 200) loss: 2.022250\n",
      "(Epoch 18 / 20) train acc: 0.203000; val_acc: 0.177778\n",
      "(Iteration 181 / 200) loss: 1.943348\n",
      "(Epoch 19 / 20) train acc: 0.237000; val_acc: 0.169444\n",
      "(Iteration 191 / 200) loss: 1.892678\n",
      "(Epoch 20 / 20) train acc: 0.200000; val_acc: 0.183333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.301852\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302745\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302184\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.301190\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.303459\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301468\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.299315\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.303866\n",
      "(Epoch 9 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.305510\n",
      "(Epoch 10 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.298373\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.297751\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.303838\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.304307\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.303605\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.305517\n",
      "(Epoch 16 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.297740\n",
      "(Epoch 17 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.298328\n",
      "(Epoch 18 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.296227\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.300766\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302320\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302572\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302934\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302473\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.301286\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302412\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.301782\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.303070\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.300392\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302168\n",
      "(Epoch 11 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.301732\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.301691\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.300801\n",
      "(Epoch 14 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302804\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.303703\n",
      "(Epoch 16 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.300145\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302735\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303083\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302361\n",
      "(Epoch 20 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 181391.943760\n",
      "(Epoch 0 / 20) train acc: 0.185000; val_acc: 0.166667\n",
      "(Epoch 1 / 20) train acc: 0.219000; val_acc: 0.180556\n",
      "(Iteration 11 / 200) loss: 98206.485391\n",
      "(Epoch 2 / 20) train acc: 0.224000; val_acc: 0.186111\n",
      "(Iteration 21 / 200) loss: 69705.893247\n",
      "(Epoch 3 / 20) train acc: 0.270000; val_acc: 0.244444\n",
      "(Iteration 31 / 200) loss: 43540.226666\n",
      "(Epoch 4 / 20) train acc: 0.353000; val_acc: 0.291667\n",
      "(Iteration 41 / 200) loss: 34310.765366\n",
      "(Epoch 5 / 20) train acc: 0.433000; val_acc: 0.330556\n",
      "(Iteration 51 / 200) loss: 33548.361770\n",
      "(Epoch 6 / 20) train acc: 0.462000; val_acc: 0.400000\n",
      "(Iteration 61 / 200) loss: 21190.895883\n",
      "(Epoch 7 / 20) train acc: 0.508000; val_acc: 0.447222\n",
      "(Iteration 71 / 200) loss: 15736.832682\n",
      "(Epoch 8 / 20) train acc: 0.607000; val_acc: 0.486111\n",
      "(Iteration 81 / 200) loss: 15926.125880\n",
      "(Epoch 9 / 20) train acc: 0.658000; val_acc: 0.519444\n",
      "(Iteration 91 / 200) loss: 11723.542961\n",
      "(Epoch 10 / 20) train acc: 0.673000; val_acc: 0.544444\n",
      "(Iteration 101 / 200) loss: 9232.691379\n",
      "(Epoch 11 / 20) train acc: 0.699000; val_acc: 0.591667\n",
      "(Iteration 111 / 200) loss: 8837.034950\n",
      "(Epoch 12 / 20) train acc: 0.730000; val_acc: 0.594444\n",
      "(Iteration 121 / 200) loss: 9618.538040\n",
      "(Epoch 13 / 20) train acc: 0.757000; val_acc: 0.608333\n",
      "(Iteration 131 / 200) loss: 8351.222457\n",
      "(Epoch 14 / 20) train acc: 0.782000; val_acc: 0.647222\n",
      "(Iteration 141 / 200) loss: 4703.760725\n",
      "(Epoch 15 / 20) train acc: 0.806000; val_acc: 0.644444\n",
      "(Iteration 151 / 200) loss: 4185.239700\n",
      "(Epoch 16 / 20) train acc: 0.792000; val_acc: 0.666667\n",
      "(Iteration 161 / 200) loss: 3714.904993\n",
      "(Epoch 17 / 20) train acc: 0.808000; val_acc: 0.688889\n",
      "(Iteration 171 / 200) loss: 3679.759117\n",
      "(Epoch 18 / 20) train acc: 0.833000; val_acc: 0.705556\n",
      "(Iteration 181 / 200) loss: 3540.222709\n",
      "(Epoch 19 / 20) train acc: 0.872000; val_acc: 0.727778\n",
      "(Iteration 191 / 200) loss: 3432.522015\n",
      "(Epoch 20 / 20) train acc: 0.851000; val_acc: 0.727778\n",
      "(Iteration 1 / 200) loss: 3.361449\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.637000; val_acc: 0.597222\n",
      "(Iteration 11 / 200) loss: 1.304987\n",
      "(Epoch 2 / 20) train acc: 0.852000; val_acc: 0.822222\n",
      "(Iteration 21 / 200) loss: 0.679733\n",
      "(Epoch 3 / 20) train acc: 0.883000; val_acc: 0.866667\n",
      "(Iteration 31 / 200) loss: 0.255142\n",
      "(Epoch 4 / 20) train acc: 0.931000; val_acc: 0.908333\n",
      "(Iteration 41 / 200) loss: 0.187252\n",
      "(Epoch 5 / 20) train acc: 0.964000; val_acc: 0.930556\n",
      "(Iteration 51 / 200) loss: 0.094976\n",
      "(Epoch 6 / 20) train acc: 0.975000; val_acc: 0.941667\n",
      "(Iteration 61 / 200) loss: 0.158331\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.958333\n",
      "(Iteration 71 / 200) loss: 0.042580\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.961111\n",
      "(Iteration 81 / 200) loss: 0.071125\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 91 / 200) loss: 0.068973\n",
      "(Epoch 10 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 101 / 200) loss: 0.055088\n",
      "(Epoch 11 / 20) train acc: 0.989000; val_acc: 0.972222\n",
      "(Iteration 111 / 200) loss: 0.037213\n",
      "(Epoch 12 / 20) train acc: 0.991000; val_acc: 0.941667\n",
      "(Iteration 121 / 200) loss: 0.041135\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.963889\n",
      "(Iteration 131 / 200) loss: 0.051731\n",
      "(Epoch 14 / 20) train acc: 0.990000; val_acc: 0.952778\n",
      "(Iteration 141 / 200) loss: 0.038245\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.963889\n",
      "(Iteration 151 / 200) loss: 0.041803\n",
      "(Epoch 16 / 20) train acc: 0.994000; val_acc: 0.977778\n",
      "(Iteration 161 / 200) loss: 0.025779\n",
      "(Epoch 17 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 171 / 200) loss: 0.026065\n",
      "(Epoch 18 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 181 / 200) loss: 0.038712\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.972222\n",
      "(Iteration 191 / 200) loss: 0.025133\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.966667\n",
      "(Iteration 1 / 200) loss: 2.302767\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.202000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2.300990\n",
      "(Epoch 2 / 20) train acc: 0.155000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 2.211329\n",
      "(Epoch 3 / 20) train acc: 0.298000; val_acc: 0.305556\n",
      "(Iteration 31 / 200) loss: 1.843694\n",
      "(Epoch 4 / 20) train acc: 0.474000; val_acc: 0.458333\n",
      "(Iteration 41 / 200) loss: 1.312563\n",
      "(Epoch 5 / 20) train acc: 0.676000; val_acc: 0.672222\n",
      "(Iteration 51 / 200) loss: 1.075746\n",
      "(Epoch 6 / 20) train acc: 0.708000; val_acc: 0.733333\n",
      "(Iteration 61 / 200) loss: 0.725102\n",
      "(Epoch 7 / 20) train acc: 0.709000; val_acc: 0.738889\n",
      "(Iteration 71 / 200) loss: 0.787194\n",
      "(Epoch 8 / 20) train acc: 0.810000; val_acc: 0.813889\n",
      "(Iteration 81 / 200) loss: 0.615307\n",
      "(Epoch 9 / 20) train acc: 0.804000; val_acc: 0.825000\n",
      "(Iteration 91 / 200) loss: 0.745847\n",
      "(Epoch 10 / 20) train acc: 0.782000; val_acc: 0.808333\n",
      "(Iteration 101 / 200) loss: 0.554537\n",
      "(Epoch 11 / 20) train acc: 0.815000; val_acc: 0.813889\n",
      "(Iteration 111 / 200) loss: 0.504901\n",
      "(Epoch 12 / 20) train acc: 0.834000; val_acc: 0.838889\n",
      "(Iteration 121 / 200) loss: 0.449805\n",
      "(Epoch 13 / 20) train acc: 0.854000; val_acc: 0.827778\n",
      "(Iteration 131 / 200) loss: 0.426710\n",
      "(Epoch 14 / 20) train acc: 0.870000; val_acc: 0.841667\n",
      "(Iteration 141 / 200) loss: 0.234709\n",
      "(Epoch 15 / 20) train acc: 0.877000; val_acc: 0.850000\n",
      "(Iteration 151 / 200) loss: 0.256118\n",
      "(Epoch 16 / 20) train acc: 0.886000; val_acc: 0.852778\n",
      "(Iteration 161 / 200) loss: 0.280597\n",
      "(Epoch 17 / 20) train acc: 0.905000; val_acc: 0.872222\n",
      "(Iteration 171 / 200) loss: 0.313103\n",
      "(Epoch 18 / 20) train acc: 0.883000; val_acc: 0.875000\n",
      "(Iteration 181 / 200) loss: 0.236775\n",
      "(Epoch 19 / 20) train acc: 0.897000; val_acc: 0.869444\n",
      "(Iteration 191 / 200) loss: 0.177890\n",
      "(Epoch 20 / 20) train acc: 0.921000; val_acc: 0.891667\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302052\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302706\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.272328\n",
      "(Epoch 4 / 20) train acc: 0.076000; val_acc: 0.066667\n",
      "(Iteration 41 / 200) loss: 2.117249\n",
      "(Epoch 5 / 20) train acc: 0.189000; val_acc: 0.161111\n",
      "(Iteration 51 / 200) loss: 2.053369\n",
      "(Epoch 6 / 20) train acc: 0.215000; val_acc: 0.177778\n",
      "(Iteration 61 / 200) loss: 2.042231\n",
      "(Epoch 7 / 20) train acc: 0.226000; val_acc: 0.172222\n",
      "(Iteration 71 / 200) loss: 1.957232\n",
      "(Epoch 8 / 20) train acc: 0.208000; val_acc: 0.166667\n",
      "(Iteration 81 / 200) loss: 1.952590\n",
      "(Epoch 9 / 20) train acc: 0.195000; val_acc: 0.161111\n",
      "(Iteration 91 / 200) loss: 2.043998\n",
      "(Epoch 10 / 20) train acc: 0.211000; val_acc: 0.177778\n",
      "(Iteration 101 / 200) loss: 1.884570\n",
      "(Epoch 11 / 20) train acc: 0.190000; val_acc: 0.175000\n",
      "(Iteration 111 / 200) loss: 1.928540\n",
      "(Epoch 12 / 20) train acc: 0.217000; val_acc: 0.180556\n",
      "(Iteration 121 / 200) loss: 1.988125\n",
      "(Epoch 13 / 20) train acc: 0.229000; val_acc: 0.175000\n",
      "(Iteration 131 / 200) loss: 1.983426\n",
      "(Epoch 14 / 20) train acc: 0.197000; val_acc: 0.172222\n",
      "(Iteration 141 / 200) loss: 1.932012\n",
      "(Epoch 15 / 20) train acc: 0.209000; val_acc: 0.183333\n",
      "(Iteration 151 / 200) loss: 1.852741\n",
      "(Epoch 16 / 20) train acc: 0.208000; val_acc: 0.161111\n",
      "(Iteration 161 / 200) loss: 1.818289\n",
      "(Epoch 17 / 20) train acc: 0.207000; val_acc: 0.166667\n",
      "(Iteration 171 / 200) loss: 1.894726\n",
      "(Epoch 18 / 20) train acc: 0.219000; val_acc: 0.186111\n",
      "(Iteration 181 / 200) loss: 1.962305\n",
      "(Epoch 19 / 20) train acc: 0.218000; val_acc: 0.172222\n",
      "(Iteration 191 / 200) loss: 1.906584\n",
      "(Epoch 20 / 20) train acc: 0.189000; val_acc: 0.172222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302961\n",
      "(Epoch 2 / 20) train acc: 0.072000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302968\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302530\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.301985\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.301502\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.303009\n",
      "(Epoch 7 / 20) train acc: 0.091000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302114\n",
      "(Epoch 8 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.301818\n",
      "(Epoch 9 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.301041\n",
      "(Epoch 10 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.305136\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.301963\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.303868\n",
      "(Epoch 13 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.304198\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.300138\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.301445\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.300985\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.303101\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301300\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302216\n",
      "(Epoch 20 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 2.302531\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 2.303265\n",
      "(Epoch 3 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302921\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302696\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302337\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301702\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302803\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.300720\n",
      "(Epoch 9 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.303725\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302754\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303147\n",
      "(Epoch 12 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.305469\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.301023\n",
      "(Epoch 14 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.301098\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302457\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.303551\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.303335\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.301780\n",
      "(Epoch 19 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301414\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 176778.698363\n",
      "(Epoch 0 / 20) train acc: 0.059000; val_acc: 0.041667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 102859.732545\n",
      "(Epoch 2 / 20) train acc: 0.136000; val_acc: 0.130556\n",
      "(Iteration 21 / 200) loss: 71716.432738\n",
      "(Epoch 3 / 20) train acc: 0.168000; val_acc: 0.213889\n",
      "(Iteration 31 / 200) loss: 44817.193222\n",
      "(Epoch 4 / 20) train acc: 0.360000; val_acc: 0.305556\n",
      "(Iteration 41 / 200) loss: 32995.294111\n",
      "(Epoch 5 / 20) train acc: 0.466000; val_acc: 0.400000\n",
      "(Iteration 51 / 200) loss: 20424.897735\n",
      "(Epoch 6 / 20) train acc: 0.527000; val_acc: 0.500000\n",
      "(Iteration 61 / 200) loss: 24161.288997\n",
      "(Epoch 7 / 20) train acc: 0.565000; val_acc: 0.566667\n",
      "(Iteration 71 / 200) loss: 12990.416578\n",
      "(Epoch 8 / 20) train acc: 0.632000; val_acc: 0.611111\n",
      "(Iteration 81 / 200) loss: 9669.588617\n",
      "(Epoch 9 / 20) train acc: 0.673000; val_acc: 0.638889\n",
      "(Iteration 91 / 200) loss: 9831.554514\n",
      "(Epoch 10 / 20) train acc: 0.713000; val_acc: 0.688889\n",
      "(Iteration 101 / 200) loss: 8760.076137\n",
      "(Epoch 11 / 20) train acc: 0.756000; val_acc: 0.711111\n",
      "(Iteration 111 / 200) loss: 7944.049081\n",
      "(Epoch 12 / 20) train acc: 0.782000; val_acc: 0.719444\n",
      "(Iteration 121 / 200) loss: 7976.310865\n",
      "(Epoch 13 / 20) train acc: 0.809000; val_acc: 0.761111\n",
      "(Iteration 131 / 200) loss: 4025.759904\n",
      "(Epoch 14 / 20) train acc: 0.831000; val_acc: 0.777778\n",
      "(Iteration 141 / 200) loss: 4585.738382\n",
      "(Epoch 15 / 20) train acc: 0.842000; val_acc: 0.786111\n",
      "(Iteration 151 / 200) loss: 4537.678481\n",
      "(Epoch 16 / 20) train acc: 0.834000; val_acc: 0.797222\n",
      "(Iteration 161 / 200) loss: 2910.031114\n",
      "(Epoch 17 / 20) train acc: 0.849000; val_acc: 0.797222\n",
      "(Iteration 171 / 200) loss: 771.456025\n",
      "(Epoch 18 / 20) train acc: 0.816000; val_acc: 0.811111\n",
      "(Iteration 181 / 200) loss: 991.395115\n",
      "(Epoch 19 / 20) train acc: 0.878000; val_acc: 0.811111\n",
      "(Iteration 191 / 200) loss: 1244.694721\n",
      "(Epoch 20 / 20) train acc: 0.885000; val_acc: 0.816667\n",
      "(Iteration 1 / 200) loss: 3.040884\n",
      "(Epoch 0 / 20) train acc: 0.137000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.670000; val_acc: 0.688889\n",
      "(Iteration 11 / 200) loss: 1.147629\n",
      "(Epoch 2 / 20) train acc: 0.901000; val_acc: 0.863889\n",
      "(Iteration 21 / 200) loss: 0.395468\n",
      "(Epoch 3 / 20) train acc: 0.939000; val_acc: 0.913889\n",
      "(Iteration 31 / 200) loss: 0.301485\n",
      "(Epoch 4 / 20) train acc: 0.954000; val_acc: 0.927778\n",
      "(Iteration 41 / 200) loss: 0.245669\n",
      "(Epoch 5 / 20) train acc: 0.975000; val_acc: 0.938889\n",
      "(Iteration 51 / 200) loss: 0.155445\n",
      "(Epoch 6 / 20) train acc: 0.975000; val_acc: 0.947222\n",
      "(Iteration 61 / 200) loss: 0.119357\n",
      "(Epoch 7 / 20) train acc: 0.991000; val_acc: 0.966667\n",
      "(Iteration 71 / 200) loss: 0.061282\n",
      "(Epoch 8 / 20) train acc: 0.992000; val_acc: 0.952778\n",
      "(Iteration 81 / 200) loss: 0.043642\n",
      "(Epoch 9 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 91 / 200) loss: 0.030251\n",
      "(Epoch 10 / 20) train acc: 0.997000; val_acc: 0.969444\n",
      "(Iteration 101 / 200) loss: 0.039820\n",
      "(Epoch 11 / 20) train acc: 0.996000; val_acc: 0.975000\n",
      "(Iteration 111 / 200) loss: 0.043739\n",
      "(Epoch 12 / 20) train acc: 0.995000; val_acc: 0.986111\n",
      "(Iteration 121 / 200) loss: 0.031236\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 131 / 200) loss: 0.028286\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.034584\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 151 / 200) loss: 0.021992\n",
      "(Epoch 16 / 20) train acc: 0.999000; val_acc: 0.986111\n",
      "(Iteration 161 / 200) loss: 0.020860\n",
      "(Epoch 17 / 20) train acc: 0.996000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.074363\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 181 / 200) loss: 0.029829\n",
      "(Epoch 19 / 20) train acc: 0.997000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.021217\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 1 / 200) loss: 2.302768\n",
      "(Epoch 0 / 20) train acc: 0.153000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.085000; val_acc: 0.094444\n",
      "(Iteration 11 / 200) loss: 2.300469\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.133333\n",
      "(Iteration 21 / 200) loss: 2.224694\n",
      "(Epoch 3 / 20) train acc: 0.190000; val_acc: 0.200000\n",
      "(Iteration 31 / 200) loss: 1.878471\n",
      "(Epoch 4 / 20) train acc: 0.488000; val_acc: 0.480556\n",
      "(Iteration 41 / 200) loss: 1.329054\n",
      "(Epoch 5 / 20) train acc: 0.554000; val_acc: 0.588889\n",
      "(Iteration 51 / 200) loss: 1.028131\n",
      "(Epoch 6 / 20) train acc: 0.625000; val_acc: 0.647222\n",
      "(Iteration 61 / 200) loss: 0.963062\n",
      "(Epoch 7 / 20) train acc: 0.612000; val_acc: 0.611111\n",
      "(Iteration 71 / 200) loss: 1.004651\n",
      "(Epoch 8 / 20) train acc: 0.704000; val_acc: 0.711111\n",
      "(Iteration 81 / 200) loss: 0.732740\n",
      "(Epoch 9 / 20) train acc: 0.709000; val_acc: 0.719444\n",
      "(Iteration 91 / 200) loss: 0.881133\n",
      "(Epoch 10 / 20) train acc: 0.707000; val_acc: 0.702778\n",
      "(Iteration 101 / 200) loss: 0.729045\n",
      "(Epoch 11 / 20) train acc: 0.726000; val_acc: 0.730556\n",
      "(Iteration 111 / 200) loss: 0.758594\n",
      "(Epoch 12 / 20) train acc: 0.750000; val_acc: 0.750000\n",
      "(Iteration 121 / 200) loss: 0.635171\n",
      "(Epoch 13 / 20) train acc: 0.760000; val_acc: 0.766667\n",
      "(Iteration 131 / 200) loss: 0.725405\n",
      "(Epoch 14 / 20) train acc: 0.747000; val_acc: 0.711111\n",
      "(Iteration 141 / 200) loss: 0.757366\n",
      "(Epoch 15 / 20) train acc: 0.834000; val_acc: 0.772222\n",
      "(Iteration 151 / 200) loss: 0.543983\n",
      "(Epoch 16 / 20) train acc: 0.803000; val_acc: 0.777778\n",
      "(Iteration 161 / 200) loss: 0.627036\n",
      "(Epoch 17 / 20) train acc: 0.783000; val_acc: 0.783333\n",
      "(Iteration 171 / 200) loss: 0.454574\n",
      "(Epoch 18 / 20) train acc: 0.824000; val_acc: 0.819444\n",
      "(Iteration 181 / 200) loss: 0.587335\n",
      "(Epoch 19 / 20) train acc: 0.839000; val_acc: 0.825000\n",
      "(Iteration 191 / 200) loss: 0.398106\n",
      "(Epoch 20 / 20) train acc: 0.862000; val_acc: 0.819444\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302934\n",
      "(Epoch 2 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302014\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.299923\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.300524\n",
      "(Epoch 5 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.287502\n",
      "(Epoch 6 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.107921\n",
      "(Epoch 7 / 20) train acc: 0.218000; val_acc: 0.194444\n",
      "(Iteration 71 / 200) loss: 2.087330\n",
      "(Epoch 8 / 20) train acc: 0.178000; val_acc: 0.194444\n",
      "(Iteration 81 / 200) loss: 1.902057\n",
      "(Epoch 9 / 20) train acc: 0.208000; val_acc: 0.197222\n",
      "(Iteration 91 / 200) loss: 2.021152\n",
      "(Epoch 10 / 20) train acc: 0.220000; val_acc: 0.238889\n",
      "(Iteration 101 / 200) loss: 2.087670\n",
      "(Epoch 11 / 20) train acc: 0.219000; val_acc: 0.197222\n",
      "(Iteration 111 / 200) loss: 1.985932\n",
      "(Epoch 12 / 20) train acc: 0.221000; val_acc: 0.200000\n",
      "(Iteration 121 / 200) loss: 2.041472\n",
      "(Epoch 13 / 20) train acc: 0.208000; val_acc: 0.194444\n",
      "(Iteration 131 / 200) loss: 1.946414\n",
      "(Epoch 14 / 20) train acc: 0.206000; val_acc: 0.200000\n",
      "(Iteration 141 / 200) loss: 2.089557\n",
      "(Epoch 15 / 20) train acc: 0.213000; val_acc: 0.200000\n",
      "(Iteration 151 / 200) loss: 2.013611\n",
      "(Epoch 16 / 20) train acc: 0.228000; val_acc: 0.197222\n",
      "(Iteration 161 / 200) loss: 2.076125\n",
      "(Epoch 17 / 20) train acc: 0.220000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 1.857033\n",
      "(Epoch 18 / 20) train acc: 0.208000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 1.970575\n",
      "(Epoch 19 / 20) train acc: 0.218000; val_acc: 0.200000\n",
      "(Iteration 191 / 200) loss: 1.926482\n",
      "(Epoch 20 / 20) train acc: 0.212000; val_acc: 0.200000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.085000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302168\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302775\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302364\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302644\n",
      "(Epoch 5 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302068\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302943\n",
      "(Epoch 7 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.303537\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302168\n",
      "(Epoch 9 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.301658\n",
      "(Epoch 10 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302044\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302048\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.299006\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.306302\n",
      "(Epoch 14 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.300738\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.305094\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301202\n",
      "(Epoch 17 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.305405\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302580\n",
      "(Epoch 19 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.306856\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302789\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.301614\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.301743\n",
      "(Epoch 4 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.300502\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.303346\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302748\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.303004\n",
      "(Epoch 8 / 20) train acc: 0.134000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.298676\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.301745\n",
      "(Epoch 10 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.298799\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.309652\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.299136\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.296024\n",
      "(Epoch 14 / 20) train acc: 0.126000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.305059\n",
      "(Epoch 15 / 20) train acc: 0.082000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.297416\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.306877\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.298589\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.305065\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.296344\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 446978.938663\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.155000; val_acc: 0.169444\n",
      "(Iteration 11 / 200) loss: 247598.111965\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 115810.757145\n",
      "(Epoch 3 / 20) train acc: 0.184000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 86450.242917\n",
      "(Epoch 4 / 20) train acc: 0.229000; val_acc: 0.252778\n",
      "(Iteration 41 / 200) loss: 57435.499097\n",
      "(Epoch 5 / 20) train acc: 0.308000; val_acc: 0.319444\n",
      "(Iteration 51 / 200) loss: 38326.840449\n",
      "(Epoch 6 / 20) train acc: 0.391000; val_acc: 0.425000\n",
      "(Iteration 61 / 200) loss: 27912.859466\n",
      "(Epoch 7 / 20) train acc: 0.458000; val_acc: 0.488889\n",
      "(Iteration 71 / 200) loss: 21034.913648\n",
      "(Epoch 8 / 20) train acc: 0.571000; val_acc: 0.536111\n",
      "(Iteration 81 / 200) loss: 17595.104172\n",
      "(Epoch 9 / 20) train acc: 0.585000; val_acc: 0.563889\n",
      "(Iteration 91 / 200) loss: 17835.952300\n",
      "(Epoch 10 / 20) train acc: 0.621000; val_acc: 0.605556\n",
      "(Iteration 101 / 200) loss: 11879.256756\n",
      "(Epoch 11 / 20) train acc: 0.662000; val_acc: 0.644444\n",
      "(Iteration 111 / 200) loss: 7672.932543\n",
      "(Epoch 12 / 20) train acc: 0.687000; val_acc: 0.669444\n",
      "(Iteration 121 / 200) loss: 9832.840917\n",
      "(Epoch 13 / 20) train acc: 0.709000; val_acc: 0.688889\n",
      "(Iteration 131 / 200) loss: 8225.351254\n",
      "(Epoch 14 / 20) train acc: 0.765000; val_acc: 0.708333\n",
      "(Iteration 141 / 200) loss: 3842.999125\n",
      "(Epoch 15 / 20) train acc: 0.753000; val_acc: 0.727778\n",
      "(Iteration 151 / 200) loss: 5413.324532\n",
      "(Epoch 16 / 20) train acc: 0.799000; val_acc: 0.750000\n",
      "(Iteration 161 / 200) loss: 3076.046568\n",
      "(Epoch 17 / 20) train acc: 0.792000; val_acc: 0.761111\n",
      "(Iteration 171 / 200) loss: 3784.784281\n",
      "(Epoch 18 / 20) train acc: 0.829000; val_acc: 0.769444\n",
      "(Iteration 181 / 200) loss: 4104.271108\n",
      "(Epoch 19 / 20) train acc: 0.826000; val_acc: 0.775000\n",
      "(Iteration 191 / 200) loss: 4001.117358\n",
      "(Epoch 20 / 20) train acc: 0.844000; val_acc: 0.775000\n",
      "(Iteration 1 / 200) loss: 2.905124\n",
      "(Epoch 0 / 20) train acc: 0.152000; val_acc: 0.155556\n",
      "(Epoch 1 / 20) train acc: 0.653000; val_acc: 0.633333\n",
      "(Iteration 11 / 200) loss: 1.418854\n",
      "(Epoch 2 / 20) train acc: 0.809000; val_acc: 0.800000\n",
      "(Iteration 21 / 200) loss: 0.606344\n",
      "(Epoch 3 / 20) train acc: 0.884000; val_acc: 0.838889\n",
      "(Iteration 31 / 200) loss: 0.460431\n",
      "(Epoch 4 / 20) train acc: 0.938000; val_acc: 0.891667\n",
      "(Iteration 41 / 200) loss: 0.234872\n",
      "(Epoch 5 / 20) train acc: 0.939000; val_acc: 0.897222\n",
      "(Iteration 51 / 200) loss: 0.154146\n",
      "(Epoch 6 / 20) train acc: 0.967000; val_acc: 0.930556\n",
      "(Iteration 61 / 200) loss: 0.107491\n",
      "(Epoch 7 / 20) train acc: 0.985000; val_acc: 0.952778\n",
      "(Iteration 71 / 200) loss: 0.132604\n",
      "(Epoch 8 / 20) train acc: 0.986000; val_acc: 0.958333\n",
      "(Iteration 81 / 200) loss: 0.066809\n",
      "(Epoch 9 / 20) train acc: 0.986000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.066660\n",
      "(Epoch 10 / 20) train acc: 0.994000; val_acc: 0.975000\n",
      "(Iteration 101 / 200) loss: 0.034271\n",
      "(Epoch 11 / 20) train acc: 0.986000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.040345\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.963889\n",
      "(Iteration 121 / 200) loss: 0.046357\n",
      "(Epoch 13 / 20) train acc: 0.995000; val_acc: 0.983333\n",
      "(Iteration 131 / 200) loss: 0.031200\n",
      "(Epoch 14 / 20) train acc: 0.997000; val_acc: 0.963889\n",
      "(Iteration 141 / 200) loss: 0.026076\n",
      "(Epoch 15 / 20) train acc: 0.997000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.035193\n",
      "(Epoch 16 / 20) train acc: 0.998000; val_acc: 0.986111\n",
      "(Iteration 161 / 200) loss: 0.022441\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 171 / 200) loss: 0.026234\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.027525\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 191 / 200) loss: 0.021884\n",
      "(Epoch 20 / 20) train acc: 0.996000; val_acc: 0.969444\n",
      "(Iteration 1 / 200) loss: 2.302766\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.176000; val_acc: 0.172222\n",
      "(Iteration 11 / 200) loss: 2.300682\n",
      "(Epoch 2 / 20) train acc: 0.148000; val_acc: 0.138889\n",
      "(Iteration 21 / 200) loss: 2.197117\n",
      "(Epoch 3 / 20) train acc: 0.208000; val_acc: 0.194444\n",
      "(Iteration 31 / 200) loss: 1.988361\n",
      "(Epoch 4 / 20) train acc: 0.236000; val_acc: 0.227778\n",
      "(Iteration 41 / 200) loss: 1.763280\n",
      "(Epoch 5 / 20) train acc: 0.285000; val_acc: 0.225000\n",
      "(Iteration 51 / 200) loss: 1.732197\n",
      "(Epoch 6 / 20) train acc: 0.320000; val_acc: 0.355556\n",
      "(Iteration 61 / 200) loss: 1.695188\n",
      "(Epoch 7 / 20) train acc: 0.363000; val_acc: 0.369444\n",
      "(Iteration 71 / 200) loss: 1.635539\n",
      "(Epoch 8 / 20) train acc: 0.391000; val_acc: 0.386111\n",
      "(Iteration 81 / 200) loss: 1.518103\n",
      "(Epoch 9 / 20) train acc: 0.440000; val_acc: 0.444444\n",
      "(Iteration 91 / 200) loss: 1.313631\n",
      "(Epoch 10 / 20) train acc: 0.412000; val_acc: 0.413889\n",
      "(Iteration 101 / 200) loss: 1.343345\n",
      "(Epoch 11 / 20) train acc: 0.500000; val_acc: 0.505556\n",
      "(Iteration 111 / 200) loss: 1.263100\n",
      "(Epoch 12 / 20) train acc: 0.557000; val_acc: 0.550000\n",
      "(Iteration 121 / 200) loss: 1.045938\n",
      "(Epoch 13 / 20) train acc: 0.586000; val_acc: 0.613889\n",
      "(Iteration 131 / 200) loss: 1.029095\n",
      "(Epoch 14 / 20) train acc: 0.619000; val_acc: 0.608333\n",
      "(Iteration 141 / 200) loss: 0.955024\n",
      "(Epoch 15 / 20) train acc: 0.695000; val_acc: 0.680556\n",
      "(Iteration 151 / 200) loss: 0.693360\n",
      "(Epoch 16 / 20) train acc: 0.724000; val_acc: 0.697222\n",
      "(Iteration 161 / 200) loss: 0.877695\n",
      "(Epoch 17 / 20) train acc: 0.697000; val_acc: 0.705556\n",
      "(Iteration 171 / 200) loss: 0.754840\n",
      "(Epoch 18 / 20) train acc: 0.720000; val_acc: 0.733333\n",
      "(Iteration 181 / 200) loss: 0.564199\n",
      "(Epoch 19 / 20) train acc: 0.759000; val_acc: 0.747222\n",
      "(Iteration 191 / 200) loss: 0.597450\n",
      "(Epoch 20 / 20) train acc: 0.742000; val_acc: 0.766667\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302539\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.303447\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.300817\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302806\n",
      "(Epoch 5 / 20) train acc: 0.079000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.303489\n",
      "(Epoch 6 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302116\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302899\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302210\n",
      "(Epoch 9 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.286738\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.114987\n",
      "(Epoch 11 / 20) train acc: 0.174000; val_acc: 0.188889\n",
      "(Iteration 111 / 200) loss: 2.062629\n",
      "(Epoch 12 / 20) train acc: 0.213000; val_acc: 0.194444\n",
      "(Iteration 121 / 200) loss: 2.036956\n",
      "(Epoch 13 / 20) train acc: 0.223000; val_acc: 0.211111\n",
      "(Iteration 131 / 200) loss: 1.958213\n",
      "(Epoch 14 / 20) train acc: 0.233000; val_acc: 0.200000\n",
      "(Iteration 141 / 200) loss: 2.002548\n",
      "(Epoch 15 / 20) train acc: 0.207000; val_acc: 0.200000\n",
      "(Iteration 151 / 200) loss: 1.937233\n",
      "(Epoch 16 / 20) train acc: 0.226000; val_acc: 0.205556\n",
      "(Iteration 161 / 200) loss: 1.823726\n",
      "(Epoch 17 / 20) train acc: 0.231000; val_acc: 0.194444\n",
      "(Iteration 171 / 200) loss: 1.913515\n",
      "(Epoch 18 / 20) train acc: 0.213000; val_acc: 0.194444\n",
      "(Iteration 181 / 200) loss: 2.068802\n",
      "(Epoch 19 / 20) train acc: 0.228000; val_acc: 0.188889\n",
      "(Iteration 191 / 200) loss: 1.849548\n",
      "(Epoch 20 / 20) train acc: 0.212000; val_acc: 0.188889\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.301984\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302382\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.273939\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 1.973464\n",
      "(Epoch 5 / 20) train acc: 0.203000; val_acc: 0.211111\n",
      "(Iteration 51 / 200) loss: 2.031207\n",
      "(Epoch 6 / 20) train acc: 0.201000; val_acc: 0.197222\n",
      "(Iteration 61 / 200) loss: 1.892967\n",
      "(Epoch 7 / 20) train acc: 0.208000; val_acc: 0.191667\n",
      "(Iteration 71 / 200) loss: 1.916896\n",
      "(Epoch 8 / 20) train acc: 0.209000; val_acc: 0.211111\n",
      "(Iteration 81 / 200) loss: 1.846931\n",
      "(Epoch 9 / 20) train acc: 0.210000; val_acc: 0.211111\n",
      "(Iteration 91 / 200) loss: 2.028679\n",
      "(Epoch 10 / 20) train acc: 0.226000; val_acc: 0.197222\n",
      "(Iteration 101 / 200) loss: 1.916440\n",
      "(Epoch 11 / 20) train acc: 0.217000; val_acc: 0.211111\n",
      "(Iteration 111 / 200) loss: 1.904975\n",
      "(Epoch 12 / 20) train acc: 0.194000; val_acc: 0.197222\n",
      "(Iteration 121 / 200) loss: 1.880256\n",
      "(Epoch 13 / 20) train acc: 0.188000; val_acc: 0.211111\n",
      "(Iteration 131 / 200) loss: 1.850958\n",
      "(Epoch 14 / 20) train acc: 0.212000; val_acc: 0.197222\n",
      "(Iteration 141 / 200) loss: 1.851493\n",
      "(Epoch 15 / 20) train acc: 0.197000; val_acc: 0.197222\n",
      "(Iteration 151 / 200) loss: 1.922762\n",
      "(Epoch 16 / 20) train acc: 0.209000; val_acc: 0.211111\n",
      "(Iteration 161 / 200) loss: 1.886115\n",
      "(Epoch 17 / 20) train acc: 0.188000; val_acc: 0.197222\n",
      "(Iteration 171 / 200) loss: 1.903156\n",
      "(Epoch 18 / 20) train acc: 0.220000; val_acc: 0.175000\n",
      "(Iteration 181 / 200) loss: 1.763554\n",
      "(Epoch 19 / 20) train acc: 0.193000; val_acc: 0.175000\n",
      "(Iteration 191 / 200) loss: 1.942015\n",
      "(Epoch 20 / 20) train acc: 0.226000; val_acc: 0.161111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.301876\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302016\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.303828\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.303288\n",
      "(Epoch 5 / 20) train acc: 0.132000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.303391\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.301612\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.301669\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.303251\n",
      "(Epoch 9 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.308663\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.295648\n",
      "(Epoch 11 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303704\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.293677\n",
      "(Epoch 13 / 20) train acc: 0.077000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.300963\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301717\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.294007\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.313040\n",
      "(Epoch 17 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.306683\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.300878\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.300934\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 179353.540012\n",
      "(Epoch 0 / 20) train acc: 0.078000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 107355.299426\n",
      "(Epoch 2 / 20) train acc: 0.249000; val_acc: 0.200000\n",
      "(Iteration 21 / 200) loss: 50256.353982\n",
      "(Epoch 3 / 20) train acc: 0.334000; val_acc: 0.286111\n",
      "(Iteration 31 / 200) loss: 35817.788574\n",
      "(Epoch 4 / 20) train acc: 0.450000; val_acc: 0.416667\n",
      "(Iteration 41 / 200) loss: 24371.200697\n",
      "(Epoch 5 / 20) train acc: 0.511000; val_acc: 0.480556\n",
      "(Iteration 51 / 200) loss: 14836.670349\n",
      "(Epoch 6 / 20) train acc: 0.642000; val_acc: 0.544444\n",
      "(Iteration 61 / 200) loss: 15191.942511\n",
      "(Epoch 7 / 20) train acc: 0.652000; val_acc: 0.611111\n",
      "(Iteration 71 / 200) loss: 11953.694683\n",
      "(Epoch 8 / 20) train acc: 0.700000; val_acc: 0.650000\n",
      "(Iteration 81 / 200) loss: 7388.624990\n",
      "(Epoch 9 / 20) train acc: 0.710000; val_acc: 0.658333\n",
      "(Iteration 91 / 200) loss: 6780.974682\n",
      "(Epoch 10 / 20) train acc: 0.741000; val_acc: 0.686111\n",
      "(Iteration 101 / 200) loss: 9101.767507\n",
      "(Epoch 11 / 20) train acc: 0.792000; val_acc: 0.694444\n",
      "(Iteration 111 / 200) loss: 4441.744085\n",
      "(Epoch 12 / 20) train acc: 0.782000; val_acc: 0.719444\n",
      "(Iteration 121 / 200) loss: 4176.382858\n",
      "(Epoch 13 / 20) train acc: 0.812000; val_acc: 0.725000\n",
      "(Iteration 131 / 200) loss: 4272.016638\n",
      "(Epoch 14 / 20) train acc: 0.831000; val_acc: 0.755556\n",
      "(Iteration 141 / 200) loss: 3213.101360\n",
      "(Epoch 15 / 20) train acc: 0.839000; val_acc: 0.766667\n",
      "(Iteration 151 / 200) loss: 4016.088276\n",
      "(Epoch 16 / 20) train acc: 0.845000; val_acc: 0.780556\n",
      "(Iteration 161 / 200) loss: 4108.220825\n",
      "(Epoch 17 / 20) train acc: 0.864000; val_acc: 0.780556\n",
      "(Iteration 171 / 200) loss: 2863.111816\n",
      "(Epoch 18 / 20) train acc: 0.839000; val_acc: 0.788889\n",
      "(Iteration 181 / 200) loss: 2422.889374\n",
      "(Epoch 19 / 20) train acc: 0.863000; val_acc: 0.805556\n",
      "(Iteration 191 / 200) loss: 1538.514754\n",
      "(Epoch 20 / 20) train acc: 0.882000; val_acc: 0.813889\n",
      "(Iteration 1 / 200) loss: 3.343719\n",
      "(Epoch 0 / 20) train acc: 0.080000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.629000; val_acc: 0.597222\n",
      "(Iteration 11 / 200) loss: 1.392946\n",
      "(Epoch 2 / 20) train acc: 0.862000; val_acc: 0.841667\n",
      "(Iteration 21 / 200) loss: 0.685717\n",
      "(Epoch 3 / 20) train acc: 0.918000; val_acc: 0.883333\n",
      "(Iteration 31 / 200) loss: 0.345899\n",
      "(Epoch 4 / 20) train acc: 0.936000; val_acc: 0.913889\n",
      "(Iteration 41 / 200) loss: 0.181031\n",
      "(Epoch 5 / 20) train acc: 0.966000; val_acc: 0.919444\n",
      "(Iteration 51 / 200) loss: 0.135676\n",
      "(Epoch 6 / 20) train acc: 0.972000; val_acc: 0.927778\n",
      "(Iteration 61 / 200) loss: 0.099456\n",
      "(Epoch 7 / 20) train acc: 0.973000; val_acc: 0.950000\n",
      "(Iteration 71 / 200) loss: 0.047719\n",
      "(Epoch 8 / 20) train acc: 0.984000; val_acc: 0.947222\n",
      "(Iteration 81 / 200) loss: 0.017625\n",
      "(Epoch 9 / 20) train acc: 0.988000; val_acc: 0.952778\n",
      "(Iteration 91 / 200) loss: 0.028811\n",
      "(Epoch 10 / 20) train acc: 0.987000; val_acc: 0.955556\n",
      "(Iteration 101 / 200) loss: 0.065966\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.961111\n",
      "(Iteration 111 / 200) loss: 0.070997\n",
      "(Epoch 12 / 20) train acc: 0.995000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.048811\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.969444\n",
      "(Iteration 131 / 200) loss: 0.020481\n",
      "(Epoch 14 / 20) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 141 / 200) loss: 0.015878\n",
      "(Epoch 15 / 20) train acc: 0.998000; val_acc: 0.966667\n",
      "(Iteration 151 / 200) loss: 0.022225\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.016177\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.966667\n",
      "(Iteration 171 / 200) loss: 0.007908\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.969444\n",
      "(Iteration 181 / 200) loss: 0.011771\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 191 / 200) loss: 0.007619\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302606\n",
      "(Epoch 0 / 20) train acc: 0.164000; val_acc: 0.163889\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.299675\n",
      "(Epoch 2 / 20) train acc: 0.177000; val_acc: 0.197222\n",
      "(Iteration 21 / 200) loss: 2.211927\n",
      "(Epoch 3 / 20) train acc: 0.254000; val_acc: 0.302778\n",
      "(Iteration 31 / 200) loss: 1.674572\n",
      "(Epoch 4 / 20) train acc: 0.456000; val_acc: 0.491667\n",
      "(Iteration 41 / 200) loss: 1.375237\n",
      "(Epoch 5 / 20) train acc: 0.610000; val_acc: 0.636111\n",
      "(Iteration 51 / 200) loss: 1.154426\n",
      "(Epoch 6 / 20) train acc: 0.618000; val_acc: 0.619444\n",
      "(Iteration 61 / 200) loss: 0.846622\n",
      "(Epoch 7 / 20) train acc: 0.684000; val_acc: 0.730556\n",
      "(Iteration 71 / 200) loss: 0.850475\n",
      "(Epoch 8 / 20) train acc: 0.699000; val_acc: 0.677778\n",
      "(Iteration 81 / 200) loss: 0.961526\n",
      "(Epoch 9 / 20) train acc: 0.722000; val_acc: 0.713889\n",
      "(Iteration 91 / 200) loss: 0.936564\n",
      "(Epoch 10 / 20) train acc: 0.746000; val_acc: 0.750000\n",
      "(Iteration 101 / 200) loss: 0.970351\n",
      "(Epoch 11 / 20) train acc: 0.709000; val_acc: 0.691667\n",
      "(Iteration 111 / 200) loss: 0.852084\n",
      "(Epoch 12 / 20) train acc: 0.744000; val_acc: 0.744444\n",
      "(Iteration 121 / 200) loss: 0.694178\n",
      "(Epoch 13 / 20) train acc: 0.761000; val_acc: 0.719444\n",
      "(Iteration 131 / 200) loss: 0.692530\n",
      "(Epoch 14 / 20) train acc: 0.767000; val_acc: 0.763889\n",
      "(Iteration 141 / 200) loss: 0.512900\n",
      "(Epoch 15 / 20) train acc: 0.799000; val_acc: 0.772222\n",
      "(Iteration 151 / 200) loss: 0.600028\n",
      "(Epoch 16 / 20) train acc: 0.774000; val_acc: 0.783333\n",
      "(Iteration 161 / 200) loss: 0.868160\n",
      "(Epoch 17 / 20) train acc: 0.759000; val_acc: 0.733333\n",
      "(Iteration 171 / 200) loss: 0.645976\n",
      "(Epoch 18 / 20) train acc: 0.810000; val_acc: 0.758333\n",
      "(Iteration 181 / 200) loss: 0.613866\n",
      "(Epoch 19 / 20) train acc: 0.789000; val_acc: 0.761111\n",
      "(Iteration 191 / 200) loss: 0.576553\n",
      "(Epoch 20 / 20) train acc: 0.811000; val_acc: 0.780556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302628\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303288\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.301473\n",
      "(Epoch 4 / 20) train acc: 0.128000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.288609\n",
      "(Epoch 5 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.168656\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.094444\n",
      "(Iteration 61 / 200) loss: 2.044596\n",
      "(Epoch 7 / 20) train acc: 0.186000; val_acc: 0.219444\n",
      "(Iteration 71 / 200) loss: 2.002130\n",
      "(Epoch 8 / 20) train acc: 0.176000; val_acc: 0.225000\n",
      "(Iteration 81 / 200) loss: 1.908903\n",
      "(Epoch 9 / 20) train acc: 0.186000; val_acc: 0.216667\n",
      "(Iteration 91 / 200) loss: 1.888701\n",
      "(Epoch 10 / 20) train acc: 0.229000; val_acc: 0.255556\n",
      "(Iteration 101 / 200) loss: 1.891807\n",
      "(Epoch 11 / 20) train acc: 0.186000; val_acc: 0.202778\n",
      "(Iteration 111 / 200) loss: 1.995466\n",
      "(Epoch 12 / 20) train acc: 0.206000; val_acc: 0.216667\n",
      "(Iteration 121 / 200) loss: 1.962857\n",
      "(Epoch 13 / 20) train acc: 0.189000; val_acc: 0.211111\n",
      "(Iteration 131 / 200) loss: 1.931303\n",
      "(Epoch 14 / 20) train acc: 0.194000; val_acc: 0.216667\n",
      "(Iteration 141 / 200) loss: 1.974095\n",
      "(Epoch 15 / 20) train acc: 0.176000; val_acc: 0.188889\n",
      "(Iteration 151 / 200) loss: 1.894436\n",
      "(Epoch 16 / 20) train acc: 0.193000; val_acc: 0.177778\n",
      "(Iteration 161 / 200) loss: 1.864573\n",
      "(Epoch 17 / 20) train acc: 0.205000; val_acc: 0.219444\n",
      "(Iteration 171 / 200) loss: 1.887233\n",
      "(Epoch 18 / 20) train acc: 0.198000; val_acc: 0.183333\n",
      "(Iteration 181 / 200) loss: 1.874102\n",
      "(Epoch 19 / 20) train acc: 0.219000; val_acc: 0.194444\n",
      "(Iteration 191 / 200) loss: 1.901753\n",
      "(Epoch 20 / 20) train acc: 0.212000; val_acc: 0.200000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302437\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302928\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302476\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302027\n",
      "(Epoch 5 / 20) train acc: 0.134000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.303163\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301723\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.300679\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302513\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.300372\n",
      "(Epoch 10 / 20) train acc: 0.084000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.303625\n",
      "(Epoch 11 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.300220\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.301457\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302545\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302664\n",
      "(Epoch 15 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302329\n",
      "(Epoch 16 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.306089\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.295971\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.305752\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.307045\n",
      "(Epoch 20 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302977\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302111\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.303568\n",
      "(Epoch 4 / 20) train acc: 0.123000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302958\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302215\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302676\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.300977\n",
      "(Epoch 8 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302487\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302149\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.296512\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.304190\n",
      "(Epoch 12 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.306627\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.305954\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.296354\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.307071\n",
      "(Epoch 16 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301032\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.299967\n",
      "(Epoch 18 / 20) train acc: 0.079000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.290042\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.295845\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 199198.241344\n",
      "(Epoch 0 / 20) train acc: 0.064000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.186000; val_acc: 0.138889\n",
      "(Iteration 11 / 200) loss: 117010.200733\n",
      "(Epoch 2 / 20) train acc: 0.300000; val_acc: 0.255556\n",
      "(Iteration 21 / 200) loss: 71748.310257\n",
      "(Epoch 3 / 20) train acc: 0.358000; val_acc: 0.322222\n",
      "(Iteration 31 / 200) loss: 48713.859823\n",
      "(Epoch 4 / 20) train acc: 0.414000; val_acc: 0.380556\n",
      "(Iteration 41 / 200) loss: 42456.914412\n",
      "(Epoch 5 / 20) train acc: 0.526000; val_acc: 0.469444\n",
      "(Iteration 51 / 200) loss: 25013.076518\n",
      "(Epoch 6 / 20) train acc: 0.589000; val_acc: 0.533333\n",
      "(Iteration 61 / 200) loss: 10098.924891\n",
      "(Epoch 7 / 20) train acc: 0.674000; val_acc: 0.586111\n",
      "(Iteration 71 / 200) loss: 10124.012036\n",
      "(Epoch 8 / 20) train acc: 0.721000; val_acc: 0.638889\n",
      "(Iteration 81 / 200) loss: 8430.417943\n",
      "(Epoch 9 / 20) train acc: 0.736000; val_acc: 0.677778\n",
      "(Iteration 91 / 200) loss: 10933.860108\n",
      "(Epoch 10 / 20) train acc: 0.785000; val_acc: 0.694444\n",
      "(Iteration 101 / 200) loss: 9060.302905\n",
      "(Epoch 11 / 20) train acc: 0.818000; val_acc: 0.713889\n",
      "(Iteration 111 / 200) loss: 3509.173835\n",
      "(Epoch 12 / 20) train acc: 0.840000; val_acc: 0.708333\n",
      "(Iteration 121 / 200) loss: 6294.086644\n",
      "(Epoch 13 / 20) train acc: 0.854000; val_acc: 0.719444\n",
      "(Iteration 131 / 200) loss: 5306.741956\n",
      "(Epoch 14 / 20) train acc: 0.838000; val_acc: 0.722222\n",
      "(Iteration 141 / 200) loss: 3773.901337\n",
      "(Epoch 15 / 20) train acc: 0.824000; val_acc: 0.738889\n",
      "(Iteration 151 / 200) loss: 1793.211352\n",
      "(Epoch 16 / 20) train acc: 0.878000; val_acc: 0.744444\n",
      "(Iteration 161 / 200) loss: 2501.197310\n",
      "(Epoch 17 / 20) train acc: 0.868000; val_acc: 0.747222\n",
      "(Iteration 171 / 200) loss: 4709.483273\n",
      "(Epoch 18 / 20) train acc: 0.876000; val_acc: 0.763889\n",
      "(Iteration 181 / 200) loss: 928.906585\n",
      "(Epoch 19 / 20) train acc: 0.892000; val_acc: 0.777778\n",
      "(Iteration 191 / 200) loss: 2794.187402\n",
      "(Epoch 20 / 20) train acc: 0.915000; val_acc: 0.769444\n",
      "(Iteration 1 / 200) loss: 2.669408\n",
      "(Epoch 0 / 20) train acc: 0.252000; val_acc: 0.205556\n",
      "(Epoch 1 / 20) train acc: 0.683000; val_acc: 0.700000\n",
      "(Iteration 11 / 200) loss: 1.341284\n",
      "(Epoch 2 / 20) train acc: 0.873000; val_acc: 0.836111\n",
      "(Iteration 21 / 200) loss: 0.550655\n",
      "(Epoch 3 / 20) train acc: 0.939000; val_acc: 0.911111\n",
      "(Iteration 31 / 200) loss: 0.273354\n",
      "(Epoch 4 / 20) train acc: 0.961000; val_acc: 0.922222\n",
      "(Iteration 41 / 200) loss: 0.208507\n",
      "(Epoch 5 / 20) train acc: 0.958000; val_acc: 0.922222\n",
      "(Iteration 51 / 200) loss: 0.201452\n",
      "(Epoch 6 / 20) train acc: 0.956000; val_acc: 0.911111\n",
      "(Iteration 61 / 200) loss: 0.182431\n",
      "(Epoch 7 / 20) train acc: 0.980000; val_acc: 0.936111\n",
      "(Iteration 71 / 200) loss: 0.083975\n",
      "(Epoch 8 / 20) train acc: 0.989000; val_acc: 0.963889\n",
      "(Iteration 81 / 200) loss: 0.048549\n",
      "(Epoch 9 / 20) train acc: 0.989000; val_acc: 0.955556\n",
      "(Iteration 91 / 200) loss: 0.021506\n",
      "(Epoch 10 / 20) train acc: 0.995000; val_acc: 0.952778\n",
      "(Iteration 101 / 200) loss: 0.040796\n",
      "(Epoch 11 / 20) train acc: 0.999000; val_acc: 0.963889\n",
      "(Iteration 111 / 200) loss: 0.023984\n",
      "(Epoch 12 / 20) train acc: 0.999000; val_acc: 0.961111\n",
      "(Iteration 121 / 200) loss: 0.016944\n",
      "(Epoch 13 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 131 / 200) loss: 0.018419\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 141 / 200) loss: 0.011163\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.983333\n",
      "(Iteration 151 / 200) loss: 0.016715\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 161 / 200) loss: 0.013305\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 171 / 200) loss: 0.009022\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.977778\n",
      "(Iteration 181 / 200) loss: 0.006769\n",
      "(Epoch 19 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.013740\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.142000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.298379\n",
      "(Epoch 2 / 20) train acc: 0.142000; val_acc: 0.147222\n",
      "(Iteration 21 / 200) loss: 2.166365\n",
      "(Epoch 3 / 20) train acc: 0.235000; val_acc: 0.211111\n",
      "(Iteration 31 / 200) loss: 1.975021\n",
      "(Epoch 4 / 20) train acc: 0.308000; val_acc: 0.275000\n",
      "(Iteration 41 / 200) loss: 1.606553\n",
      "(Epoch 5 / 20) train acc: 0.293000; val_acc: 0.330556\n",
      "(Iteration 51 / 200) loss: 1.498395\n",
      "(Epoch 6 / 20) train acc: 0.389000; val_acc: 0.422222\n",
      "(Iteration 61 / 200) loss: 1.453568\n",
      "(Epoch 7 / 20) train acc: 0.535000; val_acc: 0.536111\n",
      "(Iteration 71 / 200) loss: 1.351059\n",
      "(Epoch 8 / 20) train acc: 0.547000; val_acc: 0.511111\n",
      "(Iteration 81 / 200) loss: 1.069903\n",
      "(Epoch 9 / 20) train acc: 0.645000; val_acc: 0.627778\n",
      "(Iteration 91 / 200) loss: 0.873262\n",
      "(Epoch 10 / 20) train acc: 0.667000; val_acc: 0.661111\n",
      "(Iteration 101 / 200) loss: 0.697804\n",
      "(Epoch 11 / 20) train acc: 0.737000; val_acc: 0.713889\n",
      "(Iteration 111 / 200) loss: 0.738037\n",
      "(Epoch 12 / 20) train acc: 0.703000; val_acc: 0.725000\n",
      "(Iteration 121 / 200) loss: 0.821978\n",
      "(Epoch 13 / 20) train acc: 0.732000; val_acc: 0.700000\n",
      "(Iteration 131 / 200) loss: 0.694835\n",
      "(Epoch 14 / 20) train acc: 0.769000; val_acc: 0.738889\n",
      "(Iteration 141 / 200) loss: 0.557873\n",
      "(Epoch 15 / 20) train acc: 0.745000; val_acc: 0.747222\n",
      "(Iteration 151 / 200) loss: 0.748155\n",
      "(Epoch 16 / 20) train acc: 0.782000; val_acc: 0.761111\n",
      "(Iteration 161 / 200) loss: 0.703590\n",
      "(Epoch 17 / 20) train acc: 0.784000; val_acc: 0.794444\n",
      "(Iteration 171 / 200) loss: 0.710274\n",
      "(Epoch 18 / 20) train acc: 0.754000; val_acc: 0.744444\n",
      "(Iteration 181 / 200) loss: 0.673582\n",
      "(Epoch 19 / 20) train acc: 0.853000; val_acc: 0.802778\n",
      "(Iteration 191 / 200) loss: 0.620049\n",
      "(Epoch 20 / 20) train acc: 0.843000; val_acc: 0.819444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.086000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302878\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302861\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302934\n",
      "(Epoch 4 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302492\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.303025\n",
      "(Epoch 6 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.304232\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.303015\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302007\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.301683\n",
      "(Epoch 10 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.301311\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.300765\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.301990\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.293625\n",
      "(Epoch 14 / 20) train acc: 0.124000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.293998\n",
      "(Epoch 15 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.304475\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302648\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.298545\n",
      "(Epoch 18 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.299835\n",
      "(Epoch 19 / 20) train acc: 0.083000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.303683\n",
      "(Epoch 20 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.116000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302765\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.301993\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302442\n",
      "(Epoch 4 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302751\n",
      "(Epoch 5 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302358\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302439\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302542\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.299874\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.304507\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302595\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.301819\n",
      "(Epoch 12 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.301071\n",
      "(Epoch 13 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.300904\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.304110\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.304709\n",
      "(Epoch 16 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.308173\n",
      "(Epoch 17 / 20) train acc: 0.072000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.306289\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.305764\n",
      "(Epoch 19 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.300525\n",
      "(Epoch 20 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.301910\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302526\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302130\n",
      "(Epoch 4 / 20) train acc: 0.122000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.301889\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.299573\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.300297\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302617\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.297574\n",
      "(Epoch 9 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.303313\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.304252\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.298326\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.301977\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.303685\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.300643\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.300546\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.303753\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302536\n",
      "(Epoch 18 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303901\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.295099\n",
      "(Epoch 20 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 228337.744760\n",
      "(Epoch 0 / 20) train acc: 0.068000; val_acc: 0.069444\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 131515.344031\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 89823.583427\n",
      "(Epoch 3 / 20) train acc: 0.193000; val_acc: 0.177778\n",
      "(Iteration 31 / 200) loss: 67071.822877\n",
      "(Epoch 4 / 20) train acc: 0.284000; val_acc: 0.247222\n",
      "(Iteration 41 / 200) loss: 44423.482354\n",
      "(Epoch 5 / 20) train acc: 0.382000; val_acc: 0.313889\n",
      "(Iteration 51 / 200) loss: 35238.156849\n",
      "(Epoch 6 / 20) train acc: 0.422000; val_acc: 0.411111\n",
      "(Iteration 61 / 200) loss: 27466.386362\n",
      "(Epoch 7 / 20) train acc: 0.475000; val_acc: 0.463889\n",
      "(Iteration 71 / 200) loss: 15179.659637\n",
      "(Epoch 8 / 20) train acc: 0.574000; val_acc: 0.488889\n",
      "(Iteration 81 / 200) loss: 19028.387924\n",
      "(Epoch 9 / 20) train acc: 0.638000; val_acc: 0.530556\n",
      "(Iteration 91 / 200) loss: 13131.278720\n",
      "(Epoch 10 / 20) train acc: 0.686000; val_acc: 0.572222\n",
      "(Iteration 101 / 200) loss: 11877.324530\n",
      "(Epoch 11 / 20) train acc: 0.711000; val_acc: 0.616667\n",
      "(Iteration 111 / 200) loss: 12499.966599\n",
      "(Epoch 12 / 20) train acc: 0.746000; val_acc: 0.638889\n",
      "(Iteration 121 / 200) loss: 10269.743673\n",
      "(Epoch 13 / 20) train acc: 0.775000; val_acc: 0.658333\n",
      "(Iteration 131 / 200) loss: 8675.226379\n",
      "(Epoch 14 / 20) train acc: 0.768000; val_acc: 0.686111\n",
      "(Iteration 141 / 200) loss: 5346.156594\n",
      "(Epoch 15 / 20) train acc: 0.786000; val_acc: 0.708333\n",
      "(Iteration 151 / 200) loss: 2714.381192\n",
      "(Epoch 16 / 20) train acc: 0.829000; val_acc: 0.711111\n",
      "(Iteration 161 / 200) loss: 6019.685175\n",
      "(Epoch 17 / 20) train acc: 0.813000; val_acc: 0.719444\n",
      "(Iteration 171 / 200) loss: 5236.633854\n",
      "(Epoch 18 / 20) train acc: 0.841000; val_acc: 0.733333\n",
      "(Iteration 181 / 200) loss: 3251.460351\n",
      "(Epoch 19 / 20) train acc: 0.846000; val_acc: 0.733333\n",
      "(Iteration 191 / 200) loss: 4252.117793\n",
      "(Epoch 20 / 20) train acc: 0.856000; val_acc: 0.744444\n",
      "(Iteration 1 / 200) loss: 3.477030\n",
      "(Epoch 0 / 20) train acc: 0.064000; val_acc: 0.105556\n",
      "(Epoch 1 / 20) train acc: 0.694000; val_acc: 0.650000\n",
      "(Iteration 11 / 200) loss: 1.170485\n",
      "(Epoch 2 / 20) train acc: 0.885000; val_acc: 0.836111\n",
      "(Iteration 21 / 200) loss: 0.513844\n",
      "(Epoch 3 / 20) train acc: 0.919000; val_acc: 0.897222\n",
      "(Iteration 31 / 200) loss: 0.315966\n",
      "(Epoch 4 / 20) train acc: 0.932000; val_acc: 0.925000\n",
      "(Iteration 41 / 200) loss: 0.119488\n",
      "(Epoch 5 / 20) train acc: 0.962000; val_acc: 0.919444\n",
      "(Iteration 51 / 200) loss: 0.144359\n",
      "(Epoch 6 / 20) train acc: 0.973000; val_acc: 0.955556\n",
      "(Iteration 61 / 200) loss: 0.114433\n",
      "(Epoch 7 / 20) train acc: 0.984000; val_acc: 0.955556\n",
      "(Iteration 71 / 200) loss: 0.077911\n",
      "(Epoch 8 / 20) train acc: 0.982000; val_acc: 0.944444\n",
      "(Iteration 81 / 200) loss: 0.045516\n",
      "(Epoch 9 / 20) train acc: 0.995000; val_acc: 0.966667\n",
      "(Iteration 91 / 200) loss: 0.038662\n",
      "(Epoch 10 / 20) train acc: 0.999000; val_acc: 0.972222\n",
      "(Iteration 101 / 200) loss: 0.036762\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 111 / 200) loss: 0.055492\n",
      "(Epoch 12 / 20) train acc: 0.993000; val_acc: 0.969444\n",
      "(Iteration 121 / 200) loss: 0.028295\n",
      "(Epoch 13 / 20) train acc: 0.998000; val_acc: 0.980556\n",
      "(Iteration 131 / 200) loss: 0.021828\n",
      "(Epoch 14 / 20) train acc: 0.998000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.057606\n",
      "(Epoch 15 / 20) train acc: 0.999000; val_acc: 0.975000\n",
      "(Iteration 151 / 200) loss: 0.029976\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.975000\n",
      "(Iteration 161 / 200) loss: 0.008571\n",
      "(Epoch 17 / 20) train acc: 0.998000; val_acc: 0.975000\n",
      "(Iteration 171 / 200) loss: 0.013938\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.986111\n",
      "(Iteration 181 / 200) loss: 0.014743\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.980556\n",
      "(Iteration 191 / 200) loss: 0.007300\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.988889\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.206000; val_acc: 0.230556\n",
      "(Iteration 11 / 200) loss: 2.298935\n",
      "(Epoch 2 / 20) train acc: 0.271000; val_acc: 0.255556\n",
      "(Iteration 21 / 200) loss: 2.226975\n",
      "(Epoch 3 / 20) train acc: 0.317000; val_acc: 0.358333\n",
      "(Iteration 31 / 200) loss: 1.813266\n",
      "(Epoch 4 / 20) train acc: 0.466000; val_acc: 0.469444\n",
      "(Iteration 41 / 200) loss: 1.460601\n",
      "(Epoch 5 / 20) train acc: 0.607000; val_acc: 0.591667\n",
      "(Iteration 51 / 200) loss: 0.925307\n",
      "(Epoch 6 / 20) train acc: 0.580000; val_acc: 0.591667\n",
      "(Iteration 61 / 200) loss: 1.035134\n",
      "(Epoch 7 / 20) train acc: 0.646000; val_acc: 0.663889\n",
      "(Iteration 71 / 200) loss: 0.943545\n",
      "(Epoch 8 / 20) train acc: 0.727000; val_acc: 0.711111\n",
      "(Iteration 81 / 200) loss: 0.738278\n",
      "(Epoch 9 / 20) train acc: 0.767000; val_acc: 0.716667\n",
      "(Iteration 91 / 200) loss: 0.884576\n",
      "(Epoch 10 / 20) train acc: 0.750000; val_acc: 0.758333\n",
      "(Iteration 101 / 200) loss: 1.000716\n",
      "(Epoch 11 / 20) train acc: 0.797000; val_acc: 0.780556\n",
      "(Iteration 111 / 200) loss: 0.511579\n",
      "(Epoch 12 / 20) train acc: 0.795000; val_acc: 0.794444\n",
      "(Iteration 121 / 200) loss: 0.590391\n",
      "(Epoch 13 / 20) train acc: 0.804000; val_acc: 0.819444\n",
      "(Iteration 131 / 200) loss: 0.646357\n",
      "(Epoch 14 / 20) train acc: 0.845000; val_acc: 0.797222\n",
      "(Iteration 141 / 200) loss: 0.393754\n",
      "(Epoch 15 / 20) train acc: 0.846000; val_acc: 0.816667\n",
      "(Iteration 151 / 200) loss: 0.379109\n",
      "(Epoch 16 / 20) train acc: 0.869000; val_acc: 0.855556\n",
      "(Iteration 161 / 200) loss: 0.415798\n",
      "(Epoch 17 / 20) train acc: 0.852000; val_acc: 0.852778\n",
      "(Iteration 171 / 200) loss: 0.316139\n",
      "(Epoch 18 / 20) train acc: 0.869000; val_acc: 0.844444\n",
      "(Iteration 181 / 200) loss: 0.568900\n",
      "(Epoch 19 / 20) train acc: 0.849000; val_acc: 0.847222\n",
      "(Iteration 191 / 200) loss: 0.442128\n",
      "(Epoch 20 / 20) train acc: 0.922000; val_acc: 0.866667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.301651\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.297442\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.174894\n",
      "(Epoch 4 / 20) train acc: 0.199000; val_acc: 0.175000\n",
      "(Iteration 41 / 200) loss: 2.091109\n",
      "(Epoch 5 / 20) train acc: 0.196000; val_acc: 0.183333\n",
      "(Iteration 51 / 200) loss: 1.919732\n",
      "(Epoch 6 / 20) train acc: 0.213000; val_acc: 0.183333\n",
      "(Iteration 61 / 200) loss: 1.949904\n",
      "(Epoch 7 / 20) train acc: 0.199000; val_acc: 0.183333\n",
      "(Iteration 71 / 200) loss: 1.923159\n",
      "(Epoch 8 / 20) train acc: 0.196000; val_acc: 0.161111\n",
      "(Iteration 81 / 200) loss: 1.912045\n",
      "(Epoch 9 / 20) train acc: 0.206000; val_acc: 0.161111\n",
      "(Iteration 91 / 200) loss: 2.039586\n",
      "(Epoch 10 / 20) train acc: 0.226000; val_acc: 0.161111\n",
      "(Iteration 101 / 200) loss: 1.979187\n",
      "(Epoch 11 / 20) train acc: 0.228000; val_acc: 0.163889\n",
      "(Iteration 111 / 200) loss: 1.936494\n",
      "(Epoch 12 / 20) train acc: 0.207000; val_acc: 0.183333\n",
      "(Iteration 121 / 200) loss: 1.917163\n",
      "(Epoch 13 / 20) train acc: 0.224000; val_acc: 0.161111\n",
      "(Iteration 131 / 200) loss: 1.869656\n",
      "(Epoch 14 / 20) train acc: 0.210000; val_acc: 0.163889\n",
      "(Iteration 141 / 200) loss: 1.808342\n",
      "(Epoch 15 / 20) train acc: 0.210000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 1.989536\n",
      "(Epoch 16 / 20) train acc: 0.219000; val_acc: 0.163889\n",
      "(Iteration 161 / 200) loss: 1.956713\n",
      "(Epoch 17 / 20) train acc: 0.194000; val_acc: 0.200000\n",
      "(Iteration 171 / 200) loss: 1.891246\n",
      "(Epoch 18 / 20) train acc: 0.209000; val_acc: 0.200000\n",
      "(Iteration 181 / 200) loss: 1.904867\n",
      "(Epoch 19 / 20) train acc: 0.210000; val_acc: 0.200000\n",
      "(Iteration 191 / 200) loss: 1.776871\n",
      "(Epoch 20 / 20) train acc: 0.192000; val_acc: 0.197222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302241\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.303128\n",
      "(Epoch 3 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302106\n",
      "(Epoch 4 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302697\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.301331\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.301691\n",
      "(Epoch 7 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.250425\n",
      "(Epoch 8 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.107178\n",
      "(Epoch 9 / 20) train acc: 0.234000; val_acc: 0.194444\n",
      "(Iteration 91 / 200) loss: 1.980485\n",
      "(Epoch 10 / 20) train acc: 0.211000; val_acc: 0.194444\n",
      "(Iteration 101 / 200) loss: 1.998317\n",
      "(Epoch 11 / 20) train acc: 0.201000; val_acc: 0.183333\n",
      "(Iteration 111 / 200) loss: 1.904849\n",
      "(Epoch 12 / 20) train acc: 0.175000; val_acc: 0.136111\n",
      "(Iteration 121 / 200) loss: 1.969990\n",
      "(Epoch 13 / 20) train acc: 0.218000; val_acc: 0.186111\n",
      "(Iteration 131 / 200) loss: 1.977431\n",
      "(Epoch 14 / 20) train acc: 0.238000; val_acc: 0.188889\n",
      "(Iteration 141 / 200) loss: 2.000716\n",
      "(Epoch 15 / 20) train acc: 0.219000; val_acc: 0.194444\n",
      "(Iteration 151 / 200) loss: 1.966743\n",
      "(Epoch 16 / 20) train acc: 0.208000; val_acc: 0.197222\n",
      "(Iteration 161 / 200) loss: 1.948651\n",
      "(Epoch 17 / 20) train acc: 0.228000; val_acc: 0.205556\n",
      "(Iteration 171 / 200) loss: 1.903528\n",
      "(Epoch 18 / 20) train acc: 0.234000; val_acc: 0.197222\n",
      "(Iteration 181 / 200) loss: 1.901589\n",
      "(Epoch 19 / 20) train acc: 0.218000; val_acc: 0.205556\n",
      "(Iteration 191 / 200) loss: 1.792547\n",
      "(Epoch 20 / 20) train acc: 0.207000; val_acc: 0.208333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302250\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.303389\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302243\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.303815\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302213\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.300735\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.303542\n",
      "(Epoch 8 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.301675\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.301706\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.303191\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.298058\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302762\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.308946\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301338\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.305955\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.299829\n",
      "(Epoch 17 / 20) train acc: 0.077000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.304240\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301470\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.300516\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 396987.170405\n",
      "(Epoch 0 / 20) train acc: 0.071000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.082000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 354535.114351\n",
      "(Epoch 2 / 20) train acc: 0.076000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 339348.188430\n",
      "(Epoch 3 / 20) train acc: 0.081000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 321587.404849\n",
      "(Epoch 4 / 20) train acc: 0.077000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 320019.122065\n",
      "(Epoch 5 / 20) train acc: 0.089000; val_acc: 0.127778\n",
      "(Iteration 51 / 200) loss: 292763.679531\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.125000\n",
      "(Iteration 61 / 200) loss: 278991.337427\n",
      "(Epoch 7 / 20) train acc: 0.088000; val_acc: 0.125000\n",
      "(Iteration 71 / 200) loss: 310658.755522\n",
      "(Epoch 8 / 20) train acc: 0.077000; val_acc: 0.127778\n",
      "(Iteration 81 / 200) loss: 259546.094033\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.125000\n",
      "(Iteration 91 / 200) loss: 252703.532847\n",
      "(Epoch 10 / 20) train acc: 0.099000; val_acc: 0.125000\n",
      "(Iteration 101 / 200) loss: 234169.892334\n",
      "(Epoch 11 / 20) train acc: 0.087000; val_acc: 0.113889\n",
      "(Iteration 111 / 200) loss: 195782.492119\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.119444\n",
      "(Iteration 121 / 200) loss: 185524.432227\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.111111\n",
      "(Iteration 131 / 200) loss: 184774.972383\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.113889\n",
      "(Iteration 141 / 200) loss: 193067.792305\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 209448.372275\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.122222\n",
      "(Iteration 161 / 200) loss: 186779.273062\n",
      "(Epoch 17 / 20) train acc: 0.123000; val_acc: 0.127778\n",
      "(Iteration 171 / 200) loss: 166024.144609\n",
      "(Epoch 18 / 20) train acc: 0.125000; val_acc: 0.136111\n",
      "(Iteration 181 / 200) loss: 176746.536743\n",
      "(Epoch 19 / 20) train acc: 0.126000; val_acc: 0.136111\n",
      "(Iteration 191 / 200) loss: 145991.708955\n",
      "(Epoch 20 / 20) train acc: 0.125000; val_acc: 0.141667\n",
      "(Iteration 1 / 200) loss: 4.807590\n",
      "(Epoch 0 / 20) train acc: 0.122000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.155000; val_acc: 0.136111\n",
      "(Iteration 11 / 200) loss: 4.413949\n",
      "(Epoch 2 / 20) train acc: 0.245000; val_acc: 0.238889\n",
      "(Iteration 21 / 200) loss: 4.035543\n",
      "(Epoch 3 / 20) train acc: 0.387000; val_acc: 0.361111\n",
      "(Iteration 31 / 200) loss: 3.853016\n",
      "(Epoch 4 / 20) train acc: 0.466000; val_acc: 0.436111\n",
      "(Iteration 41 / 200) loss: 3.480794\n",
      "(Epoch 5 / 20) train acc: 0.559000; val_acc: 0.513889\n",
      "(Iteration 51 / 200) loss: 3.321820\n",
      "(Epoch 6 / 20) train acc: 0.670000; val_acc: 0.605556\n",
      "(Iteration 61 / 200) loss: 3.303123\n",
      "(Epoch 7 / 20) train acc: 0.728000; val_acc: 0.700000\n",
      "(Iteration 71 / 200) loss: 3.069362\n",
      "(Epoch 8 / 20) train acc: 0.762000; val_acc: 0.725000\n",
      "(Iteration 81 / 200) loss: 2.855727\n",
      "(Epoch 9 / 20) train acc: 0.814000; val_acc: 0.747222\n",
      "(Iteration 91 / 200) loss: 2.651148\n",
      "(Epoch 10 / 20) train acc: 0.823000; val_acc: 0.783333\n",
      "(Iteration 101 / 200) loss: 2.508493\n",
      "(Epoch 11 / 20) train acc: 0.815000; val_acc: 0.791667\n",
      "(Iteration 111 / 200) loss: 2.574134\n",
      "(Epoch 12 / 20) train acc: 0.843000; val_acc: 0.794444\n",
      "(Iteration 121 / 200) loss: 2.535410\n",
      "(Epoch 13 / 20) train acc: 0.876000; val_acc: 0.816667\n",
      "(Iteration 131 / 200) loss: 2.264366\n",
      "(Epoch 14 / 20) train acc: 0.881000; val_acc: 0.836111\n",
      "(Iteration 141 / 200) loss: 2.214986\n",
      "(Epoch 15 / 20) train acc: 0.879000; val_acc: 0.830556\n",
      "(Iteration 151 / 200) loss: 2.329996\n",
      "(Epoch 16 / 20) train acc: 0.898000; val_acc: 0.844444\n",
      "(Iteration 161 / 200) loss: 2.172383\n",
      "(Epoch 17 / 20) train acc: 0.920000; val_acc: 0.844444\n",
      "(Iteration 171 / 200) loss: 2.040522\n",
      "(Epoch 18 / 20) train acc: 0.912000; val_acc: 0.852778\n",
      "(Iteration 181 / 200) loss: 2.052727\n",
      "(Epoch 19 / 20) train acc: 0.934000; val_acc: 0.861111\n",
      "(Iteration 191 / 200) loss: 2.016337\n",
      "(Epoch 20 / 20) train acc: 0.934000; val_acc: 0.875000\n",
      "(Iteration 1 / 200) loss: 2.320749\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.317980\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.315664\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.313560\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.312069\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.310604\n",
      "(Epoch 6 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.309423\n",
      "(Epoch 7 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.307992\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.307694\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.306720\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.306114\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.305713\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304971\n",
      "(Epoch 13 / 20) train acc: 0.091000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.304718\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.304767\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.304408\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.304161\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.303569\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303326\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.303209\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302768\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302639\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302593\n",
      "(Epoch 3 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302605\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302529\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302429\n",
      "(Epoch 6 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302719\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302752\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302501\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302513\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302671\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302249\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302406\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302885\n",
      "(Epoch 14 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302788\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302596\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302158\n",
      "(Epoch 17 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.301593\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.303223\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302765\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.085000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302527\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302520\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302536\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302407\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302666\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302453\n",
      "(Epoch 7 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302669\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302563\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302584\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302826\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302434\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302810\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302527\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302217\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.303287\n",
      "(Epoch 16 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.302591\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302786\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.301889\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302912\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302567\n",
      "(Epoch 2 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302706\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302573\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302609\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302616\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302350\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302541\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302319\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302564\n",
      "(Epoch 10 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302553\n",
      "(Epoch 11 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.301887\n",
      "(Epoch 12 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302956\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302719\n",
      "(Epoch 14 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302413\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302048\n",
      "(Epoch 16 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301861\n",
      "(Epoch 17 / 20) train acc: 0.131000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302051\n",
      "(Epoch 18 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301947\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.303157\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 254638.528884\n",
      "(Epoch 0 / 20) train acc: 0.140000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.130000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 202125.852637\n",
      "(Epoch 2 / 20) train acc: 0.148000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 230028.744099\n",
      "(Epoch 3 / 20) train acc: 0.146000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 155628.686191\n",
      "(Epoch 4 / 20) train acc: 0.129000; val_acc: 0.119444\n",
      "(Iteration 41 / 200) loss: 195158.689644\n",
      "(Epoch 5 / 20) train acc: 0.148000; val_acc: 0.125000\n",
      "(Iteration 51 / 200) loss: 146879.494409\n",
      "(Epoch 6 / 20) train acc: 0.131000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 200254.419585\n",
      "(Epoch 7 / 20) train acc: 0.147000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 163910.635229\n",
      "(Epoch 8 / 20) train acc: 0.163000; val_acc: 0.130556\n",
      "(Iteration 81 / 200) loss: 141758.372336\n",
      "(Epoch 9 / 20) train acc: 0.154000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 144066.690127\n",
      "(Epoch 10 / 20) train acc: 0.159000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 140931.578608\n",
      "(Epoch 11 / 20) train acc: 0.149000; val_acc: 0.144444\n",
      "(Iteration 111 / 200) loss: 115986.407463\n",
      "(Epoch 12 / 20) train acc: 0.194000; val_acc: 0.155556\n",
      "(Iteration 121 / 200) loss: 143305.576960\n",
      "(Epoch 13 / 20) train acc: 0.176000; val_acc: 0.152778\n",
      "(Iteration 131 / 200) loss: 133987.857095\n",
      "(Epoch 14 / 20) train acc: 0.170000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 118009.947495\n",
      "(Epoch 15 / 20) train acc: 0.186000; val_acc: 0.150000\n",
      "(Iteration 151 / 200) loss: 121035.748416\n",
      "(Epoch 16 / 20) train acc: 0.183000; val_acc: 0.163889\n",
      "(Iteration 161 / 200) loss: 124649.439536\n",
      "(Epoch 17 / 20) train acc: 0.157000; val_acc: 0.163889\n",
      "(Iteration 171 / 200) loss: 92729.061001\n",
      "(Epoch 18 / 20) train acc: 0.216000; val_acc: 0.163889\n",
      "(Iteration 181 / 200) loss: 94951.472861\n",
      "(Epoch 19 / 20) train acc: 0.208000; val_acc: 0.186111\n",
      "(Iteration 191 / 200) loss: 81942.810081\n",
      "(Epoch 20 / 20) train acc: 0.204000; val_acc: 0.191667\n",
      "(Iteration 1 / 200) loss: 4.201391\n",
      "(Epoch 0 / 20) train acc: 0.137000; val_acc: 0.197222\n",
      "(Epoch 1 / 20) train acc: 0.191000; val_acc: 0.250000\n",
      "(Iteration 11 / 200) loss: 4.185756\n",
      "(Epoch 2 / 20) train acc: 0.258000; val_acc: 0.280556\n",
      "(Iteration 21 / 200) loss: 3.910674\n",
      "(Epoch 3 / 20) train acc: 0.331000; val_acc: 0.350000\n",
      "(Iteration 31 / 200) loss: 3.669746\n",
      "(Epoch 4 / 20) train acc: 0.443000; val_acc: 0.430556\n",
      "(Iteration 41 / 200) loss: 3.568654\n",
      "(Epoch 5 / 20) train acc: 0.533000; val_acc: 0.519444\n",
      "(Iteration 51 / 200) loss: 3.377258\n",
      "(Epoch 6 / 20) train acc: 0.654000; val_acc: 0.563889\n",
      "(Iteration 61 / 200) loss: 3.281558\n",
      "(Epoch 7 / 20) train acc: 0.704000; val_acc: 0.663889\n",
      "(Iteration 71 / 200) loss: 3.052591\n",
      "(Epoch 8 / 20) train acc: 0.769000; val_acc: 0.738889\n",
      "(Iteration 81 / 200) loss: 2.850769\n",
      "(Epoch 9 / 20) train acc: 0.818000; val_acc: 0.766667\n",
      "(Iteration 91 / 200) loss: 2.732128\n",
      "(Epoch 10 / 20) train acc: 0.826000; val_acc: 0.786111\n",
      "(Iteration 101 / 200) loss: 2.718247\n",
      "(Epoch 11 / 20) train acc: 0.885000; val_acc: 0.788889\n",
      "(Iteration 111 / 200) loss: 2.547702\n",
      "(Epoch 12 / 20) train acc: 0.873000; val_acc: 0.813889\n",
      "(Iteration 121 / 200) loss: 2.414203\n",
      "(Epoch 13 / 20) train acc: 0.895000; val_acc: 0.819444\n",
      "(Iteration 131 / 200) loss: 2.320953\n",
      "(Epoch 14 / 20) train acc: 0.917000; val_acc: 0.850000\n",
      "(Iteration 141 / 200) loss: 2.325518\n",
      "(Epoch 15 / 20) train acc: 0.921000; val_acc: 0.869444\n",
      "(Iteration 151 / 200) loss: 2.133362\n",
      "(Epoch 16 / 20) train acc: 0.921000; val_acc: 0.869444\n",
      "(Iteration 161 / 200) loss: 2.119458\n",
      "(Epoch 17 / 20) train acc: 0.938000; val_acc: 0.875000\n",
      "(Iteration 171 / 200) loss: 2.058637\n",
      "(Epoch 18 / 20) train acc: 0.916000; val_acc: 0.880556\n",
      "(Iteration 181 / 200) loss: 1.998303\n",
      "(Epoch 19 / 20) train acc: 0.937000; val_acc: 0.894444\n",
      "(Iteration 191 / 200) loss: 1.931352\n",
      "(Epoch 20 / 20) train acc: 0.942000; val_acc: 0.905556\n",
      "(Iteration 1 / 200) loss: 2.320834\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.318134\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.315640\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.313799\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.312165\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.310606\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.309506\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.308030\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.306977\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.306804\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.306464\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.305581\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304704\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.304848\n",
      "(Epoch 14 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.304178\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.303670\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.303642\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.303335\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303320\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.303295\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302767\n",
      "(Epoch 0 / 20) train acc: 0.079000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302607\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302761\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302640\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302381\n",
      "(Epoch 6 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302307\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302616\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302244\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302566\n",
      "(Epoch 10 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302068\n",
      "(Epoch 11 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302530\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302394\n",
      "(Epoch 13 / 20) train acc: 0.084000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302836\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302611\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302887\n",
      "(Epoch 16 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301961\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302140\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.301720\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.303050\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302544\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302644\n",
      "(Epoch 3 / 20) train acc: 0.074000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302637\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302483\n",
      "(Epoch 5 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302456\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302607\n",
      "(Epoch 7 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302493\n",
      "(Epoch 8 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302623\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302521\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302574\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302371\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302114\n",
      "(Epoch 13 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302281\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.303247\n",
      "(Epoch 15 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302005\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302218\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302266\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301572\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.303261\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.082000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302544\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302591\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302588\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302591\n",
      "(Epoch 5 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302832\n",
      "(Epoch 6 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302355\n",
      "(Epoch 7 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302549\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302471\n",
      "(Epoch 9 / 20) train acc: 0.134000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302532\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302451\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302797\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302347\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302292\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302794\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302472\n",
      "(Epoch 16 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302309\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302448\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302716\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302832\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 238000.077778\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.125000\n",
      "(Iteration 11 / 200) loss: 225810.619404\n",
      "(Epoch 2 / 20) train acc: 0.104000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 194912.810803\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.125000\n",
      "(Iteration 31 / 200) loss: 194535.284514\n",
      "(Epoch 4 / 20) train acc: 0.128000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 202289.938682\n",
      "(Epoch 5 / 20) train acc: 0.120000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 171668.773376\n",
      "(Epoch 6 / 20) train acc: 0.130000; val_acc: 0.136111\n",
      "(Iteration 61 / 200) loss: 154314.308596\n",
      "(Epoch 7 / 20) train acc: 0.119000; val_acc: 0.138889\n",
      "(Iteration 71 / 200) loss: 152230.834966\n",
      "(Epoch 8 / 20) train acc: 0.134000; val_acc: 0.141667\n",
      "(Iteration 81 / 200) loss: 155809.272456\n",
      "(Epoch 9 / 20) train acc: 0.138000; val_acc: 0.144444\n",
      "(Iteration 91 / 200) loss: 154455.830906\n",
      "(Epoch 10 / 20) train acc: 0.143000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 138367.229893\n",
      "(Epoch 11 / 20) train acc: 0.121000; val_acc: 0.150000\n",
      "(Iteration 111 / 200) loss: 121748.329312\n",
      "(Epoch 12 / 20) train acc: 0.153000; val_acc: 0.147222\n",
      "(Iteration 121 / 200) loss: 118324.919351\n",
      "(Epoch 13 / 20) train acc: 0.157000; val_acc: 0.150000\n",
      "(Iteration 131 / 200) loss: 121589.570017\n",
      "(Epoch 14 / 20) train acc: 0.128000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 111250.720884\n",
      "(Epoch 15 / 20) train acc: 0.149000; val_acc: 0.161111\n",
      "(Iteration 151 / 200) loss: 88377.452356\n",
      "(Epoch 16 / 20) train acc: 0.159000; val_acc: 0.166667\n",
      "(Iteration 161 / 200) loss: 108565.374697\n",
      "(Epoch 17 / 20) train acc: 0.166000; val_acc: 0.163889\n",
      "(Iteration 171 / 200) loss: 93649.497422\n",
      "(Epoch 18 / 20) train acc: 0.186000; val_acc: 0.166667\n",
      "(Iteration 181 / 200) loss: 95533.040210\n",
      "(Epoch 19 / 20) train acc: 0.167000; val_acc: 0.175000\n",
      "(Iteration 191 / 200) loss: 84344.533594\n",
      "(Epoch 20 / 20) train acc: 0.181000; val_acc: 0.175000\n",
      "(Iteration 1 / 200) loss: 5.356738\n",
      "(Epoch 0 / 20) train acc: 0.124000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 4.483532\n",
      "(Epoch 2 / 20) train acc: 0.172000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 4.234830\n",
      "(Epoch 3 / 20) train acc: 0.324000; val_acc: 0.263889\n",
      "(Iteration 31 / 200) loss: 3.866385\n",
      "(Epoch 4 / 20) train acc: 0.397000; val_acc: 0.352778\n",
      "(Iteration 41 / 200) loss: 3.740308\n",
      "(Epoch 5 / 20) train acc: 0.496000; val_acc: 0.469444\n",
      "(Iteration 51 / 200) loss: 3.356296\n",
      "(Epoch 6 / 20) train acc: 0.614000; val_acc: 0.563889\n",
      "(Iteration 61 / 200) loss: 3.254908\n",
      "(Epoch 7 / 20) train acc: 0.697000; val_acc: 0.677778\n",
      "(Iteration 71 / 200) loss: 3.183681\n",
      "(Epoch 8 / 20) train acc: 0.719000; val_acc: 0.711111\n",
      "(Iteration 81 / 200) loss: 2.974623\n",
      "(Epoch 9 / 20) train acc: 0.766000; val_acc: 0.755556\n",
      "(Iteration 91 / 200) loss: 2.903086\n",
      "(Epoch 10 / 20) train acc: 0.798000; val_acc: 0.786111\n",
      "(Iteration 101 / 200) loss: 2.711301\n",
      "(Epoch 11 / 20) train acc: 0.817000; val_acc: 0.808333\n",
      "(Iteration 111 / 200) loss: 2.559214\n",
      "(Epoch 12 / 20) train acc: 0.851000; val_acc: 0.833333\n",
      "(Iteration 121 / 200) loss: 2.503679\n",
      "(Epoch 13 / 20) train acc: 0.855000; val_acc: 0.841667\n",
      "(Iteration 131 / 200) loss: 2.366517\n",
      "(Epoch 14 / 20) train acc: 0.863000; val_acc: 0.863889\n",
      "(Iteration 141 / 200) loss: 2.251705\n",
      "(Epoch 15 / 20) train acc: 0.884000; val_acc: 0.872222\n",
      "(Iteration 151 / 200) loss: 2.273136\n",
      "(Epoch 16 / 20) train acc: 0.888000; val_acc: 0.883333\n",
      "(Iteration 161 / 200) loss: 2.215180\n",
      "(Epoch 17 / 20) train acc: 0.910000; val_acc: 0.888889\n",
      "(Iteration 171 / 200) loss: 2.081927\n",
      "(Epoch 18 / 20) train acc: 0.919000; val_acc: 0.894444\n",
      "(Iteration 181 / 200) loss: 2.209995\n",
      "(Epoch 19 / 20) train acc: 0.911000; val_acc: 0.900000\n",
      "(Iteration 191 / 200) loss: 1.982326\n",
      "(Epoch 20 / 20) train acc: 0.923000; val_acc: 0.897222\n",
      "(Iteration 1 / 200) loss: 2.320827\n",
      "(Epoch 0 / 20) train acc: 0.139000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.318140\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.315757\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.313703\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.312047\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.310664\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.309627\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.308099\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.307677\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.306765\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.306204\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.305709\n",
      "(Epoch 12 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304924\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.304501\n",
      "(Epoch 14 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.304036\n",
      "(Epoch 15 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.304541\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.303666\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.303374\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302956\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302781\n",
      "(Epoch 20 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302766\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302574\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302600\n",
      "(Epoch 3 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302614\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302589\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302719\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302573\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302516\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302440\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302439\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302444\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302455\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302459\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302236\n",
      "(Epoch 14 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302807\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302688\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302692\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302720\n",
      "(Epoch 18 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302775\n",
      "(Epoch 19 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302389\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302624\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302595\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302531\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302484\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302609\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302603\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302480\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302609\n",
      "(Epoch 9 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302435\n",
      "(Epoch 10 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302645\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302737\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302645\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302506\n",
      "(Epoch 14 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302507\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301835\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301608\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.301830\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302288\n",
      "(Epoch 19 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302070\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302654\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302596\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302604\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302564\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302580\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302690\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302408\n",
      "(Epoch 8 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302788\n",
      "(Epoch 9 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302416\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302412\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302373\n",
      "(Epoch 12 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302475\n",
      "(Epoch 13 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302194\n",
      "(Epoch 14 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302420\n",
      "(Epoch 15 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302352\n",
      "(Epoch 16 / 20) train acc: 0.084000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302552\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302534\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302103\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302016\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 200183.408307\n",
      "(Epoch 0 / 20) train acc: 0.063000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.061000; val_acc: 0.075000\n",
      "(Iteration 11 / 200) loss: 183564.580333\n",
      "(Epoch 2 / 20) train acc: 0.061000; val_acc: 0.072222\n",
      "(Iteration 21 / 200) loss: 158464.093441\n",
      "(Epoch 3 / 20) train acc: 0.073000; val_acc: 0.077778\n",
      "(Iteration 31 / 200) loss: 171285.606819\n",
      "(Epoch 4 / 20) train acc: 0.066000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 169135.400306\n",
      "(Epoch 5 / 20) train acc: 0.065000; val_acc: 0.069444\n",
      "(Iteration 51 / 200) loss: 134289.013860\n",
      "(Epoch 6 / 20) train acc: 0.080000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 144244.847460\n",
      "(Epoch 7 / 20) train acc: 0.072000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 123655.601104\n",
      "(Epoch 8 / 20) train acc: 0.073000; val_acc: 0.108333\n",
      "(Iteration 81 / 200) loss: 106607.754794\n",
      "(Epoch 9 / 20) train acc: 0.082000; val_acc: 0.113889\n",
      "(Iteration 91 / 200) loss: 107145.008538\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.122222\n",
      "(Iteration 101 / 200) loss: 100943.272331\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.125000\n",
      "(Iteration 111 / 200) loss: 107532.156156\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.125000\n",
      "(Iteration 121 / 200) loss: 103257.379989\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.130556\n",
      "(Iteration 131 / 200) loss: 98611.753875\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.133333\n",
      "(Iteration 141 / 200) loss: 90819.177760\n",
      "(Epoch 15 / 20) train acc: 0.134000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 78377.006678\n",
      "(Epoch 16 / 20) train acc: 0.137000; val_acc: 0.150000\n",
      "(Iteration 161 / 200) loss: 80002.480633\n",
      "(Epoch 17 / 20) train acc: 0.141000; val_acc: 0.158333\n",
      "(Iteration 171 / 200) loss: 102122.389604\n",
      "(Epoch 18 / 20) train acc: 0.131000; val_acc: 0.169444\n",
      "(Iteration 181 / 200) loss: 80732.873564\n",
      "(Epoch 19 / 20) train acc: 0.149000; val_acc: 0.180556\n",
      "(Iteration 191 / 200) loss: 82691.557543\n",
      "(Epoch 20 / 20) train acc: 0.150000; val_acc: 0.180556\n",
      "(Iteration 1 / 200) loss: 2.760987\n",
      "(Epoch 0 / 20) train acc: 0.055000; val_acc: 0.050000\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.113889\n",
      "(Iteration 11 / 200) loss: 2.536253\n",
      "(Epoch 2 / 20) train acc: 0.228000; val_acc: 0.225000\n",
      "(Iteration 21 / 200) loss: 2.254903\n",
      "(Epoch 3 / 20) train acc: 0.388000; val_acc: 0.352778\n",
      "(Iteration 31 / 200) loss: 2.035776\n",
      "(Epoch 4 / 20) train acc: 0.524000; val_acc: 0.461111\n",
      "(Iteration 41 / 200) loss: 1.885696\n",
      "(Epoch 5 / 20) train acc: 0.627000; val_acc: 0.580556\n",
      "(Iteration 51 / 200) loss: 1.777488\n",
      "(Epoch 6 / 20) train acc: 0.680000; val_acc: 0.636111\n",
      "(Iteration 61 / 200) loss: 1.558941\n",
      "(Epoch 7 / 20) train acc: 0.723000; val_acc: 0.680556\n",
      "(Iteration 71 / 200) loss: 1.497719\n",
      "(Epoch 8 / 20) train acc: 0.764000; val_acc: 0.722222\n",
      "(Iteration 81 / 200) loss: 1.362749\n",
      "(Epoch 9 / 20) train acc: 0.789000; val_acc: 0.758333\n",
      "(Iteration 91 / 200) loss: 1.159290\n",
      "(Epoch 10 / 20) train acc: 0.828000; val_acc: 0.794444\n",
      "(Iteration 101 / 200) loss: 1.091248\n",
      "(Epoch 11 / 20) train acc: 0.876000; val_acc: 0.808333\n",
      "(Iteration 111 / 200) loss: 0.909374\n",
      "(Epoch 12 / 20) train acc: 0.886000; val_acc: 0.827778\n",
      "(Iteration 121 / 200) loss: 0.945674\n",
      "(Epoch 13 / 20) train acc: 0.890000; val_acc: 0.844444\n",
      "(Iteration 131 / 200) loss: 0.788475\n",
      "(Epoch 14 / 20) train acc: 0.896000; val_acc: 0.866667\n",
      "(Iteration 141 / 200) loss: 0.729711\n",
      "(Epoch 15 / 20) train acc: 0.907000; val_acc: 0.866667\n",
      "(Iteration 151 / 200) loss: 0.735803\n",
      "(Epoch 16 / 20) train acc: 0.917000; val_acc: 0.866667\n",
      "(Iteration 161 / 200) loss: 0.603207\n",
      "(Epoch 17 / 20) train acc: 0.917000; val_acc: 0.869444\n",
      "(Iteration 171 / 200) loss: 0.598160\n",
      "(Epoch 18 / 20) train acc: 0.932000; val_acc: 0.877778\n",
      "(Iteration 181 / 200) loss: 0.454817\n",
      "(Epoch 19 / 20) train acc: 0.937000; val_acc: 0.883333\n",
      "(Iteration 191 / 200) loss: 0.456513\n",
      "(Epoch 20 / 20) train acc: 0.936000; val_acc: 0.886111\n",
      "(Iteration 1 / 200) loss: 2.304399\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.121000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.304087\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.303646\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.303622\n",
      "(Epoch 4 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.303535\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.303209\n",
      "(Epoch 6 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.303137\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302720\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.301667\n",
      "(Epoch 9 / 20) train acc: 0.211000; val_acc: 0.241667\n",
      "(Iteration 91 / 200) loss: 2.300669\n",
      "(Epoch 10 / 20) train acc: 0.243000; val_acc: 0.236111\n",
      "(Iteration 101 / 200) loss: 2.294169\n",
      "(Epoch 11 / 20) train acc: 0.195000; val_acc: 0.216667\n",
      "(Iteration 111 / 200) loss: 2.274217\n",
      "(Epoch 12 / 20) train acc: 0.185000; val_acc: 0.202778\n",
      "(Iteration 121 / 200) loss: 2.230506\n",
      "(Epoch 13 / 20) train acc: 0.289000; val_acc: 0.294444\n",
      "(Iteration 131 / 200) loss: 2.153760\n",
      "(Epoch 14 / 20) train acc: 0.310000; val_acc: 0.302778\n",
      "(Iteration 141 / 200) loss: 2.023452\n",
      "(Epoch 15 / 20) train acc: 0.275000; val_acc: 0.288889\n",
      "(Iteration 151 / 200) loss: 1.902412\n",
      "(Epoch 16 / 20) train acc: 0.252000; val_acc: 0.286111\n",
      "(Iteration 161 / 200) loss: 1.793964\n",
      "(Epoch 17 / 20) train acc: 0.242000; val_acc: 0.275000\n",
      "(Iteration 171 / 200) loss: 1.764670\n",
      "(Epoch 18 / 20) train acc: 0.312000; val_acc: 0.350000\n",
      "(Iteration 181 / 200) loss: 1.678885\n",
      "(Epoch 19 / 20) train acc: 0.345000; val_acc: 0.344444\n",
      "(Iteration 191 / 200) loss: 1.568720\n",
      "(Epoch 20 / 20) train acc: 0.397000; val_acc: 0.347222\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302564\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302544\n",
      "(Epoch 3 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302544\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302563\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302424\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302535\n",
      "(Epoch 7 / 20) train acc: 0.127000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302469\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302445\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302396\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302838\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303170\n",
      "(Epoch 12 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302534\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302678\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302268\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302581\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302285\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302159\n",
      "(Epoch 18 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302135\n",
      "(Epoch 19 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302816\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302577\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302554\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302581\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302482\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302469\n",
      "(Epoch 7 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302530\n",
      "(Epoch 8 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302158\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302281\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302416\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302719\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302552\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302555\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302565\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302132\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302155\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302524\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301841\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302197\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302570\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302537\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302583\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302466\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302726\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302701\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302471\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302480\n",
      "(Epoch 11 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302312\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302649\n",
      "(Epoch 13 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302172\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302725\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302601\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302719\n",
      "(Epoch 17 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302600\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302473\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302736\n",
      "(Epoch 20 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 354330.613010\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.075000\n",
      "(Iteration 11 / 200) loss: 313107.025637\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 295107.699518\n",
      "(Epoch 3 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 291884.413520\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 257039.087599\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 281517.561750\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 255994.455957\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 191712.810222\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.077778\n",
      "(Iteration 81 / 200) loss: 217434.744508\n",
      "(Epoch 9 / 20) train acc: 0.127000; val_acc: 0.077778\n",
      "(Iteration 91 / 200) loss: 223488.718793\n",
      "(Epoch 10 / 20) train acc: 0.130000; val_acc: 0.075000\n",
      "(Iteration 101 / 200) loss: 200674.713147\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.075000\n",
      "(Iteration 111 / 200) loss: 197284.647595\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.077778\n",
      "(Iteration 121 / 200) loss: 190043.382051\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.077778\n",
      "(Iteration 131 / 200) loss: 187318.476551\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.075000\n",
      "(Iteration 141 / 200) loss: 159875.281083\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 151167.865656\n",
      "(Epoch 16 / 20) train acc: 0.124000; val_acc: 0.091667\n",
      "(Iteration 161 / 200) loss: 167705.200307\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 149499.745034\n",
      "(Epoch 18 / 20) train acc: 0.147000; val_acc: 0.100000\n",
      "(Iteration 181 / 200) loss: 166434.319770\n",
      "(Epoch 19 / 20) train acc: 0.134000; val_acc: 0.102778\n",
      "(Iteration 191 / 200) loss: 123968.974546\n",
      "(Epoch 20 / 20) train acc: 0.126000; val_acc: 0.100000\n",
      "(Iteration 1 / 200) loss: 3.039258\n",
      "(Epoch 0 / 20) train acc: 0.072000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 2.805351\n",
      "(Epoch 2 / 20) train acc: 0.163000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 2.498830\n",
      "(Epoch 3 / 20) train acc: 0.253000; val_acc: 0.247222\n",
      "(Iteration 31 / 200) loss: 2.231684\n",
      "(Epoch 4 / 20) train acc: 0.359000; val_acc: 0.336111\n",
      "(Iteration 41 / 200) loss: 2.086638\n",
      "(Epoch 5 / 20) train acc: 0.460000; val_acc: 0.450000\n",
      "(Iteration 51 / 200) loss: 1.909229\n",
      "(Epoch 6 / 20) train acc: 0.538000; val_acc: 0.547222\n",
      "(Iteration 61 / 200) loss: 1.764146\n",
      "(Epoch 7 / 20) train acc: 0.649000; val_acc: 0.633333\n",
      "(Iteration 71 / 200) loss: 1.626001\n",
      "(Epoch 8 / 20) train acc: 0.727000; val_acc: 0.705556\n",
      "(Iteration 81 / 200) loss: 1.468712\n",
      "(Epoch 9 / 20) train acc: 0.773000; val_acc: 0.758333\n",
      "(Iteration 91 / 200) loss: 1.333718\n",
      "(Epoch 10 / 20) train acc: 0.800000; val_acc: 0.772222\n",
      "(Iteration 101 / 200) loss: 1.074145\n",
      "(Epoch 11 / 20) train acc: 0.814000; val_acc: 0.794444\n",
      "(Iteration 111 / 200) loss: 0.995616\n",
      "(Epoch 12 / 20) train acc: 0.870000; val_acc: 0.808333\n",
      "(Iteration 121 / 200) loss: 1.031300\n",
      "(Epoch 13 / 20) train acc: 0.868000; val_acc: 0.833333\n",
      "(Iteration 131 / 200) loss: 0.832593\n",
      "(Epoch 14 / 20) train acc: 0.904000; val_acc: 0.855556\n",
      "(Iteration 141 / 200) loss: 0.828901\n",
      "(Epoch 15 / 20) train acc: 0.895000; val_acc: 0.877778\n",
      "(Iteration 151 / 200) loss: 0.695699\n",
      "(Epoch 16 / 20) train acc: 0.907000; val_acc: 0.888889\n",
      "(Iteration 161 / 200) loss: 0.729464\n",
      "(Epoch 17 / 20) train acc: 0.931000; val_acc: 0.894444\n",
      "(Iteration 171 / 200) loss: 0.548496\n",
      "(Epoch 18 / 20) train acc: 0.922000; val_acc: 0.900000\n",
      "(Iteration 181 / 200) loss: 0.600537\n",
      "(Epoch 19 / 20) train acc: 0.933000; val_acc: 0.911111\n",
      "(Iteration 191 / 200) loss: 0.552608\n",
      "(Epoch 20 / 20) train acc: 0.948000; val_acc: 0.922222\n",
      "(Iteration 1 / 200) loss: 2.304410\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.304182\n",
      "(Epoch 2 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303862\n",
      "(Epoch 3 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.303539\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.303052\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.303386\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302689\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302806\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.300564\n",
      "(Epoch 9 / 20) train acc: 0.179000; val_acc: 0.166667\n",
      "(Iteration 91 / 200) loss: 2.296542\n",
      "(Epoch 10 / 20) train acc: 0.197000; val_acc: 0.175000\n",
      "(Iteration 101 / 200) loss: 2.279596\n",
      "(Epoch 11 / 20) train acc: 0.184000; val_acc: 0.169444\n",
      "(Iteration 111 / 200) loss: 2.267108\n",
      "(Epoch 12 / 20) train acc: 0.193000; val_acc: 0.186111\n",
      "(Iteration 121 / 200) loss: 2.213354\n",
      "(Epoch 13 / 20) train acc: 0.189000; val_acc: 0.183333\n",
      "(Iteration 131 / 200) loss: 2.090380\n",
      "(Epoch 14 / 20) train acc: 0.193000; val_acc: 0.191667\n",
      "(Iteration 141 / 200) loss: 2.032371\n",
      "(Epoch 15 / 20) train acc: 0.192000; val_acc: 0.188889\n",
      "(Iteration 151 / 200) loss: 1.951990\n",
      "(Epoch 16 / 20) train acc: 0.194000; val_acc: 0.202778\n",
      "(Iteration 161 / 200) loss: 1.909345\n",
      "(Epoch 17 / 20) train acc: 0.241000; val_acc: 0.200000\n",
      "(Iteration 171 / 200) loss: 1.917373\n",
      "(Epoch 18 / 20) train acc: 0.236000; val_acc: 0.219444\n",
      "(Iteration 181 / 200) loss: 1.849392\n",
      "(Epoch 19 / 20) train acc: 0.302000; val_acc: 0.250000\n",
      "(Iteration 191 / 200) loss: 1.803474\n",
      "(Epoch 20 / 20) train acc: 0.304000; val_acc: 0.266667\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302624\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302592\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302615\n",
      "(Epoch 4 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302588\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302417\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302635\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302595\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302467\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302590\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302554\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302307\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302247\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302364\n",
      "(Epoch 14 / 20) train acc: 0.083000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302435\n",
      "(Epoch 15 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302917\n",
      "(Epoch 16 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302187\n",
      "(Epoch 17 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302851\n",
      "(Epoch 18 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302340\n",
      "(Epoch 19 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302770\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.088000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302508\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302495\n",
      "(Epoch 4 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302556\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302512\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302612\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302511\n",
      "(Epoch 8 / 20) train acc: 0.131000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302887\n",
      "(Epoch 9 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302546\n",
      "(Epoch 10 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.301980\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302198\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302296\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302488\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302494\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301934\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301758\n",
      "(Epoch 17 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302998\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302173\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302433\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302621\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302630\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302552\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302520\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302586\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302660\n",
      "(Epoch 8 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302539\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302532\n",
      "(Epoch 10 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302736\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302667\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302509\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302358\n",
      "(Epoch 14 / 20) train acc: 0.128000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302348\n",
      "(Epoch 15 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302502\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302501\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302389\n",
      "(Epoch 18 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302133\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302448\n",
      "(Epoch 20 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 202783.967431\n",
      "(Epoch 0 / 20) train acc: 0.141000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.166000; val_acc: 0.136111\n",
      "(Iteration 11 / 200) loss: 177829.401479\n",
      "(Epoch 2 / 20) train acc: 0.158000; val_acc: 0.147222\n",
      "(Iteration 21 / 200) loss: 162555.986691\n",
      "(Epoch 3 / 20) train acc: 0.146000; val_acc: 0.158333\n",
      "(Iteration 31 / 200) loss: 163702.952198\n",
      "(Epoch 4 / 20) train acc: 0.151000; val_acc: 0.155556\n",
      "(Iteration 41 / 200) loss: 165510.377799\n",
      "(Epoch 5 / 20) train acc: 0.155000; val_acc: 0.158333\n",
      "(Iteration 51 / 200) loss: 169670.023572\n",
      "(Epoch 6 / 20) train acc: 0.139000; val_acc: 0.161111\n",
      "(Iteration 61 / 200) loss: 153125.399453\n",
      "(Epoch 7 / 20) train acc: 0.158000; val_acc: 0.172222\n",
      "(Iteration 71 / 200) loss: 136202.535400\n",
      "(Epoch 8 / 20) train acc: 0.183000; val_acc: 0.180556\n",
      "(Iteration 81 / 200) loss: 126240.041414\n",
      "(Epoch 9 / 20) train acc: 0.171000; val_acc: 0.186111\n",
      "(Iteration 91 / 200) loss: 115096.687515\n",
      "(Epoch 10 / 20) train acc: 0.184000; val_acc: 0.191667\n",
      "(Iteration 101 / 200) loss: 111897.583705\n",
      "(Epoch 11 / 20) train acc: 0.191000; val_acc: 0.188889\n",
      "(Iteration 111 / 200) loss: 121396.719951\n",
      "(Epoch 12 / 20) train acc: 0.194000; val_acc: 0.197222\n",
      "(Iteration 121 / 200) loss: 89889.436215\n",
      "(Epoch 13 / 20) train acc: 0.189000; val_acc: 0.200000\n",
      "(Iteration 131 / 200) loss: 91651.652522\n",
      "(Epoch 14 / 20) train acc: 0.187000; val_acc: 0.219444\n",
      "(Iteration 141 / 200) loss: 92395.588861\n",
      "(Epoch 15 / 20) train acc: 0.182000; val_acc: 0.225000\n",
      "(Iteration 151 / 200) loss: 75297.715223\n",
      "(Epoch 16 / 20) train acc: 0.219000; val_acc: 0.222222\n",
      "(Iteration 161 / 200) loss: 79187.191601\n",
      "(Epoch 17 / 20) train acc: 0.208000; val_acc: 0.236111\n",
      "(Iteration 171 / 200) loss: 75971.348019\n",
      "(Epoch 18 / 20) train acc: 0.241000; val_acc: 0.258333\n",
      "(Iteration 181 / 200) loss: 68131.314458\n",
      "(Epoch 19 / 20) train acc: 0.233000; val_acc: 0.258333\n",
      "(Iteration 191 / 200) loss: 54756.665935\n",
      "(Epoch 20 / 20) train acc: 0.238000; val_acc: 0.275000\n",
      "(Iteration 1 / 200) loss: 3.187834\n",
      "(Epoch 0 / 20) train acc: 0.050000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.128000; val_acc: 0.161111\n",
      "(Iteration 11 / 200) loss: 2.747257\n",
      "(Epoch 2 / 20) train acc: 0.195000; val_acc: 0.225000\n",
      "(Iteration 21 / 200) loss: 2.418055\n",
      "(Epoch 3 / 20) train acc: 0.295000; val_acc: 0.277778\n",
      "(Iteration 31 / 200) loss: 2.362611\n",
      "(Epoch 4 / 20) train acc: 0.369000; val_acc: 0.383333\n",
      "(Iteration 41 / 200) loss: 2.152870\n",
      "(Epoch 5 / 20) train acc: 0.531000; val_acc: 0.480556\n",
      "(Iteration 51 / 200) loss: 1.872151\n",
      "(Epoch 6 / 20) train acc: 0.588000; val_acc: 0.572222\n",
      "(Iteration 61 / 200) loss: 1.800462\n",
      "(Epoch 7 / 20) train acc: 0.667000; val_acc: 0.616667\n",
      "(Iteration 71 / 200) loss: 1.651052\n",
      "(Epoch 8 / 20) train acc: 0.686000; val_acc: 0.677778\n",
      "(Iteration 81 / 200) loss: 1.536338\n",
      "(Epoch 9 / 20) train acc: 0.760000; val_acc: 0.700000\n",
      "(Iteration 91 / 200) loss: 1.397445\n",
      "(Epoch 10 / 20) train acc: 0.783000; val_acc: 0.727778\n",
      "(Iteration 101 / 200) loss: 1.313380\n",
      "(Epoch 11 / 20) train acc: 0.802000; val_acc: 0.761111\n",
      "(Iteration 111 / 200) loss: 1.204091\n",
      "(Epoch 12 / 20) train acc: 0.850000; val_acc: 0.797222\n",
      "(Iteration 121 / 200) loss: 0.920716\n",
      "(Epoch 13 / 20) train acc: 0.878000; val_acc: 0.819444\n",
      "(Iteration 131 / 200) loss: 1.073268\n",
      "(Epoch 14 / 20) train acc: 0.888000; val_acc: 0.847222\n",
      "(Iteration 141 / 200) loss: 0.778459\n",
      "(Epoch 15 / 20) train acc: 0.891000; val_acc: 0.866667\n",
      "(Iteration 151 / 200) loss: 0.762736\n",
      "(Epoch 16 / 20) train acc: 0.887000; val_acc: 0.886111\n",
      "(Iteration 161 / 200) loss: 0.622800\n",
      "(Epoch 17 / 20) train acc: 0.907000; val_acc: 0.888889\n",
      "(Iteration 171 / 200) loss: 0.680637\n",
      "(Epoch 18 / 20) train acc: 0.938000; val_acc: 0.908333\n",
      "(Iteration 181 / 200) loss: 0.587267\n",
      "(Epoch 19 / 20) train acc: 0.925000; val_acc: 0.922222\n",
      "(Iteration 191 / 200) loss: 0.484533\n",
      "(Epoch 20 / 20) train acc: 0.933000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 2.304404\n",
      "(Epoch 0 / 20) train acc: 0.129000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.196000; val_acc: 0.200000\n",
      "(Iteration 11 / 200) loss: 2.304131\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.303734\n",
      "(Epoch 3 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.303739\n",
      "(Epoch 4 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.303260\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.303145\n",
      "(Epoch 6 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302371\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 2.299752\n",
      "(Epoch 8 / 20) train acc: 0.124000; val_acc: 0.111111\n",
      "(Iteration 81 / 200) loss: 2.293844\n",
      "(Epoch 9 / 20) train acc: 0.203000; val_acc: 0.188889\n",
      "(Iteration 91 / 200) loss: 2.272085\n",
      "(Epoch 10 / 20) train acc: 0.278000; val_acc: 0.241667\n",
      "(Iteration 101 / 200) loss: 2.214110\n",
      "(Epoch 11 / 20) train acc: 0.284000; val_acc: 0.266667\n",
      "(Iteration 111 / 200) loss: 2.176635\n",
      "(Epoch 12 / 20) train acc: 0.236000; val_acc: 0.252778\n",
      "(Iteration 121 / 200) loss: 2.045430\n",
      "(Epoch 13 / 20) train acc: 0.265000; val_acc: 0.272222\n",
      "(Iteration 131 / 200) loss: 1.880056\n",
      "(Epoch 14 / 20) train acc: 0.225000; val_acc: 0.255556\n",
      "(Iteration 141 / 200) loss: 1.870266\n",
      "(Epoch 15 / 20) train acc: 0.274000; val_acc: 0.275000\n",
      "(Iteration 151 / 200) loss: 1.733391\n",
      "(Epoch 16 / 20) train acc: 0.277000; val_acc: 0.286111\n",
      "(Iteration 161 / 200) loss: 1.731751\n",
      "(Epoch 17 / 20) train acc: 0.337000; val_acc: 0.330556\n",
      "(Iteration 171 / 200) loss: 1.644684\n",
      "(Epoch 18 / 20) train acc: 0.371000; val_acc: 0.363889\n",
      "(Iteration 181 / 200) loss: 1.610327\n",
      "(Epoch 19 / 20) train acc: 0.394000; val_acc: 0.377778\n",
      "(Iteration 191 / 200) loss: 1.639730\n",
      "(Epoch 20 / 20) train acc: 0.373000; val_acc: 0.358333\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302559\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302579\n",
      "(Epoch 3 / 20) train acc: 0.124000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302491\n",
      "(Epoch 4 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302521\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302644\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302575\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302518\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302357\n",
      "(Epoch 9 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302624\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302603\n",
      "(Epoch 11 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302353\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302537\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302032\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302389\n",
      "(Epoch 15 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.303039\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.301834\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.301894\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302653\n",
      "(Epoch 19 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302964\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302580\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302539\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302565\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302598\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302735\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302291\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302825\n",
      "(Epoch 8 / 20) train acc: 0.089000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302555\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302423\n",
      "(Epoch 10 / 20) train acc: 0.128000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302524\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302318\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302530\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302711\n",
      "(Epoch 14 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.303288\n",
      "(Epoch 15 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302399\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302783\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302564\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.301918\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.301663\n",
      "(Epoch 20 / 20) train acc: 0.125000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302557\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302576\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 2.302597\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302501\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302660\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302593\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302490\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302419\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302290\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302331\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.303083\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302307\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.301892\n",
      "(Epoch 14 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302626\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302602\n",
      "(Epoch 16 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.301823\n",
      "(Epoch 17 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301822\n",
      "(Epoch 18 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301859\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301565\n",
      "(Epoch 20 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 281995.609494\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.111111\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.111111\n",
      "(Iteration 11 / 200) loss: 229888.568851\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.111111\n",
      "(Iteration 21 / 200) loss: 234813.728316\n",
      "(Epoch 3 / 20) train acc: 0.090000; val_acc: 0.108333\n",
      "(Iteration 31 / 200) loss: 216276.087817\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.102778\n",
      "(Iteration 41 / 200) loss: 209107.547328\n",
      "(Epoch 5 / 20) train acc: 0.086000; val_acc: 0.102778\n",
      "(Iteration 51 / 200) loss: 199675.626847\n",
      "(Epoch 6 / 20) train acc: 0.086000; val_acc: 0.105556\n",
      "(Iteration 61 / 200) loss: 163801.506373\n",
      "(Epoch 7 / 20) train acc: 0.088000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 177869.085914\n",
      "(Epoch 8 / 20) train acc: 0.090000; val_acc: 0.094444\n",
      "(Iteration 81 / 200) loss: 169026.325471\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.088889\n",
      "(Iteration 91 / 200) loss: 159333.975038\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 169130.844608\n",
      "(Epoch 11 / 20) train acc: 0.117000; val_acc: 0.086111\n",
      "(Iteration 111 / 200) loss: 163495.244184\n",
      "(Epoch 12 / 20) train acc: 0.090000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 134347.663762\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.088889\n",
      "(Iteration 131 / 200) loss: 157542.233345\n",
      "(Epoch 14 / 20) train acc: 0.090000; val_acc: 0.088889\n",
      "(Iteration 141 / 200) loss: 118240.562928\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 151 / 200) loss: 105439.002516\n",
      "(Epoch 16 / 20) train acc: 0.092000; val_acc: 0.094444\n",
      "(Iteration 161 / 200) loss: 107104.562109\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.094444\n",
      "(Iteration 171 / 200) loss: 115061.701709\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.102778\n",
      "(Iteration 181 / 200) loss: 112252.181311\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.105556\n",
      "(Iteration 191 / 200) loss: 97873.840915\n",
      "(Epoch 20 / 20) train acc: 0.119000; val_acc: 0.108333\n",
      "(Iteration 1 / 200) loss: 2.753619\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.072222\n",
      "(Epoch 1 / 20) train acc: 0.150000; val_acc: 0.122222\n",
      "(Iteration 11 / 200) loss: 2.670420\n",
      "(Epoch 2 / 20) train acc: 0.232000; val_acc: 0.219444\n",
      "(Iteration 21 / 200) loss: 2.229357\n",
      "(Epoch 3 / 20) train acc: 0.349000; val_acc: 0.305556\n",
      "(Iteration 31 / 200) loss: 2.011850\n",
      "(Epoch 4 / 20) train acc: 0.469000; val_acc: 0.416667\n",
      "(Iteration 41 / 200) loss: 1.831431\n",
      "(Epoch 5 / 20) train acc: 0.606000; val_acc: 0.522222\n",
      "(Iteration 51 / 200) loss: 1.717175\n",
      "(Epoch 6 / 20) train acc: 0.624000; val_acc: 0.619444\n",
      "(Iteration 61 / 200) loss: 1.527836\n",
      "(Epoch 7 / 20) train acc: 0.719000; val_acc: 0.666667\n",
      "(Iteration 71 / 200) loss: 1.379736\n",
      "(Epoch 8 / 20) train acc: 0.753000; val_acc: 0.730556\n",
      "(Iteration 81 / 200) loss: 1.162361\n",
      "(Epoch 9 / 20) train acc: 0.802000; val_acc: 0.772222\n",
      "(Iteration 91 / 200) loss: 1.008518\n",
      "(Epoch 10 / 20) train acc: 0.833000; val_acc: 0.791667\n",
      "(Iteration 101 / 200) loss: 0.885079\n",
      "(Epoch 11 / 20) train acc: 0.844000; val_acc: 0.797222\n",
      "(Iteration 111 / 200) loss: 0.715379\n",
      "(Epoch 12 / 20) train acc: 0.863000; val_acc: 0.811111\n",
      "(Iteration 121 / 200) loss: 0.775971\n",
      "(Epoch 13 / 20) train acc: 0.871000; val_acc: 0.838889\n",
      "(Iteration 131 / 200) loss: 0.600310\n",
      "(Epoch 14 / 20) train acc: 0.896000; val_acc: 0.844444\n",
      "(Iteration 141 / 200) loss: 0.529148\n",
      "(Epoch 15 / 20) train acc: 0.903000; val_acc: 0.866667\n",
      "(Iteration 151 / 200) loss: 0.466485\n",
      "(Epoch 16 / 20) train acc: 0.907000; val_acc: 0.872222\n",
      "(Iteration 161 / 200) loss: 0.496177\n",
      "(Epoch 17 / 20) train acc: 0.913000; val_acc: 0.875000\n",
      "(Iteration 171 / 200) loss: 0.347559\n",
      "(Epoch 18 / 20) train acc: 0.922000; val_acc: 0.888889\n",
      "(Iteration 181 / 200) loss: 0.296365\n",
      "(Epoch 19 / 20) train acc: 0.921000; val_acc: 0.886111\n",
      "(Iteration 191 / 200) loss: 0.441191\n",
      "(Epoch 20 / 20) train acc: 0.914000; val_acc: 0.916667\n",
      "(Iteration 1 / 200) loss: 2.302768\n",
      "(Epoch 0 / 20) train acc: 0.150000; val_acc: 0.166667\n",
      "(Epoch 1 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302701\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302476\n",
      "(Epoch 3 / 20) train acc: 0.145000; val_acc: 0.141667\n",
      "(Iteration 31 / 200) loss: 2.302408\n",
      "(Epoch 4 / 20) train acc: 0.137000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 2.301994\n",
      "(Epoch 5 / 20) train acc: 0.128000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 2.301042\n",
      "(Epoch 6 / 20) train acc: 0.212000; val_acc: 0.211111\n",
      "(Iteration 61 / 200) loss: 2.299598\n",
      "(Epoch 7 / 20) train acc: 0.234000; val_acc: 0.233333\n",
      "(Iteration 71 / 200) loss: 2.293664\n",
      "(Epoch 8 / 20) train acc: 0.241000; val_acc: 0.225000\n",
      "(Iteration 81 / 200) loss: 2.279345\n",
      "(Epoch 9 / 20) train acc: 0.215000; val_acc: 0.205556\n",
      "(Iteration 91 / 200) loss: 2.246585\n",
      "(Epoch 10 / 20) train acc: 0.302000; val_acc: 0.266667\n",
      "(Iteration 101 / 200) loss: 2.210163\n",
      "(Epoch 11 / 20) train acc: 0.300000; val_acc: 0.327778\n",
      "(Iteration 111 / 200) loss: 2.103015\n",
      "(Epoch 12 / 20) train acc: 0.339000; val_acc: 0.347222\n",
      "(Iteration 121 / 200) loss: 2.005140\n",
      "(Epoch 13 / 20) train acc: 0.408000; val_acc: 0.436111\n",
      "(Iteration 131 / 200) loss: 1.921951\n",
      "(Epoch 14 / 20) train acc: 0.440000; val_acc: 0.450000\n",
      "(Iteration 141 / 200) loss: 1.717448\n",
      "(Epoch 15 / 20) train acc: 0.462000; val_acc: 0.488889\n",
      "(Iteration 151 / 200) loss: 1.619840\n",
      "(Epoch 16 / 20) train acc: 0.534000; val_acc: 0.572222\n",
      "(Iteration 161 / 200) loss: 1.473844\n",
      "(Epoch 17 / 20) train acc: 0.545000; val_acc: 0.580556\n",
      "(Iteration 171 / 200) loss: 1.374784\n",
      "(Epoch 18 / 20) train acc: 0.585000; val_acc: 0.630556\n",
      "(Iteration 181 / 200) loss: 1.227343\n",
      "(Epoch 19 / 20) train acc: 0.596000; val_acc: 0.622222\n",
      "(Iteration 191 / 200) loss: 1.167534\n",
      "(Epoch 20 / 20) train acc: 0.611000; val_acc: 0.641667\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302571\n",
      "(Epoch 2 / 20) train acc: 0.091000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302604\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302593\n",
      "(Epoch 4 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302527\n",
      "(Epoch 5 / 20) train acc: 0.079000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302563\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 61 / 200) loss: 2.302725\n",
      "(Epoch 7 / 20) train acc: 0.087000; val_acc: 0.091667\n",
      "(Iteration 71 / 200) loss: 2.302593\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.302454\n",
      "(Epoch 9 / 20) train acc: 0.122000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.302518\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 101 / 200) loss: 2.302771\n",
      "(Epoch 11 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302563\n",
      "(Epoch 12 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302596\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302157\n",
      "(Epoch 14 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302262\n",
      "(Epoch 15 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302332\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302553\n",
      "(Epoch 17 / 20) train acc: 0.084000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.301824\n",
      "(Epoch 18 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302481\n",
      "(Epoch 19 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302948\n",
      "(Epoch 20 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302538\n",
      "(Epoch 2 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302635\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302598\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302720\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302478\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302676\n",
      "(Epoch 7 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302441\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302502\n",
      "(Epoch 9 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302507\n",
      "(Epoch 10 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302393\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302268\n",
      "(Epoch 12 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302500\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.303090\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302565\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301129\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302325\n",
      "(Epoch 17 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302327\n",
      "(Epoch 18 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302266\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.300914\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302606\n",
      "(Epoch 2 / 20) train acc: 0.078000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302516\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302577\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302599\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302537\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302610\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302567\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302483\n",
      "(Epoch 9 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302660\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302517\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302521\n",
      "(Epoch 12 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302586\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302477\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302347\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302749\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302375\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302560\n",
      "(Epoch 18 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302699\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302264\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 252138.244349\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.083000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 238646.783693\n",
      "(Epoch 2 / 20) train acc: 0.087000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 200732.543159\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.094444\n",
      "(Iteration 31 / 200) loss: 187553.462651\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.102778\n",
      "(Iteration 41 / 200) loss: 203744.282159\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.111111\n",
      "(Iteration 51 / 200) loss: 158652.361674\n",
      "(Epoch 6 / 20) train acc: 0.096000; val_acc: 0.122222\n",
      "(Iteration 61 / 200) loss: 173320.441192\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.130556\n",
      "(Iteration 71 / 200) loss: 163856.900715\n",
      "(Epoch 8 / 20) train acc: 0.135000; val_acc: 0.133333\n",
      "(Iteration 81 / 200) loss: 164522.720247\n",
      "(Epoch 9 / 20) train acc: 0.132000; val_acc: 0.136111\n",
      "(Iteration 91 / 200) loss: 164206.989785\n",
      "(Epoch 10 / 20) train acc: 0.132000; val_acc: 0.133333\n",
      "(Iteration 101 / 200) loss: 156734.619327\n",
      "(Epoch 11 / 20) train acc: 0.133000; val_acc: 0.138889\n",
      "(Iteration 111 / 200) loss: 124779.668873\n",
      "(Epoch 12 / 20) train acc: 0.129000; val_acc: 0.147222\n",
      "(Iteration 121 / 200) loss: 134897.068427\n",
      "(Epoch 13 / 20) train acc: 0.144000; val_acc: 0.147222\n",
      "(Iteration 131 / 200) loss: 124496.767982\n",
      "(Epoch 14 / 20) train acc: 0.183000; val_acc: 0.150000\n",
      "(Iteration 141 / 200) loss: 128312.097542\n",
      "(Epoch 15 / 20) train acc: 0.153000; val_acc: 0.155556\n",
      "(Iteration 151 / 200) loss: 116237.757103\n",
      "(Epoch 16 / 20) train acc: 0.146000; val_acc: 0.158333\n",
      "(Iteration 161 / 200) loss: 112731.656668\n",
      "(Epoch 17 / 20) train acc: 0.172000; val_acc: 0.163889\n",
      "(Iteration 171 / 200) loss: 107914.066238\n",
      "(Epoch 18 / 20) train acc: 0.174000; val_acc: 0.169444\n",
      "(Iteration 181 / 200) loss: 112461.325811\n",
      "(Epoch 19 / 20) train acc: 0.182000; val_acc: 0.172222\n",
      "(Iteration 191 / 200) loss: 96268.395387\n",
      "(Epoch 20 / 20) train acc: 0.178000; val_acc: 0.183333\n",
      "(Iteration 1 / 200) loss: 2.750301\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.151000; val_acc: 0.158333\n",
      "(Iteration 11 / 200) loss: 2.353707\n",
      "(Epoch 2 / 20) train acc: 0.328000; val_acc: 0.333333\n",
      "(Iteration 21 / 200) loss: 1.939927\n",
      "(Epoch 3 / 20) train acc: 0.463000; val_acc: 0.427778\n",
      "(Iteration 31 / 200) loss: 1.819064\n",
      "(Epoch 4 / 20) train acc: 0.562000; val_acc: 0.516667\n",
      "(Iteration 41 / 200) loss: 1.608195\n",
      "(Epoch 5 / 20) train acc: 0.634000; val_acc: 0.616667\n",
      "(Iteration 51 / 200) loss: 1.535063\n",
      "(Epoch 6 / 20) train acc: 0.682000; val_acc: 0.686111\n",
      "(Iteration 61 / 200) loss: 1.419087\n",
      "(Epoch 7 / 20) train acc: 0.748000; val_acc: 0.736111\n",
      "(Iteration 71 / 200) loss: 1.302804\n",
      "(Epoch 8 / 20) train acc: 0.766000; val_acc: 0.769444\n",
      "(Iteration 81 / 200) loss: 1.057462\n",
      "(Epoch 9 / 20) train acc: 0.805000; val_acc: 0.780556\n",
      "(Iteration 91 / 200) loss: 0.910051\n",
      "(Epoch 10 / 20) train acc: 0.826000; val_acc: 0.802778\n",
      "(Iteration 101 / 200) loss: 0.813797\n",
      "(Epoch 11 / 20) train acc: 0.857000; val_acc: 0.827778\n",
      "(Iteration 111 / 200) loss: 0.686131\n",
      "(Epoch 12 / 20) train acc: 0.859000; val_acc: 0.830556\n",
      "(Iteration 121 / 200) loss: 0.614752\n",
      "(Epoch 13 / 20) train acc: 0.872000; val_acc: 0.838889\n",
      "(Iteration 131 / 200) loss: 0.608383\n",
      "(Epoch 14 / 20) train acc: 0.897000; val_acc: 0.855556\n",
      "(Iteration 141 / 200) loss: 0.583654\n",
      "(Epoch 15 / 20) train acc: 0.907000; val_acc: 0.858333\n",
      "(Iteration 151 / 200) loss: 0.473264\n",
      "(Epoch 16 / 20) train acc: 0.905000; val_acc: 0.850000\n",
      "(Iteration 161 / 200) loss: 0.606138\n",
      "(Epoch 17 / 20) train acc: 0.915000; val_acc: 0.863889\n",
      "(Iteration 171 / 200) loss: 0.338216\n",
      "(Epoch 18 / 20) train acc: 0.914000; val_acc: 0.880556\n",
      "(Iteration 181 / 200) loss: 0.380205\n",
      "(Epoch 19 / 20) train acc: 0.929000; val_acc: 0.886111\n",
      "(Iteration 191 / 200) loss: 0.410900\n",
      "(Epoch 20 / 20) train acc: 0.916000; val_acc: 0.886111\n",
      "(Iteration 1 / 200) loss: 2.302768\n",
      "(Epoch 0 / 20) train acc: 0.162000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302629\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302524\n",
      "(Epoch 3 / 20) train acc: 0.179000; val_acc: 0.161111\n",
      "(Iteration 31 / 200) loss: 2.302524\n",
      "(Epoch 4 / 20) train acc: 0.197000; val_acc: 0.161111\n",
      "(Iteration 41 / 200) loss: 2.302055\n",
      "(Epoch 5 / 20) train acc: 0.212000; val_acc: 0.172222\n",
      "(Iteration 51 / 200) loss: 2.301297\n",
      "(Epoch 6 / 20) train acc: 0.386000; val_acc: 0.325000\n",
      "(Iteration 61 / 200) loss: 2.299443\n",
      "(Epoch 7 / 20) train acc: 0.387000; val_acc: 0.322222\n",
      "(Iteration 71 / 200) loss: 2.294256\n",
      "(Epoch 8 / 20) train acc: 0.398000; val_acc: 0.344444\n",
      "(Iteration 81 / 200) loss: 2.286072\n",
      "(Epoch 9 / 20) train acc: 0.398000; val_acc: 0.308333\n",
      "(Iteration 91 / 200) loss: 2.258548\n",
      "(Epoch 10 / 20) train acc: 0.405000; val_acc: 0.338889\n",
      "(Iteration 101 / 200) loss: 2.225343\n",
      "(Epoch 11 / 20) train acc: 0.461000; val_acc: 0.400000\n",
      "(Iteration 111 / 200) loss: 2.158614\n",
      "(Epoch 12 / 20) train acc: 0.470000; val_acc: 0.416667\n",
      "(Iteration 121 / 200) loss: 2.058681\n",
      "(Epoch 13 / 20) train acc: 0.488000; val_acc: 0.452778\n",
      "(Iteration 131 / 200) loss: 1.951448\n",
      "(Epoch 14 / 20) train acc: 0.462000; val_acc: 0.461111\n",
      "(Iteration 141 / 200) loss: 1.714972\n",
      "(Epoch 15 / 20) train acc: 0.553000; val_acc: 0.491667\n",
      "(Iteration 151 / 200) loss: 1.565222\n",
      "(Epoch 16 / 20) train acc: 0.505000; val_acc: 0.511111\n",
      "(Iteration 161 / 200) loss: 1.489055\n",
      "(Epoch 17 / 20) train acc: 0.566000; val_acc: 0.519444\n",
      "(Iteration 171 / 200) loss: 1.362420\n",
      "(Epoch 18 / 20) train acc: 0.589000; val_acc: 0.552778\n",
      "(Iteration 181 / 200) loss: 1.235008\n",
      "(Epoch 19 / 20) train acc: 0.587000; val_acc: 0.575000\n",
      "(Iteration 191 / 200) loss: 1.194689\n",
      "(Epoch 20 / 20) train acc: 0.611000; val_acc: 0.638889\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302649\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302573\n",
      "(Epoch 3 / 20) train acc: 0.083000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302505\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302540\n",
      "(Epoch 5 / 20) train acc: 0.081000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302711\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302587\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302529\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302781\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302700\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302415\n",
      "(Epoch 11 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302680\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302936\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302034\n",
      "(Epoch 14 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302544\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302619\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302216\n",
      "(Epoch 17 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302733\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302162\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.301666\n",
      "(Epoch 20 / 20) train acc: 0.079000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302636\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302658\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302562\n",
      "(Epoch 6 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302502\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302587\n",
      "(Epoch 8 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302324\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302428\n",
      "(Epoch 10 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302838\n",
      "(Epoch 11 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302188\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302578\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302475\n",
      "(Epoch 14 / 20) train acc: 0.085000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302363\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302783\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302724\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302460\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302486\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302452\n",
      "(Epoch 20 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302603\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302514\n",
      "(Epoch 3 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302500\n",
      "(Epoch 4 / 20) train acc: 0.131000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302480\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302249\n",
      "(Epoch 6 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302501\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302696\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302559\n",
      "(Epoch 9 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302810\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302368\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302682\n",
      "(Epoch 12 / 20) train acc: 0.081000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302588\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302480\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302845\n",
      "(Epoch 15 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302108\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302180\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302490\n",
      "(Epoch 18 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302554\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302128\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 269461.279867\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.125000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 287230.299391\n",
      "(Epoch 2 / 20) train acc: 0.130000; val_acc: 0.105556\n",
      "(Iteration 21 / 200) loss: 194354.519034\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.111111\n",
      "(Iteration 31 / 200) loss: 232560.138700\n",
      "(Epoch 4 / 20) train acc: 0.126000; val_acc: 0.119444\n",
      "(Iteration 41 / 200) loss: 204530.178376\n",
      "(Epoch 5 / 20) train acc: 0.127000; val_acc: 0.122222\n",
      "(Iteration 51 / 200) loss: 191548.458062\n",
      "(Epoch 6 / 20) train acc: 0.135000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 180795.717753\n",
      "(Epoch 7 / 20) train acc: 0.126000; val_acc: 0.133333\n",
      "(Iteration 71 / 200) loss: 182820.937448\n",
      "(Epoch 8 / 20) train acc: 0.139000; val_acc: 0.133333\n",
      "(Iteration 81 / 200) loss: 179681.257148\n",
      "(Epoch 9 / 20) train acc: 0.126000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 159303.466852\n",
      "(Epoch 10 / 20) train acc: 0.124000; val_acc: 0.144444\n",
      "(Iteration 101 / 200) loss: 138886.826557\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.141667\n",
      "(Iteration 111 / 200) loss: 157317.346268\n",
      "(Epoch 12 / 20) train acc: 0.122000; val_acc: 0.138889\n",
      "(Iteration 121 / 200) loss: 139997.465981\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.133333\n",
      "(Iteration 131 / 200) loss: 148528.215697\n",
      "(Epoch 14 / 20) train acc: 0.132000; val_acc: 0.133333\n",
      "(Iteration 141 / 200) loss: 117806.755411\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.133333\n",
      "(Iteration 151 / 200) loss: 117961.865128\n",
      "(Epoch 16 / 20) train acc: 0.141000; val_acc: 0.133333\n",
      "(Iteration 161 / 200) loss: 117279.334851\n",
      "(Epoch 17 / 20) train acc: 0.161000; val_acc: 0.138889\n",
      "(Iteration 171 / 200) loss: 103643.544572\n",
      "(Epoch 18 / 20) train acc: 0.145000; val_acc: 0.136111\n",
      "(Iteration 181 / 200) loss: 123670.354293\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.144444\n",
      "(Iteration 191 / 200) loss: 80549.579019\n",
      "(Epoch 20 / 20) train acc: 0.163000; val_acc: 0.152778\n",
      "(Iteration 1 / 200) loss: 3.036089\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.167000; val_acc: 0.158333\n",
      "(Iteration 11 / 200) loss: 2.743142\n",
      "(Epoch 2 / 20) train acc: 0.263000; val_acc: 0.230556\n",
      "(Iteration 21 / 200) loss: 2.185065\n",
      "(Epoch 3 / 20) train acc: 0.349000; val_acc: 0.347222\n",
      "(Iteration 31 / 200) loss: 1.920465\n",
      "(Epoch 4 / 20) train acc: 0.483000; val_acc: 0.450000\n",
      "(Iteration 41 / 200) loss: 1.811002\n",
      "(Epoch 5 / 20) train acc: 0.557000; val_acc: 0.513889\n",
      "(Iteration 51 / 200) loss: 1.658840\n",
      "(Epoch 6 / 20) train acc: 0.646000; val_acc: 0.588889\n",
      "(Iteration 61 / 200) loss: 1.393277\n",
      "(Epoch 7 / 20) train acc: 0.721000; val_acc: 0.663889\n",
      "(Iteration 71 / 200) loss: 1.377291\n",
      "(Epoch 8 / 20) train acc: 0.759000; val_acc: 0.702778\n",
      "(Iteration 81 / 200) loss: 1.022613\n",
      "(Epoch 9 / 20) train acc: 0.813000; val_acc: 0.738889\n",
      "(Iteration 91 / 200) loss: 1.011466\n",
      "(Epoch 10 / 20) train acc: 0.809000; val_acc: 0.766667\n",
      "(Iteration 101 / 200) loss: 0.828308\n",
      "(Epoch 11 / 20) train acc: 0.837000; val_acc: 0.791667\n",
      "(Iteration 111 / 200) loss: 0.875353\n",
      "(Epoch 12 / 20) train acc: 0.858000; val_acc: 0.819444\n",
      "(Iteration 121 / 200) loss: 0.711405\n",
      "(Epoch 13 / 20) train acc: 0.898000; val_acc: 0.847222\n",
      "(Iteration 131 / 200) loss: 0.604696\n",
      "(Epoch 14 / 20) train acc: 0.898000; val_acc: 0.858333\n",
      "(Iteration 141 / 200) loss: 0.680888\n",
      "(Epoch 15 / 20) train acc: 0.894000; val_acc: 0.875000\n",
      "(Iteration 151 / 200) loss: 0.476660\n",
      "(Epoch 16 / 20) train acc: 0.891000; val_acc: 0.891667\n",
      "(Iteration 161 / 200) loss: 0.504654\n",
      "(Epoch 17 / 20) train acc: 0.914000; val_acc: 0.891667\n",
      "(Iteration 171 / 200) loss: 0.320012\n",
      "(Epoch 18 / 20) train acc: 0.936000; val_acc: 0.913889\n",
      "(Iteration 181 / 200) loss: 0.312436\n",
      "(Epoch 19 / 20) train acc: 0.945000; val_acc: 0.919444\n",
      "(Iteration 191 / 200) loss: 0.342426\n",
      "(Epoch 20 / 20) train acc: 0.946000; val_acc: 0.919444\n",
      "(Iteration 1 / 200) loss: 2.302768\n",
      "(Epoch 0 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.167000; val_acc: 0.180556\n",
      "(Iteration 11 / 200) loss: 2.302701\n",
      "(Epoch 2 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302621\n",
      "(Epoch 3 / 20) train acc: 0.115000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 2.302442\n",
      "(Epoch 4 / 20) train acc: 0.279000; val_acc: 0.283333\n",
      "(Iteration 41 / 200) loss: 2.301913\n",
      "(Epoch 5 / 20) train acc: 0.337000; val_acc: 0.316667\n",
      "(Iteration 51 / 200) loss: 2.301100\n",
      "(Epoch 6 / 20) train acc: 0.340000; val_acc: 0.283333\n",
      "(Iteration 61 / 200) loss: 2.297868\n",
      "(Epoch 7 / 20) train acc: 0.353000; val_acc: 0.294444\n",
      "(Iteration 71 / 200) loss: 2.291042\n",
      "(Epoch 8 / 20) train acc: 0.348000; val_acc: 0.297222\n",
      "(Iteration 81 / 200) loss: 2.270733\n",
      "(Epoch 9 / 20) train acc: 0.401000; val_acc: 0.358333\n",
      "(Iteration 91 / 200) loss: 2.249834\n",
      "(Epoch 10 / 20) train acc: 0.346000; val_acc: 0.341667\n",
      "(Iteration 101 / 200) loss: 2.208789\n",
      "(Epoch 11 / 20) train acc: 0.356000; val_acc: 0.347222\n",
      "(Iteration 111 / 200) loss: 2.063972\n",
      "(Epoch 12 / 20) train acc: 0.372000; val_acc: 0.352778\n",
      "(Iteration 121 / 200) loss: 1.975963\n",
      "(Epoch 13 / 20) train acc: 0.356000; val_acc: 0.397222\n",
      "(Iteration 131 / 200) loss: 1.777320\n",
      "(Epoch 14 / 20) train acc: 0.404000; val_acc: 0.408333\n",
      "(Iteration 141 / 200) loss: 1.601749\n",
      "(Epoch 15 / 20) train acc: 0.440000; val_acc: 0.433333\n",
      "(Iteration 151 / 200) loss: 1.356666\n",
      "(Epoch 16 / 20) train acc: 0.494000; val_acc: 0.494444\n",
      "(Iteration 161 / 200) loss: 1.245529\n",
      "(Epoch 17 / 20) train acc: 0.545000; val_acc: 0.527778\n",
      "(Iteration 171 / 200) loss: 1.422518\n",
      "(Epoch 18 / 20) train acc: 0.528000; val_acc: 0.563889\n",
      "(Iteration 181 / 200) loss: 1.157328\n",
      "(Epoch 19 / 20) train acc: 0.550000; val_acc: 0.569444\n",
      "(Iteration 191 / 200) loss: 1.091749\n",
      "(Epoch 20 / 20) train acc: 0.610000; val_acc: 0.608333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302591\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302490\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302427\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302509\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302637\n",
      "(Epoch 6 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302654\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302488\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302302\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302451\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302778\n",
      "(Epoch 11 / 20) train acc: 0.082000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302752\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302786\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302426\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302695\n",
      "(Epoch 15 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.301958\n",
      "(Epoch 16 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302695\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.303154\n",
      "(Epoch 18 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302445\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302661\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302524\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302471\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302644\n",
      "(Epoch 4 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302566\n",
      "(Epoch 5 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302628\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302721\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302725\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302238\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302268\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302250\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302846\n",
      "(Epoch 12 / 20) train acc: 0.080000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302602\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.301945\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302064\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302743\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302329\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302895\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302099\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302389\n",
      "(Epoch 20 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302605\n",
      "(Epoch 2 / 20) train acc: 0.082000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302513\n",
      "(Epoch 3 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302462\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302624\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302574\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302327\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302237\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302453\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302438\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302342\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302609\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302501\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302326\n",
      "(Epoch 14 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302631\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302289\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302401\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.301774\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302584\n",
      "(Epoch 19 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302124\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 213133.921186\n",
      "(Epoch 0 / 20) train acc: 0.079000; val_acc: 0.075000\n",
      "(Epoch 1 / 20) train acc: 0.091000; val_acc: 0.072222\n",
      "(Iteration 11 / 200) loss: 206002.101118\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.069444\n",
      "(Iteration 21 / 200) loss: 187995.761061\n",
      "(Epoch 3 / 20) train acc: 0.088000; val_acc: 0.063889\n",
      "(Iteration 31 / 200) loss: 164745.431006\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.066667\n",
      "(Iteration 41 / 200) loss: 157917.080952\n",
      "(Epoch 5 / 20) train acc: 0.091000; val_acc: 0.066667\n",
      "(Iteration 51 / 200) loss: 161976.990898\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.069444\n",
      "(Iteration 61 / 200) loss: 153815.560846\n",
      "(Epoch 7 / 20) train acc: 0.086000; val_acc: 0.069444\n",
      "(Iteration 71 / 200) loss: 133441.540794\n",
      "(Epoch 8 / 20) train acc: 0.092000; val_acc: 0.069444\n",
      "(Iteration 81 / 200) loss: 126441.480743\n",
      "(Epoch 9 / 20) train acc: 0.092000; val_acc: 0.072222\n",
      "(Iteration 91 / 200) loss: 131200.890693\n",
      "(Epoch 10 / 20) train acc: 0.089000; val_acc: 0.077778\n",
      "(Iteration 101 / 200) loss: 118416.470644\n",
      "(Epoch 11 / 20) train acc: 0.080000; val_acc: 0.075000\n",
      "(Iteration 111 / 200) loss: 109627.780596\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.086111\n",
      "(Iteration 121 / 200) loss: 108919.500549\n",
      "(Epoch 13 / 20) train acc: 0.090000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 100769.350501\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.105556\n",
      "(Iteration 141 / 200) loss: 107663.590454\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.108333\n",
      "(Iteration 151 / 200) loss: 88551.520408\n",
      "(Epoch 16 / 20) train acc: 0.133000; val_acc: 0.108333\n",
      "(Iteration 161 / 200) loss: 93196.750361\n",
      "(Epoch 17 / 20) train acc: 0.114000; val_acc: 0.111111\n",
      "(Iteration 171 / 200) loss: 79529.770315\n",
      "(Epoch 18 / 20) train acc: 0.115000; val_acc: 0.111111\n",
      "(Iteration 181 / 200) loss: 89216.510269\n",
      "(Epoch 19 / 20) train acc: 0.136000; val_acc: 0.119444\n",
      "(Iteration 191 / 200) loss: 71994.745223\n",
      "(Epoch 20 / 20) train acc: 0.142000; val_acc: 0.127778\n",
      "(Iteration 1 / 200) loss: 3.774721\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.140000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 2.724457\n",
      "(Epoch 2 / 20) train acc: 0.232000; val_acc: 0.211111\n",
      "(Iteration 21 / 200) loss: 2.214824\n",
      "(Epoch 3 / 20) train acc: 0.291000; val_acc: 0.272222\n",
      "(Iteration 31 / 200) loss: 2.098207\n",
      "(Epoch 4 / 20) train acc: 0.408000; val_acc: 0.358333\n",
      "(Iteration 41 / 200) loss: 1.944586\n",
      "(Epoch 5 / 20) train acc: 0.483000; val_acc: 0.416667\n",
      "(Iteration 51 / 200) loss: 1.673288\n",
      "(Epoch 6 / 20) train acc: 0.525000; val_acc: 0.491667\n",
      "(Iteration 61 / 200) loss: 1.606833\n",
      "(Epoch 7 / 20) train acc: 0.625000; val_acc: 0.563889\n",
      "(Iteration 71 / 200) loss: 1.408627\n",
      "(Epoch 8 / 20) train acc: 0.641000; val_acc: 0.619444\n",
      "(Iteration 81 / 200) loss: 1.290118\n",
      "(Epoch 9 / 20) train acc: 0.715000; val_acc: 0.677778\n",
      "(Iteration 91 / 200) loss: 1.223314\n",
      "(Epoch 10 / 20) train acc: 0.779000; val_acc: 0.727778\n",
      "(Iteration 101 / 200) loss: 0.967560\n",
      "(Epoch 11 / 20) train acc: 0.797000; val_acc: 0.763889\n",
      "(Iteration 111 / 200) loss: 0.901387\n",
      "(Epoch 12 / 20) train acc: 0.831000; val_acc: 0.791667\n",
      "(Iteration 121 / 200) loss: 0.704720\n",
      "(Epoch 13 / 20) train acc: 0.869000; val_acc: 0.813889\n",
      "(Iteration 131 / 200) loss: 0.647891\n",
      "(Epoch 14 / 20) train acc: 0.876000; val_acc: 0.816667\n",
      "(Iteration 141 / 200) loss: 0.655545\n",
      "(Epoch 15 / 20) train acc: 0.875000; val_acc: 0.830556\n",
      "(Iteration 151 / 200) loss: 0.623638\n",
      "(Epoch 16 / 20) train acc: 0.898000; val_acc: 0.863889\n",
      "(Iteration 161 / 200) loss: 0.598449\n",
      "(Epoch 17 / 20) train acc: 0.891000; val_acc: 0.872222\n",
      "(Iteration 171 / 200) loss: 0.463182\n",
      "(Epoch 18 / 20) train acc: 0.917000; val_acc: 0.875000\n",
      "(Iteration 181 / 200) loss: 0.360613\n",
      "(Epoch 19 / 20) train acc: 0.917000; val_acc: 0.886111\n",
      "(Iteration 191 / 200) loss: 0.415124\n",
      "(Epoch 20 / 20) train acc: 0.929000; val_acc: 0.888889\n",
      "(Iteration 1 / 200) loss: 2.302600\n",
      "(Epoch 0 / 20) train acc: 0.192000; val_acc: 0.144444\n",
      "(Epoch 1 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302615\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302368\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302342\n",
      "(Epoch 4 / 20) train acc: 0.190000; val_acc: 0.144444\n",
      "(Iteration 41 / 200) loss: 2.301701\n",
      "(Epoch 5 / 20) train acc: 0.220000; val_acc: 0.166667\n",
      "(Iteration 51 / 200) loss: 2.299227\n",
      "(Epoch 6 / 20) train acc: 0.202000; val_acc: 0.161111\n",
      "(Iteration 61 / 200) loss: 2.294849\n",
      "(Epoch 7 / 20) train acc: 0.200000; val_acc: 0.158333\n",
      "(Iteration 71 / 200) loss: 2.285875\n",
      "(Epoch 8 / 20) train acc: 0.179000; val_acc: 0.155556\n",
      "(Iteration 81 / 200) loss: 2.256413\n",
      "(Epoch 9 / 20) train acc: 0.212000; val_acc: 0.194444\n",
      "(Iteration 91 / 200) loss: 2.204263\n",
      "(Epoch 10 / 20) train acc: 0.207000; val_acc: 0.216667\n",
      "(Iteration 101 / 200) loss: 2.111780\n",
      "(Epoch 11 / 20) train acc: 0.300000; val_acc: 0.233333\n",
      "(Iteration 111 / 200) loss: 2.022744\n",
      "(Epoch 12 / 20) train acc: 0.248000; val_acc: 0.233333\n",
      "(Iteration 121 / 200) loss: 1.824819\n",
      "(Epoch 13 / 20) train acc: 0.217000; val_acc: 0.250000\n",
      "(Iteration 131 / 200) loss: 1.821389\n",
      "(Epoch 14 / 20) train acc: 0.200000; val_acc: 0.202778\n",
      "(Iteration 141 / 200) loss: 1.666028\n",
      "(Epoch 15 / 20) train acc: 0.207000; val_acc: 0.208333\n",
      "(Iteration 151 / 200) loss: 1.728331\n",
      "(Epoch 16 / 20) train acc: 0.246000; val_acc: 0.222222\n",
      "(Iteration 161 / 200) loss: 1.737063\n",
      "(Epoch 17 / 20) train acc: 0.300000; val_acc: 0.266667\n",
      "(Iteration 171 / 200) loss: 1.758865\n",
      "(Epoch 18 / 20) train acc: 0.311000; val_acc: 0.266667\n",
      "(Iteration 181 / 200) loss: 1.665508\n",
      "(Epoch 19 / 20) train acc: 0.282000; val_acc: 0.286111\n",
      "(Iteration 191 / 200) loss: 1.646338\n",
      "(Epoch 20 / 20) train acc: 0.295000; val_acc: 0.283333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.133000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302600\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302479\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302566\n",
      "(Epoch 4 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302594\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302356\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302499\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302728\n",
      "(Epoch 8 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302304\n",
      "(Epoch 9 / 20) train acc: 0.102000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302569\n",
      "(Epoch 10 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302667\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302498\n",
      "(Epoch 12 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302363\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302235\n",
      "(Epoch 14 / 20) train acc: 0.137000; val_acc: 0.108333\n",
      "(Iteration 141 / 200) loss: 2.302192\n",
      "(Epoch 15 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301148\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.298321\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.286443\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.243710\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.203245\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302579\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302497\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302500\n",
      "(Epoch 4 / 20) train acc: 0.084000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302444\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302505\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302567\n",
      "(Epoch 7 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302341\n",
      "(Epoch 8 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302495\n",
      "(Epoch 9 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302790\n",
      "(Epoch 10 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302117\n",
      "(Epoch 11 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302172\n",
      "(Epoch 12 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302193\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302118\n",
      "(Epoch 14 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302013\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302301\n",
      "(Epoch 16 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302458\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302182\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.301014\n",
      "(Epoch 19 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302792\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302516\n",
      "(Epoch 2 / 20) train acc: 0.075000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302547\n",
      "(Epoch 3 / 20) train acc: 0.133000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302459\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302550\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302461\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302155\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302788\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302631\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302139\n",
      "(Epoch 10 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302749\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302255\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302178\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302729\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302656\n",
      "(Epoch 15 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302655\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302316\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302284\n",
      "(Epoch 18 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.301884\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.301823\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 172157.784564\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 173335.084509\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 164193.594463\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.122222\n",
      "(Iteration 31 / 200) loss: 150167.274420\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.122222\n",
      "(Iteration 41 / 200) loss: 128897.194377\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 137290.894335\n",
      "(Epoch 6 / 20) train acc: 0.145000; val_acc: 0.133333\n",
      "(Iteration 61 / 200) loss: 125084.334294\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.138889\n",
      "(Iteration 71 / 200) loss: 126979.024253\n",
      "(Epoch 8 / 20) train acc: 0.134000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 94922.114213\n",
      "(Epoch 9 / 20) train acc: 0.145000; val_acc: 0.150000\n",
      "(Iteration 91 / 200) loss: 114713.404172\n",
      "(Epoch 10 / 20) train acc: 0.139000; val_acc: 0.169444\n",
      "(Iteration 101 / 200) loss: 99008.114132\n",
      "(Epoch 11 / 20) train acc: 0.157000; val_acc: 0.177778\n",
      "(Iteration 111 / 200) loss: 104498.094092\n",
      "(Epoch 12 / 20) train acc: 0.158000; val_acc: 0.197222\n",
      "(Iteration 121 / 200) loss: 106675.974052\n",
      "(Epoch 13 / 20) train acc: 0.188000; val_acc: 0.213889\n",
      "(Iteration 131 / 200) loss: 80886.284011\n",
      "(Epoch 14 / 20) train acc: 0.230000; val_acc: 0.222222\n",
      "(Iteration 141 / 200) loss: 84589.433972\n",
      "(Epoch 15 / 20) train acc: 0.232000; val_acc: 0.225000\n",
      "(Iteration 151 / 200) loss: 87177.423933\n",
      "(Epoch 16 / 20) train acc: 0.252000; val_acc: 0.238889\n",
      "(Iteration 161 / 200) loss: 66582.533894\n",
      "(Epoch 17 / 20) train acc: 0.258000; val_acc: 0.250000\n",
      "(Iteration 171 / 200) loss: 85104.933856\n",
      "(Epoch 18 / 20) train acc: 0.260000; val_acc: 0.263889\n",
      "(Iteration 181 / 200) loss: 72986.798817\n",
      "(Epoch 19 / 20) train acc: 0.281000; val_acc: 0.272222\n",
      "(Iteration 191 / 200) loss: 68440.288779\n",
      "(Epoch 20 / 20) train acc: 0.255000; val_acc: 0.277778\n",
      "(Iteration 1 / 200) loss: 3.053312\n",
      "(Epoch 0 / 20) train acc: 0.139000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.177000; val_acc: 0.147222\n",
      "(Iteration 11 / 200) loss: 2.476002\n",
      "(Epoch 2 / 20) train acc: 0.310000; val_acc: 0.252778\n",
      "(Iteration 21 / 200) loss: 2.249889\n",
      "(Epoch 3 / 20) train acc: 0.405000; val_acc: 0.366667\n",
      "(Iteration 31 / 200) loss: 1.923227\n",
      "(Epoch 4 / 20) train acc: 0.460000; val_acc: 0.461111\n",
      "(Iteration 41 / 200) loss: 1.728586\n",
      "(Epoch 5 / 20) train acc: 0.589000; val_acc: 0.533333\n",
      "(Iteration 51 / 200) loss: 1.535811\n",
      "(Epoch 6 / 20) train acc: 0.664000; val_acc: 0.608333\n",
      "(Iteration 61 / 200) loss: 1.316467\n",
      "(Epoch 7 / 20) train acc: 0.703000; val_acc: 0.683333\n",
      "(Iteration 71 / 200) loss: 1.235159\n",
      "(Epoch 8 / 20) train acc: 0.770000; val_acc: 0.730556\n",
      "(Iteration 81 / 200) loss: 1.174626\n",
      "(Epoch 9 / 20) train acc: 0.818000; val_acc: 0.777778\n",
      "(Iteration 91 / 200) loss: 0.910391\n",
      "(Epoch 10 / 20) train acc: 0.837000; val_acc: 0.805556\n",
      "(Iteration 101 / 200) loss: 0.876656\n",
      "(Epoch 11 / 20) train acc: 0.855000; val_acc: 0.822222\n",
      "(Iteration 111 / 200) loss: 0.822285\n",
      "(Epoch 12 / 20) train acc: 0.879000; val_acc: 0.833333\n",
      "(Iteration 121 / 200) loss: 0.720547\n",
      "(Epoch 13 / 20) train acc: 0.877000; val_acc: 0.841667\n",
      "(Iteration 131 / 200) loss: 0.760992\n",
      "(Epoch 14 / 20) train acc: 0.884000; val_acc: 0.861111\n",
      "(Iteration 141 / 200) loss: 0.605158\n",
      "(Epoch 15 / 20) train acc: 0.904000; val_acc: 0.866667\n",
      "(Iteration 151 / 200) loss: 0.565959\n",
      "(Epoch 16 / 20) train acc: 0.896000; val_acc: 0.872222\n",
      "(Iteration 161 / 200) loss: 0.424510\n",
      "(Epoch 17 / 20) train acc: 0.921000; val_acc: 0.877778\n",
      "(Iteration 171 / 200) loss: 0.420175\n",
      "(Epoch 18 / 20) train acc: 0.924000; val_acc: 0.891667\n",
      "(Iteration 181 / 200) loss: 0.345696\n",
      "(Epoch 19 / 20) train acc: 0.941000; val_acc: 0.911111\n",
      "(Iteration 191 / 200) loss: 0.332049\n",
      "(Epoch 20 / 20) train acc: 0.946000; val_acc: 0.911111\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.166000; val_acc: 0.172222\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302560\n",
      "(Epoch 2 / 20) train acc: 0.210000; val_acc: 0.175000\n",
      "(Iteration 21 / 200) loss: 2.302486\n",
      "(Epoch 3 / 20) train acc: 0.223000; val_acc: 0.175000\n",
      "(Iteration 31 / 200) loss: 2.302203\n",
      "(Epoch 4 / 20) train acc: 0.200000; val_acc: 0.172222\n",
      "(Iteration 41 / 200) loss: 2.301667\n",
      "(Epoch 5 / 20) train acc: 0.202000; val_acc: 0.169444\n",
      "(Iteration 51 / 200) loss: 2.299018\n",
      "(Epoch 6 / 20) train acc: 0.217000; val_acc: 0.169444\n",
      "(Iteration 61 / 200) loss: 2.294852\n",
      "(Epoch 7 / 20) train acc: 0.203000; val_acc: 0.183333\n",
      "(Iteration 71 / 200) loss: 2.280561\n",
      "(Epoch 8 / 20) train acc: 0.234000; val_acc: 0.230556\n",
      "(Iteration 81 / 200) loss: 2.249308\n",
      "(Epoch 9 / 20) train acc: 0.245000; val_acc: 0.222222\n",
      "(Iteration 91 / 200) loss: 2.191701\n",
      "(Epoch 10 / 20) train acc: 0.229000; val_acc: 0.244444\n",
      "(Iteration 101 / 200) loss: 2.085885\n",
      "(Epoch 11 / 20) train acc: 0.238000; val_acc: 0.236111\n",
      "(Iteration 111 / 200) loss: 1.950018\n",
      "(Epoch 12 / 20) train acc: 0.254000; val_acc: 0.227778\n",
      "(Iteration 121 / 200) loss: 1.894652\n",
      "(Epoch 13 / 20) train acc: 0.264000; val_acc: 0.216667\n",
      "(Iteration 131 / 200) loss: 1.777445\n",
      "(Epoch 14 / 20) train acc: 0.272000; val_acc: 0.230556\n",
      "(Iteration 141 / 200) loss: 1.736178\n",
      "(Epoch 15 / 20) train acc: 0.275000; val_acc: 0.255556\n",
      "(Iteration 151 / 200) loss: 1.658663\n",
      "(Epoch 16 / 20) train acc: 0.251000; val_acc: 0.213889\n",
      "(Iteration 161 / 200) loss: 1.829888\n",
      "(Epoch 17 / 20) train acc: 0.257000; val_acc: 0.219444\n",
      "(Iteration 171 / 200) loss: 1.709867\n",
      "(Epoch 18 / 20) train acc: 0.287000; val_acc: 0.275000\n",
      "(Iteration 181 / 200) loss: 1.670834\n",
      "(Epoch 19 / 20) train acc: 0.314000; val_acc: 0.283333\n",
      "(Iteration 191 / 200) loss: 1.619493\n",
      "(Epoch 20 / 20) train acc: 0.286000; val_acc: 0.255556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302553\n",
      "(Epoch 2 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302573\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302649\n",
      "(Epoch 5 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302668\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302607\n",
      "(Epoch 7 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302611\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302792\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302442\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302448\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302558\n",
      "(Epoch 12 / 20) train acc: 0.088000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302855\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302050\n",
      "(Epoch 14 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.301921\n",
      "(Epoch 15 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.299658\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.291102\n",
      "(Epoch 17 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.260031\n",
      "(Epoch 18 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.244006\n",
      "(Epoch 19 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.191548\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.129000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302623\n",
      "(Epoch 2 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302623\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302442\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302719\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302541\n",
      "(Epoch 6 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302479\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302676\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302735\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302467\n",
      "(Epoch 10 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302298\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302675\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302117\n",
      "(Epoch 13 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302096\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302298\n",
      "(Epoch 15 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302799\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.303004\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.300958\n",
      "(Epoch 18 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.299237\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.295570\n",
      "(Epoch 20 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302659\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302599\n",
      "(Epoch 3 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302505\n",
      "(Epoch 4 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302608\n",
      "(Epoch 5 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302546\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302379\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302464\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302604\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302594\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302267\n",
      "(Epoch 11 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302828\n",
      "(Epoch 12 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302567\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302519\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302406\n",
      "(Epoch 15 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302486\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302454\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302748\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302319\n",
      "(Epoch 19 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302177\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 176332.143235\n",
      "(Epoch 0 / 20) train acc: 0.074000; val_acc: 0.052778\n",
      "(Epoch 1 / 20) train acc: 0.069000; val_acc: 0.047222\n",
      "(Iteration 11 / 200) loss: 193101.603185\n",
      "(Epoch 2 / 20) train acc: 0.062000; val_acc: 0.058333\n",
      "(Iteration 21 / 200) loss: 198975.403144\n",
      "(Epoch 3 / 20) train acc: 0.070000; val_acc: 0.069444\n",
      "(Iteration 31 / 200) loss: 179724.203105\n",
      "(Epoch 4 / 20) train acc: 0.085000; val_acc: 0.075000\n",
      "(Iteration 41 / 200) loss: 150195.903068\n",
      "(Epoch 5 / 20) train acc: 0.070000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 127818.793031\n",
      "(Epoch 6 / 20) train acc: 0.079000; val_acc: 0.105556\n",
      "(Iteration 61 / 200) loss: 126906.202995\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.105556\n",
      "(Iteration 71 / 200) loss: 115878.192959\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.111111\n",
      "(Iteration 81 / 200) loss: 111939.042923\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.113889\n",
      "(Iteration 91 / 200) loss: 105936.172888\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.122222\n",
      "(Iteration 101 / 200) loss: 81087.917853\n",
      "(Epoch 11 / 20) train acc: 0.157000; val_acc: 0.125000\n",
      "(Iteration 111 / 200) loss: 89990.112818\n",
      "(Epoch 12 / 20) train acc: 0.125000; val_acc: 0.133333\n",
      "(Iteration 121 / 200) loss: 109585.422783\n",
      "(Epoch 13 / 20) train acc: 0.159000; val_acc: 0.133333\n",
      "(Iteration 131 / 200) loss: 87804.712749\n",
      "(Epoch 14 / 20) train acc: 0.142000; val_acc: 0.141667\n",
      "(Iteration 141 / 200) loss: 95354.102715\n",
      "(Epoch 15 / 20) train acc: 0.146000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 88632.182680\n",
      "(Epoch 16 / 20) train acc: 0.151000; val_acc: 0.150000\n",
      "(Iteration 161 / 200) loss: 83059.447646\n",
      "(Epoch 17 / 20) train acc: 0.161000; val_acc: 0.158333\n",
      "(Iteration 171 / 200) loss: 75778.657613\n",
      "(Epoch 18 / 20) train acc: 0.175000; val_acc: 0.161111\n",
      "(Iteration 181 / 200) loss: 89757.642580\n",
      "(Epoch 19 / 20) train acc: 0.173000; val_acc: 0.158333\n",
      "(Iteration 191 / 200) loss: 66537.902547\n",
      "(Epoch 20 / 20) train acc: 0.190000; val_acc: 0.161111\n",
      "(Iteration 1 / 200) loss: 2.995062\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.147000; val_acc: 0.166667\n",
      "(Iteration 11 / 200) loss: 2.536827\n",
      "(Epoch 2 / 20) train acc: 0.240000; val_acc: 0.219444\n",
      "(Iteration 21 / 200) loss: 2.143143\n",
      "(Epoch 3 / 20) train acc: 0.292000; val_acc: 0.305556\n",
      "(Iteration 31 / 200) loss: 1.831455\n",
      "(Epoch 4 / 20) train acc: 0.429000; val_acc: 0.380556\n",
      "(Iteration 41 / 200) loss: 1.841066\n",
      "(Epoch 5 / 20) train acc: 0.502000; val_acc: 0.452778\n",
      "(Iteration 51 / 200) loss: 1.667208\n",
      "(Epoch 6 / 20) train acc: 0.625000; val_acc: 0.580556\n",
      "(Iteration 61 / 200) loss: 1.457873\n",
      "(Epoch 7 / 20) train acc: 0.680000; val_acc: 0.658333\n",
      "(Iteration 71 / 200) loss: 1.244899\n",
      "(Epoch 8 / 20) train acc: 0.733000; val_acc: 0.711111\n",
      "(Iteration 81 / 200) loss: 1.083569\n",
      "(Epoch 9 / 20) train acc: 0.771000; val_acc: 0.761111\n",
      "(Iteration 91 / 200) loss: 1.168619\n",
      "(Epoch 10 / 20) train acc: 0.803000; val_acc: 0.791667\n",
      "(Iteration 101 / 200) loss: 1.056635\n",
      "(Epoch 11 / 20) train acc: 0.822000; val_acc: 0.816667\n",
      "(Iteration 111 / 200) loss: 0.849155\n",
      "(Epoch 12 / 20) train acc: 0.868000; val_acc: 0.847222\n",
      "(Iteration 121 / 200) loss: 0.825492\n",
      "(Epoch 13 / 20) train acc: 0.869000; val_acc: 0.861111\n",
      "(Iteration 131 / 200) loss: 0.590857\n",
      "(Epoch 14 / 20) train acc: 0.891000; val_acc: 0.886111\n",
      "(Iteration 141 / 200) loss: 0.621287\n",
      "(Epoch 15 / 20) train acc: 0.890000; val_acc: 0.902778\n",
      "(Iteration 151 / 200) loss: 0.586036\n",
      "(Epoch 16 / 20) train acc: 0.914000; val_acc: 0.908333\n",
      "(Iteration 161 / 200) loss: 0.516977\n",
      "(Epoch 17 / 20) train acc: 0.925000; val_acc: 0.922222\n",
      "(Iteration 171 / 200) loss: 0.373666\n",
      "(Epoch 18 / 20) train acc: 0.933000; val_acc: 0.922222\n",
      "(Iteration 181 / 200) loss: 0.371536\n",
      "(Epoch 19 / 20) train acc: 0.905000; val_acc: 0.925000\n",
      "(Iteration 191 / 200) loss: 0.294859\n",
      "(Epoch 20 / 20) train acc: 0.937000; val_acc: 0.936111\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302574\n",
      "(Epoch 2 / 20) train acc: 0.179000; val_acc: 0.169444\n",
      "(Iteration 21 / 200) loss: 2.302432\n",
      "(Epoch 3 / 20) train acc: 0.287000; val_acc: 0.258333\n",
      "(Iteration 31 / 200) loss: 2.302269\n",
      "(Epoch 4 / 20) train acc: 0.407000; val_acc: 0.383333\n",
      "(Iteration 41 / 200) loss: 2.301909\n",
      "(Epoch 5 / 20) train acc: 0.333000; val_acc: 0.294444\n",
      "(Iteration 51 / 200) loss: 2.300340\n",
      "(Epoch 6 / 20) train acc: 0.319000; val_acc: 0.308333\n",
      "(Iteration 61 / 200) loss: 2.298213\n",
      "(Epoch 7 / 20) train acc: 0.358000; val_acc: 0.336111\n",
      "(Iteration 71 / 200) loss: 2.289059\n",
      "(Epoch 8 / 20) train acc: 0.325000; val_acc: 0.322222\n",
      "(Iteration 81 / 200) loss: 2.276730\n",
      "(Epoch 9 / 20) train acc: 0.300000; val_acc: 0.294444\n",
      "(Iteration 91 / 200) loss: 2.248541\n",
      "(Epoch 10 / 20) train acc: 0.283000; val_acc: 0.283333\n",
      "(Iteration 101 / 200) loss: 2.193403\n",
      "(Epoch 11 / 20) train acc: 0.265000; val_acc: 0.255556\n",
      "(Iteration 111 / 200) loss: 2.081395\n",
      "(Epoch 12 / 20) train acc: 0.253000; val_acc: 0.247222\n",
      "(Iteration 121 / 200) loss: 1.931924\n",
      "(Epoch 13 / 20) train acc: 0.288000; val_acc: 0.252778\n",
      "(Iteration 131 / 200) loss: 1.895747\n",
      "(Epoch 14 / 20) train acc: 0.276000; val_acc: 0.275000\n",
      "(Iteration 141 / 200) loss: 1.767950\n",
      "(Epoch 15 / 20) train acc: 0.280000; val_acc: 0.286111\n",
      "(Iteration 151 / 200) loss: 1.704997\n",
      "(Epoch 16 / 20) train acc: 0.320000; val_acc: 0.322222\n",
      "(Iteration 161 / 200) loss: 1.653821\n",
      "(Epoch 17 / 20) train acc: 0.440000; val_acc: 0.402778\n",
      "(Iteration 171 / 200) loss: 1.549853\n",
      "(Epoch 18 / 20) train acc: 0.514000; val_acc: 0.463889\n",
      "(Iteration 181 / 200) loss: 1.527828\n",
      "(Epoch 19 / 20) train acc: 0.539000; val_acc: 0.488889\n",
      "(Iteration 191 / 200) loss: 1.466413\n",
      "(Epoch 20 / 20) train acc: 0.529000; val_acc: 0.502778\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302576\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302681\n",
      "(Epoch 3 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302494\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302672\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302543\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302754\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302451\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302258\n",
      "(Epoch 9 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302421\n",
      "(Epoch 10 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302427\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302646\n",
      "(Epoch 12 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302538\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.301759\n",
      "(Epoch 14 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.301930\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.297675\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.296464\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.256188\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.231775\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.248934\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.122222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302595\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302666\n",
      "(Epoch 3 / 20) train acc: 0.081000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302585\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302609\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302779\n",
      "(Epoch 6 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302550\n",
      "(Epoch 7 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302676\n",
      "(Epoch 8 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302801\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302470\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302613\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302849\n",
      "(Epoch 12 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302417\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302474\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302846\n",
      "(Epoch 15 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302055\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302325\n",
      "(Epoch 17 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302829\n",
      "(Epoch 18 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302646\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302665\n",
      "(Epoch 20 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302600\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302605\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302466\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302465\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302471\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302693\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302383\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302542\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302659\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302512\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302590\n",
      "(Epoch 12 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302538\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302451\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302448\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.301958\n",
      "(Epoch 16 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302665\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302408\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302462\n",
      "(Epoch 19 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302532\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 310781.360530\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.069444\n",
      "(Epoch 1 / 20) train acc: 0.076000; val_acc: 0.069444\n",
      "(Iteration 11 / 200) loss: 290480.074722\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.066667\n",
      "(Iteration 21 / 200) loss: 313846.449749\n",
      "(Epoch 3 / 20) train acc: 0.082000; val_acc: 0.066667\n",
      "(Iteration 31 / 200) loss: 305996.744932\n",
      "(Epoch 4 / 20) train acc: 0.085000; val_acc: 0.066667\n",
      "(Iteration 41 / 200) loss: 317640.340137\n",
      "(Epoch 5 / 20) train acc: 0.089000; val_acc: 0.063889\n",
      "(Iteration 51 / 200) loss: 260586.795410\n",
      "(Epoch 6 / 20) train acc: 0.067000; val_acc: 0.066667\n",
      "(Iteration 61 / 200) loss: 298850.770708\n",
      "(Epoch 7 / 20) train acc: 0.073000; val_acc: 0.066667\n",
      "(Iteration 71 / 200) loss: 324676.785996\n",
      "(Epoch 8 / 20) train acc: 0.071000; val_acc: 0.066667\n",
      "(Iteration 81 / 200) loss: 285395.161265\n",
      "(Epoch 9 / 20) train acc: 0.086000; val_acc: 0.063889\n",
      "(Iteration 91 / 200) loss: 275675.436592\n",
      "(Epoch 10 / 20) train acc: 0.082000; val_acc: 0.063889\n",
      "(Iteration 101 / 200) loss: 272655.611951\n",
      "(Epoch 11 / 20) train acc: 0.079000; val_acc: 0.063889\n",
      "(Iteration 111 / 200) loss: 287994.747295\n",
      "(Epoch 12 / 20) train acc: 0.074000; val_acc: 0.066667\n",
      "(Iteration 121 / 200) loss: 300063.682598\n",
      "(Epoch 13 / 20) train acc: 0.079000; val_acc: 0.066667\n",
      "(Iteration 131 / 200) loss: 295443.637937\n",
      "(Epoch 14 / 20) train acc: 0.068000; val_acc: 0.066667\n",
      "(Iteration 141 / 200) loss: 273317.413264\n",
      "(Epoch 15 / 20) train acc: 0.075000; val_acc: 0.066667\n",
      "(Iteration 151 / 200) loss: 265874.048599\n",
      "(Epoch 16 / 20) train acc: 0.089000; val_acc: 0.066667\n",
      "(Iteration 161 / 200) loss: 259881.463940\n",
      "(Epoch 17 / 20) train acc: 0.070000; val_acc: 0.066667\n",
      "(Iteration 171 / 200) loss: 278447.779297\n",
      "(Epoch 18 / 20) train acc: 0.082000; val_acc: 0.066667\n",
      "(Iteration 181 / 200) loss: 276842.634668\n",
      "(Epoch 19 / 20) train acc: 0.074000; val_acc: 0.066667\n",
      "(Iteration 191 / 200) loss: 259770.190032\n",
      "(Epoch 20 / 20) train acc: 0.064000; val_acc: 0.066667\n",
      "(Iteration 1 / 200) loss: 4.849506\n",
      "(Epoch 0 / 20) train acc: 0.034000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.035000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 4.991368\n",
      "(Epoch 2 / 20) train acc: 0.027000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 4.487832\n",
      "(Epoch 3 / 20) train acc: 0.056000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 4.727479\n",
      "(Epoch 4 / 20) train acc: 0.062000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 4.768111\n",
      "(Epoch 5 / 20) train acc: 0.052000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 4.569751\n",
      "(Epoch 6 / 20) train acc: 0.075000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 4.657131\n",
      "(Epoch 7 / 20) train acc: 0.060000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 4.413255\n",
      "(Epoch 8 / 20) train acc: 0.071000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 4.418284\n",
      "(Epoch 9 / 20) train acc: 0.070000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 4.335417\n",
      "(Epoch 10 / 20) train acc: 0.073000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 4.358613\n",
      "(Epoch 11 / 20) train acc: 0.077000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 4.363561\n",
      "(Epoch 12 / 20) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 4.304087\n",
      "(Epoch 13 / 20) train acc: 0.086000; val_acc: 0.111111\n",
      "(Iteration 131 / 200) loss: 4.460251\n",
      "(Epoch 14 / 20) train acc: 0.090000; val_acc: 0.122222\n",
      "(Iteration 141 / 200) loss: 4.310288\n",
      "(Epoch 15 / 20) train acc: 0.080000; val_acc: 0.127778\n",
      "(Iteration 151 / 200) loss: 4.293389\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.136111\n",
      "(Iteration 161 / 200) loss: 4.176552\n",
      "(Epoch 17 / 20) train acc: 0.105000; val_acc: 0.141667\n",
      "(Iteration 171 / 200) loss: 4.257355\n",
      "(Epoch 18 / 20) train acc: 0.099000; val_acc: 0.147222\n",
      "(Iteration 181 / 200) loss: 4.242660\n",
      "(Epoch 19 / 20) train acc: 0.117000; val_acc: 0.152778\n",
      "(Iteration 191 / 200) loss: 4.231468\n",
      "(Epoch 20 / 20) train acc: 0.122000; val_acc: 0.169444\n",
      "(Iteration 1 / 200) loss: 2.320839\n",
      "(Epoch 0 / 20) train acc: 0.157000; val_acc: 0.122222\n",
      "(Epoch 1 / 20) train acc: 0.078000; val_acc: 0.075000\n",
      "(Iteration 11 / 200) loss: 2.320540\n",
      "(Epoch 2 / 20) train acc: 0.096000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 2.320265\n",
      "(Epoch 3 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.319967\n",
      "(Epoch 4 / 20) train acc: 0.085000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.319712\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.319410\n",
      "(Epoch 6 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.319130\n",
      "(Epoch 7 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.318894\n",
      "(Epoch 8 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.318624\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.318390\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.318094\n",
      "(Epoch 11 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.317898\n",
      "(Epoch 12 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.317680\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.317400\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.317174\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.316962\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.316727\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.316455\n",
      "(Epoch 18 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.316220\n",
      "(Epoch 19 / 20) train acc: 0.128000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.316076\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302766\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302738\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302714\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302693\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302670\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302668\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302653\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302647\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302635\n",
      "(Epoch 9 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302639\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302619\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302600\n",
      "(Epoch 12 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.302619\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302613\n",
      "(Epoch 14 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302602\n",
      "(Epoch 15 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302602\n",
      "(Epoch 16 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302600\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302586\n",
      "(Epoch 18 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302594\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302567\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.105000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302592\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302578\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302565\n",
      "(Epoch 5 / 20) train acc: 0.125000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302598\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302579\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302566\n",
      "(Epoch 8 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302577\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302581\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302577\n",
      "(Epoch 11 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302576\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302555\n",
      "(Epoch 13 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302575\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302614\n",
      "(Epoch 15 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.302573\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.302512\n",
      "(Epoch 17 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.302527\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.302549\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.302509\n",
      "(Epoch 20 / 20) train acc: 0.087000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302584\n",
      "(Epoch 3 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302590\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302575\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302559\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302549\n",
      "(Epoch 7 / 20) train acc: 0.123000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302551\n",
      "(Epoch 8 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302561\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302601\n",
      "(Epoch 10 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302557\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302568\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.302568\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302593\n",
      "(Epoch 14 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302540\n",
      "(Epoch 15 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302579\n",
      "(Epoch 16 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302607\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302594\n",
      "(Epoch 18 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302606\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302521\n",
      "(Epoch 20 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 238595.556077\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.133333\n",
      "(Epoch 1 / 20) train acc: 0.124000; val_acc: 0.133333\n",
      "(Iteration 11 / 200) loss: 238581.230381\n",
      "(Epoch 2 / 20) train acc: 0.125000; val_acc: 0.133333\n",
      "(Iteration 21 / 200) loss: 220346.605635\n",
      "(Epoch 3 / 20) train acc: 0.136000; val_acc: 0.133333\n",
      "(Iteration 31 / 200) loss: 218879.301123\n",
      "(Epoch 4 / 20) train acc: 0.121000; val_acc: 0.133333\n",
      "(Iteration 41 / 200) loss: 227355.396704\n",
      "(Epoch 5 / 20) train acc: 0.135000; val_acc: 0.133333\n",
      "(Iteration 51 / 200) loss: 224805.172297\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.133333\n",
      "(Iteration 61 / 200) loss: 238709.247930\n",
      "(Epoch 7 / 20) train acc: 0.127000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 240342.203516\n",
      "(Epoch 8 / 20) train acc: 0.105000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 241010.159104\n",
      "(Epoch 9 / 20) train acc: 0.121000; val_acc: 0.138889\n",
      "(Iteration 91 / 200) loss: 199147.814680\n",
      "(Epoch 10 / 20) train acc: 0.122000; val_acc: 0.141667\n",
      "(Iteration 101 / 200) loss: 215892.970266\n",
      "(Epoch 11 / 20) train acc: 0.127000; val_acc: 0.144444\n",
      "(Iteration 111 / 200) loss: 279246.925889\n",
      "(Epoch 12 / 20) train acc: 0.128000; val_acc: 0.144444\n",
      "(Iteration 121 / 200) loss: 199275.701472\n",
      "(Epoch 13 / 20) train acc: 0.126000; val_acc: 0.144444\n",
      "(Iteration 131 / 200) loss: 216211.857036\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.144444\n",
      "(Iteration 141 / 200) loss: 216839.712615\n",
      "(Epoch 15 / 20) train acc: 0.135000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 221020.448286\n",
      "(Epoch 16 / 20) train acc: 0.122000; val_acc: 0.141667\n",
      "(Iteration 161 / 200) loss: 208346.083960\n",
      "(Epoch 17 / 20) train acc: 0.123000; val_acc: 0.144444\n",
      "(Iteration 171 / 200) loss: 192863.919641\n",
      "(Epoch 18 / 20) train acc: 0.144000; val_acc: 0.144444\n",
      "(Iteration 181 / 200) loss: 208916.575327\n",
      "(Epoch 19 / 20) train acc: 0.132000; val_acc: 0.138889\n",
      "(Iteration 191 / 200) loss: 204283.730955\n",
      "(Epoch 20 / 20) train acc: 0.131000; val_acc: 0.138889\n",
      "(Iteration 1 / 200) loss: 4.957006\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 4.831794\n",
      "(Epoch 2 / 20) train acc: 0.123000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 4.790953\n",
      "(Epoch 3 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 4.908803\n",
      "(Epoch 4 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 4.596407\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.105556\n",
      "(Iteration 51 / 200) loss: 4.778050\n",
      "(Epoch 6 / 20) train acc: 0.121000; val_acc: 0.108333\n",
      "(Iteration 61 / 200) loss: 4.383373\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.102778\n",
      "(Iteration 71 / 200) loss: 4.587918\n",
      "(Epoch 8 / 20) train acc: 0.126000; val_acc: 0.102778\n",
      "(Iteration 81 / 200) loss: 4.507070\n",
      "(Epoch 9 / 20) train acc: 0.125000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 4.766851\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.102778\n",
      "(Iteration 101 / 200) loss: 4.457159\n",
      "(Epoch 11 / 20) train acc: 0.119000; val_acc: 0.102778\n",
      "(Iteration 111 / 200) loss: 4.285656\n",
      "(Epoch 12 / 20) train acc: 0.136000; val_acc: 0.111111\n",
      "(Iteration 121 / 200) loss: 4.421597\n",
      "(Epoch 13 / 20) train acc: 0.144000; val_acc: 0.113889\n",
      "(Iteration 131 / 200) loss: 4.203475\n",
      "(Epoch 14 / 20) train acc: 0.184000; val_acc: 0.119444\n",
      "(Iteration 141 / 200) loss: 4.295167\n",
      "(Epoch 15 / 20) train acc: 0.147000; val_acc: 0.125000\n",
      "(Iteration 151 / 200) loss: 4.389029\n",
      "(Epoch 16 / 20) train acc: 0.140000; val_acc: 0.138889\n",
      "(Iteration 161 / 200) loss: 4.272359\n",
      "(Epoch 17 / 20) train acc: 0.147000; val_acc: 0.141667\n",
      "(Iteration 171 / 200) loss: 4.143574\n",
      "(Epoch 18 / 20) train acc: 0.167000; val_acc: 0.147222\n",
      "(Iteration 181 / 200) loss: 4.154760\n",
      "(Epoch 19 / 20) train acc: 0.145000; val_acc: 0.158333\n",
      "(Iteration 191 / 200) loss: 4.169102\n",
      "(Epoch 20 / 20) train acc: 0.166000; val_acc: 0.175000\n",
      "(Iteration 1 / 200) loss: 2.320883\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.047000; val_acc: 0.047222\n",
      "(Iteration 11 / 200) loss: 2.320583\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.320302\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.320018\n",
      "(Epoch 4 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.319744\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.319464\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.319221\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 2.318930\n",
      "(Epoch 8 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.318680\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.318410\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.318181\n",
      "(Epoch 11 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.317910\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.317701\n",
      "(Epoch 13 / 20) train acc: 0.120000; val_acc: 0.094444\n",
      "(Iteration 131 / 200) loss: 2.317468\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.317267\n",
      "(Epoch 15 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.316978\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.316755\n",
      "(Epoch 17 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.316559\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.316312\n",
      "(Epoch 19 / 20) train acc: 0.216000; val_acc: 0.155556\n",
      "(Iteration 191 / 200) loss: 2.316077\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302766\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302739\n",
      "(Epoch 2 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302727\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302710\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Iteration 41 / 200) loss: 2.302672\n",
      "(Epoch 5 / 20) train acc: 0.093000; val_acc: 0.127778\n",
      "(Iteration 51 / 200) loss: 2.302660\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 2.302646\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 2.302614\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Iteration 81 / 200) loss: 2.302636\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.127778\n",
      "(Iteration 91 / 200) loss: 2.302615\n",
      "(Epoch 10 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302624\n",
      "(Epoch 11 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302613\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302602\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302610\n",
      "(Epoch 14 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302609\n",
      "(Epoch 15 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302573\n",
      "(Epoch 16 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302626\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302584\n",
      "(Epoch 18 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302571\n",
      "(Epoch 19 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302585\n",
      "(Epoch 20 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.086000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302570\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302578\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302558\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302565\n",
      "(Epoch 7 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302581\n",
      "(Epoch 8 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302584\n",
      "(Epoch 9 / 20) train acc: 0.082000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302575\n",
      "(Epoch 10 / 20) train acc: 0.089000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302570\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302528\n",
      "(Epoch 12 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302574\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302582\n",
      "(Epoch 14 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302575\n",
      "(Epoch 15 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302572\n",
      "(Epoch 16 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302553\n",
      "(Epoch 17 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302547\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302621\n",
      "(Epoch 19 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302585\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.125000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302580\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302589\n",
      "(Epoch 3 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302569\n",
      "(Epoch 4 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302580\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302588\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302586\n",
      "(Epoch 7 / 20) train acc: 0.132000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302575\n",
      "(Epoch 8 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302583\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302571\n",
      "(Epoch 10 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302590\n",
      "(Epoch 11 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302592\n",
      "(Epoch 12 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302611\n",
      "(Epoch 13 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302590\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302529\n",
      "(Epoch 15 / 20) train acc: 0.082000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302594\n",
      "(Epoch 16 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302556\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302601\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302575\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302519\n",
      "(Epoch 20 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 176199.895049\n",
      "(Epoch 0 / 20) train acc: 0.050000; val_acc: 0.066667\n",
      "(Epoch 1 / 20) train acc: 0.048000; val_acc: 0.066667\n",
      "(Iteration 11 / 200) loss: 183948.929207\n",
      "(Epoch 2 / 20) train acc: 0.042000; val_acc: 0.063889\n",
      "(Iteration 21 / 200) loss: 190073.164287\n",
      "(Epoch 3 / 20) train acc: 0.031000; val_acc: 0.061111\n",
      "(Iteration 31 / 200) loss: 183588.719573\n",
      "(Epoch 4 / 20) train acc: 0.045000; val_acc: 0.061111\n",
      "(Iteration 41 / 200) loss: 175015.894941\n",
      "(Epoch 5 / 20) train acc: 0.040000; val_acc: 0.061111\n",
      "(Iteration 51 / 200) loss: 190144.850312\n",
      "(Epoch 6 / 20) train acc: 0.043000; val_acc: 0.058333\n",
      "(Iteration 61 / 200) loss: 190804.105659\n",
      "(Epoch 7 / 20) train acc: 0.046000; val_acc: 0.058333\n",
      "(Iteration 71 / 200) loss: 176429.321013\n",
      "(Epoch 8 / 20) train acc: 0.044000; val_acc: 0.055556\n",
      "(Iteration 81 / 200) loss: 195911.696372\n",
      "(Epoch 9 / 20) train acc: 0.043000; val_acc: 0.055556\n",
      "(Iteration 91 / 200) loss: 173179.791724\n",
      "(Epoch 10 / 20) train acc: 0.040000; val_acc: 0.055556\n",
      "(Iteration 101 / 200) loss: 170265.147114\n",
      "(Epoch 11 / 20) train acc: 0.044000; val_acc: 0.052778\n",
      "(Iteration 111 / 200) loss: 190060.382539\n",
      "(Epoch 12 / 20) train acc: 0.037000; val_acc: 0.052778\n",
      "(Iteration 121 / 200) loss: 178492.157949\n",
      "(Epoch 13 / 20) train acc: 0.037000; val_acc: 0.052778\n",
      "(Iteration 131 / 200) loss: 189094.773408\n",
      "(Epoch 14 / 20) train acc: 0.046000; val_acc: 0.052778\n",
      "(Iteration 141 / 200) loss: 182260.368914\n",
      "(Epoch 15 / 20) train acc: 0.031000; val_acc: 0.052778\n",
      "(Iteration 151 / 200) loss: 187952.224404\n",
      "(Epoch 16 / 20) train acc: 0.038000; val_acc: 0.052778\n",
      "(Iteration 161 / 200) loss: 165336.019910\n",
      "(Epoch 17 / 20) train acc: 0.036000; val_acc: 0.050000\n",
      "(Iteration 171 / 200) loss: 178035.335410\n",
      "(Epoch 18 / 20) train acc: 0.037000; val_acc: 0.050000\n",
      "(Iteration 181 / 200) loss: 172748.230903\n",
      "(Epoch 19 / 20) train acc: 0.043000; val_acc: 0.050000\n",
      "(Iteration 191 / 200) loss: 162141.206426\n",
      "(Epoch 20 / 20) train acc: 0.034000; val_acc: 0.050000\n",
      "(Iteration 1 / 200) loss: 4.435354\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.119444\n",
      "(Epoch 1 / 20) train acc: 0.107000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 4.355550\n",
      "(Epoch 2 / 20) train acc: 0.121000; val_acc: 0.125000\n",
      "(Iteration 21 / 200) loss: 4.377292\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 4.385988\n",
      "(Epoch 4 / 20) train acc: 0.126000; val_acc: 0.133333\n",
      "(Iteration 41 / 200) loss: 4.262748\n",
      "(Epoch 5 / 20) train acc: 0.154000; val_acc: 0.133333\n",
      "(Iteration 51 / 200) loss: 3.996184\n",
      "(Epoch 6 / 20) train acc: 0.145000; val_acc: 0.130556\n",
      "(Iteration 61 / 200) loss: 4.217543\n",
      "(Epoch 7 / 20) train acc: 0.142000; val_acc: 0.141667\n",
      "(Iteration 71 / 200) loss: 4.277410\n",
      "(Epoch 8 / 20) train acc: 0.145000; val_acc: 0.141667\n",
      "(Iteration 81 / 200) loss: 4.080249\n",
      "(Epoch 9 / 20) train acc: 0.176000; val_acc: 0.144444\n",
      "(Iteration 91 / 200) loss: 4.101675\n",
      "(Epoch 10 / 20) train acc: 0.176000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 4.072084\n",
      "(Epoch 11 / 20) train acc: 0.168000; val_acc: 0.161111\n",
      "(Iteration 111 / 200) loss: 4.069293\n",
      "(Epoch 12 / 20) train acc: 0.187000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 3.996469\n",
      "(Epoch 13 / 20) train acc: 0.165000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 4.033081\n",
      "(Epoch 14 / 20) train acc: 0.187000; val_acc: 0.191667\n",
      "(Iteration 141 / 200) loss: 3.950849\n",
      "(Epoch 15 / 20) train acc: 0.203000; val_acc: 0.202778\n",
      "(Iteration 151 / 200) loss: 3.876968\n",
      "(Epoch 16 / 20) train acc: 0.225000; val_acc: 0.208333\n",
      "(Iteration 161 / 200) loss: 3.818803\n",
      "(Epoch 17 / 20) train acc: 0.240000; val_acc: 0.216667\n",
      "(Iteration 171 / 200) loss: 3.947316\n",
      "(Epoch 18 / 20) train acc: 0.240000; val_acc: 0.225000\n",
      "(Iteration 181 / 200) loss: 3.942156\n",
      "(Epoch 19 / 20) train acc: 0.240000; val_acc: 0.225000\n",
      "(Iteration 191 / 200) loss: 3.886222\n",
      "(Epoch 20 / 20) train acc: 0.250000; val_acc: 0.247222\n",
      "(Iteration 1 / 200) loss: 2.320665\n",
      "(Epoch 0 / 20) train acc: 0.136000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.105556\n",
      "(Iteration 11 / 200) loss: 2.320367\n",
      "(Epoch 2 / 20) train acc: 0.185000; val_acc: 0.188889\n",
      "(Iteration 21 / 200) loss: 2.320084\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.319799\n",
      "(Epoch 4 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.319528\n",
      "(Epoch 5 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.319279\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.319020\n",
      "(Epoch 7 / 20) train acc: 0.086000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.318731\n",
      "(Epoch 8 / 20) train acc: 0.118000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.318447\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.318209\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.317960\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.317707\n",
      "(Epoch 12 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.317479\n",
      "(Epoch 13 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.317240\n",
      "(Epoch 14 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.317003\n",
      "(Epoch 15 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.316756\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.316548\n",
      "(Epoch 17 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.316394\n",
      "(Epoch 18 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.316120\n",
      "(Epoch 19 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.315917\n",
      "(Epoch 20 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302766\n",
      "(Epoch 0 / 20) train acc: 0.114000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302731\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302712\n",
      "(Epoch 3 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302695\n",
      "(Epoch 4 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302626\n",
      "(Epoch 5 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302668\n",
      "(Epoch 6 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302653\n",
      "(Epoch 7 / 20) train acc: 0.085000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302619\n",
      "(Epoch 8 / 20) train acc: 0.085000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302661\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302641\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302621\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302597\n",
      "(Epoch 12 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302590\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302638\n",
      "(Epoch 14 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302596\n",
      "(Epoch 15 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302578\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302589\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302537\n",
      "(Epoch 18 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302541\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302560\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302588\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302570\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302580\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302577\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302571\n",
      "(Epoch 6 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302588\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302582\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302581\n",
      "(Epoch 9 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302614\n",
      "(Epoch 10 / 20) train acc: 0.092000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302592\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302605\n",
      "(Epoch 12 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302572\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302591\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302534\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302589\n",
      "(Epoch 16 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302634\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302565\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302559\n",
      "(Epoch 19 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302614\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302577\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302574\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302578\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302580\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302572\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302537\n",
      "(Epoch 8 / 20) train acc: 0.090000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302579\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302597\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302570\n",
      "(Epoch 11 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302547\n",
      "(Epoch 12 / 20) train acc: 0.131000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302547\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302568\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302589\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302565\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302616\n",
      "(Epoch 17 / 20) train acc: 0.085000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302582\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302541\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302534\n",
      "(Epoch 20 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 239701.730400\n",
      "(Epoch 0 / 20) train acc: 0.109000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.113889\n",
      "(Iteration 11 / 200) loss: 243056.609848\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.113889\n",
      "(Iteration 21 / 200) loss: 201941.749362\n",
      "(Epoch 3 / 20) train acc: 0.107000; val_acc: 0.113889\n",
      "(Iteration 31 / 200) loss: 235123.828896\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 208251.808432\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.113889\n",
      "(Iteration 51 / 200) loss: 213097.027971\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.113889\n",
      "(Iteration 61 / 200) loss: 190121.007510\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.113889\n",
      "(Iteration 71 / 200) loss: 211893.367056\n",
      "(Epoch 8 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 205445.346600\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 194630.766146\n",
      "(Epoch 10 / 20) train acc: 0.083000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 203210.325693\n",
      "(Epoch 11 / 20) train acc: 0.096000; val_acc: 0.111111\n",
      "(Iteration 111 / 200) loss: 193466.165243\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.111111\n",
      "(Iteration 121 / 200) loss: 231215.904792\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.105556\n",
      "(Iteration 131 / 200) loss: 183260.184339\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.105556\n",
      "(Iteration 141 / 200) loss: 213099.943889\n",
      "(Epoch 15 / 20) train acc: 0.114000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 210897.023437\n",
      "(Epoch 16 / 20) train acc: 0.116000; val_acc: 0.111111\n",
      "(Iteration 161 / 200) loss: 199646.402981\n",
      "(Epoch 17 / 20) train acc: 0.093000; val_acc: 0.111111\n",
      "(Iteration 171 / 200) loss: 202147.742525\n",
      "(Epoch 18 / 20) train acc: 0.116000; val_acc: 0.111111\n",
      "(Iteration 181 / 200) loss: 190052.702073\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.111111\n",
      "(Iteration 191 / 200) loss: 212116.041625\n",
      "(Epoch 20 / 20) train acc: 0.085000; val_acc: 0.113889\n",
      "(Iteration 1 / 200) loss: 2.747594\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 2.657764\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.094444\n",
      "(Iteration 21 / 200) loss: 2.741169\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.111111\n",
      "(Iteration 31 / 200) loss: 2.678200\n",
      "(Epoch 4 / 20) train acc: 0.149000; val_acc: 0.127778\n",
      "(Iteration 41 / 200) loss: 2.815492\n",
      "(Epoch 5 / 20) train acc: 0.148000; val_acc: 0.138889\n",
      "(Iteration 51 / 200) loss: 2.595555\n",
      "(Epoch 6 / 20) train acc: 0.146000; val_acc: 0.150000\n",
      "(Iteration 61 / 200) loss: 2.684402\n",
      "(Epoch 7 / 20) train acc: 0.164000; val_acc: 0.155556\n",
      "(Iteration 71 / 200) loss: 2.471962\n",
      "(Epoch 8 / 20) train acc: 0.156000; val_acc: 0.163889\n",
      "(Iteration 81 / 200) loss: 2.580458\n",
      "(Epoch 9 / 20) train acc: 0.185000; val_acc: 0.169444\n",
      "(Iteration 91 / 200) loss: 2.548166\n",
      "(Epoch 10 / 20) train acc: 0.184000; val_acc: 0.186111\n",
      "(Iteration 101 / 200) loss: 2.474046\n",
      "(Epoch 11 / 20) train acc: 0.189000; val_acc: 0.194444\n",
      "(Iteration 111 / 200) loss: 2.320369\n",
      "(Epoch 12 / 20) train acc: 0.226000; val_acc: 0.205556\n",
      "(Iteration 121 / 200) loss: 2.549662\n",
      "(Epoch 13 / 20) train acc: 0.205000; val_acc: 0.213889\n",
      "(Iteration 131 / 200) loss: 2.447268\n",
      "(Epoch 14 / 20) train acc: 0.211000; val_acc: 0.216667\n",
      "(Iteration 141 / 200) loss: 2.347714\n",
      "(Epoch 15 / 20) train acc: 0.235000; val_acc: 0.225000\n",
      "(Iteration 151 / 200) loss: 2.453664\n",
      "(Epoch 16 / 20) train acc: 0.257000; val_acc: 0.233333\n",
      "(Iteration 161 / 200) loss: 2.260219\n",
      "(Epoch 17 / 20) train acc: 0.250000; val_acc: 0.233333\n",
      "(Iteration 171 / 200) loss: 2.317552\n",
      "(Epoch 18 / 20) train acc: 0.282000; val_acc: 0.238889\n",
      "(Iteration 181 / 200) loss: 2.279815\n",
      "(Epoch 19 / 20) train acc: 0.284000; val_acc: 0.261111\n",
      "(Iteration 191 / 200) loss: 2.194189\n",
      "(Epoch 20 / 20) train acc: 0.295000; val_acc: 0.277778\n",
      "(Iteration 1 / 200) loss: 2.304407\n",
      "(Epoch 0 / 20) train acc: 0.056000; val_acc: 0.052778\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.075000\n",
      "(Iteration 11 / 200) loss: 2.304376\n",
      "(Epoch 2 / 20) train acc: 0.108000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.304342\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.304317\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.304278\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.304254\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.304224\n",
      "(Epoch 7 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.304204\n",
      "(Epoch 8 / 20) train acc: 0.130000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.304179\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.086111\n",
      "(Iteration 91 / 200) loss: 2.304127\n",
      "(Epoch 10 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.304111\n",
      "(Epoch 11 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.304083\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304071\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.304015\n",
      "(Epoch 14 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.304059\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.303982\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.303976\n",
      "(Epoch 17 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.303866\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.303926\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.105556\n",
      "(Iteration 191 / 200) loss: 2.303948\n",
      "(Epoch 20 / 20) train acc: 0.150000; val_acc: 0.105556\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302601\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302594\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302604\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302604\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302579\n",
      "(Epoch 6 / 20) train acc: 0.109000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302581\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302582\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302582\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302592\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302580\n",
      "(Epoch 11 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302614\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302544\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302587\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302556\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302550\n",
      "(Epoch 16 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302580\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302539\n",
      "(Epoch 18 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302581\n",
      "(Epoch 19 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302557\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302580\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302582\n",
      "(Epoch 3 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302578\n",
      "(Epoch 5 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302592\n",
      "(Epoch 6 / 20) train acc: 0.120000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302594\n",
      "(Epoch 7 / 20) train acc: 0.087000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302576\n",
      "(Epoch 8 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302568\n",
      "(Epoch 9 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302533\n",
      "(Epoch 10 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302615\n",
      "(Epoch 11 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302561\n",
      "(Epoch 12 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302583\n",
      "(Epoch 13 / 20) train acc: 0.134000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302590\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302524\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.302571\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.302615\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.302562\n",
      "(Epoch 18 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.302552\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 191 / 200) loss: 2.302527\n",
      "(Epoch 20 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.126000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302577\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302590\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302567\n",
      "(Epoch 6 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302573\n",
      "(Epoch 7 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302618\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302571\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302588\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302593\n",
      "(Epoch 11 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302572\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302554\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302544\n",
      "(Epoch 14 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302582\n",
      "(Epoch 15 / 20) train acc: 0.126000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302562\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302535\n",
      "(Epoch 17 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302569\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302555\n",
      "(Epoch 19 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302516\n",
      "(Epoch 20 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 258062.202710\n",
      "(Epoch 0 / 20) train acc: 0.057000; val_acc: 0.052778\n",
      "(Epoch 1 / 20) train acc: 0.057000; val_acc: 0.052778\n",
      "(Iteration 11 / 200) loss: 263821.002126\n",
      "(Epoch 2 / 20) train acc: 0.048000; val_acc: 0.052778\n",
      "(Iteration 21 / 200) loss: 262296.701641\n",
      "(Epoch 3 / 20) train acc: 0.053000; val_acc: 0.052778\n",
      "(Iteration 31 / 200) loss: 251394.801173\n",
      "(Epoch 4 / 20) train acc: 0.052000; val_acc: 0.052778\n",
      "(Iteration 41 / 200) loss: 281717.840711\n",
      "(Epoch 5 / 20) train acc: 0.055000; val_acc: 0.052778\n",
      "(Iteration 51 / 200) loss: 266953.680249\n",
      "(Epoch 6 / 20) train acc: 0.072000; val_acc: 0.050000\n",
      "(Iteration 61 / 200) loss: 231159.659783\n",
      "(Epoch 7 / 20) train acc: 0.057000; val_acc: 0.050000\n",
      "(Iteration 71 / 200) loss: 224821.459323\n",
      "(Epoch 8 / 20) train acc: 0.055000; val_acc: 0.050000\n",
      "(Iteration 81 / 200) loss: 241828.838866\n",
      "(Epoch 9 / 20) train acc: 0.050000; val_acc: 0.052778\n",
      "(Iteration 91 / 200) loss: 254867.558403\n",
      "(Epoch 10 / 20) train acc: 0.050000; val_acc: 0.052778\n",
      "(Iteration 101 / 200) loss: 247030.557942\n",
      "(Epoch 11 / 20) train acc: 0.058000; val_acc: 0.047222\n",
      "(Iteration 111 / 200) loss: 240575.497481\n",
      "(Epoch 12 / 20) train acc: 0.050000; val_acc: 0.050000\n",
      "(Iteration 121 / 200) loss: 264378.417022\n",
      "(Epoch 13 / 20) train acc: 0.048000; val_acc: 0.047222\n",
      "(Iteration 131 / 200) loss: 242554.096561\n",
      "(Epoch 14 / 20) train acc: 0.055000; val_acc: 0.047222\n",
      "(Iteration 141 / 200) loss: 242789.356101\n",
      "(Epoch 15 / 20) train acc: 0.056000; val_acc: 0.047222\n",
      "(Iteration 151 / 200) loss: 234462.095640\n",
      "(Epoch 16 / 20) train acc: 0.047000; val_acc: 0.047222\n",
      "(Iteration 161 / 200) loss: 235372.015180\n",
      "(Epoch 17 / 20) train acc: 0.045000; val_acc: 0.044444\n",
      "(Iteration 171 / 200) loss: 235678.554720\n",
      "(Epoch 18 / 20) train acc: 0.048000; val_acc: 0.044444\n",
      "(Iteration 181 / 200) loss: 209320.974263\n",
      "(Epoch 19 / 20) train acc: 0.040000; val_acc: 0.044444\n",
      "(Iteration 191 / 200) loss: 227788.033808\n",
      "(Epoch 20 / 20) train acc: 0.048000; val_acc: 0.044444\n",
      "(Iteration 1 / 200) loss: 3.042805\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.069444\n",
      "(Epoch 1 / 20) train acc: 0.084000; val_acc: 0.069444\n",
      "(Iteration 11 / 200) loss: 3.180731\n",
      "(Epoch 2 / 20) train acc: 0.094000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 2.969933\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.077778\n",
      "(Iteration 31 / 200) loss: 2.956135\n",
      "(Epoch 4 / 20) train acc: 0.078000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.794714\n",
      "(Epoch 5 / 20) train acc: 0.089000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.714854\n",
      "(Epoch 6 / 20) train acc: 0.094000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 2.682212\n",
      "(Epoch 7 / 20) train acc: 0.103000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 2.776673\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.094444\n",
      "(Iteration 81 / 200) loss: 2.846557\n",
      "(Epoch 9 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 91 / 200) loss: 2.712302\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.486192\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.102778\n",
      "(Iteration 111 / 200) loss: 2.589480\n",
      "(Epoch 12 / 20) train acc: 0.123000; val_acc: 0.105556\n",
      "(Iteration 121 / 200) loss: 2.481442\n",
      "(Epoch 13 / 20) train acc: 0.139000; val_acc: 0.108333\n",
      "(Iteration 131 / 200) loss: 2.641039\n",
      "(Epoch 14 / 20) train acc: 0.133000; val_acc: 0.130556\n",
      "(Iteration 141 / 200) loss: 2.499025\n",
      "(Epoch 15 / 20) train acc: 0.138000; val_acc: 0.127778\n",
      "(Iteration 151 / 200) loss: 2.658191\n",
      "(Epoch 16 / 20) train acc: 0.166000; val_acc: 0.130556\n",
      "(Iteration 161 / 200) loss: 2.416652\n",
      "(Epoch 17 / 20) train acc: 0.149000; val_acc: 0.136111\n",
      "(Iteration 171 / 200) loss: 2.407631\n",
      "(Epoch 18 / 20) train acc: 0.184000; val_acc: 0.147222\n",
      "(Iteration 181 / 200) loss: 2.369765\n",
      "(Epoch 19 / 20) train acc: 0.177000; val_acc: 0.158333\n",
      "(Iteration 191 / 200) loss: 2.304737\n",
      "(Epoch 20 / 20) train acc: 0.189000; val_acc: 0.172222\n",
      "(Iteration 1 / 200) loss: 2.304406\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.138889\n",
      "(Epoch 1 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.304373\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.304330\n",
      "(Epoch 3 / 20) train acc: 0.147000; val_acc: 0.166667\n",
      "(Iteration 31 / 200) loss: 2.304308\n",
      "(Epoch 4 / 20) train acc: 0.179000; val_acc: 0.158333\n",
      "(Iteration 41 / 200) loss: 2.304279\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.304259\n",
      "(Epoch 6 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.304235\n",
      "(Epoch 7 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.304217\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.304177\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.304117\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.122222\n",
      "(Iteration 101 / 200) loss: 2.304164\n",
      "(Epoch 11 / 20) train acc: 0.203000; val_acc: 0.158333\n",
      "(Iteration 111 / 200) loss: 2.304074\n",
      "(Epoch 12 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.304113\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.304104\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.304038\n",
      "(Epoch 15 / 20) train acc: 0.205000; val_acc: 0.155556\n",
      "(Iteration 151 / 200) loss: 2.304013\n",
      "(Epoch 16 / 20) train acc: 0.214000; val_acc: 0.158333\n",
      "(Iteration 161 / 200) loss: 2.304016\n",
      "(Epoch 17 / 20) train acc: 0.208000; val_acc: 0.166667\n",
      "(Iteration 171 / 200) loss: 2.304007\n",
      "(Epoch 18 / 20) train acc: 0.240000; val_acc: 0.225000\n",
      "(Iteration 181 / 200) loss: 2.303927\n",
      "(Epoch 19 / 20) train acc: 0.278000; val_acc: 0.250000\n",
      "(Iteration 191 / 200) loss: 2.303870\n",
      "(Epoch 20 / 20) train acc: 0.244000; val_acc: 0.233333\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302590\n",
      "(Epoch 2 / 20) train acc: 0.084000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302601\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302603\n",
      "(Epoch 4 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302604\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302587\n",
      "(Epoch 6 / 20) train acc: 0.131000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302601\n",
      "(Epoch 7 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302607\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302583\n",
      "(Epoch 9 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302598\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302560\n",
      "(Epoch 11 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302592\n",
      "(Epoch 12 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302619\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302583\n",
      "(Epoch 14 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302579\n",
      "(Epoch 15 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302581\n",
      "(Epoch 16 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302575\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302596\n",
      "(Epoch 18 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302632\n",
      "(Epoch 19 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302561\n",
      "(Epoch 20 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302589\n",
      "(Epoch 2 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302584\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302608\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302576\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302578\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302562\n",
      "(Epoch 8 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302575\n",
      "(Epoch 9 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302570\n",
      "(Epoch 10 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302549\n",
      "(Epoch 11 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302602\n",
      "(Epoch 12 / 20) train acc: 0.091000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302577\n",
      "(Epoch 13 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302575\n",
      "(Epoch 14 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302583\n",
      "(Epoch 15 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302576\n",
      "(Epoch 16 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302596\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302574\n",
      "(Epoch 18 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302584\n",
      "(Epoch 19 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302579\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.086000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302578\n",
      "(Epoch 2 / 20) train acc: 0.084000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302571\n",
      "(Epoch 3 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.127000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302589\n",
      "(Epoch 5 / 20) train acc: 0.089000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302593\n",
      "(Epoch 6 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302570\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302575\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302567\n",
      "(Epoch 9 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302548\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302615\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302581\n",
      "(Epoch 12 / 20) train acc: 0.087000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302588\n",
      "(Epoch 13 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302568\n",
      "(Epoch 14 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302602\n",
      "(Epoch 15 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302540\n",
      "(Epoch 16 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302566\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302597\n",
      "(Epoch 18 / 20) train acc: 0.088000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302573\n",
      "(Epoch 19 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302584\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 161599.447718\n",
      "(Epoch 0 / 20) train acc: 0.128000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 175491.167096\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 156047.496572\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 161235.946070\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 141739.505569\n",
      "(Epoch 5 / 20) train acc: 0.120000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 149518.815069\n",
      "(Epoch 6 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 145383.084568\n",
      "(Epoch 7 / 20) train acc: 0.129000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 143848.474068\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.102778\n",
      "(Iteration 81 / 200) loss: 146645.483571\n",
      "(Epoch 9 / 20) train acc: 0.125000; val_acc: 0.102778\n",
      "(Iteration 91 / 200) loss: 142924.083072\n",
      "(Epoch 10 / 20) train acc: 0.143000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 149774.262574\n",
      "(Epoch 11 / 20) train acc: 0.130000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 139073.412079\n",
      "(Epoch 12 / 20) train acc: 0.121000; val_acc: 0.102778\n",
      "(Iteration 121 / 200) loss: 132437.641584\n",
      "(Epoch 13 / 20) train acc: 0.134000; val_acc: 0.105556\n",
      "(Iteration 131 / 200) loss: 140595.081087\n",
      "(Epoch 14 / 20) train acc: 0.121000; val_acc: 0.105556\n",
      "(Iteration 141 / 200) loss: 136381.930593\n",
      "(Epoch 15 / 20) train acc: 0.119000; val_acc: 0.105556\n",
      "(Iteration 151 / 200) loss: 140960.020098\n",
      "(Epoch 16 / 20) train acc: 0.124000; val_acc: 0.102778\n",
      "(Iteration 161 / 200) loss: 132931.679605\n",
      "(Epoch 17 / 20) train acc: 0.128000; val_acc: 0.102778\n",
      "(Iteration 171 / 200) loss: 134846.489112\n",
      "(Epoch 18 / 20) train acc: 0.137000; val_acc: 0.102778\n",
      "(Iteration 181 / 200) loss: 154421.518619\n",
      "(Epoch 19 / 20) train acc: 0.124000; val_acc: 0.102778\n",
      "(Iteration 191 / 200) loss: 139078.978126\n",
      "(Epoch 20 / 20) train acc: 0.129000; val_acc: 0.105556\n",
      "(Iteration 1 / 200) loss: 2.840520\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.126000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.708572\n",
      "(Epoch 2 / 20) train acc: 0.127000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.673264\n",
      "(Epoch 3 / 20) train acc: 0.144000; val_acc: 0.102778\n",
      "(Iteration 31 / 200) loss: 2.367521\n",
      "(Epoch 4 / 20) train acc: 0.146000; val_acc: 0.113889\n",
      "(Iteration 41 / 200) loss: 2.486952\n",
      "(Epoch 5 / 20) train acc: 0.155000; val_acc: 0.119444\n",
      "(Iteration 51 / 200) loss: 2.494687\n",
      "(Epoch 6 / 20) train acc: 0.170000; val_acc: 0.133333\n",
      "(Iteration 61 / 200) loss: 2.429270\n",
      "(Epoch 7 / 20) train acc: 0.178000; val_acc: 0.133333\n",
      "(Iteration 71 / 200) loss: 2.488539\n",
      "(Epoch 8 / 20) train acc: 0.182000; val_acc: 0.141667\n",
      "(Iteration 81 / 200) loss: 2.467054\n",
      "(Epoch 9 / 20) train acc: 0.174000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 2.397989\n",
      "(Epoch 10 / 20) train acc: 0.204000; val_acc: 0.155556\n",
      "(Iteration 101 / 200) loss: 2.316505\n",
      "(Epoch 11 / 20) train acc: 0.181000; val_acc: 0.163889\n",
      "(Iteration 111 / 200) loss: 2.325286\n",
      "(Epoch 12 / 20) train acc: 0.254000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 2.391827\n",
      "(Epoch 13 / 20) train acc: 0.240000; val_acc: 0.197222\n",
      "(Iteration 131 / 200) loss: 2.342183\n",
      "(Epoch 14 / 20) train acc: 0.234000; val_acc: 0.216667\n",
      "(Iteration 141 / 200) loss: 2.317056\n",
      "(Epoch 15 / 20) train acc: 0.250000; val_acc: 0.233333\n",
      "(Iteration 151 / 200) loss: 2.351256\n",
      "(Epoch 16 / 20) train acc: 0.265000; val_acc: 0.238889\n",
      "(Iteration 161 / 200) loss: 2.195763\n",
      "(Epoch 17 / 20) train acc: 0.279000; val_acc: 0.247222\n",
      "(Iteration 171 / 200) loss: 2.270927\n",
      "(Epoch 18 / 20) train acc: 0.318000; val_acc: 0.252778\n",
      "(Iteration 181 / 200) loss: 2.195133\n",
      "(Epoch 19 / 20) train acc: 0.327000; val_acc: 0.275000\n",
      "(Iteration 191 / 200) loss: 2.192073\n",
      "(Epoch 20 / 20) train acc: 0.337000; val_acc: 0.277778\n",
      "(Iteration 1 / 200) loss: 2.304420\n",
      "(Epoch 0 / 20) train acc: 0.083000; val_acc: 0.094444\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 2.304394\n",
      "(Epoch 2 / 20) train acc: 0.144000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.304363\n",
      "(Epoch 3 / 20) train acc: 0.114000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.304339\n",
      "(Epoch 4 / 20) train acc: 0.100000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.304299\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.304267\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.304245\n",
      "(Epoch 7 / 20) train acc: 0.208000; val_acc: 0.155556\n",
      "(Iteration 71 / 200) loss: 2.304200\n",
      "(Epoch 8 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.304169\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.304191\n",
      "(Epoch 10 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.304150\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.304125\n",
      "(Epoch 12 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.304089\n",
      "(Epoch 13 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.304152\n",
      "(Epoch 14 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.304073\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.304051\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.304019\n",
      "(Epoch 17 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.303929\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.303950\n",
      "(Epoch 19 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.303840\n",
      "(Epoch 20 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302604\n",
      "(Epoch 0 / 20) train acc: 0.115000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302605\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302594\n",
      "(Epoch 3 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302584\n",
      "(Epoch 4 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302589\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302589\n",
      "(Epoch 6 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302591\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302562\n",
      "(Epoch 8 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302555\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302579\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302599\n",
      "(Epoch 11 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302605\n",
      "(Epoch 12 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302595\n",
      "(Epoch 13 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302573\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302546\n",
      "(Epoch 15 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302548\n",
      "(Epoch 16 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302621\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302614\n",
      "(Epoch 18 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302575\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302590\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.118000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302586\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302582\n",
      "(Epoch 5 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302569\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302585\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302590\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302577\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302553\n",
      "(Epoch 10 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302574\n",
      "(Epoch 11 / 20) train acc: 0.094000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302588\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302566\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302582\n",
      "(Epoch 14 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302575\n",
      "(Epoch 15 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302616\n",
      "(Epoch 16 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302581\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302565\n",
      "(Epoch 18 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302606\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302600\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.118000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302582\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302586\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302584\n",
      "(Epoch 5 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302586\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302598\n",
      "(Epoch 7 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302577\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302566\n",
      "(Epoch 9 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302613\n",
      "(Epoch 10 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302555\n",
      "(Epoch 11 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302585\n",
      "(Epoch 12 / 20) train acc: 0.129000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302569\n",
      "(Epoch 13 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302618\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302560\n",
      "(Epoch 15 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302586\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302606\n",
      "(Epoch 17 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302568\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302574\n",
      "(Epoch 19 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302565\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 240227.953704\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 232964.973655\n",
      "(Epoch 2 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 211417.453614\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 204359.873573\n",
      "(Epoch 4 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 226486.813534\n",
      "(Epoch 5 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 219857.893495\n",
      "(Epoch 6 / 20) train acc: 0.113000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 197948.453456\n",
      "(Epoch 7 / 20) train acc: 0.104000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 202884.453417\n",
      "(Epoch 8 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 216121.033378\n",
      "(Epoch 9 / 20) train acc: 0.090000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 210262.693339\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 222318.473300\n",
      "(Epoch 11 / 20) train acc: 0.110000; val_acc: 0.077778\n",
      "(Iteration 111 / 200) loss: 207151.453262\n",
      "(Epoch 12 / 20) train acc: 0.110000; val_acc: 0.077778\n",
      "(Iteration 121 / 200) loss: 233801.353223\n",
      "(Epoch 13 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 214382.333184\n",
      "(Epoch 14 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 211951.393146\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 236141.633108\n",
      "(Epoch 16 / 20) train acc: 0.121000; val_acc: 0.086111\n",
      "(Iteration 161 / 200) loss: 232586.313069\n",
      "(Epoch 17 / 20) train acc: 0.101000; val_acc: 0.088889\n",
      "(Iteration 171 / 200) loss: 205888.813031\n",
      "(Epoch 18 / 20) train acc: 0.113000; val_acc: 0.088889\n",
      "(Iteration 181 / 200) loss: 205990.992993\n",
      "(Epoch 19 / 20) train acc: 0.118000; val_acc: 0.088889\n",
      "(Iteration 191 / 200) loss: 186870.172955\n",
      "(Epoch 20 / 20) train acc: 0.106000; val_acc: 0.088889\n",
      "(Iteration 1 / 200) loss: 2.787271\n",
      "(Epoch 0 / 20) train acc: 0.111000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 3.070903\n",
      "(Epoch 2 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.882737\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 2.714511\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.777802\n",
      "(Epoch 5 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.565327\n",
      "(Epoch 6 / 20) train acc: 0.128000; val_acc: 0.077778\n",
      "(Iteration 61 / 200) loss: 2.575007\n",
      "(Epoch 7 / 20) train acc: 0.131000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.565677\n",
      "(Epoch 8 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 81 / 200) loss: 2.552243\n",
      "(Epoch 9 / 20) train acc: 0.117000; val_acc: 0.094444\n",
      "(Iteration 91 / 200) loss: 2.443148\n",
      "(Epoch 10 / 20) train acc: 0.141000; val_acc: 0.108333\n",
      "(Iteration 101 / 200) loss: 2.371754\n",
      "(Epoch 11 / 20) train acc: 0.148000; val_acc: 0.119444\n",
      "(Iteration 111 / 200) loss: 2.453822\n",
      "(Epoch 12 / 20) train acc: 0.168000; val_acc: 0.125000\n",
      "(Iteration 121 / 200) loss: 2.374806\n",
      "(Epoch 13 / 20) train acc: 0.153000; val_acc: 0.136111\n",
      "(Iteration 131 / 200) loss: 2.420901\n",
      "(Epoch 14 / 20) train acc: 0.166000; val_acc: 0.155556\n",
      "(Iteration 141 / 200) loss: 2.338387\n",
      "(Epoch 15 / 20) train acc: 0.190000; val_acc: 0.163889\n",
      "(Iteration 151 / 200) loss: 2.237255\n",
      "(Epoch 16 / 20) train acc: 0.195000; val_acc: 0.180556\n",
      "(Iteration 161 / 200) loss: 2.227244\n",
      "(Epoch 17 / 20) train acc: 0.230000; val_acc: 0.197222\n",
      "(Iteration 171 / 200) loss: 2.216401\n",
      "(Epoch 18 / 20) train acc: 0.234000; val_acc: 0.205556\n",
      "(Iteration 181 / 200) loss: 2.266909\n",
      "(Epoch 19 / 20) train acc: 0.272000; val_acc: 0.205556\n",
      "(Iteration 191 / 200) loss: 2.070808\n",
      "(Epoch 20 / 20) train acc: 0.252000; val_acc: 0.208333\n",
      "(Iteration 1 / 200) loss: 2.302766\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.094000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302758\n",
      "(Epoch 2 / 20) train acc: 0.154000; val_acc: 0.130556\n",
      "(Iteration 21 / 200) loss: 2.302755\n",
      "(Epoch 3 / 20) train acc: 0.187000; val_acc: 0.180556\n",
      "(Iteration 31 / 200) loss: 2.302726\n",
      "(Epoch 4 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302762\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302745\n",
      "(Epoch 6 / 20) train acc: 0.140000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302735\n",
      "(Epoch 7 / 20) train acc: 0.199000; val_acc: 0.200000\n",
      "(Iteration 71 / 200) loss: 2.302710\n",
      "(Epoch 8 / 20) train acc: 0.202000; val_acc: 0.200000\n",
      "(Iteration 81 / 200) loss: 2.302686\n",
      "(Epoch 9 / 20) train acc: 0.205000; val_acc: 0.188889\n",
      "(Iteration 91 / 200) loss: 2.302679\n",
      "(Epoch 10 / 20) train acc: 0.138000; val_acc: 0.138889\n",
      "(Iteration 101 / 200) loss: 2.302651\n",
      "(Epoch 11 / 20) train acc: 0.273000; val_acc: 0.250000\n",
      "(Iteration 111 / 200) loss: 2.302643\n",
      "(Epoch 12 / 20) train acc: 0.304000; val_acc: 0.288889\n",
      "(Iteration 121 / 200) loss: 2.302673\n",
      "(Epoch 13 / 20) train acc: 0.303000; val_acc: 0.286111\n",
      "(Iteration 131 / 200) loss: 2.302658\n",
      "(Epoch 14 / 20) train acc: 0.235000; val_acc: 0.205556\n",
      "(Iteration 141 / 200) loss: 2.302584\n",
      "(Epoch 15 / 20) train acc: 0.201000; val_acc: 0.186111\n",
      "(Iteration 151 / 200) loss: 2.302588\n",
      "(Epoch 16 / 20) train acc: 0.173000; val_acc: 0.175000\n",
      "(Iteration 161 / 200) loss: 2.302582\n",
      "(Epoch 17 / 20) train acc: 0.178000; val_acc: 0.186111\n",
      "(Iteration 171 / 200) loss: 2.302528\n",
      "(Epoch 18 / 20) train acc: 0.245000; val_acc: 0.227778\n",
      "(Iteration 181 / 200) loss: 2.302563\n",
      "(Epoch 19 / 20) train acc: 0.290000; val_acc: 0.263889\n",
      "(Iteration 191 / 200) loss: 2.302472\n",
      "(Epoch 20 / 20) train acc: 0.299000; val_acc: 0.291667\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.100000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302581\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302585\n",
      "(Epoch 3 / 20) train acc: 0.126000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302580\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302597\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302597\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302576\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302586\n",
      "(Epoch 9 / 20) train acc: 0.111000; val_acc: 0.097222\n",
      "(Iteration 91 / 200) loss: 2.302585\n",
      "(Epoch 10 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 101 / 200) loss: 2.302562\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302612\n",
      "(Epoch 12 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302585\n",
      "(Epoch 13 / 20) train acc: 0.085000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302565\n",
      "(Epoch 14 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302589\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302581\n",
      "(Epoch 16 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302591\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302534\n",
      "(Epoch 18 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302554\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302581\n",
      "(Epoch 20 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302586\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 2.302585\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302585\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302594\n",
      "(Epoch 9 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302557\n",
      "(Epoch 10 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302560\n",
      "(Epoch 11 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302569\n",
      "(Epoch 12 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302601\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302551\n",
      "(Epoch 14 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302564\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302608\n",
      "(Epoch 16 / 20) train acc: 0.135000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302569\n",
      "(Epoch 17 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302539\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302601\n",
      "(Epoch 19 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302521\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.096000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 2.302589\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 2.302586\n",
      "(Epoch 3 / 20) train acc: 0.091000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 2.302578\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302591\n",
      "(Epoch 5 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302576\n",
      "(Epoch 6 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302575\n",
      "(Epoch 7 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302577\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302607\n",
      "(Epoch 9 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302575\n",
      "(Epoch 10 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302570\n",
      "(Epoch 11 / 20) train acc: 0.095000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302574\n",
      "(Epoch 12 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302587\n",
      "(Epoch 13 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302613\n",
      "(Epoch 14 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302546\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302622\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302591\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302568\n",
      "(Epoch 18 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302573\n",
      "(Epoch 19 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302591\n",
      "(Epoch 20 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 226883.631014\n",
      "(Epoch 0 / 20) train acc: 0.027000; val_acc: 0.030556\n",
      "(Epoch 1 / 20) train acc: 0.028000; val_acc: 0.030556\n",
      "(Iteration 11 / 200) loss: 219844.190947\n",
      "(Epoch 2 / 20) train acc: 0.031000; val_acc: 0.030556\n",
      "(Iteration 21 / 200) loss: 210711.450888\n",
      "(Epoch 3 / 20) train acc: 0.025000; val_acc: 0.030556\n",
      "(Iteration 31 / 200) loss: 213856.470832\n",
      "(Epoch 4 / 20) train acc: 0.028000; val_acc: 0.030556\n",
      "(Iteration 41 / 200) loss: 203735.050776\n",
      "(Epoch 5 / 20) train acc: 0.027000; val_acc: 0.030556\n",
      "(Iteration 51 / 200) loss: 192748.130720\n",
      "(Epoch 6 / 20) train acc: 0.023000; val_acc: 0.027778\n",
      "(Iteration 61 / 200) loss: 212416.010665\n",
      "(Epoch 7 / 20) train acc: 0.025000; val_acc: 0.027778\n",
      "(Iteration 71 / 200) loss: 196864.570609\n",
      "(Epoch 8 / 20) train acc: 0.025000; val_acc: 0.022222\n",
      "(Iteration 81 / 200) loss: 199292.650554\n",
      "(Epoch 9 / 20) train acc: 0.025000; val_acc: 0.022222\n",
      "(Iteration 91 / 200) loss: 224612.410499\n",
      "(Epoch 10 / 20) train acc: 0.031000; val_acc: 0.022222\n",
      "(Iteration 101 / 200) loss: 240252.110444\n",
      "(Epoch 11 / 20) train acc: 0.029000; val_acc: 0.025000\n",
      "(Iteration 111 / 200) loss: 201030.450389\n",
      "(Epoch 12 / 20) train acc: 0.025000; val_acc: 0.025000\n",
      "(Iteration 121 / 200) loss: 225035.170333\n",
      "(Epoch 13 / 20) train acc: 0.029000; val_acc: 0.025000\n",
      "(Iteration 131 / 200) loss: 215894.930278\n",
      "(Epoch 14 / 20) train acc: 0.026000; val_acc: 0.025000\n",
      "(Iteration 141 / 200) loss: 238816.650223\n",
      "(Epoch 15 / 20) train acc: 0.035000; val_acc: 0.022222\n",
      "(Iteration 151 / 200) loss: 219573.070168\n",
      "(Epoch 16 / 20) train acc: 0.029000; val_acc: 0.022222\n",
      "(Iteration 161 / 200) loss: 212864.570113\n",
      "(Epoch 17 / 20) train acc: 0.034000; val_acc: 0.022222\n",
      "(Iteration 171 / 200) loss: 207176.550058\n",
      "(Epoch 18 / 20) train acc: 0.023000; val_acc: 0.022222\n",
      "(Iteration 181 / 200) loss: 211326.770003\n",
      "(Epoch 19 / 20) train acc: 0.033000; val_acc: 0.025000\n",
      "(Iteration 191 / 200) loss: 213298.509948\n",
      "(Epoch 20 / 20) train acc: 0.028000; val_acc: 0.025000\n",
      "(Iteration 1 / 200) loss: 3.505662\n",
      "(Epoch 0 / 20) train acc: 0.093000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 3.309691\n",
      "(Epoch 2 / 20) train acc: 0.109000; val_acc: 0.122222\n",
      "(Iteration 21 / 200) loss: 3.128593\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.127778\n",
      "(Iteration 31 / 200) loss: 3.020256\n",
      "(Epoch 4 / 20) train acc: 0.124000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 3.010350\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 2.968789\n",
      "(Epoch 6 / 20) train acc: 0.121000; val_acc: 0.133333\n",
      "(Iteration 61 / 200) loss: 2.928106\n",
      "(Epoch 7 / 20) train acc: 0.116000; val_acc: 0.136111\n",
      "(Iteration 71 / 200) loss: 2.707828\n",
      "(Epoch 8 / 20) train acc: 0.137000; val_acc: 0.138889\n",
      "(Iteration 81 / 200) loss: 2.767803\n",
      "(Epoch 9 / 20) train acc: 0.145000; val_acc: 0.141667\n",
      "(Iteration 91 / 200) loss: 2.619271\n",
      "(Epoch 10 / 20) train acc: 0.128000; val_acc: 0.141667\n",
      "(Iteration 101 / 200) loss: 2.671532\n",
      "(Epoch 11 / 20) train acc: 0.126000; val_acc: 0.144444\n",
      "(Iteration 111 / 200) loss: 2.389760\n",
      "(Epoch 12 / 20) train acc: 0.146000; val_acc: 0.147222\n",
      "(Iteration 121 / 200) loss: 2.444086\n",
      "(Epoch 13 / 20) train acc: 0.145000; val_acc: 0.150000\n",
      "(Iteration 131 / 200) loss: 2.636779\n",
      "(Epoch 14 / 20) train acc: 0.149000; val_acc: 0.150000\n",
      "(Iteration 141 / 200) loss: 2.532663\n",
      "(Epoch 15 / 20) train acc: 0.142000; val_acc: 0.144444\n",
      "(Iteration 151 / 200) loss: 2.374410\n",
      "(Epoch 16 / 20) train acc: 0.157000; val_acc: 0.150000\n",
      "(Iteration 161 / 200) loss: 2.418169\n",
      "(Epoch 17 / 20) train acc: 0.174000; val_acc: 0.155556\n",
      "(Iteration 171 / 200) loss: 2.170741\n",
      "(Epoch 18 / 20) train acc: 0.153000; val_acc: 0.152778\n",
      "(Iteration 181 / 200) loss: 2.263047\n",
      "(Epoch 19 / 20) train acc: 0.168000; val_acc: 0.158333\n",
      "(Iteration 191 / 200) loss: 2.290747\n",
      "(Epoch 20 / 20) train acc: 0.183000; val_acc: 0.163889\n",
      "(Iteration 1 / 200) loss: 2.302767\n",
      "(Epoch 0 / 20) train acc: 0.131000; val_acc: 0.113889\n",
      "(Epoch 1 / 20) train acc: 0.180000; val_acc: 0.141667\n",
      "(Iteration 11 / 200) loss: 2.302755\n",
      "(Epoch 2 / 20) train acc: 0.205000; val_acc: 0.183333\n",
      "(Iteration 21 / 200) loss: 2.302762\n",
      "(Epoch 3 / 20) train acc: 0.180000; val_acc: 0.147222\n",
      "(Iteration 31 / 200) loss: 2.302754\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302743\n",
      "(Epoch 5 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302719\n",
      "(Epoch 6 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302745\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302722\n",
      "(Epoch 8 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302719\n",
      "(Epoch 9 / 20) train acc: 0.093000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302716\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302723\n",
      "(Epoch 11 / 20) train acc: 0.174000; val_acc: 0.125000\n",
      "(Iteration 111 / 200) loss: 2.302706\n",
      "(Epoch 12 / 20) train acc: 0.187000; val_acc: 0.127778\n",
      "(Iteration 121 / 200) loss: 2.302675\n",
      "(Epoch 13 / 20) train acc: 0.209000; val_acc: 0.152778\n",
      "(Iteration 131 / 200) loss: 2.302659\n",
      "(Epoch 14 / 20) train acc: 0.220000; val_acc: 0.161111\n",
      "(Iteration 141 / 200) loss: 2.302687\n",
      "(Epoch 15 / 20) train acc: 0.213000; val_acc: 0.163889\n",
      "(Iteration 151 / 200) loss: 2.302607\n",
      "(Epoch 16 / 20) train acc: 0.229000; val_acc: 0.163889\n",
      "(Iteration 161 / 200) loss: 2.302588\n",
      "(Epoch 17 / 20) train acc: 0.212000; val_acc: 0.161111\n",
      "(Iteration 171 / 200) loss: 2.302588\n",
      "(Epoch 18 / 20) train acc: 0.209000; val_acc: 0.161111\n",
      "(Iteration 181 / 200) loss: 2.302553\n",
      "(Epoch 19 / 20) train acc: 0.218000; val_acc: 0.163889\n",
      "(Iteration 191 / 200) loss: 2.302473\n",
      "(Epoch 20 / 20) train acc: 0.212000; val_acc: 0.163889\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302584\n",
      "(Epoch 2 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302596\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302575\n",
      "(Epoch 4 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302566\n",
      "(Epoch 5 / 20) train acc: 0.122000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302572\n",
      "(Epoch 6 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302559\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302589\n",
      "(Epoch 8 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302541\n",
      "(Epoch 9 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302579\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302596\n",
      "(Epoch 11 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302573\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302586\n",
      "(Epoch 13 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302551\n",
      "(Epoch 14 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302545\n",
      "(Epoch 15 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302564\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302575\n",
      "(Epoch 17 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302593\n",
      "(Epoch 18 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302582\n",
      "(Epoch 19 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302558\n",
      "(Epoch 20 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302591\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302582\n",
      "(Epoch 3 / 20) train acc: 0.131000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302572\n",
      "(Epoch 4 / 20) train acc: 0.112000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302570\n",
      "(Epoch 5 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302583\n",
      "(Epoch 6 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302586\n",
      "(Epoch 7 / 20) train acc: 0.098000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302567\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302580\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302578\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302599\n",
      "(Epoch 11 / 20) train acc: 0.088000; val_acc: 0.116667\n",
      "(Iteration 111 / 200) loss: 2.302593\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302561\n",
      "(Epoch 13 / 20) train acc: 0.093000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302503\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302610\n",
      "(Epoch 15 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302548\n",
      "(Epoch 16 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302534\n",
      "(Epoch 17 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302595\n",
      "(Epoch 18 / 20) train acc: 0.113000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302605\n",
      "(Epoch 19 / 20) train acc: 0.103000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302558\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.104000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 11 / 200) loss: 2.302590\n",
      "(Epoch 2 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 21 / 200) loss: 2.302585\n",
      "(Epoch 3 / 20) train acc: 0.116000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302585\n",
      "(Epoch 4 / 20) train acc: 0.108000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302590\n",
      "(Epoch 5 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302573\n",
      "(Epoch 6 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302586\n",
      "(Epoch 7 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302584\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302581\n",
      "(Epoch 9 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302592\n",
      "(Epoch 10 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302575\n",
      "(Epoch 11 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302594\n",
      "(Epoch 12 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 121 / 200) loss: 2.302581\n",
      "(Epoch 13 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302563\n",
      "(Epoch 14 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302575\n",
      "(Epoch 15 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302553\n",
      "(Epoch 16 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302578\n",
      "(Epoch 17 / 20) train acc: 0.096000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302542\n",
      "(Epoch 18 / 20) train acc: 0.097000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302599\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302555\n",
      "(Epoch 20 / 20) train acc: 0.117000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 251398.867256\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.126000; val_acc: 0.127778\n",
      "(Iteration 11 / 200) loss: 253863.187193\n",
      "(Epoch 2 / 20) train acc: 0.137000; val_acc: 0.127778\n",
      "(Iteration 21 / 200) loss: 266999.647140\n",
      "(Epoch 3 / 20) train acc: 0.108000; val_acc: 0.130556\n",
      "(Iteration 31 / 200) loss: 258288.287087\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.130556\n",
      "(Iteration 41 / 200) loss: 262630.347037\n",
      "(Epoch 5 / 20) train acc: 0.113000; val_acc: 0.130556\n",
      "(Iteration 51 / 200) loss: 245942.286986\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.130556\n",
      "(Iteration 61 / 200) loss: 268547.806936\n",
      "(Epoch 7 / 20) train acc: 0.117000; val_acc: 0.130556\n",
      "(Iteration 71 / 200) loss: 241414.386885\n",
      "(Epoch 8 / 20) train acc: 0.130000; val_acc: 0.130556\n",
      "(Iteration 81 / 200) loss: 273103.826835\n",
      "(Epoch 9 / 20) train acc: 0.114000; val_acc: 0.130556\n",
      "(Iteration 91 / 200) loss: 253137.846785\n",
      "(Epoch 10 / 20) train acc: 0.130000; val_acc: 0.130556\n",
      "(Iteration 101 / 200) loss: 229085.646735\n",
      "(Epoch 11 / 20) train acc: 0.126000; val_acc: 0.130556\n",
      "(Iteration 111 / 200) loss: 255340.046685\n",
      "(Epoch 12 / 20) train acc: 0.125000; val_acc: 0.130556\n",
      "(Iteration 121 / 200) loss: 254795.626635\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.130556\n",
      "(Iteration 131 / 200) loss: 246068.266585\n",
      "(Epoch 14 / 20) train acc: 0.137000; val_acc: 0.130556\n",
      "(Iteration 141 / 200) loss: 241554.646535\n",
      "(Epoch 15 / 20) train acc: 0.124000; val_acc: 0.130556\n",
      "(Iteration 151 / 200) loss: 221889.246485\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.130556\n",
      "(Iteration 161 / 200) loss: 251100.966435\n",
      "(Epoch 17 / 20) train acc: 0.123000; val_acc: 0.130556\n",
      "(Iteration 171 / 200) loss: 280375.626386\n",
      "(Epoch 18 / 20) train acc: 0.135000; val_acc: 0.127778\n",
      "(Iteration 181 / 200) loss: 239533.746336\n",
      "(Epoch 19 / 20) train acc: 0.132000; val_acc: 0.125000\n",
      "(Iteration 191 / 200) loss: 234438.606287\n",
      "(Epoch 20 / 20) train acc: 0.116000; val_acc: 0.125000\n",
      "(Iteration 1 / 200) loss: 3.432705\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.147222\n",
      "(Epoch 1 / 20) train acc: 0.129000; val_acc: 0.150000\n",
      "(Iteration 11 / 200) loss: 3.289712\n",
      "(Epoch 2 / 20) train acc: 0.113000; val_acc: 0.150000\n",
      "(Iteration 21 / 200) loss: 3.296299\n",
      "(Epoch 3 / 20) train acc: 0.121000; val_acc: 0.150000\n",
      "(Iteration 31 / 200) loss: 3.096564\n",
      "(Epoch 4 / 20) train acc: 0.122000; val_acc: 0.155556\n",
      "(Iteration 41 / 200) loss: 2.993394\n",
      "(Epoch 5 / 20) train acc: 0.135000; val_acc: 0.169444\n",
      "(Iteration 51 / 200) loss: 3.443912\n",
      "(Epoch 6 / 20) train acc: 0.135000; val_acc: 0.172222\n",
      "(Iteration 61 / 200) loss: 2.876244\n",
      "(Epoch 7 / 20) train acc: 0.120000; val_acc: 0.166667\n",
      "(Iteration 71 / 200) loss: 3.022591\n",
      "(Epoch 8 / 20) train acc: 0.138000; val_acc: 0.166667\n",
      "(Iteration 81 / 200) loss: 3.033766\n",
      "(Epoch 9 / 20) train acc: 0.155000; val_acc: 0.175000\n",
      "(Iteration 91 / 200) loss: 2.880101\n",
      "(Epoch 10 / 20) train acc: 0.153000; val_acc: 0.175000\n",
      "(Iteration 101 / 200) loss: 2.592913\n",
      "(Epoch 11 / 20) train acc: 0.153000; val_acc: 0.172222\n",
      "(Iteration 111 / 200) loss: 2.718952\n",
      "(Epoch 12 / 20) train acc: 0.145000; val_acc: 0.177778\n",
      "(Iteration 121 / 200) loss: 2.596307\n",
      "(Epoch 13 / 20) train acc: 0.159000; val_acc: 0.183333\n",
      "(Iteration 131 / 200) loss: 2.647016\n",
      "(Epoch 14 / 20) train acc: 0.161000; val_acc: 0.194444\n",
      "(Iteration 141 / 200) loss: 2.701130\n",
      "(Epoch 15 / 20) train acc: 0.165000; val_acc: 0.197222\n",
      "(Iteration 151 / 200) loss: 2.327142\n",
      "(Epoch 16 / 20) train acc: 0.157000; val_acc: 0.213889\n",
      "(Iteration 161 / 200) loss: 2.597257\n",
      "(Epoch 17 / 20) train acc: 0.207000; val_acc: 0.211111\n",
      "(Iteration 171 / 200) loss: 2.329691\n",
      "(Epoch 18 / 20) train acc: 0.219000; val_acc: 0.216667\n",
      "(Iteration 181 / 200) loss: 2.420537\n",
      "(Epoch 19 / 20) train acc: 0.204000; val_acc: 0.227778\n",
      "(Iteration 191 / 200) loss: 2.310091\n",
      "(Epoch 20 / 20) train acc: 0.202000; val_acc: 0.247222\n",
      "(Iteration 1 / 200) loss: 2.302768\n",
      "(Epoch 0 / 20) train acc: 0.095000; val_acc: 0.127778\n",
      "(Epoch 1 / 20) train acc: 0.102000; val_acc: 0.119444\n",
      "(Iteration 11 / 200) loss: 2.302756\n",
      "(Epoch 2 / 20) train acc: 0.093000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302746\n",
      "(Epoch 3 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302748\n",
      "(Epoch 4 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302739\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302755\n",
      "(Epoch 6 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302757\n",
      "(Epoch 7 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302715\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302694\n",
      "(Epoch 9 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302728\n",
      "(Epoch 10 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302681\n",
      "(Epoch 11 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302707\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302719\n",
      "(Epoch 13 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302711\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302668\n",
      "(Epoch 15 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302668\n",
      "(Epoch 16 / 20) train acc: 0.087000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302626\n",
      "(Epoch 17 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302662\n",
      "(Epoch 18 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302570\n",
      "(Epoch 19 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302586\n",
      "(Epoch 20 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302587\n",
      "(Epoch 0 / 20) train acc: 0.098000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.087000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.128000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.302590\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302571\n",
      "(Epoch 5 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302580\n",
      "(Epoch 6 / 20) train acc: 0.100000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302577\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302582\n",
      "(Epoch 8 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302595\n",
      "(Epoch 9 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302581\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302588\n",
      "(Epoch 11 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302595\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302567\n",
      "(Epoch 13 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302553\n",
      "(Epoch 14 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302551\n",
      "(Epoch 15 / 20) train acc: 0.107000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302542\n",
      "(Epoch 16 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 161 / 200) loss: 2.302564\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 171 / 200) loss: 2.302558\n",
      "(Epoch 18 / 20) train acc: 0.112000; val_acc: 0.116667\n",
      "(Iteration 181 / 200) loss: 2.302526\n",
      "(Epoch 19 / 20) train acc: 0.117000; val_acc: 0.116667\n",
      "(Iteration 191 / 200) loss: 2.302565\n",
      "(Epoch 20 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.086111\n",
      "(Epoch 1 / 20) train acc: 0.097000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302583\n",
      "(Epoch 2 / 20) train acc: 0.107000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302583\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302576\n",
      "(Epoch 4 / 20) train acc: 0.082000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 2.302571\n",
      "(Epoch 5 / 20) train acc: 0.102000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 2.302596\n",
      "(Epoch 6 / 20) train acc: 0.092000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 2.302578\n",
      "(Epoch 7 / 20) train acc: 0.140000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302587\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302598\n",
      "(Epoch 9 / 20) train acc: 0.086000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302590\n",
      "(Epoch 10 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302588\n",
      "(Epoch 11 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302568\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302586\n",
      "(Epoch 13 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302601\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302576\n",
      "(Epoch 15 / 20) train acc: 0.125000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.302596\n",
      "(Epoch 16 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 161 / 200) loss: 2.302652\n",
      "(Epoch 17 / 20) train acc: 0.098000; val_acc: 0.097222\n",
      "(Iteration 171 / 200) loss: 2.302602\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 181 / 200) loss: 2.302560\n",
      "(Epoch 19 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302600\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.108000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302590\n",
      "(Epoch 2 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302580\n",
      "(Epoch 3 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302589\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302579\n",
      "(Epoch 5 / 20) train acc: 0.110000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302572\n",
      "(Epoch 6 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302585\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302586\n",
      "(Epoch 8 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302578\n",
      "(Epoch 9 / 20) train acc: 0.090000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302579\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302565\n",
      "(Epoch 11 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302581\n",
      "(Epoch 12 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302567\n",
      "(Epoch 13 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302595\n",
      "(Epoch 14 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302564\n",
      "(Epoch 15 / 20) train acc: 0.110000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302582\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302540\n",
      "(Epoch 17 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302588\n",
      "(Epoch 18 / 20) train acc: 0.118000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302606\n",
      "(Epoch 19 / 20) train acc: 0.114000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302536\n",
      "(Epoch 20 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 242772.520798\n",
      "(Epoch 0 / 20) train acc: 0.085000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.079000; val_acc: 0.077778\n",
      "(Iteration 11 / 200) loss: 286857.880790\n",
      "(Epoch 2 / 20) train acc: 0.063000; val_acc: 0.075000\n",
      "(Iteration 21 / 200) loss: 233824.880784\n",
      "(Epoch 3 / 20) train acc: 0.086000; val_acc: 0.075000\n",
      "(Iteration 31 / 200) loss: 242825.120777\n",
      "(Epoch 4 / 20) train acc: 0.077000; val_acc: 0.075000\n",
      "(Iteration 41 / 200) loss: 224753.360771\n",
      "(Epoch 5 / 20) train acc: 0.066000; val_acc: 0.069444\n",
      "(Iteration 51 / 200) loss: 217770.720764\n",
      "(Epoch 6 / 20) train acc: 0.068000; val_acc: 0.066667\n",
      "(Iteration 61 / 200) loss: 248460.660758\n",
      "(Epoch 7 / 20) train acc: 0.062000; val_acc: 0.066667\n",
      "(Iteration 71 / 200) loss: 237218.860752\n",
      "(Epoch 8 / 20) train acc: 0.067000; val_acc: 0.063889\n",
      "(Iteration 81 / 200) loss: 224398.240745\n",
      "(Epoch 9 / 20) train acc: 0.070000; val_acc: 0.063889\n",
      "(Iteration 91 / 200) loss: 226155.900739\n",
      "(Epoch 10 / 20) train acc: 0.069000; val_acc: 0.063889\n",
      "(Iteration 101 / 200) loss: 221841.700733\n",
      "(Epoch 11 / 20) train acc: 0.064000; val_acc: 0.063889\n",
      "(Iteration 111 / 200) loss: 222882.460727\n",
      "(Epoch 12 / 20) train acc: 0.074000; val_acc: 0.063889\n",
      "(Iteration 121 / 200) loss: 228070.960720\n",
      "(Epoch 13 / 20) train acc: 0.063000; val_acc: 0.061111\n",
      "(Iteration 131 / 200) loss: 226602.100714\n",
      "(Epoch 14 / 20) train acc: 0.074000; val_acc: 0.061111\n",
      "(Iteration 141 / 200) loss: 226349.060708\n",
      "(Epoch 15 / 20) train acc: 0.059000; val_acc: 0.061111\n",
      "(Iteration 151 / 200) loss: 213051.720701\n",
      "(Epoch 16 / 20) train acc: 0.080000; val_acc: 0.061111\n",
      "(Iteration 161 / 200) loss: 205148.740695\n",
      "(Epoch 17 / 20) train acc: 0.071000; val_acc: 0.061111\n",
      "(Iteration 171 / 200) loss: 214704.880689\n",
      "(Epoch 18 / 20) train acc: 0.058000; val_acc: 0.058333\n",
      "(Iteration 181 / 200) loss: 224133.920683\n",
      "(Epoch 19 / 20) train acc: 0.048000; val_acc: 0.058333\n",
      "(Iteration 191 / 200) loss: 200447.300676\n",
      "(Epoch 20 / 20) train acc: 0.055000; val_acc: 0.055556\n",
      "(Iteration 1 / 200) loss: 2.771937\n",
      "(Epoch 0 / 20) train acc: 0.127000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.137000; val_acc: 0.108333\n",
      "(Iteration 11 / 200) loss: 2.683332\n",
      "(Epoch 2 / 20) train acc: 0.151000; val_acc: 0.119444\n",
      "(Iteration 21 / 200) loss: 2.589626\n",
      "(Epoch 3 / 20) train acc: 0.139000; val_acc: 0.116667\n",
      "(Iteration 31 / 200) loss: 2.474937\n",
      "(Epoch 4 / 20) train acc: 0.154000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.604901\n",
      "(Epoch 5 / 20) train acc: 0.149000; val_acc: 0.119444\n",
      "(Iteration 51 / 200) loss: 2.448955\n",
      "(Epoch 6 / 20) train acc: 0.164000; val_acc: 0.127778\n",
      "(Iteration 61 / 200) loss: 2.682581\n",
      "(Epoch 7 / 20) train acc: 0.146000; val_acc: 0.136111\n",
      "(Iteration 71 / 200) loss: 2.351048\n",
      "(Epoch 8 / 20) train acc: 0.187000; val_acc: 0.150000\n",
      "(Iteration 81 / 200) loss: 2.347057\n",
      "(Epoch 9 / 20) train acc: 0.170000; val_acc: 0.155556\n",
      "(Iteration 91 / 200) loss: 2.275852\n",
      "(Epoch 10 / 20) train acc: 0.202000; val_acc: 0.158333\n",
      "(Iteration 101 / 200) loss: 2.276703\n",
      "(Epoch 11 / 20) train acc: 0.186000; val_acc: 0.169444\n",
      "(Iteration 111 / 200) loss: 2.277325\n",
      "(Epoch 12 / 20) train acc: 0.211000; val_acc: 0.169444\n",
      "(Iteration 121 / 200) loss: 2.218695\n",
      "(Epoch 13 / 20) train acc: 0.199000; val_acc: 0.175000\n",
      "(Iteration 131 / 200) loss: 2.162597\n",
      "(Epoch 14 / 20) train acc: 0.246000; val_acc: 0.186111\n",
      "(Iteration 141 / 200) loss: 2.322664\n",
      "(Epoch 15 / 20) train acc: 0.239000; val_acc: 0.197222\n",
      "(Iteration 151 / 200) loss: 2.117868\n",
      "(Epoch 16 / 20) train acc: 0.255000; val_acc: 0.208333\n",
      "(Iteration 161 / 200) loss: 2.085838\n",
      "(Epoch 17 / 20) train acc: 0.241000; val_acc: 0.219444\n",
      "(Iteration 171 / 200) loss: 2.103756\n",
      "(Epoch 18 / 20) train acc: 0.274000; val_acc: 0.227778\n",
      "(Iteration 181 / 200) loss: 1.985002\n",
      "(Epoch 19 / 20) train acc: 0.263000; val_acc: 0.233333\n",
      "(Iteration 191 / 200) loss: 1.929740\n",
      "(Epoch 20 / 20) train acc: 0.304000; val_acc: 0.252778\n",
      "(Iteration 1 / 200) loss: 2.302603\n",
      "(Epoch 0 / 20) train acc: 0.073000; val_acc: 0.077778\n",
      "(Epoch 1 / 20) train acc: 0.100000; val_acc: 0.102778\n",
      "(Iteration 11 / 200) loss: 2.302596\n",
      "(Epoch 2 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302599\n",
      "(Epoch 3 / 20) train acc: 0.127000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302578\n",
      "(Epoch 4 / 20) train acc: 0.109000; val_acc: 0.116667\n",
      "(Iteration 41 / 200) loss: 2.302608\n",
      "(Epoch 5 / 20) train acc: 0.190000; val_acc: 0.172222\n",
      "(Iteration 51 / 200) loss: 2.302538\n",
      "(Epoch 6 / 20) train acc: 0.172000; val_acc: 0.136111\n",
      "(Iteration 61 / 200) loss: 2.302570\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302577\n",
      "(Epoch 8 / 20) train acc: 0.112000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302576\n",
      "(Epoch 9 / 20) train acc: 0.198000; val_acc: 0.180556\n",
      "(Iteration 91 / 200) loss: 2.302551\n",
      "(Epoch 10 / 20) train acc: 0.358000; val_acc: 0.344444\n",
      "(Iteration 101 / 200) loss: 2.302545\n",
      "(Epoch 11 / 20) train acc: 0.441000; val_acc: 0.391667\n",
      "(Iteration 111 / 200) loss: 2.302555\n",
      "(Epoch 12 / 20) train acc: 0.459000; val_acc: 0.394444\n",
      "(Iteration 121 / 200) loss: 2.302507\n",
      "(Epoch 13 / 20) train acc: 0.484000; val_acc: 0.441667\n",
      "(Iteration 131 / 200) loss: 2.302501\n",
      "(Epoch 14 / 20) train acc: 0.484000; val_acc: 0.441667\n",
      "(Iteration 141 / 200) loss: 2.302512\n",
      "(Epoch 15 / 20) train acc: 0.457000; val_acc: 0.419444\n",
      "(Iteration 151 / 200) loss: 2.302447\n",
      "(Epoch 16 / 20) train acc: 0.319000; val_acc: 0.311111\n",
      "(Iteration 161 / 200) loss: 2.302579\n",
      "(Epoch 17 / 20) train acc: 0.359000; val_acc: 0.313889\n",
      "(Iteration 171 / 200) loss: 2.302475\n",
      "(Epoch 18 / 20) train acc: 0.356000; val_acc: 0.313889\n",
      "(Iteration 181 / 200) loss: 2.302386\n",
      "(Epoch 19 / 20) train acc: 0.286000; val_acc: 0.272222\n",
      "(Iteration 191 / 200) loss: 2.302325\n",
      "(Epoch 20 / 20) train acc: 0.307000; val_acc: 0.294444\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 2.302593\n",
      "(Epoch 2 / 20) train acc: 0.099000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 2.302594\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 2.302591\n",
      "(Epoch 4 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302581\n",
      "(Epoch 5 / 20) train acc: 0.097000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302593\n",
      "(Epoch 6 / 20) train acc: 0.093000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302582\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 71 / 200) loss: 2.302589\n",
      "(Epoch 8 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 81 / 200) loss: 2.302566\n",
      "(Epoch 9 / 20) train acc: 0.100000; val_acc: 0.086111\n",
      "(Iteration 91 / 200) loss: 2.302601\n",
      "(Epoch 10 / 20) train acc: 0.101000; val_acc: 0.086111\n",
      "(Iteration 101 / 200) loss: 2.302577\n",
      "(Epoch 11 / 20) train acc: 0.113000; val_acc: 0.086111\n",
      "(Iteration 111 / 200) loss: 2.302593\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.086111\n",
      "(Iteration 121 / 200) loss: 2.302593\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 131 / 200) loss: 2.302578\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 141 / 200) loss: 2.302550\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302547\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302558\n",
      "(Epoch 17 / 20) train acc: 0.118000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302584\n",
      "(Epoch 18 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302593\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302590\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.095000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.092000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302587\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.113000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302586\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.116667\n",
      "(Iteration 51 / 200) loss: 2.302585\n",
      "(Epoch 6 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 61 / 200) loss: 2.302592\n",
      "(Epoch 7 / 20) train acc: 0.077000; val_acc: 0.116667\n",
      "(Iteration 71 / 200) loss: 2.302592\n",
      "(Epoch 8 / 20) train acc: 0.102000; val_acc: 0.116667\n",
      "(Iteration 81 / 200) loss: 2.302568\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302583\n",
      "(Epoch 10 / 20) train acc: 0.088000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302582\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.097222\n",
      "(Iteration 111 / 200) loss: 2.302599\n",
      "(Epoch 12 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 121 / 200) loss: 2.302611\n",
      "(Epoch 13 / 20) train acc: 0.100000; val_acc: 0.097222\n",
      "(Iteration 131 / 200) loss: 2.302568\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302583\n",
      "(Epoch 15 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 151 / 200) loss: 2.302551\n",
      "(Epoch 16 / 20) train acc: 0.108000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302563\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302579\n",
      "(Epoch 18 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302614\n",
      "(Epoch 19 / 20) train acc: 0.098000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302612\n",
      "(Epoch 20 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.089000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302579\n",
      "(Epoch 2 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302582\n",
      "(Epoch 3 / 20) train acc: 0.103000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302575\n",
      "(Epoch 4 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302560\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302566\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302561\n",
      "(Epoch 7 / 20) train acc: 0.116000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302589\n",
      "(Epoch 8 / 20) train acc: 0.089000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302596\n",
      "(Epoch 9 / 20) train acc: 0.097000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302569\n",
      "(Epoch 10 / 20) train acc: 0.113000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302602\n",
      "(Epoch 11 / 20) train acc: 0.124000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302624\n",
      "(Epoch 12 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302591\n",
      "(Epoch 13 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302568\n",
      "(Epoch 14 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302519\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302552\n",
      "(Epoch 16 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302631\n",
      "(Epoch 17 / 20) train acc: 0.112000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302541\n",
      "(Epoch 18 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302607\n",
      "(Epoch 19 / 20) train acc: 0.094000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302508\n",
      "(Epoch 20 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 166689.661476\n",
      "(Epoch 0 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.123000; val_acc: 0.086111\n",
      "(Iteration 11 / 200) loss: 171629.941470\n",
      "(Epoch 2 / 20) train acc: 0.120000; val_acc: 0.086111\n",
      "(Iteration 21 / 200) loss: 162112.351464\n",
      "(Epoch 3 / 20) train acc: 0.096000; val_acc: 0.086111\n",
      "(Iteration 31 / 200) loss: 153008.261459\n",
      "(Epoch 4 / 20) train acc: 0.122000; val_acc: 0.086111\n",
      "(Iteration 41 / 200) loss: 174100.961453\n",
      "(Epoch 5 / 20) train acc: 0.096000; val_acc: 0.086111\n",
      "(Iteration 51 / 200) loss: 166233.081448\n",
      "(Epoch 6 / 20) train acc: 0.095000; val_acc: 0.086111\n",
      "(Iteration 61 / 200) loss: 170821.341443\n",
      "(Epoch 7 / 20) train acc: 0.112000; val_acc: 0.086111\n",
      "(Iteration 71 / 200) loss: 163923.841438\n",
      "(Epoch 8 / 20) train acc: 0.127000; val_acc: 0.086111\n",
      "(Iteration 81 / 200) loss: 162258.631432\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.086111\n",
      "(Iteration 91 / 200) loss: 164545.991427\n",
      "(Epoch 10 / 20) train acc: 0.111000; val_acc: 0.088889\n",
      "(Iteration 101 / 200) loss: 160958.091422\n",
      "(Epoch 11 / 20) train acc: 0.096000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 166020.261417\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 178664.341412\n",
      "(Epoch 13 / 20) train acc: 0.123000; val_acc: 0.088889\n",
      "(Iteration 131 / 200) loss: 172412.801407\n",
      "(Epoch 14 / 20) train acc: 0.120000; val_acc: 0.088889\n",
      "(Iteration 141 / 200) loss: 158775.431401\n",
      "(Epoch 15 / 20) train acc: 0.111000; val_acc: 0.088889\n",
      "(Iteration 151 / 200) loss: 157765.931396\n",
      "(Epoch 16 / 20) train acc: 0.106000; val_acc: 0.088889\n",
      "(Iteration 161 / 200) loss: 158761.731391\n",
      "(Epoch 17 / 20) train acc: 0.110000; val_acc: 0.088889\n",
      "(Iteration 171 / 200) loss: 142359.581386\n",
      "(Epoch 18 / 20) train acc: 0.129000; val_acc: 0.088889\n",
      "(Iteration 181 / 200) loss: 169166.321381\n",
      "(Epoch 19 / 20) train acc: 0.107000; val_acc: 0.088889\n",
      "(Iteration 191 / 200) loss: 151381.431376\n",
      "(Epoch 20 / 20) train acc: 0.115000; val_acc: 0.086111\n",
      "(Iteration 1 / 200) loss: 2.691375\n",
      "(Epoch 0 / 20) train acc: 0.214000; val_acc: 0.230556\n",
      "(Epoch 1 / 20) train acc: 0.269000; val_acc: 0.236111\n",
      "(Iteration 11 / 200) loss: 2.824511\n",
      "(Epoch 2 / 20) train acc: 0.235000; val_acc: 0.250000\n",
      "(Iteration 21 / 200) loss: 2.689788\n",
      "(Epoch 3 / 20) train acc: 0.253000; val_acc: 0.255556\n",
      "(Iteration 31 / 200) loss: 2.742599\n",
      "(Epoch 4 / 20) train acc: 0.282000; val_acc: 0.266667\n",
      "(Iteration 41 / 200) loss: 2.449826\n",
      "(Epoch 5 / 20) train acc: 0.274000; val_acc: 0.272222\n",
      "(Iteration 51 / 200) loss: 2.412142\n",
      "(Epoch 6 / 20) train acc: 0.282000; val_acc: 0.286111\n",
      "(Iteration 61 / 200) loss: 2.405283\n",
      "(Epoch 7 / 20) train acc: 0.276000; val_acc: 0.291667\n",
      "(Iteration 71 / 200) loss: 2.435266\n",
      "(Epoch 8 / 20) train acc: 0.310000; val_acc: 0.308333\n",
      "(Iteration 81 / 200) loss: 2.399657\n",
      "(Epoch 9 / 20) train acc: 0.325000; val_acc: 0.313889\n",
      "(Iteration 91 / 200) loss: 2.356962\n",
      "(Epoch 10 / 20) train acc: 0.326000; val_acc: 0.330556\n",
      "(Iteration 101 / 200) loss: 2.297153\n",
      "(Epoch 11 / 20) train acc: 0.337000; val_acc: 0.336111\n",
      "(Iteration 111 / 200) loss: 2.112904\n",
      "(Epoch 12 / 20) train acc: 0.344000; val_acc: 0.355556\n",
      "(Iteration 121 / 200) loss: 2.275483\n",
      "(Epoch 13 / 20) train acc: 0.340000; val_acc: 0.369444\n",
      "(Iteration 131 / 200) loss: 2.099191\n",
      "(Epoch 14 / 20) train acc: 0.353000; val_acc: 0.377778\n",
      "(Iteration 141 / 200) loss: 2.352647\n",
      "(Epoch 15 / 20) train acc: 0.392000; val_acc: 0.386111\n",
      "(Iteration 151 / 200) loss: 2.174831\n",
      "(Epoch 16 / 20) train acc: 0.366000; val_acc: 0.391667\n",
      "(Iteration 161 / 200) loss: 2.115167\n",
      "(Epoch 17 / 20) train acc: 0.374000; val_acc: 0.400000\n",
      "(Iteration 171 / 200) loss: 2.279127\n",
      "(Epoch 18 / 20) train acc: 0.350000; val_acc: 0.397222\n",
      "(Iteration 181 / 200) loss: 2.203264\n",
      "(Epoch 19 / 20) train acc: 0.411000; val_acc: 0.400000\n",
      "(Iteration 191 / 200) loss: 2.206116\n",
      "(Epoch 20 / 20) train acc: 0.419000; val_acc: 0.400000\n",
      "(Iteration 1 / 200) loss: 2.302599\n",
      "(Epoch 0 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 11 / 200) loss: 2.302605\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 21 / 200) loss: 2.302576\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302580\n",
      "(Epoch 4 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 41 / 200) loss: 2.302551\n",
      "(Epoch 5 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 51 / 200) loss: 2.302570\n",
      "(Epoch 6 / 20) train acc: 0.132000; val_acc: 0.080556\n",
      "(Iteration 61 / 200) loss: 2.302547\n",
      "(Epoch 7 / 20) train acc: 0.099000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302568\n",
      "(Epoch 8 / 20) train acc: 0.082000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302545\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.080556\n",
      "(Iteration 91 / 200) loss: 2.302570\n",
      "(Epoch 10 / 20) train acc: 0.116000; val_acc: 0.080556\n",
      "(Iteration 101 / 200) loss: 2.302489\n",
      "(Epoch 11 / 20) train acc: 0.092000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302564\n",
      "(Epoch 12 / 20) train acc: 0.145000; val_acc: 0.091667\n",
      "(Iteration 121 / 200) loss: 2.302501\n",
      "(Epoch 13 / 20) train acc: 0.262000; val_acc: 0.197222\n",
      "(Iteration 131 / 200) loss: 2.302492\n",
      "(Epoch 14 / 20) train acc: 0.313000; val_acc: 0.236111\n",
      "(Iteration 141 / 200) loss: 2.302455\n",
      "(Epoch 15 / 20) train acc: 0.310000; val_acc: 0.236111\n",
      "(Iteration 151 / 200) loss: 2.302485\n",
      "(Epoch 16 / 20) train acc: 0.259000; val_acc: 0.213889\n",
      "(Iteration 161 / 200) loss: 2.302392\n",
      "(Epoch 17 / 20) train acc: 0.252000; val_acc: 0.208333\n",
      "(Iteration 171 / 200) loss: 2.302466\n",
      "(Epoch 18 / 20) train acc: 0.214000; val_acc: 0.177778\n",
      "(Iteration 181 / 200) loss: 2.302399\n",
      "(Epoch 19 / 20) train acc: 0.239000; val_acc: 0.191667\n",
      "(Iteration 191 / 200) loss: 2.302262\n",
      "(Epoch 20 / 20) train acc: 0.243000; val_acc: 0.211111\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.107000; val_acc: 0.097222\n",
      "(Epoch 1 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302586\n",
      "(Epoch 2 / 20) train acc: 0.102000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302579\n",
      "(Epoch 3 / 20) train acc: 0.105000; val_acc: 0.097222\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.096000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302590\n",
      "(Epoch 5 / 20) train acc: 0.112000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302569\n",
      "(Epoch 6 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302594\n",
      "(Epoch 7 / 20) train acc: 0.081000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302571\n",
      "(Epoch 8 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302574\n",
      "(Epoch 9 / 20) train acc: 0.123000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302584\n",
      "(Epoch 10 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302579\n",
      "(Epoch 11 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302557\n",
      "(Epoch 12 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302586\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302562\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302579\n",
      "(Epoch 15 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302563\n",
      "(Epoch 16 / 20) train acc: 0.130000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302639\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302582\n",
      "(Epoch 18 / 20) train acc: 0.125000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302633\n",
      "(Epoch 19 / 20) train acc: 0.111000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302588\n",
      "(Epoch 20 / 20) train acc: 0.103000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.087000; val_acc: 0.091667\n",
      "(Epoch 1 / 20) train acc: 0.090000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302587\n",
      "(Epoch 2 / 20) train acc: 0.081000; val_acc: 0.091667\n",
      "(Iteration 21 / 200) loss: 2.302594\n",
      "(Epoch 3 / 20) train acc: 0.097000; val_acc: 0.091667\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.122000; val_acc: 0.091667\n",
      "(Iteration 41 / 200) loss: 2.302600\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.091667\n",
      "(Iteration 51 / 200) loss: 2.302579\n",
      "(Epoch 6 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302577\n",
      "(Epoch 7 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302587\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302569\n",
      "(Epoch 9 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302582\n",
      "(Epoch 10 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302587\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302596\n",
      "(Epoch 12 / 20) train acc: 0.106000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302549\n",
      "(Epoch 13 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302603\n",
      "(Epoch 14 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 141 / 200) loss: 2.302607\n",
      "(Epoch 15 / 20) train acc: 0.119000; val_acc: 0.116667\n",
      "(Iteration 151 / 200) loss: 2.302567\n",
      "(Epoch 16 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302575\n",
      "(Epoch 17 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302581\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302571\n",
      "(Epoch 19 / 20) train acc: 0.102000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302549\n",
      "(Epoch 20 / 20) train acc: 0.096000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.116667\n",
      "(Epoch 1 / 20) train acc: 0.101000; val_acc: 0.100000\n",
      "(Iteration 11 / 200) loss: 2.302589\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302590\n",
      "(Epoch 3 / 20) train acc: 0.101000; val_acc: 0.080556\n",
      "(Iteration 31 / 200) loss: 2.302583\n",
      "(Epoch 4 / 20) train acc: 0.101000; val_acc: 0.097222\n",
      "(Iteration 41 / 200) loss: 2.302585\n",
      "(Epoch 5 / 20) train acc: 0.094000; val_acc: 0.097222\n",
      "(Iteration 51 / 200) loss: 2.302591\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.097222\n",
      "(Iteration 61 / 200) loss: 2.302585\n",
      "(Epoch 7 / 20) train acc: 0.113000; val_acc: 0.080556\n",
      "(Iteration 71 / 200) loss: 2.302601\n",
      "(Epoch 8 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 81 / 200) loss: 2.302577\n",
      "(Epoch 9 / 20) train acc: 0.105000; val_acc: 0.116667\n",
      "(Iteration 91 / 200) loss: 2.302567\n",
      "(Epoch 10 / 20) train acc: 0.110000; val_acc: 0.116667\n",
      "(Iteration 101 / 200) loss: 2.302570\n",
      "(Epoch 11 / 20) train acc: 0.106000; val_acc: 0.080556\n",
      "(Iteration 111 / 200) loss: 2.302562\n",
      "(Epoch 12 / 20) train acc: 0.114000; val_acc: 0.116667\n",
      "(Iteration 121 / 200) loss: 2.302599\n",
      "(Epoch 13 / 20) train acc: 0.096000; val_acc: 0.116667\n",
      "(Iteration 131 / 200) loss: 2.302555\n",
      "(Epoch 14 / 20) train acc: 0.117000; val_acc: 0.097222\n",
      "(Iteration 141 / 200) loss: 2.302593\n",
      "(Epoch 15 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 151 / 200) loss: 2.302612\n",
      "(Epoch 16 / 20) train acc: 0.115000; val_acc: 0.080556\n",
      "(Iteration 161 / 200) loss: 2.302547\n",
      "(Epoch 17 / 20) train acc: 0.129000; val_acc: 0.080556\n",
      "(Iteration 171 / 200) loss: 2.302567\n",
      "(Epoch 18 / 20) train acc: 0.095000; val_acc: 0.080556\n",
      "(Iteration 181 / 200) loss: 2.302566\n",
      "(Epoch 19 / 20) train acc: 0.104000; val_acc: 0.080556\n",
      "(Iteration 191 / 200) loss: 2.302556\n",
      "(Epoch 20 / 20) train acc: 0.107000; val_acc: 0.080556\n",
      "(Iteration 1 / 200) loss: 270347.840827\n",
      "(Epoch 0 / 20) train acc: 0.076000; val_acc: 0.088889\n",
      "(Epoch 1 / 20) train acc: 0.104000; val_acc: 0.088889\n",
      "(Iteration 11 / 200) loss: 288337.600820\n",
      "(Epoch 2 / 20) train acc: 0.106000; val_acc: 0.088889\n",
      "(Iteration 21 / 200) loss: 271748.800814\n",
      "(Epoch 3 / 20) train acc: 0.080000; val_acc: 0.088889\n",
      "(Iteration 31 / 200) loss: 260274.000808\n",
      "(Epoch 4 / 20) train acc: 0.081000; val_acc: 0.088889\n",
      "(Iteration 41 / 200) loss: 249403.600803\n",
      "(Epoch 5 / 20) train acc: 0.091000; val_acc: 0.088889\n",
      "(Iteration 51 / 200) loss: 265674.620797\n",
      "(Epoch 6 / 20) train acc: 0.072000; val_acc: 0.088889\n",
      "(Iteration 61 / 200) loss: 236283.000792\n",
      "(Epoch 7 / 20) train acc: 0.108000; val_acc: 0.088889\n",
      "(Iteration 71 / 200) loss: 274285.200786\n",
      "(Epoch 8 / 20) train acc: 0.091000; val_acc: 0.088889\n",
      "(Iteration 81 / 200) loss: 239889.440781\n",
      "(Epoch 9 / 20) train acc: 0.084000; val_acc: 0.088889\n",
      "(Iteration 91 / 200) loss: 242697.680775\n",
      "(Epoch 10 / 20) train acc: 0.084000; val_acc: 0.088889\n",
      "(Iteration 101 / 200) loss: 246009.820770\n",
      "(Epoch 11 / 20) train acc: 0.092000; val_acc: 0.088889\n",
      "(Iteration 111 / 200) loss: 234076.300764\n",
      "(Epoch 12 / 20) train acc: 0.095000; val_acc: 0.088889\n",
      "(Iteration 121 / 200) loss: 250914.080759\n",
      "(Epoch 13 / 20) train acc: 0.081000; val_acc: 0.088889\n",
      "(Iteration 131 / 200) loss: 240588.300754\n",
      "(Epoch 14 / 20) train acc: 0.107000; val_acc: 0.088889\n",
      "(Iteration 141 / 200) loss: 281843.360748\n",
      "(Epoch 15 / 20) train acc: 0.097000; val_acc: 0.088889\n",
      "(Iteration 151 / 200) loss: 279376.200743\n",
      "(Epoch 16 / 20) train acc: 0.105000; val_acc: 0.088889\n",
      "(Iteration 161 / 200) loss: 242146.280737\n",
      "(Epoch 17 / 20) train acc: 0.074000; val_acc: 0.091667\n",
      "(Iteration 171 / 200) loss: 202309.400732\n",
      "(Epoch 18 / 20) train acc: 0.093000; val_acc: 0.094444\n",
      "(Iteration 181 / 200) loss: 244397.060726\n",
      "(Epoch 19 / 20) train acc: 0.087000; val_acc: 0.100000\n",
      "(Iteration 191 / 200) loss: 208494.180721\n",
      "(Epoch 20 / 20) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 1 / 200) loss: 3.103268\n",
      "(Epoch 0 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Epoch 1 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.828798\n",
      "(Epoch 2 / 20) train acc: 0.126000; val_acc: 0.102778\n",
      "(Iteration 21 / 200) loss: 2.966954\n",
      "(Epoch 3 / 20) train acc: 0.135000; val_acc: 0.102778\n",
      "(Iteration 31 / 200) loss: 2.766396\n",
      "(Epoch 4 / 20) train acc: 0.116000; val_acc: 0.102778\n",
      "(Iteration 41 / 200) loss: 2.749207\n",
      "(Epoch 5 / 20) train acc: 0.132000; val_acc: 0.111111\n",
      "(Iteration 51 / 200) loss: 2.624028\n",
      "(Epoch 6 / 20) train acc: 0.151000; val_acc: 0.119444\n",
      "(Iteration 61 / 200) loss: 2.568814\n",
      "(Epoch 7 / 20) train acc: 0.132000; val_acc: 0.127778\n",
      "(Iteration 71 / 200) loss: 2.550961\n",
      "(Epoch 8 / 20) train acc: 0.153000; val_acc: 0.136111\n",
      "(Iteration 81 / 200) loss: 2.509908\n",
      "(Epoch 9 / 20) train acc: 0.160000; val_acc: 0.150000\n",
      "(Iteration 91 / 200) loss: 2.426987\n",
      "(Epoch 10 / 20) train acc: 0.142000; val_acc: 0.161111\n",
      "(Iteration 101 / 200) loss: 2.479956\n",
      "(Epoch 11 / 20) train acc: 0.177000; val_acc: 0.169444\n",
      "(Iteration 111 / 200) loss: 2.397444\n",
      "(Epoch 12 / 20) train acc: 0.213000; val_acc: 0.175000\n",
      "(Iteration 121 / 200) loss: 2.250325\n",
      "(Epoch 13 / 20) train acc: 0.205000; val_acc: 0.180556\n",
      "(Iteration 131 / 200) loss: 2.391808\n",
      "(Epoch 14 / 20) train acc: 0.204000; val_acc: 0.188889\n",
      "(Iteration 141 / 200) loss: 2.356121\n",
      "(Epoch 15 / 20) train acc: 0.223000; val_acc: 0.194444\n",
      "(Iteration 151 / 200) loss: 2.271257\n",
      "(Epoch 16 / 20) train acc: 0.228000; val_acc: 0.205556\n",
      "(Iteration 161 / 200) loss: 2.171636\n",
      "(Epoch 17 / 20) train acc: 0.237000; val_acc: 0.219444\n",
      "(Iteration 171 / 200) loss: 2.415856\n",
      "(Epoch 18 / 20) train acc: 0.257000; val_acc: 0.227778\n",
      "(Iteration 181 / 200) loss: 2.195640\n",
      "(Epoch 19 / 20) train acc: 0.283000; val_acc: 0.247222\n",
      "(Iteration 191 / 200) loss: 2.011973\n",
      "(Epoch 20 / 20) train acc: 0.290000; val_acc: 0.252778\n",
      "(Iteration 1 / 200) loss: 2.302602\n",
      "(Epoch 0 / 20) train acc: 0.118000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.197000; val_acc: 0.175000\n",
      "(Iteration 11 / 200) loss: 2.302597\n",
      "(Epoch 2 / 20) train acc: 0.186000; val_acc: 0.177778\n",
      "(Iteration 21 / 200) loss: 2.302608\n",
      "(Epoch 3 / 20) train acc: 0.185000; val_acc: 0.180556\n",
      "(Iteration 31 / 200) loss: 2.302574\n",
      "(Epoch 4 / 20) train acc: 0.186000; val_acc: 0.180556\n",
      "(Iteration 41 / 200) loss: 2.302592\n",
      "(Epoch 5 / 20) train acc: 0.213000; val_acc: 0.188889\n",
      "(Iteration 51 / 200) loss: 2.302565\n",
      "(Epoch 6 / 20) train acc: 0.198000; val_acc: 0.177778\n",
      "(Iteration 61 / 200) loss: 2.302567\n",
      "(Epoch 7 / 20) train acc: 0.314000; val_acc: 0.238889\n",
      "(Iteration 71 / 200) loss: 2.302577\n",
      "(Epoch 8 / 20) train acc: 0.200000; val_acc: 0.163889\n",
      "(Iteration 81 / 200) loss: 2.302533\n",
      "(Epoch 9 / 20) train acc: 0.220000; val_acc: 0.163889\n",
      "(Iteration 91 / 200) loss: 2.302576\n",
      "(Epoch 10 / 20) train acc: 0.292000; val_acc: 0.241667\n",
      "(Iteration 101 / 200) loss: 2.302564\n",
      "(Epoch 11 / 20) train acc: 0.321000; val_acc: 0.319444\n",
      "(Iteration 111 / 200) loss: 2.302520\n",
      "(Epoch 12 / 20) train acc: 0.397000; val_acc: 0.388889\n",
      "(Iteration 121 / 200) loss: 2.302532\n",
      "(Epoch 13 / 20) train acc: 0.427000; val_acc: 0.383333\n",
      "(Iteration 131 / 200) loss: 2.302459\n",
      "(Epoch 14 / 20) train acc: 0.441000; val_acc: 0.391667\n",
      "(Iteration 141 / 200) loss: 2.302463\n",
      "(Epoch 15 / 20) train acc: 0.441000; val_acc: 0.394444\n",
      "(Iteration 151 / 200) loss: 2.302382\n",
      "(Epoch 16 / 20) train acc: 0.444000; val_acc: 0.391667\n",
      "(Iteration 161 / 200) loss: 2.302384\n",
      "(Epoch 17 / 20) train acc: 0.472000; val_acc: 0.416667\n",
      "(Iteration 171 / 200) loss: 2.302355\n",
      "(Epoch 18 / 20) train acc: 0.432000; val_acc: 0.402778\n",
      "(Iteration 181 / 200) loss: 2.302313\n",
      "(Epoch 19 / 20) train acc: 0.431000; val_acc: 0.405556\n",
      "(Iteration 191 / 200) loss: 2.302249\n",
      "(Epoch 20 / 20) train acc: 0.456000; val_acc: 0.416667\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.078000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.114000; val_acc: 0.091667\n",
      "(Iteration 11 / 200) loss: 2.302578\n",
      "(Epoch 2 / 20) train acc: 0.103000; val_acc: 0.097222\n",
      "(Iteration 21 / 200) loss: 2.302578\n",
      "(Epoch 3 / 20) train acc: 0.092000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302587\n",
      "(Epoch 4 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302557\n",
      "(Epoch 5 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302592\n",
      "(Epoch 6 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302593\n",
      "(Epoch 7 / 20) train acc: 0.115000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302571\n",
      "(Epoch 8 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302603\n",
      "(Epoch 9 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302598\n",
      "(Epoch 10 / 20) train acc: 0.120000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302599\n",
      "(Epoch 11 / 20) train acc: 0.105000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302569\n",
      "(Epoch 12 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302590\n",
      "(Epoch 13 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302613\n",
      "(Epoch 14 / 20) train acc: 0.108000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302569\n",
      "(Epoch 15 / 20) train acc: 0.121000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302552\n",
      "(Epoch 16 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302574\n",
      "(Epoch 17 / 20) train acc: 0.117000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302559\n",
      "(Epoch 18 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302554\n",
      "(Epoch 19 / 20) train acc: 0.091000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302521\n",
      "(Epoch 20 / 20) train acc: 0.114000; val_acc: 0.083333\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.091000; val_acc: 0.125000\n",
      "(Epoch 1 / 20) train acc: 0.086000; val_acc: 0.097222\n",
      "(Iteration 11 / 200) loss: 2.302577\n",
      "(Epoch 2 / 20) train acc: 0.119000; val_acc: 0.100000\n",
      "(Iteration 21 / 200) loss: 2.302578\n",
      "(Epoch 3 / 20) train acc: 0.099000; val_acc: 0.100000\n",
      "(Iteration 31 / 200) loss: 2.302584\n",
      "(Epoch 4 / 20) train acc: 0.104000; val_acc: 0.100000\n",
      "(Iteration 41 / 200) loss: 2.302584\n",
      "(Epoch 5 / 20) train acc: 0.117000; val_acc: 0.100000\n",
      "(Iteration 51 / 200) loss: 2.302598\n",
      "(Epoch 6 / 20) train acc: 0.115000; val_acc: 0.100000\n",
      "(Iteration 61 / 200) loss: 2.302590\n",
      "(Epoch 7 / 20) train acc: 0.083000; val_acc: 0.100000\n",
      "(Iteration 71 / 200) loss: 2.302577\n",
      "(Epoch 8 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 81 / 200) loss: 2.302589\n",
      "(Epoch 9 / 20) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 91 / 200) loss: 2.302578\n",
      "(Epoch 10 / 20) train acc: 0.093000; val_acc: 0.100000\n",
      "(Iteration 101 / 200) loss: 2.302586\n",
      "(Epoch 11 / 20) train acc: 0.090000; val_acc: 0.100000\n",
      "(Iteration 111 / 200) loss: 2.302614\n",
      "(Epoch 12 / 20) train acc: 0.097000; val_acc: 0.100000\n",
      "(Iteration 121 / 200) loss: 2.302620\n",
      "(Epoch 13 / 20) train acc: 0.108000; val_acc: 0.100000\n",
      "(Iteration 131 / 200) loss: 2.302592\n",
      "(Epoch 14 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 141 / 200) loss: 2.302569\n",
      "(Epoch 15 / 20) train acc: 0.105000; val_acc: 0.100000\n",
      "(Iteration 151 / 200) loss: 2.302572\n",
      "(Epoch 16 / 20) train acc: 0.096000; val_acc: 0.100000\n",
      "(Iteration 161 / 200) loss: 2.302575\n",
      "(Epoch 17 / 20) train acc: 0.095000; val_acc: 0.100000\n",
      "(Iteration 171 / 200) loss: 2.302593\n",
      "(Epoch 18 / 20) train acc: 0.115000; val_acc: 0.100000\n",
      "(Iteration 181 / 200) loss: 2.302529\n",
      "(Epoch 19 / 20) train acc: 0.088000; val_acc: 0.100000\n",
      "(Iteration 191 / 200) loss: 2.302588\n",
      "(Epoch 20 / 20) train acc: 0.109000; val_acc: 0.100000\n",
      "(Iteration 1 / 200) loss: 2.302585\n",
      "(Epoch 0 / 20) train acc: 0.094000; val_acc: 0.100000\n",
      "(Epoch 1 / 20) train acc: 0.099000; val_acc: 0.083333\n",
      "(Iteration 11 / 200) loss: 2.302585\n",
      "(Epoch 2 / 20) train acc: 0.122000; val_acc: 0.083333\n",
      "(Iteration 21 / 200) loss: 2.302581\n",
      "(Epoch 3 / 20) train acc: 0.098000; val_acc: 0.083333\n",
      "(Iteration 31 / 200) loss: 2.302577\n",
      "(Epoch 4 / 20) train acc: 0.106000; val_acc: 0.083333\n",
      "(Iteration 41 / 200) loss: 2.302571\n",
      "(Epoch 5 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 51 / 200) loss: 2.302596\n",
      "(Epoch 6 / 20) train acc: 0.095000; val_acc: 0.083333\n",
      "(Iteration 61 / 200) loss: 2.302601\n",
      "(Epoch 7 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 71 / 200) loss: 2.302601\n",
      "(Epoch 8 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 81 / 200) loss: 2.302599\n",
      "(Epoch 9 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 91 / 200) loss: 2.302594\n",
      "(Epoch 10 / 20) train acc: 0.104000; val_acc: 0.083333\n",
      "(Iteration 101 / 200) loss: 2.302581\n",
      "(Epoch 11 / 20) train acc: 0.100000; val_acc: 0.083333\n",
      "(Iteration 111 / 200) loss: 2.302601\n",
      "(Epoch 12 / 20) train acc: 0.103000; val_acc: 0.083333\n",
      "(Iteration 121 / 200) loss: 2.302583\n",
      "(Epoch 13 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 131 / 200) loss: 2.302594\n",
      "(Epoch 14 / 20) train acc: 0.109000; val_acc: 0.083333\n",
      "(Iteration 141 / 200) loss: 2.302576\n",
      "(Epoch 15 / 20) train acc: 0.101000; val_acc: 0.083333\n",
      "(Iteration 151 / 200) loss: 2.302558\n",
      "(Epoch 16 / 20) train acc: 0.125000; val_acc: 0.083333\n",
      "(Iteration 161 / 200) loss: 2.302513\n",
      "(Epoch 17 / 20) train acc: 0.111000; val_acc: 0.083333\n",
      "(Iteration 171 / 200) loss: 2.302536\n",
      "(Epoch 18 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "(Iteration 181 / 200) loss: 2.302523\n",
      "(Epoch 19 / 20) train acc: 0.082000; val_acc: 0.083333\n",
      "(Iteration 191 / 200) loss: 2.302545\n",
      "(Epoch 20 / 20) train acc: 0.119000; val_acc: 0.083333\n",
      "\t best accuracy of 0.9916666666666667, best_model: <scripts.classifiers.fc_net.FullyConnectedNet object at 0x0000023B65AC3EB0>, best_layer: [100, 100], best_learning_rate: 0.001, best_reg: 0.01, best_decay: 1.0, best_weight_scale: 0.1 \n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# find batch/layer normalization and dropout useful. Store your best model in  #\n",
    "# the best_model variable.                                                     #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "layers = [[100, 100], [100, 100, 100], [100, 100, 100, 100]]\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "regs = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "decays = [0.9, 0.95, 1.0]\n",
    "weight_scales = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Перебираем все комбинации гиперпараметров\n",
    "for layer in layers:\n",
    "    for learning_rate in learning_rates:\n",
    "        for reg in regs:\n",
    "            for decay in decays:\n",
    "                for weight_scale in weight_scales:\n",
    "                    model = FullyConnectedNet(layer, input_dim=8*8, reg=reg, weight_scale=weight_scale)\n",
    "                    solver = Solver(model, data, num_epochs=20, batch_size=100, update_rule='adam',\n",
    "                                 optim_config={\n",
    "                                'learning_rate': learning_rate,\n",
    "                                'decay_rate': decay\n",
    "                                }\n",
    "                                )\n",
    "                    solver.train()\n",
    "                    if solver.val_acc_history[-1] > best_accuracy:\n",
    "                        best_accuracy = solver.val_acc_history[-1]\n",
    "                        best_model = model\n",
    "                        best_layer = layer\n",
    "                        best_learning_rate = learning_rate\n",
    "                        best_reg = reg\n",
    "                        best_decay = decay\n",
    "                        best_weight_scale = weight_scale\n",
    "\n",
    "print(f\"\\t best accuracy of {best_accuracy}, best_model: {best_model}, best_layer: {best_layer}, best_learning_rate: {best_learning_rate}, best_reg: {best_reg}, best_decay: {best_decay}, best_weight_scale: {best_weight_scale} \")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best accuracy of 0.9916666666666667, best_layer: [100, 100], best_learning_rate: 0.001, best_reg: 0.01, best_decay: 1.0, best_weight_scale: 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получите оценку accuracy для валидационной и тестовой выборок. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.9916666666666667\n",
      "Test set accuracy:  0.975\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация по мини-батчам\n",
    "\n",
    "Идея нормализации по мини-батчам предложена в работе [1]\n",
    "\n",
    "[1] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", ICML 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте прямой проход для слоя батч-нормализации - функция batchnorm_forward в scripts/layers.py . Проверьте свою реализацию, запустив следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ -2.3814598  -13.18038246   1.91780462]\n",
      "  stds:   [27.18502186 34.21455511 37.68611762]\n",
      "\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  means:  [4.21884749e-17 5.82867088e-17 8.32667268e-19]\n",
      "  stds:   [0.99999999 1.         1.        ]\n",
      "\n",
      "After batch normalization (gamma= [1. 2. 3.] , beta= [11. 12. 13.] )\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:   [0.99999999 1.99999999 2.99999999]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "print('After batch normalization (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.03927354 -0.04349152 -0.10452688]\n",
      "  stds:   [1.01531428 1.01238373 0.97819988]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "for t in range(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте обратный проход в функции batchnorm_backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.6674604875341426e-09\n",
      "dgamma error:  7.417225040694815e-13\n",
      "dbeta error:  2.379446949959628e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "#You should expect to see relative errors between 1e-13 and 1e-8\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измените реализацию класса FullyConnectedNet, добавив батч-нормализацию. \n",
    "Если флаг normalization == \"batchnorm\", то вам необходимо вставить слой батч-нормализации перед каждым слоем активации ReLU, кроме выхода сети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  2.2611955101340957\n",
      "W1 relative error: 1.10e-04\n",
      "W2 relative error: 3.11e-06\n",
      "W3 relative error: 4.05e-10\n",
      "b1 relative error: 4.44e-08\n",
      "b2 relative error: 2.22e-08\n",
      "b3 relative error: 1.01e-10\n",
      "beta1 relative error: 7.33e-09\n",
      "beta2 relative error: 1.89e-09\n",
      "gamma1 relative error: 6.96e-09\n",
      "gamma2 relative error: 2.41e-09\n",
      "\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  5.884829928987633\n",
      "W1 relative error: 1.98e-06\n",
      "W2 relative error: 2.29e-06\n",
      "W3 relative error: 6.29e-10\n",
      "b1 relative error: 5.55e-09\n",
      "b2 relative error: 2.22e-08\n",
      "b3 relative error: 2.10e-10\n",
      "beta1 relative error: 6.65e-09\n",
      "beta2 relative error: 3.39e-09\n",
      "gamma1 relative error: 6.27e-09\n",
      "gamma2 relative error: 5.28e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# You should expect losses between 1e-4~1e-10 for W, \n",
    "# losses between 1e-08~1e-10 for b,\n",
    "# and losses between 1e-08~1e-09 for beta and gammas.\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            normalization='batchnorm')\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  if reg == 0: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите 6-ти слойную сеть на наборе из 1000 изображений с батч-нормализацией и без нее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver with batch norm:\n",
      "(Iteration 1 / 200) loss: 2.324979\n",
      "(Epoch 0 / 10) train acc: 0.093000; val_acc: 0.086111\n",
      "(Epoch 1 / 10) train acc: 0.942000; val_acc: 0.933333\n",
      "(Iteration 21 / 200) loss: 1.116172\n",
      "(Epoch 2 / 10) train acc: 0.964000; val_acc: 0.961111\n",
      "(Iteration 41 / 200) loss: 0.530194\n",
      "(Epoch 3 / 10) train acc: 0.987000; val_acc: 0.969444\n",
      "(Iteration 61 / 200) loss: 0.316331\n",
      "(Epoch 4 / 10) train acc: 0.985000; val_acc: 0.966667\n",
      "(Iteration 81 / 200) loss: 0.141624\n",
      "(Epoch 5 / 10) train acc: 0.992000; val_acc: 0.963889\n",
      "(Iteration 101 / 200) loss: 0.103517\n",
      "(Epoch 6 / 10) train acc: 0.993000; val_acc: 0.980556\n",
      "(Iteration 121 / 200) loss: 0.129526\n",
      "(Epoch 7 / 10) train acc: 0.993000; val_acc: 0.977778\n",
      "(Iteration 141 / 200) loss: 0.105744\n",
      "(Epoch 8 / 10) train acc: 0.992000; val_acc: 0.955556\n",
      "(Iteration 161 / 200) loss: 0.070285\n",
      "(Epoch 9 / 10) train acc: 0.996000; val_acc: 0.972222\n",
      "(Iteration 181 / 200) loss: 0.051985\n",
      "(Epoch 10 / 10) train acc: 0.987000; val_acc: 0.983333\n",
      "\n",
      "Solver without batch norm:\n",
      "(Iteration 1 / 200) loss: 2.302592\n",
      "(Epoch 0 / 10) train acc: 0.123000; val_acc: 0.127778\n",
      "(Epoch 1 / 10) train acc: 0.186000; val_acc: 0.180556\n",
      "(Iteration 21 / 200) loss: 2.254467\n",
      "(Epoch 2 / 10) train acc: 0.255000; val_acc: 0.227778\n",
      "(Iteration 41 / 200) loss: 1.775613\n",
      "(Epoch 3 / 10) train acc: 0.525000; val_acc: 0.508333\n",
      "(Iteration 61 / 200) loss: 1.386529\n",
      "(Epoch 4 / 10) train acc: 0.618000; val_acc: 0.627778\n",
      "(Iteration 81 / 200) loss: 1.130167\n",
      "(Epoch 5 / 10) train acc: 0.704000; val_acc: 0.683333\n",
      "(Iteration 101 / 200) loss: 0.686155\n",
      "(Epoch 6 / 10) train acc: 0.770000; val_acc: 0.758333\n",
      "(Iteration 121 / 200) loss: 0.589071\n",
      "(Epoch 7 / 10) train acc: 0.790000; val_acc: 0.786111\n",
      "(Iteration 141 / 200) loss: 0.520840\n",
      "(Epoch 8 / 10) train acc: 0.772000; val_acc: 0.780556\n",
      "(Iteration 161 / 200) loss: 0.823805\n",
      "(Epoch 9 / 10) train acc: 0.806000; val_acc: 0.750000\n",
      "(Iteration 181 / 200) loss: 0.479706\n",
      "(Epoch 10 / 10) train acc: 0.749000; val_acc: 0.702778\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Try training a very deep net with batchnorm\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims, input_dim=8*8, weight_scale=weight_scale, normalization='batchnorm')\n",
    "model = FullyConnectedNet(hidden_dims, input_dim=8*8, weight_scale=weight_scale, normalization=None)\n",
    "\n",
    "print('Solver with batch norm:')\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True,print_every=20)\n",
    "bn_solver.train()\n",
    "\n",
    "print('\\nSolver without batch norm:')\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируйте процесс обучения для двух сетей. Увеличилась ли скорость сходимости в случае с батч-нормализацией? Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABL4AAATYCAYAAAARTw5LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde3xT9f3H8XdSaEuhLZRbW0RAdEKtiqgoeEEdSh2CbjqVjYnO6UTYRJzzMhXRTXTOiVOHuykq3p3KRWU/vOAVxYkoWHWCFTZoQUBabuWS5PdHmpCkOck5yUlykr6ej4cP7OlJ8k1ykua88/l+vi6fz+cTAAAAAAAAkGPcmR4AAAAAAAAAkAoEXwAAAAAAAMhJBF8AAAAAAADISQRfAAAAAAAAyEkEXwAAAAAAAMhJBF8AAAAAAADISQRfAAAAAAAAyEkEXwAAAAAAAMhJBF8AAAAAAADISQRfAAAANrjwwgvVt2/fhC578803y+Vy2Tsgk5IZNwAAgNMRfAEAgJzmcrlM/bdo0aJMDxUAAAA2c/l8Pl+mBwEAAJAqs2fPDvv5kUce0cKFC/Xoo4+GbT/11FPVs2fPhG9nz5498nq9KigosHzZvXv3au/evSosLEz49hN14YUXatGiRfr666/TftsAAACp1i7TAwAAAEilcePGhf383nvvaeHCha22R9qxY4eKiopM30779u0TGp8ktWvXTu3a8bEMAADAbkx1BAAAbd5JJ52k6upqffjhhzrxxBNVVFSk66+/XpI0Z84cjRo1SpWVlSooKFD//v116623yuPxhF1HZK+sr7/+Wi6XS3/4wx/017/+Vf3791dBQYGOPvpoffDBB2GXjdbjy+VyadKkSXrhhRdUXV2tgoICHXLIIVqwYEGr8S9atEhHHXWUCgsL1b9/f/3lL39Jqm/Y9u3bddVVV6l3794qKCjQwQcfrD/84Q+KnCiwcOFCHX/88ercubM6deqkgw8+OPi4Bdx777065JBDVFRUpC5duuioo47S448/ntC4AAAArOKrRQAAAEmbNm3S6aefrvPPP1/jxo0LTnucNWuWOnXqpClTpqhTp0567bXXdNNNN6mpqUl33nln3Ot9/PHHtXXrVv385z+Xy+XS73//e/3gBz/QV199FbdK7O2339Zzzz2nyy+/XMXFxfrTn/6ks88+W2vWrFHXrl0lSR999JFqampUUVGhadOmyePx6JZbblH37t0Tehx8Pp/GjBmj119/XRdffLEGDRqkf/3rX7r66qu1du1a3X333ZKkTz/9VGeccYYOO+ww3XLLLSooKNDKlSv1zjvvBK/rb3/7m375y1/qnHPO0RVXXKHm5mZ98sknev/99/WjH/0oofEBAABYQfAFAAAgqaGhQQ888IB+/vOfh21//PHH1aFDh+DPl112mS677DL9+c9/1m9/+9u4Pb3WrFmjL7/8Ul26dJEkHXzwwTrzzDP1r3/9S2eccUbMy3722Weqra1V//79JUknn3yyDj/8cD3xxBOaNGmSJGnq1KnKy8vTO++8o8rKSknSueeeq4EDB1p7AFrMnTtXr732mn7729/qN7/5jSRp4sSJ+uEPf6h77rlHkyZNUv/+/bVw4ULt3r1bL7/8srp16xb1ul588UUdcsgheuaZZxIaCwAAQLKY6ggAACCpoKBAF110UavtoaHX1q1btXHjRp1wwgnasWOHPv/887jXe9555wVDL0k64YQTJElfffVV3MuOGDEiGHpJ0mGHHaaSkpLgZT0ej1555RWdddZZwdBLkg488ECdfvrpca8/mpdeekl5eXn65S9/Gbb9qquuks/n08svvyxJ6ty5syT/VFCv1xv1ujp37qz//e9/raZ2AgAApAvBFwAAgKRevXopPz+/1fZPP/1U3//+91VaWqqSkhJ179492Bi/sbEx7vXuv//+YT8HQrBvv/3W8mUDlw9cdsOGDdq5c6cOPPDAVvtF22bG6tWrVVlZqeLi4rDtgQqy1atXS/IHescdd5x+9rOfqWfPnjr//PP19NNPh4Vg11xzjTp16qQhQ4booIMO0sSJE8OmQgIAAKQawRcAAIDCK7sCtmzZouHDh+vjjz/WLbfconnz5mnhwoW64447JMmw0ilUXl5e1O2RjeLtvmyqdejQQW+++aZeeeUV/eQnP9Enn3yi8847T6eeemqw8f/AgQP1xRdf6Mknn9Txxx+vf/7znzr++OM1derUDI8eAAC0FQRfAAAABhYtWqRNmzZp1qxZuuKKK3TGGWdoxIgRYVMXM6lHjx4qLCzUypUrW/0u2jYz+vTpo3Xr1mnr1q1h2wPTOvv06RPc5na79d3vfld//OMfVVtbq9/97nd67bXX9Prrrwf36dixo8477zw99NBDWrNmjUaNGqXf/e53am5uTmh8AAAAVhB8AQAAGAhUXIVWWO3evVt//vOfMzWkMHl5eRoxYoReeOEFrVu3Lrh95cqVwV5cVn3ve9+Tx+PRfffdF7b97rvvlsvlCvYO27x5c6vLDho0SJK0a9cuSf6VMkPl5+erqqpKPp9Pe/bsSWh8AAAAVrCqIwAAgIFhw4apS5cuGj9+vH75y1/K5XLp0UcfdcRUw4Cbb75Z//d//6fjjjtOEyZMCIZW1dXVWrZsmeXrGz16tE4++WT95je/0ddff63DDz9c//d//6c5c+Zo8uTJwWb7t9xyi958802NGjVKffr00YYNG/TnP/9Z++23n44//nhJ0mmnnaby8nIdd9xx6tmzpz777DPdd999GjVqVKseYgAAAKlA8AUAAGCga9eumj9/vq666irdcMMN6tKli8aNG6fvfve7GjlyZKaHJ0k68sgj9fLLL+tXv/qVbrzxRvXu3Vu33HKLPvvsM1OrTkZyu92aO3eubrrpJj311FN66KGH1LdvX91555266qqrgvuNGTNGX3/9tR588EFt3LhR3bp10/DhwzVt2jSVlpZKkn7+85/rscce0x//+Edt27ZN++23n375y1/qhhtusO3+AwAAxOLyOekrSwAAANjirLPO0qeffqovv/wy00MBAADIGHp8AQAAZLmdO3eG/fzll1/qpZde0kknnZSZAQEAADgEFV8AAABZrqKiQhdeeKEOOOAArV69WjNnztSuXbv00Ucf6aCDDsr08AAAADKGHl8AAABZrqamRk888YQaGhpUUFCgoUOH6rbbbiP0AgAAbR4VXwAAAAAAAMhJ9PgCAAAAAABATiL4AgAAAAAAQE7Kih5fXq9X69atU3FxsVwuV6aHAwAAAAAAgAzx+XzaunWrKisr5XbHrunKiuBr3bp16t27d6aHAQAAAAAAAIf473//q/322y/mPlkRfBUXF0vy36GSkpIMjwYAAAAAAACZ0tTUpN69ewfzoliyIvgKTG8sKSkh+AIAAAAAAICpdlg0twcAAAAAAEBOIvgCAAAAAABATiL4AgAAAAAAQE4i+AIAAAAAAEBOIvgCAAAAAABATiL4AgAAAAAAQE4i+AIAAAAAAEBOapfpAbRFHq9PS+o2a8PWZvUoLtSQfmXKc7syPSwAAAAAAICcQvCVZgtW1GvavFrVNzYHt1WUFmrq6CrVVFdkcGQAAAAAAAC5hamOabRgRb0mzF4aFnpJUkNjsybMXqoFK+ozNDIAAAAAAIDcQ8VXmni8Pk2bVyufJLe8GuL+XD20RRvUWUu8A+SSNHfO0zrNWyl3cbnUZ5j/gqvflbatlzr1NN7mzpO8HnP7Wrm8O6/1HTHaz+zlAQAAAAAA0oTgK02W1G1WfWOzRrqXaGr7R1Tp2hz83WZfJ0lS2Z5t0nMtGzt0keSSdu7bL+q2kkqp+hxpxbNS07rY+1q5fEmldNp0qWPXfWHWjk3Sv65rvZ/Zy9sR3BGmAQAAAAAAk1w+n8+X6UHE09TUpNLSUjU2NqqkpCTTw0nInGVr9dLTf9XM9jMkSaG97APPgCvX+9snG9wZhWkEZwAAAAAAtBlWciKCrzRZ/OUG9Zl9jMq1WSzgaCOrwVnNHVLVmLQPEwAAAAAA2MNKTsRUxzQZkve58kKmN8ImTeukd//UevvOb6PsWy89/RPppOulrv2pAgMAAAAAIMcRfKVJ3vYNmR4C1FLcuOi2fZtiTZ8EAAAAAABZjeArXTr1zPQIEE3TOunZ8eHbmBIJAAAAAEBOIPhKlz7D/IFKU72ClUdwpqZ66ekLpHNmUQkGAAAAAEAWI/hKF3eev4ro6QskuUT45WQtz80/L5J83n2bqQQDAAAAACCruDM9gDalaox07iNSSUX49g5l/v8S2VbSSxr2S38oY+flzUr28k4WGnpJ+5rjL7pDWv6sVPeW5PX4/6t7K3wbAAAAAADIOJfP53N86ZGVZSqzgtcjrX43fAqdlPg2d15y1xnt8js2Sf+6zt8DK6Ckl3TabdGn/5m5fCB027k5/Dqrz5ZWPBt/Xyfq0EWSK+I+URkGAAAAAECqWMmJCL5gLFqYZqXHldkwzmxwZxTGOS44c/n/OfcRwi8AAAAAAGxG8IXcZRTGxQvONq2SFk1vuZJ0HPIuqbhC+v4D0vZvaI4PAAAAAIBNCL6ykMfr05K6zdqwtVk9igs1pF+Z8tyuTA8rt9TOlRZcE14Zlk5MgQQAAAAAIGkEX1lmwYp6TZtXq/rG5uC2itJCTR1dpZrqihiXhGVmepG53K0b29uiZTXPk66XuvanCgwAAAAAgAQQfGWRBSvqNWH20laT7wK1XjPHDSb8SrVoYdgzF7b8MsUvD6rAAAAAAACwxEpO5E7TmBCFx+vTtHm1UaOVwLZp82rl8To+m8xu7jyp3wnSoef4/z3kLH9j+pI0BI5N9dLTF/inYQIAAAAAAFsRfGXQkrrNYdMbI/kk1Tc2a0ldJlYmbOOqxkiTV0jj50tn/8M/PVEu7avFs0tLqLngWn/lGQAAAAAAsE27TA+gLduw1Tj0SmQ/2CxQCRbQY2Dr5vgdyvz/7kwmnPRJTWv90y1Dbw8AAAAAACSF4CuDehQX2rofUqxqjDRgVHg/sD7D/L8LbCvqJs2Z4J/CaLU/2Lb1tg8ZAAAAAIC2jOArg4b0K1NFaaEaGpujRiQuSeWlhRrSryzdQ4ORyCqwgNBtNXf4+3YFVnE0q1PPZEcHAAAAAABC0OMrg/LcLk0dXSWpdeeowM9TR1cpz213XymkVNUYi83xXVJJr33VYwAAAAAAwBYun8/n+CUDrSxTmY0WrKjXtHm1YY3uK0oLNXV0lWqq07CyIFLD69k3BXLTKmnR9JZfhL7kWqrCTrpe6tp/3/RJd14GBgwAAAAAgPNZyYkIvhzC4/VpSd1mbdjarB7F/umNVHrlmNq55prjl1T6p0tWjWl9HaFhGiEZAAAAAKANIvgCnCpqFVjkS7Al8Dz3kfDwK1pwFiskAwAAAAAgB1nJiejxBaRToDn+Id+Xls5S9Ob3LdsWXOsPyiR/6PX0BeGhl+RfPfLpC/y/BwAAAAAAYQi+gExY/W7rECuMT2pa69/P6/FXepkNyQAAAAAAgCSCL0fzeH1avGqT5ixbq8WrNsnjdfysVJi1bb35/ayEZAAAAAAAIKhdpgeA6FjpMcd16ml+PyshGQAAAAAACKLiy4EWrKjXhNlLw0IvSWpobNaE2Uu1YEV9hkYG2/QZ5m9ML6OVO11SSS//flZCMgAAAAAAEETw5TAer0/T5tXG6uakafNqmfaY7dx5/tUYJbUOv1p+rrndv5+VkAwAAAAAAAQRfDnMkrrNrSq9Qvkk1Tc2a0nd5vQNCqlRNUY69xGpJGLqakmlf/uAUVLdW9Knz0uDL2z5ZZyQDAAAAAAABNHjy2E2bDUOvRLZDw5XNcYfcK1+19+jq1NPf+XW5y9KM6rDm9p36CLJJe0MCT1LKv2hV9WYtA8dAAAAAACnI/hymB7FhbbuhyzgzpP6nbDv59q50tMXSJETXndu8W876Xqpa/99IRmVXgAAAAAAREXw5TBD+pWporRQDY3NUft8uSSVlxZqSL+ydA8N6eD1SAuuUavQS2rZ5pKWPixNXk7gBQAAAABAHPT4cpg8t0tTR1dJMuzmpKmjq5TnNmp0jqy2+t3w6Y2t+KSmtf79AAAAAABATARfDlRTXaGZ4warvDR8OmN5aaFmjhusmuoKg0si621bb+9+AAAAAAC0YUx1dKia6gqdWlWuJXWbtWFrs3oU+6c3UumV4zr1tG8/r6d103yp9TamTAIAAAAAchTBl4PluV0a2r9rpoeBdOozzL9SY1O9ovf5cvl/HwixjNTO9fcKM7Uq5B2sCgkAAAAAyElMdQScxJ3nD6IkGXZ5q7k9vErL65Hq3pKWP+v/99MX/KtCRvYK2/lteOgl+QO2py/wB2UAAAAAAOQYKr4Ap6kaI537SOuKrZJKf+gVWp0VrbLL5Vb0arFoWlaKXHCtNGAU0x4BAAAAADmF4Atwoqox/iAqVj+u2rn+aq3IkMvntXhjIStF9jsh2ZEDAAAAAOAYlqY6Tp8+XUcffbSKi4vVo0cPnXXWWfriiy/iXu6ZZ57RgAEDVFhYqEMPPVQvvfRSwgMG2gx3nj+IOvQc/7+R0xsXXCPzlV0mfDbXP1XS67HvOgEAAAAAyCBLwdcbb7yhiRMn6r333tPChQu1Z88enXbaadq+fbvhZd59912NHTtWF198sT766COdddZZOuuss7RixYqkBw+0Wavfbd3DK1lL/io9fIY0o5qeXwAAAACAnODy+XwJl4x888036tGjh9544w2deOKJUfc577zztH37ds2fPz+47dhjj9WgQYP0wAMPmLqdpqYmlZaWqrGxUSUlJYkONyd4vD4tqdusDVub1aO4UEP6lSnPHdkEHTlv+bPSPy9O0ZW3HE/nPsJqjwAAAAAAx7GSEyXV46uxsVGSVFZWZrjP4sWLNWXKlLBtI0eO1AsvvGB4mV27dmnXrl3Bn5uampIZZs5YsKJe0+bVqr6xObitorRQU0dXqaa6IoMjQ9p16pnCK4/R8N7rid13zOp+AAAAAACkUMLBl9fr1eTJk3XcccepurracL+Ghgb17Bl+kt6zZ081NDQYXmb69OmaNm1aokPLSQtW1GvC7KWtOjo1NDZrwuylmjluMOFXW9JnmH+Vx6Z6Gfb5crnDG913aAmod242cQNRGt5HW0GypFKquSP+SpPR9gMAAAAAIMUs9fgKNXHiRK1YsUJPPvmkneORJF133XVqbGwM/vff//7X9tvIJh6vT9Pm1UaNNwLbps2rlcdrY6NzOJs7zx8kSQpOTQxy+f87+yFp/Hzp7H/4/716pf+/8fOlIZeau51t6/3/BlaQjOwr1lTv3x7oCWZ2PwAAAAAA0iCh4GvSpEmaP3++Xn/9de23334x9y0vL9f69evDtq1fv17l5eWGlykoKFBJSUnYf23ZkrrNYdMbI/kk1Tc2a0mdmUoe5IyqMf4+XCURlX4llf7t1We1XhUysFLkQJOVV516xllBsmXbgmulvbvN7ceqkQAAAACANLE01dHn8+kXv/iFnn/+eS1atEj9+vWLe5mhQ4fq1Vdf1eTJk4PbFi5cqKFDh1oebFu1Yatx6JXIfsghVWP8fbis9tOKO1XS5f99n2EmVpBsmRb5wd/M7Rc6fRIAAAAAgBSyFHxNnDhRjz/+uObMmaPi4uJgn67S0lJ16NBBknTBBReoV69emj59uiTpiiuu0PDhw3XXXXdp1KhRevLJJ/Xvf/9bf/3rX22+K7mrR3Ghqf02bt2lOcvWstpjWxOo4rJ6mZo7/NMP5VJ4+NVy3NTc7t9v2/ooVxDFt1+b28/s9QEAAAAAkCRLwdfMmTMlSSeddFLY9oceekgXXnihJGnNmjVyu/fNoBw2bJgef/xx3XDDDbr++ut10EEH6YUXXojZEB/hhvQrU0VpoRoam43amMvtkm598bPgz6z2iLgCUyWjNqK/fV8jerMrSHbpa26/lK5ICQAAAADAPi6fz+f4juhNTU0qLS1VY2Njm+33FVjVUTJcwy9MoNaL1R4Rl9cTe6qk1yPNqI4/LfKXy6Q/HR5/v8nL40/FBAAAAADAgJWcKOFVHZFeNdUVmjlusMpLw6c9Gs1mZLVHmBaYKhnaBD/y9zFXkJS/Qqxdvrn9CL0AAAAAAGlCxVeW8Xh9WlK3WRu2Nmvj1l1h0xuNPHHJsRrav2saRoecVjs3yrTIXuHTIq3sBwAAAABAAqzkRJZ6fCHz8tyuYIg1Z9laU5d5eUW9JNHwHtZEToEcMMrcCpKJrjQJAAAAAIDNCL6ymNnVHh9ZvFqPLF5Nw3uYF7Vqq9I/ldFM1VYiK00CAAAAAGAzenxlscBqj2ZruBoam3XZ7KW655X/aM6ytVq8ahP9v9Ba7Vzp6QvCQy/J37T+6Qv8vwcAAAAAIAvQ4yvLWV3tMRJVYAgTXMFxncEOrMwIAAAAAMgsVnVsQ4xWezSrobFZE2Yv1YKWPmBo41a/GyP0kiSf1LTWvx8AAAAAAA5Hj68cUFNdoVOryrWkbrNeXlGvRxavNn1ZnySXpGnzanVqVTnN79u6bevt3Q+JiVxYgMUBAAAAACAhBF85InS1RyvBl+QPv+obm7WkbnPwOtBGdepp736wzurCAoRkAAAAAGCI4CvHBBreNzQ2W+75tWFrc0rGhCzSZ5g/ZGmqV/SucS09vvoMS/fI2obAwgKRj31gYYFzHwkPv5JdfRMAAAAAchw9vnJMntulqaOrJMn0ao8BPYoT6xOGHOLO84cmklofQS0/19xORVEqeD3+ECtq4NiybcG1/v0kVt8EAAAAABMIvnKQ1Yb3LvlXdxzSryy1A0N2qBrjrywqiVjps6SydcUR7GNlYQGrIRkAAAAAtFFMdcxRoQ3vN2xt1tcbd2jGK/+RFH6qHKjpmTq6isb22KdqjDRgFL2j0snKwgJWQrJ+J9gyPAAAAADIRgRfOSy04b0kHVzeSdPm1aq+cV8vr/LSQk0dXaWa6opoV4G2zJ1nf2hCI3ZjVhYWYPVNAAAAADCF4KsNiawC61Hsn95IpRfSgkbsscVdWEBSUTdpa720/Rtz18nqmwAAAADaOJfP57O6+F/aNTU1qbS0VI2NjSopKcn0cABYZbRaYWCyLb3D/IKPk2QYfgW43JLPa/RLf4g2eTkVdQAAAAByjpWciOb2AFKLRuzmGS0sEE2s0Eti9U0AAAAAEMEXgFSz0ogd/vBr8gpp/HzpB3/zT2+MxRXxNs7qmwAAAAAQRI8vAKmV6kbsudgwP7CwQN1b0o6Nsff1eaWRt/nve67cfwAAAACwCcEXJEker4+m90gNK6sVBpgNs3K9Yb7ZMLBTT+nQc1I7FgAAAADIQgRf0IIV9Zo2r1b1jc3BbRWlhbpx1EB16VhAGIbkxF2tsKURe59h/h/NhllGDfOb6v3bc2G6XyKhIQAAAAAgiFUd27gFK+o1YfbSeOvHSfKHYVNHV6mm2kTjbSCU4WqFEas6ml390euRZlTH6B2WI6saBu9nnNAw2+8nAAAAAFjAqo4wxeP1adq8WlOhlyQ1NDZrwuylWrCiPqXjQg4yWq0wtBG7ldUf20rDfHeev9JNUjD8C2L1RgAAAACIh6mObdiSus1h0xvj8cl/qj1tXq1OrSpn2iOsqRojDRhl3LvLSpiV6ob5ThIIDaNO/7w9+6dzAgAAAEAKEXy1YRu2mg+9AnyS6hubtaRus4b272r/oJDbAqsVRmMlzGprva/ihYYAAAAAgKgIvtqwHsWFCV82kdAMiMlKmGW1YX4uiBUaAgAAAACiosdXGzakX5kqSgtbdQ4yI5nQDAjj9Uh1b0lb66WibmrdyyrAJZX02lfpFLP3lU8aPF769Hn/dXs9KRs+AAAAAMC5qPhqw/LcLk0dXaUJs5cGooK4XJLKSws1pF9ZikeHrBFoNp/IFLzaua17V0UVpZG7Ue+rDl38/y66bd+2kkp/UEY/LAAAAABoU1w+n8/son4ZY2WZSli3YEW9ps2rjdvoPlBXM3PcYNVUV8TcF21EtODKbMhUO1d6+gKZilxLehk3cg8N3jatkhZNj3KdLUdvYAVJAAAAAEDWspITEXxBkuTx+rSkbrM2bG1Wj+JCfbt9t259MTwMqygt1NTRVYRe8DMMrkyETF6PNKM6dqVXUTepZrpUXGGuiizudbb0/Zq8PPGm8MlUtwEAAAAAbGElJ2KqIyT5pz1GrtI4sro8LAwb0q9Mee5EOoIh53g9/kqvqNVaPkkuacG1/pUIowVDq9+NP71xx0Z/6GW2oXvc6/RJTWv9+yXSJD6Z6jYAAAAAQEYQfMFQtDAMkJR8yLRtvbnbMbtfqq4zwKi6ranev50plAAAAADgSKzqCMC6ZEOmTj3NXd7sfqm6TslEdZv81W2sHAkAAAAAjkPwBcC6ZEOmPsP80wRlNHXW5W9o32eY+TGl4jola9VtAAAAAABHIfgCYF2yIZM7z98bK7Bv5GUl/yqOkf3BvB6p7i1p+bP+f0OrrBK9znhSOYUSAAAAAJBSBF8ArLMjZKoa4++NVRKxSmhJZfSeWbVz/as2PnyG9M+L/f/OqPZvT/Q6zUikui1aQBcrtAMAAAAApITL5/NFa1zjKFaWqQSQRlFXOuzlD73Mhkxej3+a4Lb1/vCoz7DWgZlRc/lAyBYZapm5TrO8Hn/A1lQf5fZbxlBSKU1e7r+NaI9Jhy7+/XZu3rct3StC2vmYAAAAAEAGWcmJCL4AJCfVgUoweDLqsxURPKVCMHiTwsOviODNMKCLxiC0S4WoAWWagzcAAAAAsImVnIipjgCS486T+p0gHXqO/1+7wycnNJc3M4Uy5uqP0aRpRchAGBf5GDbV+7eHThUFAAAAgBzTLtMDAICY7Gwub7Y6Ldp+VWOkAaOMLx83oIsmJLTrd4LFy5oQM4zzSXJJL18jFZZK27/Zd58kpkUCAAAAyAkEX7DE4/VpSd1mbdjarB7FhRrSr0x5bqOV/QAbJNJcPhqz0/3i7WcUUCWzqmOqVoQ0Uy23dZ30SMj9d0I/MgAAAACwCcEXTFuwol7T5tWqvrE5uK2itFBTR1epproixiWBJPQZ5g9e4jWXD1QqRWPUeysw3S9ej67I/aIxG9DZfdlYEgnUdn7bepuZ+w8AAAAADkSPL5iyYEW9JsxeGhZ6SVJDY7MmzF6qlz5Zp8WrNmnOsrVavGqTPF7Hr5mAbOHO81cbSQo2hA9q+bnmduOpeHGn+8nfZ2vvbnP7GfXjCgR0rcYYi8u/Cmas0C4ZtgVqaepHBgAAAAA2o+ILcXm8Pk2bVxsrDtCkJz5SaNZFJRhsFWguH3UK4u2xq5DMNsefM8l8E/1o0x0DAd3TF8gffsULf02EdokK9CjbWi8VdZN2bDIxnnhS3I8MAAAAAFKA4AtxLanb3KrSK1JkgVegEmzmuMGEX7BHvObyRsxO91v+lLn9Iq8vshH+D2dJ/7ouPETrUOb/t1XfrDihXSKi9SizU6r6kQEAAABAChB8Ia4NW2OHXtEEcrDrn1+unXu8Ki+hET5s4M6zXm1kd/+s0OszaoR/2nSpY9fwgE5K/UqJRj3K7JSqfmQAAAAAkAIEX4irR3FhwpfdvH2PrnxqmSSmPyJD4jbHNyuiiX6sRvjPXuifmnnoOeG/S+UUwZi9zFoUdZNqpksde0hzJlh8TEwsIgAAAAAADkNze8Q1pF+ZKkoLLbXsjiYw/XHBinpbxgWYErM5vlkR/bjMNsw30wje65Hq3pKWP+v/N9Hm8XF7mUnasVEqrpD6n2TxMUlhPzIAAAAASCGCL8SV53Zp6ugqSYnHBtK+iGDavFpWfUR6BZrjlyRYbVhS6b98oB+X2Yb5q9+Nfb21c6UZ1dLDZ0j/vNj/74xq/3arzPbeCuxn9Jh0KNvXkywg8v4DAAAAQJZgqiNMqamu0MxxgzVtXm1Yo3u3q3Vj+1h8kuobm7WkbrOG9u9q/0ABI6HN8evekN68M/5lTrhaOmB4635cVkOmaGJNlXz6AutBk9neW6H7GS0YIJnvRxbZ3D8VvcsAAAAAIEEEXzCtprpCp1aVa0ndZm3Y2qwexYX6dvtuTXx8qSRr3ZNebpnuSMN7pFWgOX6fYdKyx2L0uGrpZ3XyddFDnERCplBxp0q6/FMlB4wyHyLF7WVm0KPLaMEAM/3IjJr719xBdRgAAAAAR2CqIyzJc7s0tH9XnTmol4b276rvHeavBCsvtdYA/5HFqzX2b+/p+Dteo+cX0i9m3y8T/awCIZPh5F+XVNLLuBG8XVMlQyV7n6wKVKxF3o9AxVoi0zUBAAAAwGYEX0haTXWF3r7mFD1xybG6+9zDVdYx33QvMBreI2OMelyZ6WeVbMhkx1TJaJK5T/GENuFftci+5v4AAAAAkEJMdYQtApVgktQhP08TZi+VS/GnP7ZM6tK0ebU6taqcaY9IL6MeV2aqogIhU9SpfrfHDpmSnSoZb1yJ3icj0aY0xhRSsWZmyiQAAAAApAjBF2xn1AjfCA3vkVFGPa7MSDRkSrQfl1nJ3KdIRk34zbBasQYAAAAANiP4QkqENsJ/eUW9Hlm8Ou5lNmyNH5IBjpNIyBSYKvn0BVKr2sgU9ONKVMwm/CYkUrEGAAAAADay3OPrzTff1OjRo1VZWSmXy6UXXngh5v6LFi2Sy+Vq9V9DQ0OiY0aWCEx/PL26Iv7OknoUW2uQD2S1VPbjskvcJvxG4jT3BwAAAIA0sVzxtX37dh1++OH66U9/qh/84AemL/fFF1+opKQk+HOPHj2s3jSy1JB+ZaooLVRDY7PRpC6VlxZqSL+ydA8NyKxU9OOyU0JTFR1UsQYAAACgzbMcfJ1++uk6/fTTLd9Qjx491LlzZ8uXQ/bLc7s0dXRV1Ib3gVb2U0dX0dgebZOd/bjslshURTPN/QEAAAAgTdLW42vQoEHatWuXqqurdfPNN+u4444z3HfXrl3atWtX8OempqZ0DBEpZNTwvry0UFNHV6nG5HRIAGlkpgl/cYX0/Qek7d84r2INAAAAQJuX8uCroqJCDzzwgI466ijt2rVLf//733XSSSfp/fff1+DBg6NeZvr06Zo2bVqqh4Y0C214v2Frs3oU+6c3UukFOJSZJvyn3yEdMDwDgwMAAACA+Fw+ny/B5bokl8ul559/XmeddZalyw0fPlz777+/Hn300ai/j1bx1bt3bzU2Nob1CQMApEHtXP/qjqGN7kt6MaURAAAAQEY0NTWptLTUVE6UtqmOoYYMGaK3337b8PcFBQUqKChI44gAAIac3oQfAAAAAAxkJPhatmyZKiro6QQAWSNVTfi9HgI1AAAAACljOfjatm2bVq5cGfy5rq5Oy5YtU1lZmfbff39dd911Wrt2rR555BFJ0owZM9SvXz8dcsgham5u1t///ne99tpr+r//+z/77gUAIPtEnUJZ6e8rxhRKAAAAADawHHz9+9//1sknnxz8ecqUKZKk8ePHa9asWaqvr9eaNWuCv9+9e7euuuoqrV27VkVFRTrssMP0yiuvhF0HACCHmKniqp3b0jQ/os1kU71/+7mPEH4BAAAASFpSze3TxUrTMuQOj9dnegVIK/sCSCEzVVxejzSjOnyfMC7/ZSYvZ9ojAAAAgFYc39weiGfBinpNm1er+sbm4LaK0kJNHV2lmuqKhPcFkEJmq7hWvxsj9JL/8k1r/fuloq8YAAAAgDbDnekBAJK/Ymvxqk2as2yt7nnlS02YvTQsyJKkhsZmTZi9VAtW1Ae3LVhRb3pfACnk9fgrvSJDL2nftgXX+vfbtt7cdZrdDwAAAAAMUPGFjItWsRVN4HT6+ueXa+cer3p0KtDNc2sNT7NdkqbNq9WpVeVMewRSzUoVV6ee5q7T7H4AAAAAYIDgCxkVqNiy0mhu8/Y9uvKpZXH380mqb2zWkrrNGtq/a6JDBGCG2eqsz+ZKB4/y9/Bqqlf0CrGWHl99htk5QgAAAABtEMEXMsbj9WnavOgVW3basDV2JRkAG5itzlryV/9/HbpoX21m6LtAS3Vmze00tgcAAACQNHp8IWOW1G2OO73RDj2KC1N+G0Cb12eYv0pLJqcV79zi/7dDl/DtJZX7muADAAAAQJKo+ELGpLoSyyWpvLRQQ/qVpfR2AMhfnVVzR8uqjpFVXNG0VHu1K5QumCtt/8ZfNdZnGJVeAAAAAGxDxRcyJpWVWIHT7vOP7q35n6zT4lWb5PGmelIl0MZVjfFXa5VUmLyAT9q6TnK5pUPPkfqdQOgFAAAAwFZUfCFjhvQrU0VpoRoam23v81Va1F6SdPcrXwa3VZQWauroKtVUmz0pB2BZ1RhpwCj/6o2fzfX384onsjG+1+O//Lb1VIEBAAAASAoVX8iYPLdLU0dXSWrdFSjw85UjDtLd5x6uso75hp2DXJLKSwr02M+O0T3nD9KVI76jxh17tGXHnrD9GhqbNWH2Ui1YUW/n3QAQyZ3nr94aaLJPV2hj/Nq50oxq6eEzpH9e7P93RrV/OwAAAABYRPCFjKqprtDMcYNVXho+7bG8tFAPjBusK0Z8R98fvJ9u+361JOOA7OYxh+i4A7vpjMMq9eQHa6JWkAW2TZtXy7RHIB3iNrx3SSW9/PtJ/nDr6QukpnXhuzXV+7dHhl9ej1T3lrT8Wf+/Xo/d9wAAAABAlmOqIzKuprpCp1aVa0ndZm3Y2qwexf6G9HluV9g+M8cN1rR5tWErQZZHTF+Mt1KkT1J9Y7OW1G3W0P5dU3afAChOw/uW13fN7f79vB5pwTWK3hS/pRH+gmv90yjdef4QbME14SFZSaX/9lgREgAAAEALgi84Qp7bFTeIMhOQmV0pMtUrSgJoEWh4HzWkun1fSLX63daVXmF8UtNa/347v20J0yJCskBl2LmPEH4BAAAAkETwhSwTLyAzu1JkKleUBBAhtOG9UcP6yAb3RmpfkD59QaYrwxJBc30AAAAgZxB8IafEWynSJf/0yCH9ytI9NKBtCzS8NxLa4D6WD/4eZ4eQyrBYt2eEKZQAAABATqG5PXKKmZUip46uCpseCcAB4jbCt8hsBVkoq831AQAAADgewRdyTqyVImeOGxxshA/AQQKN8CXZEn6ZrSALiNtcX/4plKwcCQAAAGQVpjoiJ5lphA/AYYwa4Vvi8leO9Rlm7WJWmusnMoUSAAAAQEYQfCFnmVkpUpI8Xh8BGeAUoY3wP5srLfmrhQu3vG5rbrfejN7s1MhEplACAAAAyBiCL7RpC1bUa9q8WtU3Nge3VZQWauroKqZEApkS2gjfSvBVUukPvRJpQm92aqTVKZQAAAAAMorgC21KaHXX1xt3aMYr/2nV0aehsVkTZi+lHxiQaYGG9031it57S1JRN6lmulRc4d/faqWX6dtKcAol/LwefxXftvX+8DCZ5woAAACwgOALbUa06q5ofPJPmJo2r1anVpUz7RHIlEDD+6cvkP9VGRpItbwuz7g7sQqvRG4rkSmU8K+GGdm3raTS/3jb8dwBAAAAMbCqI9qEBSvqNWH20rihV4BPUn1js5bUbU7twADEFmh4XxJRfVlS6d9uZ3CSzttqK2rn+sPEyIUDmur922vnZmZcAAAAaDOo+ELO83h9mjav1miiVEwbtpoLygCkUGjD+1RPlUvnbeU6r8df6RX13beltnbBtf7Hm8cXAAAAKULwhZy3pG6z6UqvSD2KC20eDYCEhDa8z6XbymWr321d6RXGJzWt9e/H4w0AAIAUIfhCzkukasslqby0UEP6ldk/IABoC7att3c/AAAAIAEEX8h5Vqu2Aq3sp46uorE9ACSqU0979wMAAHASVq3OGgRfyHlD+pWporRQDY3Npvp8lZcWauroKtVUV8TfGQAQXZ9h/oUBmuoVvc+Xy//7PsPSPTIAAIDksGp1ViH4Qs7Lc7s0dXSVJsxeKpfCT78CP1854iD17dZRPYr90xup9AKAJLnz/B/+nr5AivruK6nmdr4ZBQAA2SWwanXkF3uBVatZDdxxXD6fL5HF7tKqqalJpaWlamxsVElJSaaHgyy1YEW9ps2rDWt0X0F1F9A2UIqeOVG/Ee3lD734UAgAALKJ1yPNqI6xgE9LRfvk5XzWTDErOREVX2gzaqordGpVuZbUbdaGrc1UdwFtBaXomVU1RhowiuAxUwh9AQCwD6tWZyWCL7QpeW6XhvbvmulhAEgXu0rRCQ+S487jw18mEPoCAGAvVq3OSgRfAIDcEgipttZLC65T9MbqPkkuacG1/mqkWCEW4QGyEf1HAACwH6tWZyV3pgcAAIBtauf6+y48fIb03CXSjo0xdg4pRY91fU9f0LqkPRAe1M61ZdiArbwef1hrGPrKH/p6PekcFQAA2S+warWM2uW4/L1MWbXaUQi+AAC5wSikiseoFJ3wANnKSv8RAABgXmDVakmtwy9WrXYqgi8AQPaLGVLFYVSK7qTwwOuR6t6Slj/r/5ewDbHQfwQAgNSpGuNvGVBSEb69pJJWAg5Fjy8AQPaLG1JF07LctFEpulPCA3qMmcciBH70HwEAILVYtTqrEHwBFni8Pi2p26wNW5vVo7hQQ/qVKc9tNL8bQNpYDp9MlKI7ITxId4PybA6OCAj3CfQfaapX9CrIOKEvAACIj1WrswbBF2DSghX1mjavVvWNzcFtFaWFmjq6SjXVFTEuCSDlrIZPJZX+0CtWIJLp8CBujzGTq1Kalc3BESsYhgv0H3n6AvlD3tDHhf4jAACgbXH5fL4EGqKkV1NTk0pLS9XY2KiSkpJMDwdtRGh119cbd2jGK/9pdfoZqPWaOW4w4ReQSV6PfzVHw5BKUlE3qWa6VFyxL6yKV90UDFSkqOFBKgKVQNVV3RvSm3fG33/8/OS/bTQKjlJ5P+0SfO6Nprq2BJSTlzs/6LG74i5qmNkrfugLAADgcFZyIiq+gCiiVXdF01JzoWnzanVqVTnTHoFMMVPhcsbd+072zVY3BZqXRt03BeFBtHHFk2yPsXRXltnNyiIETp6OkIqKO/qPAAAAEHwBkRasqNeE2UtNrw3nk1Tf2KwldZs1tH/XVA4NQCxmQyqr0+LSFR4YVl3FkWyPsWwPjpyyCEEyUjlVk/4jAACgjSP4AkJ4vD5Nm1dr9bRTkrRha+zqMABpEC+kSrS6KdXhQcxxGbGpx1i2B0dOWIQgGU6vuMvmBQ8AAABE8AWEWVK3Oe70RiM9igttHg2AhMQKqZxa3RR3XJFsbFCe7cFRphchSJZTj0kpuxc8AAAAaOHO9AAAJ0mkassl/+qOQ/qV2T8gAPZyanWT1dsrqbSv4XwgOJJRj0KXvyG6U4OjQH83Sa3vQ8vPp93mD46WPyvVveWvYnIKpx6TgemXkaFcYPpl7dz0jgcAACBBVHwBIaxWbQVOsaaOrqKxPZANnFrdZPb2TrhaOmC4vdPNzCwMYEdlWSrF6u9Wfbb0f9c5t2rJicek06dfAgAAWEDwBYQY0q9MFaWFamhsNtVpp7y0UFNHV6mmuiLlYwNgA6dOizM7rpOvS03QkO7VK1MhWn+3HZukZy5USprG28WJx6STp18CAABYRPAFhMhzuzR1dJUmzF4ate7BJ+nKEQepb7eO6lG8b3rj4lWbtGFrc3Ab1V+AQzm1uskJ40rX6pWpFNrfzeuRZlTL8VVLTnjuIzl1+iUAFpwAgAQQfAERaqorNHPcYE2bVxvW6D5addeCFfWt9qugCgxwNruqm+w++XBC1VWqV69Mp2yqWnLCcx/KidMvAbDgBAAkyOXz+aysnZ4RTU1NKi0tVWNjo0pKSjI9HLQRHq9PS+o2G1ZyLVhRrwmzl7aqJQjsMXPcYMIvwMmSCa5SefLBt/n2WP6s9M+L4+939j+kQ89J/XjMcMpzH6iWizf9cvJyjk0gXQILThh98nTC1G0ASCMrOREVX4CBPLdLQ/t3jfo7j9enafNqY02g0bR5tTq1qpxpj4BTJVrdZHTyYVffqFyqusqkbKxacspz78Tpl0BbxoITAJAUd6YHAGSjJXWbw6Y3RvJJqm9s1qx36jRn2VotXrVJHq/jiysBxBP35EPSy9dIX73hrziqe8t/GaRfoGm8jL58cEklvdK/kEG2CEy/LImoXC6ppLIESDcrU7cBAK1Q8QUkYMNW49Ar1K0vfhb8/4rSQt04aqC6dCygET6QrcycfGxdJz0SEgoYTYF0yrS2XJUrVUuZPE5yYcEDIBew4AQAJIXgC0hAj+JCy5epb2zW5Y9/FLaNMAzIMomcVESbAkmD4vRwWtN4q5xwnGR6+iUBMZCdU7cBwEFobg8kwOP16fg7XlNDY3PUCU/JYFVIwMHq3pIePiOBC4Y0A//8RRoUp1s2hic0snZG8Ac4AQtOAEArVnIienwBCchzuzR1dJUk4+4xiWpobNaE2Uu1YEW9zdcMIGlx+0YZaem/UvdW/B5hC66lL5jdAlVLh57j/9fpJ4Zmesnl+nESCP4ipxYHKihr52ZmXEAmBKZuS2r99yeLpm4DQIZYDr7efPNNjR49WpWVlXK5XHrhhRfiXmbRokUaPHiwCgoKdOCBB2rWrFkJDBVwlprqCs0cN1jlpdanPcYSOM2ZNq+WhviA08Q8+TBh9ds0KEZ8bb2RNcEf0BoLThjzevxfLLGoDAADlnt8bd++XYcffrh++tOf6gc/+EHc/evq6jRq1Chddtlleuyxx/Tqq6/qZz/7mSoqKjRy5MiEBg04RU11hU6tKteSus3asLVZG7fuCmton6jAqpBL6jZraP+uyQ8UgH2M+kaZYTbLpkFx29bWG1lbCf4y2X8MSDcWnGiNKdEATLAcfJ1++uk6/fTTTe//wAMPqF+/frrrrrskSQMHDtTbb7+tu+++m+ALOSHP7QqGUx6vT39/u8623l9mV48EkGaRJx9F3aQ5E+L3X+l3gvTWnfGvPxsaFGdj36xQTh5/W29k3daDPyCWTC844SRGvRCjLSoDoE1L+aqOixcv1ogRI8K2jRw5UpMnTza8zK5du7Rr167gz01NTakaHmCrQO+vCbOXyiXzxR1GElk9EkCaRJ581NzR8gE88tUf0n+l7/H+ACxeQNZnWMqGbYts/4bdyvgzEZAFesll+3GSqLYe/AGIL+6UaJd/SvSAUc75UgNAxqS8uX1DQ4N69gz/YNKzZ081NTVp586dUS8zffp0lZaWBv/r3bt3qocJ2MaO3l8u+Vd3HNKvzL6BAUgtM/1XUt2gOB19TrK96biV8dfO9a+k9vAZ0j8v9v87ozr197GtN7KOu4iESyrplbvBH4D42novRACWpLziKxHXXXedpkyZEvy5qamJ8AtZJbL3V4/iQn27fbdufbFW9Y2xpy8GPuZPHV2lPLfda0YCSCkz/VeMeoSVVPrDjEQrptJRhZXt37BbGf/nL2Z2Ck2qjpNsEAj+4lVQOvEYA5AeTIkGYEHKg6/y8nKtXx/+hrN+/XqVlJSoQ4cOUS9TUFCggoKCVA8NSKnQ3l8BI6vjh2HlpYWaOrpKNdUVkVcJIBuY6b9id4PidPU5yfam42bHX/eWMwK+ttzI2q7gz8m93AAkjinRACxIefA1dOhQvfTSS2HbFi5cqKFDh6b6pgHHMROGDelXRqUX0BbY1aA4nVVYdn7DnolAwuz4V7/tnICvLTeyTjb4y/ZedACMtfVeiAAssRx8bdu2TStXrgz+XFdXp2XLlqmsrEz777+/rrvuOq1du1aPPPKIJOmyyy7Tfffdp1//+tf66U9/qtdee01PP/20XnzxRfvuBZDFooVhAGBaOquw7PqGPVOBhNnxm12ZJJkpNFQimZNo8Mdqb0BuY0o0AAssN7f/97//rSOOOEJHHHGEJGnKlCk64ogjdNNNN0mS6uvrtWbNmuD+/fr104svvqiFCxfq8MMP11133aW///3vGjlypE13AQCANiydfU7saDqeyeb4ZsdvNmhJdApNpprmtxVxqyDlr4JMxeIP6ZCORSyAbGBmURkAkOTy+Xxmv9fMmKamJpWWlqqxsVElJSWZHg7gKB6vj6mSQFtW95Y/OIln/Hx7pswFK2mkqN+wxzrZ8Hr8AY9hhVrL1JTJy1P3Lb2Z8Q8Y1TLOOFNoEhmnUSWSmccP5qT7NZFOTN8EWqOCFmiTrOREjlzVEUB0kSFXtOb4FTTHB9qWdPc5SabpuBOa45sdfyqm0GT7qpjZIldXe2P6JhBdW+6FCMAUgi8gSyxYUa9p88JDrmgaGps1YfZS3f+jI9SlYwGVYECuS7TPSTLfkCfadNwpzfHNjN+uVQVDOSH4awtycbU3QlMAABJG8AU4VGh119cbd2jGK/8x1W85sM+kJz6SN+QCVIIBGZTqaRhWQxo7pksl8g27k5rjmxl/sqsKRsrVSiSnycXV3ghNAQBIGMEX4EBmq7ti8UZ81g9Ugs0cN5jwC0indPXkMRvSZHK6lB2BRLrHb+cUmlysRHKiXFztjdAUAICEWV7VEUBqLVhRrwmzlyYVekUT+Ng/bV6tPJGpGIDUSPcKhoGQ5tBz/P9Gm96YyGp3dq0iFwgkJLVeWdFEIJHtq/XZsSomzMm11d4ITQEASBgVX4CDeLw+TZtXa2pKYyJ8kuobm7WkbrOG9u+aolsBIMmZPXkSmS5ld8VatjfHT0YuViI5md1TVTMpF6dvAgCQJgRfgIMsqdtse6VXNBu2JnYbkatK0jAfiMGJIY3V6VKpmlbohOb4mZKKpvnwM+ql58QQ1CpCUwAAEkbwBThIooGUVT2KCy1fJlrfMRrmAzE4MaSxMl0q1RVrmWyOn2mpqkRK9SIKidxOusaUrl56mURoCgBAQgi+AAdJJJCK5Ha1bmwf4JJUXuqv1LIi0Hcs8mppmA/EkOqQJpFAwcp0KSdWrOXSdC+7K5HSFfxYuZ1UjSny2N+xSXrmQmVkwYZ0y6XpmwAApAnBF+AgQ/qVqaK0UA2Nzab6fFWUFurGUQPVpWNBcPrht9t3a+LjSyVFnQihqaOrLE1PjNV3rKXmQ9Pm1erUqnKmPQKhUhnSJBooWJku5cSKNaZ7RZeulS6t3E6qxhTt2He5W9+OpIz10ku1aKFpuirrAADIQgRfgIPkuV2aOrpKE2YvjXpK55N05YiD1Ldbx5g9tma6B7eallie4LTEeH3HaJgPGEhVSJNsoGB2ulQmphWaOXl3ynQvpwQN6VpEwcrtSKkZk9Gx7/PGuJDDFzywQ1uY5gkAQBIIvgCHqamu0MxxyQVXNdUVOrWq3JZG9Gb7jqWrPxmQVewOaewKOcxMl0r3tEIrJ++Znu7lpKAhXVNSrdyOZP+YYh77Jjh5wYNkpKvaD3ACp3zhACDrEHwBDmRHcJXndtlSgWW275gd/cmAnGRnSGNnyBGvx1Q6pxUmcvKeqdX6nBY0JDol1eoJZCqmvlrZN+6xH4fTFzxIRDqq/Qga4BRO+sIBQNYh+AIcyq7gKlnx+o4l2jAfaFPsCmnS3XcrHdMK0zVVzw5OHGsiU1ITOYFMxdRXK/smfExn0YIHVqW62o+gAU7htC8cAGQdd6YHAMDZAn3HpH0N8gMSbZgPIEGZ6LtVNUaavEIaP186+x/+fycvt+8kw+oUukwyO9bXp0t1b/mDslQLTElt9Q4d4JJKeu0LfgInkJH3I3ACWTs3+duxOiYzEjqmc3zBg1QG4YkeJ4Dd4n7hIP8XDul4vwWQtQi+AMQV6DtWXho+nbG8tFAzxw223DAfQIJSESiYEahYO/Qc/792hghOXD0y2TG8daf08BnSjOrUBwSBKamSDL+eCAQ/yZxAWrkdK/uaFffYV8vqjiFKKnO7EiRVQThBA5wkm74cAeBYTHUE2jiP12eql5idDfMBJCidfbfSJRNVbEbi9TOyOoZ0TcMxOyU12alxVqa+2j1N1syxf/ZDUseubacfVaoWoEjXggmAGdn05QgAxyL4AtqwBSvqW60eWRFj9Uin9B0D2rR09N1Kp3SvHmnETD+juGONlMa+X2YWUbDjBNLKYg12r76Za8d+slIVhBM0wEmc9OUIgKzl8vl8Ca4LnT5NTU0qLS1VY2OjSkpKMj0cICcsWFGvCbOXtjp1C9RvMYURcLhcWm0t2LhYinrynuqKKaPGydFu33CscYyfn/nqmLq3/FMw43HCWGPJpWPfDlFD216Jh4G5cpwgN3g9/mnj8b4cmby8bb8PAG2QlZyI4Atogzxen46/47WwSq9QgZUa377mFKYyAjAvmUDC7pN3s4InVUZTu6KcVEUbazxDLpUGjslsSMMJZO6yMwzkOIHTZPrLETgDX3ogAsEXgJgWr9qksX97L+5+T1xyLFMbAZhjZqpgPJn4UJtodUtgrHVvSG/eaf72rD4mdot5AumTTrpe6tqfk4q2jqDBGk7IUy9TX47AGez4jIGcQ/AFIKpAI/uXV9TrkcWr4+5/z/mDdOagXmkYGYCUSvVJmZWpgolI5fiXPyv98+L4+539D/+qltHGFrM6JpIDgoNoJxAdyvz/7ty8b1s2BJdIHYIGc3LhhDxbXrvZMk7YK9WfMZC1CL4AtBKtkX08VHwBOSDVJ2WJTBW0ItXjt6OfkeW+Xw6YKhZ6ArlplbRoupI6qciFk3+0RtAQWy6ckPPahZOl+jMGspqVnMidpjEByKBAI3uzoZdLUnlJgbw+n+YsW6vFqzbJ43V8Rg4gUuCkLPIDY1O9f3vt3ORvY/W7cXpd+aSmtf79rErH+AMrNcqon6HLX+USa1XJwGqDJWYXBEniMbGLO88f5B3yfWnpLEUP7Fq2LbjWf/JhJB3PUzp4Pf4gdPmz/n+9nujb2pLAcXLoOf5/ObHcx+vxB0bJvHYyzSmv3bb+OoOxVH7GQJvSLtMDAJBaHq9P0+bVml57LLAgevNer3789/eD2ytKCzV1dBUrPQLZIu5Jmct/UjZgVHIns9vW27tfQLrG787zVzY8fYH2vQMGtIRhNbfHv42qMf6xrH5X+myutOSv8W/b6mOSClZOKqJVvKXreUq1qNM/u0hytZ7+edp0qWNXqqDaqtD+fsm8djLNKa9dKs4QS6o+Y6DNoeILyHFL6jZbmt5YWtRekrRlx56w7Q2NzZowe6kWrKi3dXwAUiRd35J26mnvfgHp/JbXqGKrpNLaVKVAdcxAk/tbfUxSIdmTilz4Nt6o6mXnt+Ghl+Tf59nx/umx/7zY/++M6uQrY6h4yQ61c/3P98NnmF/Uwqkn5E547Tql4gzOlarPGNmGvxFJo+ILyHEbtpoLvS4Y2kcjq8p11TMfS9rT6vct3/1p2rxanVpVrjy30bQgAI6Qrm9JA1MFDZu7t/TfiDVVMJlx2XVSGVqxlWwlT6oek1RI9qQi27+Nj1n1YlLgJD3Rfk5UvGQHw35ecWz43H+i6rTKwEy/dp1ScQZny6a/p6nC3whbUPEF5LgexYWm9ju9ukJut0sNTcZBmU9SfWOzltRtNtwHgEOk61vSwFRBSa37ZFmYKpjouOz8lteufkapekxSIdkeZ9n+bXzcqhczkujnRMVLdkgmIH3rTvsqA+2U6deuEyrO4HzZ9Pc0FfgbYRuCLyDHDelXporSwlinNKooLdSQfmWmq8PM7gcgg+xo2m6WXVMFQ6Vz/KmQisckFZI9qbDzecrEVA7bqlkSOEnPhebobYUdAanTTlQz/drNdMUZske2/D21G38jbMVURyDH5bldmjq6ShNmLzVq26ypo6uU53aZrg4zux+ADLKrabtZdk4VlNI//lSw+zFJlcBJRdSpFLfHPqmw63nK1FQOu6tZrJykJ7uwANLHlvDFYdP3Mv3azXTFWSYEFkZw8t8Dp8qWv6d24m+ErQi+gDagprpCM8cN1rR5tWGN7ssjVmoMVIc1NDYbzaJXeUt1GIAskEygkYjAVEG7pHv8qWD3YxJg9wlUMicVyT5PRr2Tku2dZUbc/jEWbVvvr3ox8/ilo+KFE217mA1fDj1PWv5UjB0cdqKaydduW+vdRJ+m5KXq76lTURVpK5fP57Phr3xqNTU1qbS0VI2NjSopKcn0cICs5fH6tKRuszZsbVaPYn+AFdmkfsGKek2YvVRS9OqwmeMGB4MyAFki209+s338dnPqCVQiz5PX4+99ZPitdsvJ7+TlqXvOgyfvUlLhl8st+bz7fo73nNS95e/9FM/4+Ymd7Dn1OMlGweM0Tkgz4mbpuUviX9/Z//D3EnSKTL12DV97LZ86c2Uam+HCCC2VdiddL3Xtz983hEv134gcYCUnIvgC0MqCFfWtqsMqIqrDAAAZEPMEStlxohh6kr1tvfSv6+NfJtUf7KOFRB1aqpt3Jrqgi8FzErj/W+ulBddJOzYpZpiSSOiXC8eJ05gJaTp0aTsnqnadlEcNaHtlT1VvPHEDwgiE0wgwG7in8oshh7OSEzHVEUArNdUVOrWqPG51WLqZqVgDgJwVt9Gtg/oHGYl2kmtGqqdyGE31lMK37dgk/eu68PFHVnoFRXlOTN//JPrY5cJx4kRmpgV6PW1n+p5d07ByvXeT1YUR0jHF207ZXpHt5PHnQq9TByH4AhBVntulof27xt0vXWEUVWgA2rxsb3RrWIVkgp0Nro1OdIz6x0RuGzjaQsVayHOy81vz9z+ZPnbZfpw4WbyQpi2dqNrZnD6XezdZDu2zKJzO9unU2TD+XOh16hAEXwASlq4wKtB3LPJUoaGxWRNmL6XvGIC2IZsb3casQorF5goZO050Qk/Slz9r7jJb66VXpirm/S/qJtVMl4orkqs6yObjJBvEC2nayolqNjanz0R1T0KhfRaE05lclMQO2TT+XK+KTBOCLwAJSVcY5fH6NG1ebawJG5o2r1anVpUz7RFAbrOzwiLdrE73kWR7hUwqTnTMPtbbv4l//3ds9IdeyZ7oZvNxkitinag6eWqVFdlW3Zap6p5kVo51ajid7dOps3H8uVwVmSbuTA8AQPaJF0ZJ/jDK401+7YwldZvDKsqi3V59Y7OW1CXafBgAskTgBEpGIb/L3xTaSRUWAYmcwJVU2vete9wTHflPdLwea9dr9jnp2N3c9dlxopvNx0kuCZyoHnqO/99Aj7cZ1f6m8P+82P/vjGr/9mwUqG4rifii087Xrh0CoXdk+BwIvVP5+AcCQknGr0kD6QinvR7/QgXLn/X/a+Y90Mp0aifK9vEjIVR8AbDMShgVr09YvB5hG7Ya304os/sBQNbKtgqLUGZP4Ebe5t/X7kqYVPW9MvucdOhi7vrsONHN5uMkl2XT1CorrEzDykS1mxOqe4ymvxpK0TTRyMc/2mIdZqrgsn06dbaPHwkh+AJgmV1hlJkeYT2KC03dltn9ACCrZWv/ILP9gI65LDUnn6k80XHian/pPk5yZfpeqjghfEklM9OwMjXV0CmLPUQGhJtWSYum7xtDUIrCabMrysYKYgOv828+N3ebTp1OzXTwNongC4BldoRRZnuEDelXporSQjU0NhudKqi81F8pBgBtQjY2us10FVKqT3ScuNpfuo6TbFgZLdOcEr5kSiar3ZxU3RMZEPYYmJ5w2tKKugZBrNngTJIjFzaQ9gV3W+v9i4ns2KSsWZgBSSP4AmBZsmGU1Yb1U0dXacLspUanCpo6uorG9gDalmxsdJvJarV0rEDnxNX+Un2cJBJotMXqMKeEL21xqqGTq3tSGU6HhjwLrpO1xvoRQayl4Myh06lNB3cOHT+SRvAFwLJkwyirPcJqqis0c9zgVtMiyyOmRQIAHC5T1WqZrjgLcGq1XiKBSCKBRlutDnNC+NJWpxqmI/RORirCaUvVWTFsWx/ndR6FE6fdWwnuSiql027z92Vc/qxz3qORNIIvAAmJFUbdOGqgSjvka86ytbY1rK+prtCpVeUxG+EDALJApqrVMlFxZRQoOalaL9FAxGqg4ZTm7pmoesp0+NKWpxo6JfROlWgN65+5UNYqvAx06mnidd7ihKulA4Y7LyQyE9wVdZNqpkvFFYk3/IfjEXwBSFi0MOrb7bt164upaVif53bFXSUSAABD6ay4yobqpmSmKn4219xtxK0aSWNz90w9J4mEL3YFdJl+7J1Q7Zati4LEE+14drmVfOgVEsR++ry5i/QY4KxAP8BMcLdjoz/02vlt9NAw21dehSSCLwBJCg2jFqyo18THjRvW3/+jI9SlY4EaGneqrGO+vt2+m4b1AID0SkfFlV0VNqmsTrJrqmI8pqpG0tDcPdMVZ1bCFzsDukw/9pmudgtw6jTjRBkdzz5vklccEcQ6IbhMhtlKwq310itTlfFwHilD8AXAFvEa1kvSpCc+kjfOl1A0rAcAZDW7KmxSXZ1k11RFQwlUjUSepOZK1VOAmfDF7oCOqYbhY3FiVZJVVvtuWREZxDoluEyU2UBu+zeZD+eRUgRfAGwRr2G9pLihl0TDegBAlrOjwiYd1UlWAhHLJ9o2VI3kUtVTqFjhSyoCOidU7OTqVMNMMdt3K56SXv5G7h27GgexTgouE2E2uOvY3dz1pXrlVaQMwRcAW5htWB9NWcf2uvGMQ1ReYk/Deo/XRxN8AEBmJFthk67qJCuBiNUT7WSrRnKt6smsVAR0TqnYybWphpmUzHEa2sjd7OOfzcGl2eCuQxdz1+fUKZ2Ii+ALgC3MNqyPZvP2PSovKbSlcf2CFfWtVpqMbK4PAEDKJFthk67qJCuBiNmpikMulQaOSa5qxK7gL3SapNmgINMntakI6JxUsZMrUw0zLaHjtOW5PuPuxIKqbA4uzQR3Xo8zAmIpMyvPtgEEXwBsMaRfmSpKC9XQ2JxQx4FkKsYCFqyo14TZxs31Z44bTPgFAEitZCts0lWdZCUQMXuiPXCMcbBhtmrErqmi0Va7M2z87ZA+RamalpjNFTtoLe57jFof73Y819kcXMYL7pwSEGfDasBZiuALgC3y3C5NHV2lCbOXtvpzYUaiFWOBaY0NjTt164ufxfp+WNPm1erUqnKmPQIAUifZE6hU92SKrCb44SzpX9fFDkTsmi5npmok2eDP8mp3DupTlMppidlcsZOIVFTNOKUSx8x7zNkPxe7dlSuiPSdS9OcpXnAXLyAeMEqqeyt1j2mmV57NcQRfAGxTU12hmeMGt5pq6HYZN7Z3yd/Qfki/suA2sz26ok1rNOKTVN/YrCV1m22ZUgkAgKFkKmxSGX4YVROcNj19Da7jnXwmE/yZacIfrRLmtNv8PX6WP5uZkCD05H3whdKi6UpJ1Uk6K3YyGRKlomrGaZU4VPFFf046dJHkknZu3rfNyvNkFBB//qI0ozp1z79TVp7NYS6fz5eCdVDt1dTUpNLSUjU2NqqkpCTTwwEQR2Rw9e323Zr4+FJJUT/ChU1BNNujy2haYzyTTu6vg3oW0/QeAJB6iZ78B7/5l6L+5Uzkm3+jagIr1xn15L+XvSfaXk/LCWac4G/y8taPZd1b0sNnxL+Nkbf5n49OPaUdmwwq3tIUaJg+ebf5cU6lTIZEdhznUvhrd9OqljAyyetMBadUoaWb4fMcTZLPk13HVCxm37vGz48fXrehY8JKTkTwBSAtzARaRmFWZEDm8fp0/B2vmar0ioWm9wAAx7IzZAqGSUa9s2KESdGuK9UnVYkGf8uflf55cfzrP/sf0qHnpOeENpaYt++TTrpe6to/81VoCYW2GXhM7TrOo732DFl47VjRVsKLRO5n3Oc5mgSfJzvfO2Ox+t4VOr7Qxy/TQX6aWcmJmOoIIC1qqit0alW54RRGj9enafNqTfXoWlK3OenQS6LpPQDAwezsyWTnSpHpmC6X6DQuK9MkMz21yMztL33Y/kDFjEQrtjL9mNq1MILpSiKT12mV06ZVpkqi9zPu8xxNgs9TulbZTWSKt9mAlh5hkiR3Ihe6//771bdvXxUWFuqYY47RkiVLDPedNWuWXC5X2H+FhYk1sQaQ3fLcLg3t31VnDuqlof27hk0zjBdmhfbosmMFyMB1Sv5AzWPUhAwAgEwJhEyHnuP/N9GwIF0rRdqpaow0eYV/as/Z//D/O3m5uf5oMmpj4PJXzfUZZu2ENhUyfftGAsFP5NgCJ8+1c40vm+n7lOxxbqZHXLK3HU8yj382SeZ+JvNYW71sut47rbx3ScaPX1Qtx/OCa/3HeBtlOfh66qmnNGXKFE2dOlVLly7V4YcfrpEjR2rDhg2GlykpKVF9fX3wv9WrVyc1aAC5x2yYFagWs0tooAYAQE5K9UqRRrwef++a5c/6/7V60hUt+It1nYEm/JJan0BGNIfPdBiY6duPJm7FlmKfPGf6PiV7nCdUSWTxtmNJ9vHPFsnez2Qea6uXTdd7p5X3roQC2pbQ+fXpib0X5wDLwdcf//hHXXLJJbroootUVVWlBx54QEVFRXrwwQcNL+NyuVReXh78r2dPm/+oAsh6ZsOswBTJitJCw+9EJKmsY3vdfd4gTTr5QFPX+/KKei1etckRlV8er0+LV23SnGVrHTMmAEAWs1pNYIfauf7eOA+f4e9d8/AZ/p+jVXKYDcjMXGdgmmRJRAuDksrwqT6ZCgOtXm+qbj+aZCu2Mn2fkj3OEwrkbHztZLpizi7xXs/J3s+4z3M0CT5P6XzvNPvelUxA+9adsd+Lc5ilHl+7d+/Whx9+qOuuuy64ze12a8SIEVq8eLHh5bZt26Y+ffrI6/Vq8ODBuu2223TIIYcY7r9r1y7t2rUr+HNTU5OVYQLIQoEwq6Gx2WgNJ5WX7usLNnV0lSbMXmq02Ldu+/6hqqmu0OJVm3Tf6yvj3v4ji1frkcWrM97w3uyqlgAAmBaoJnj6AsnoL2egmsAORn2SovWaMdvnx8p1mumPFjihjbd6pJ1hYKhM3340yVZsZfo+JXqcBxqEf/O5xRu0+bWT6Yo5O5h5PZsd/2ctwUzkazfm8xxNEs9Tut87zbx32fH8t8G+X5YqvjZu3CiPx9OqYqtnz55qaGiIepmDDz5YDz74oObMmaPZs2fL6/Vq2LBh+t///md4O9OnT1dpaWnwv969e1sZJoAsFAizJMMCX00dXRXsC1ZTXaGZ4warvDS8Uqy8tDCsWb2Z6rBQgYb3C1bUJ3hPEhdY1TKy11kmxwQAyBFmqwmSZWUak9k+P4lMjYrXH83K1KJUyPTtR5NsxZYT7pPV4zy0ivDNO63dlt2vnUxXzCXL7OvZ7PiX/NW4Osnoee5Q5v8vVLLPUyrfO6NVx8V777Ll+c+hqbMmuXw+n+k5NOvWrVOvXr307rvvaujQocHtv/71r/XGG2/o/fffj3sde/bs0cCBAzV27FjdeuutUfeJVvHVu3dvU8tUAshuViuePF6f4UqRodc5YfZSSeZmwweqy96+5pRW15UqHq9Px9/xmmGD/0yMCQCQgwLVLcmuFGmk7i3/yWo8P5kjzZkQY8pOS3XQ5OX+8Zq5zvHzra+sFrVCpVfs1SPtlOnbD+X1+EOGeBVb8VaadMJ9MnOcW1rBsaXa56Trpa79U/Pasevxz4Tg2E28nqU49zPKZaXoIVO051lKzXuc3e+dyayeaunxiyOR902HaGpqUmlpqamcyNJUx27duikvL0/r14eX161fv17l5eWmrqN9+/Y64ogjtHKl8dSjgoICFRQUWBkagBxRU12hU6vK44ZZAYGVIuNd58xxg1sFakZCG97Hu267WFnVMt6YzISBAIA2KlBNkCpmp+Gsftt8n59UTgEzM7UolTJ9+6HsmtblhPsU7zi32iC8pDL1wV26p9XZyUrfrn4nWJyq6PPvt+Ba/3EVOe0x2vOcivc4O987rUzdjjYOS49fHE6eOmsjS8FXfn6+jjzySL366qs666yzJEler1evvvqqJk2aZOo6PB6Pli9fru9973uWBwugbTATZlkVGqi9vKJejyyOv7rsyy1TC2MFR2ZDpnj7WVnVMhZ6hAEAMsrsNByz52qB4MTO246U6jAw1bdvZyVKYFpX1EoUC8FPph/TeMw2CD/haumA4ekL7ux6/NPNajhtdD8NRQRn2Szu1G2DkC+U4XHSSzrtNqljV6nuDXPTd506ddZmloIvSZoyZYrGjx+vo446SkOGDNGMGTO0fft2XXTRRZKkCy64QL169dL06dMlSbfccouOPfZYHXjggdqyZYvuvPNOrV69Wj/72c/svScAEEdooGYm+IpseB9Zifbt9t269cX4IZOZMMrKqpZGAlM6I/+MBnqEhfY+AwAgJcw2OO93gn+FsXgCQY7TGsE7RaLTpWJxQsVWqqfkmg1qegxIf9DihMffKrPhybb1/n5WnXr672Pgfn4219/Ty8zls53V6jgj8Y6TPsOkZY/xvtnCcvB13nnn6ZtvvtFNN92khoYGDRo0SAsWLAg2vF+zZo3c7n0987/99ltdcsklamhoUJcuXXTkkUfq3XffVVVVlX33AgAsiLeCZKSGxmZdNnupOhe115Yde+LuGxoymQ2jrKxqGY3H69O0ebWxvjvStHm1OrWqPCunPTJ9EwCyhNnpWn2PNx9mZfMUsFRKZrpUPJms2EpFmBfJ6Y3koz3+qQ4DkxE3nJbkckv/un7fz5HPqZngKxeqk+ycuh3rdcr7ZhhLze0zxUrTMgAww2rDeysCIdUbV5+s4Xe+brphvdGYAvFOrIqtxas2aezf3os7ticuOTZtfcvswvRNAMhCZhqcB4MbKepfvsjgxglN053CSjPxbDqxNWw4H6PBeSKyrZF8OsLAZBm+no2EPKcDRmXX85EMswuA2NV0PoffN63kRARfANqsaIGKnc4aVKkXlsXvW3DjqIHqVlxgafpkpDnL1uqKJ5fFva17zh+kMwf1MjV+JzCqmDMTBgIAMszsynpWTsrSuYqbk6X75Dkd0h3mWQ1eo0lHFVa6wkA7RHs9u9ySz2twgZDn9PMXk38+skEmQlcnVwsmgeALAEwKTKEz2/A+1SpKC3XjqIHq0rHA0rS+XKz48nh9Ov6O10xXzAEAslQyJ2XZUAmTCsuflf55cfz9zv6HdOg5qR+PHTIR5iVTDRPr2LOrR1c2VvaFvp63rQ+f3mgk8JzGeD48A0bnTtsLO0JXWMqJLPf4AoBcYrXhfao1NDZr4uMfaea4wZYqs5LtEeZES+o2x6zG80mqb2zWkrrNWRPmAQCiSLSfVCp7XDmd03tUJcLO3kdmJdpIPuax9xOpQ5m0c/O+7YmGsXY1Qk+n0Nfz8mfNXSZ0tccoz8eC2g2aFvFlaFa3vcjW1TuzGMEXAMh6w/tUSbQRfZ7bpamjqzRh9lKj9pWaOroqq74Z27DV3BRUs/sBAHKI1+M/aYy1rMuCa/0n0YlW2jh5apCZZuJF3aSt9f5KKqeNP5pMhXlWg9e4x57CQy8p8TA2E2GgnRJ5TiOej5xdtTwbV+/MYu74uwBA7gsER9K+oChTQiuZ4vF4fVq8apPmLFur0g75uv9Hg1VeWhi2T3lpYVZ+KOhRXBh/Jwv7AQByiJVKGKtq5/qnlz18hn864cNn+H+unZvwcG0XWLFNkuEnlx0bpecuceb4owmEeYafxFz+aW+Bvm6ZEvfYi6YltllwrT84MyvbK/uSfE7jrVou+b8s9ngd370pukDId+g5/n8JvVKGii8AaFFTXaGZ4wa3anjfuai9tuzY06qSKtXiVTIZrXaYSI8wJ8rF6ZsAAJukqhImm6ZPGk2XisaJ448UCPOevkAyql+vuT3z4UDC1VUJTEuMW9nX0uMr02GgkSSfU9pewC5UfAFAiJrqCr19zSl64pJjdc/5g/TEJcfqwxtO1QPjWldSVZQW6s8/OkJPXHKsJp3c39T1Tzr5QN04aqCpfUMrmUIruxav2qSXPvGXfUd+GAj0CGvcuVtnDuqlof27ZmXoJcWuwsvW6ZsAAJukohLGzBQ2qxU7qVY1Rpq8wt8c/Ad/809vjMqh448UCPNKIqrUSyqdE9olW11lJTiLWdnnoDAwliSeU9pewC5UfAFAhNCG9wE11RU6tarccDWZIf3K9M+la+NWJ1156nckSX9/u850JVO0yi63K2ZXE8s9wpzKqAqvPJsbmgIAkpeKSphsbCQu7ZsuVfeWf3qjoSTHn66+Z07vfWSmv1osVoOzXGiEnuBzStsL2IXgCwBMihaIhf7OSnN5s/saNfSM1cog18q+44WO2Mfj9fE4AWgbUjEtLtsbiady/LVzDYKXBFYqNCPRlT7TIeaxF0sS0xKdHgaakcBzStsL2IWpjgBgk0B1kpnm8mb2jdXQ04xcKvsOhI7ZPn0zlRasqNfxd7ymsX97T1c8uUxj//aejr/jNS1YUZ/poQFAatg9LS7bG4mnavyBvmeR1XCBvmFOb5qfCkbHXodAAJOCaYnpbITu9fgrCJc/6/83Q9NjaXsBu7h8Pp/jl0BoampSaWmpGhsbVVJSkunhAEBMVqpuYu27eNUmjf3bewmP44lLjs2Jii/EZ1QZGDjqsnFVTwAwza4peF6Pf/XDeNMnJy93ZqVNKsYfvE6jKaAOf0xSLdqx9/mLUarjemXPtMR0V/eZYLSgU1a1vUjXVOE2xEpORPAFAA41Z9laXfHkMsuXC5R9v33NKXwD1gZ4vD4df8drhqsecTwAgAXBVR2lqNMnndJg3Yjd4697S3r4jPj7jZ/v3KmJmZCtIYfRqqYOOP6zup2DA8PEXGAlJ2KqIwA4VCKNOp1a9h25KqUnVpMyWGJlqW8AQBzZsKpgLHaPP9v7nmVKOqcl2sXhq5pmbdsLpgo7As3tAcCh4jX0lPyrO4ZmSFZXO0zHt2c5UZ7uYCz1DQA2y/ZG4naOP9v7nsG8bF3V1Mnihokuf5g4YFT2vL9kKYIvAHAoMytF3jf2CHXpWBA3uIoWcC2sbUh5IGXUe6qhsVkTZi+l95QNWOobQKpk9dSiZDl5VUEz7Bp/n2H+arF4fcMSWakQzkJ1n/0IEx2D4AsAHCyw+mNkQGWlsitaxVXnovbasmNPq33tDKRirUrZ8h2Xps2r1alV5W3nRCoFWOobQCpQrQtJ/gCt5o6Wvk8GX8Mls1KhgTYdumYK1X32I0x0DIIvAHC4muoKnVpVntAHQKOKq2ihl7Tv4+z1zy/Xzj1elZdYX5VS8vedemflN6Z6T816p07digv4YJsgM5WBTuv5BsDZqNZFmEDfsKjNue1fqZDQNUOo7rMfYaJjsKojAOSoeKv9mVVRWqgbRw1sNaUy2lTJzkXtJRkHa2Zuiw+2ieFEAYAdnL5SLJVAGZSGlQqNQtfAM0zommLZvqqpE4S+Toq6SXMmxA8TJy+nx1cCrOREBF8AkKMWr9qksX97LyXXbTRVMll8sE0OJ4QAkmX2b8cTlxyrof27pmFE+xDw5zanh65tRu3cKNV9vVJS3Zdzoj12HbpIO7+V4VRhwsSEWcmJmOoIADkqlav4pSL0ktpW769UhFSBpb6BRBCcQnLuSrFMv8x9S+o2m2qRsKRuM3/rUinbVzXNlGC1XMS71M4t/n87dJF2bt63PUVThREdwRcA5KhsXcUv8MH27oX/0XEHdsvJk2+qFuA0HJMIcOJKsSyW0jY4NXRtk7J9VdN083r8lV6x3qXaFUoXzJW2f0OYmAHuTA8AAJAagdX+svUU4L7XV2rs397T8Xe8pgUr6jM9HNsEqhYiv9UOVC1Eu68er0+LV23SnGVrtXjVJnm8ju9SgCySyDGJ3BXvb4dL/lA0nSvFWqkEQvZyYugKmLL63fDpja34pK3rJJdbOvQcf6hI6JVWBF8AkKMCq/1JanUCE/g50Iw+XSadfKBuHDXQ0mVy6eQ7XtWC5K9aCA22Fqyo1/F3vKaxf3tPVzy5LCfDQGROIsdkKsdCwJt5Zv52pHul2FyqBOI4N+bE0BUwZdt6e/eD7ZjqCAA5rKa6QjPHDW41ham8ZQrTqVXlWlK3WQ2NO3Xri5/p2+27o54AJyvQkPbKU78jSfr723VqaGw2dVu5NI3Fav8Setog1ZzSU4epls4S729Hup+TXKkE4jiPLRC6Tpi91KgNeNpDV8CUTj3t3Q+2I/gCgBxXU10RDLiiNa0OnMx2yM+L+mEzWdE+rBp9sDWSipPvTDTytlK1QE8bpIMTKmkIeJ0p3t+OdApUAhl9YRL4csXJlUAc5+Y4LXQFTOkzzN+svqle0T/Zuvy/7zMs3SNDC4IvAGgDzKz2Z/RhM5pAYNW5qH3YCo+BqZOh26J9WLVyW6HsOvnO1LfuVqoWnFKJ05a0xVUNM11JQ8DrbMmuFGvXayrbK4E4zq1xUugKmOLOk2ruaFnV0eBdquZ2+nplEMEXACAo2ofNb7fv1q0vxp4qGfrBVJKpD6uht/XOym903+ur4o4v9OQ70ROqTH7rbqVqYf4nsZqk7pMNPW2SlY5Aqq1OQcp0JQ0Bb+6y+zWVzZVAHOfWJRu6AmlXNUY69xH/6o6hje5LKv2hV9WYzI0NBF8AgHDRPmyOrI4/VTKU2Q+rgdsa0q9M/1y61vTJd6InVHZ+655IGGOmauHGUQO1pG6zvly/NeZ1BaSjp026KqGi3c7C2oaUB1JteQpSpitpzAa3L7cs5kDVR3ZI1WvKqZVA8d4jnTClGEAaVI2RBozyr/K4bb2/p1efYVR6OYDL5/M5fimRpqYmlZaWqrGxUSUlJZkeDgAgBQInSlL0k+/AiZLRCVXkftEsXrVJY//2XtyxPHHJsTHDu2QrGYwuP+bwCs39uN7U9M9AGPj2Naek9KTPaKw3jhqoLh0LbDv5jHY7kVNpA8w812Z5vD4df8drho95uh7nTMtUxZvZ12Q6x4TktLXXlJnXjl1/ewAA+1jJiaj4AgA4gplpLMlWbNnxrbsdlQxGU0onPt76eqNJV08bo/ta39isyx//KGxbMmGY0e1EC70ke3viMAXJL1OVNPGmWkZqC1V42a4tvabM/j3I9JRiAGjrCL4AAI4R7+Q70ROqwDSUZKcP2jlVMnRKaaBCwmwJdjp62sS6r9EYhWHJTD+Nxa6TZ6Yg7ZOJnjqxplpGQyNw52srrymrfw+yuTl/PG1xYRAA2YXgCwDgKLFOvs2eKL2z8puYzfmNxPvWPVWVDPGuN2DSyQfquAO7peWkwuyYYjFTnZPs7SR78pzpVQ1hfZVXK68zTsjTr628pqz+Pcjm5vyxtNWFQQBkF4IvAEDWMHuiZGaFyEixvnUPnDwHGmzHYzWMMbv/QT07hVWJmT2hT+Tk345qDDunnxpJ9uTZ6VOQ2kpwE1rt+fKKej2yeHXcy8Q7drLphDzbn+fQ8XfrWKDykkKtb3Lma8ouiVS2ObU5f6La8sIgALILwRcAIGtY7QdkReBb91OryrV41aaEKsYCrIYxViskrJzQJ3ryb1c1RqDqYdY7depWXBA80ZNkafppJLtOnu2cgmR3eOHU4CZVIU1otaeZ4CvWMZpNJ+ROfZ7NMlqYIhB859q0voBEK9vMTil2ehhq59R/AEg1VnUEAGQVo9UfExU6fXBhbYPp6VbRuCT1LCnQXecO0sZtu1qFPEYnMIEeX/Gqjt6+5hQtrG0wvaplMitgxhtTMjoXtZdk3Lw+HjtXdQxI1UqdiYYXyTx3qZSOkMbK6yHaCXU2rSoY73m+/0dH2Lp6qt1ijd+n1iuzZlOgF0+yx2ks2RCGslIlso3Tw2RYZyUnIvgCAGSdaCcFibrn/EE6c1AvwxM4s4xO9KKFPNFOYIwCvdCg49SqctMn9JKSPvm3O2S0yq6TZ7MfdhP9UGx3SOXU4CadYZyZ14PRbWXLCXm851mS3C7JG/IAJLN6qt3MHKfRvghI11jTcZKbzHEa7zqdFnpHmrNsra54clnc/QJ/Y4FMyoYwGdZZyYmY6ggAyDqRfVK+XL9N972+MqHr6lFcmPDKgqFKW8KZyCqmaFVN0aZbmWl8vHjVJtPNlNXy/2b2NeobdmpVuaWm43YLnX6a6AmslQ+7iaxqmIrpPqlaRCEZ6Z7WlEwj8EytKmg1aDGzsIM34gFPdPXUVDBznDY07ZLb5Up78JGuk1y7G9Y7ffpg6DG+cesuU5fJ9kUM2qpcqo5K99T3XHrscgnBFwAgK4WGFItXbbIcfIX2iEpmZcELhvbRyKpyXfXMx5LMTd0zOoGJ1/g4FSf0gX1jnSi+fc0pYWNKpO+ZFdFWr4wMeMx8sIz1Yfey2Ut15YiD1Ldbx4SqwALb31n5je0hVaaCm1gyEcYl2gg80d5LyZysGL1+YlVn2fX8ZapvmROPUyn9J7l2Nqx3YugdEO0Yj6xIDJUrixjEk+0hR7TxR2v7kK3VUakOkyMfv2ifj7L1scs1BF8AgKxntel9ZIPlZE7MTm/5INPQZO06jE5gYlUdJXpCH8uX67fpnle+1IxX/mPpRHFkdXnKwrDQ1SujMVPNEe/DriTd/cqXhpePdTtjDq/Q3I/rLd1XK8dYJoKbeDIVciRShZfISp3JVAgZBS3xqrPsXEAiE5VAqXg/SlamKqYSOU6jybYwMVboJWVuYZB0yfbpc0YLU5itVM8GqQyTzbbdyNbHLtcQfAEAsl6slfmiiZyGksiJWejJ8/xP1lm+fICVE5h4J/Qd2rn0ne4ddFhFkSRpUEWRvtkWezrK8/+ukyRVFucZ7vPAq5/rhAM6tzoROaJXR0kdW37qqOEHDtMn/9uizdt369vtuzXzjVVm71qYHkVuNTdHf1ze+s8G3TqvVm5JvULH7N2jW+d8rDzvHp3wnR76aM23cnv3hO8TS8TlY93O/I/WtN4ex9qNTVr8H7cO26/14xjpsIqiuM9d904FOqyiKPg4vfWfDbr/9VVhl+neqUATT+6vE77Tw/Q4jfQocpu6v7Geu3S6ZdRBunlereHvbx51kPbs3qU9Mn9MRePx+vTAq5/HfP2ECbnOYQd2N3yevT7p22avmveam4CdiUqgRALGVEtHxVQqewZmW5gYEFn5ZWWqZ7aGR9m0cmw0RuM3WnTGCVNtE2H2M9bLK+olKenentFk62OXa2huDwDIGYlMN5Ksr2AY2WTYbDPtaKw22I7WTNkl6eyBHXXKAZ1UXlKoDvn+k/Cduz3avH132L6J6t4pXwXtzQc9Pp+0vqlZHq/P9G275A8xe5YUyhXlc2HgOvcalRlIynP5+63t9fjU1LzX9HjDb79A65t2xbydRLVzu1TaoX3wOTJi9NwFHpayjvmtnmejZtih+yYq3vMZ77mLvK7dez3y+HzKc7mU3y4v7mUSsXO3R40794Q9j5GPf7xjKt792rXHo2+27bY8tuBx6vVp607/cRo+Ap/2eHx69atteu6z7aZfQ5GNxFNdSZOK5u6hrI4/1Q3XzYY0iYY5qVwpMpZYj7PZv283jhqobsUFGV0YJF2cugBJqFjPqZmFNWLJ9MIg8UT2orv1xc9MX9bK6zSRx8/pj122obk9AKBNSrTXSrIVY1anWkqJV0NEa6b8g4EdNWZgZ/XrXaFunUvkCjlD39q8Wxuadmuv12vpdiJVlBaqpEO+pct0b96tdVusfTCs7Fyo4sLot7N9117tKdph6nraSUq0JqJzcaH2FKW2aqlbjPsZEO25a+d2q0dJvjoVtNeO3R7t9Xj1zbbd6l5q/Py2c7vVt3vHsOMiEfGez1jPXUDgPrm83uCHUJ/bre4l+XEvmwifz6cduz3yeL3Kc7tVlJ8X9jiYPaZ6dilSx4LWH5ubdu6WK4npve0kFXX1j8cbGr75fPLt3a0zWgK6f3623dT1hVYCpaqSJvKE+v4fDW41zTnR5u6hEhl/KqcJm63wSaYSKNbfIqvTB82K9zibrZjpVlxgKUx0eiP/WDLVi81sEBzvOU2mr6lkvToqnaz2ootk5nWazOOX7mnK2IfgCwCQUxLttWK0OpeZijGrwVmyJzChAd83TTu0X16j9q8sV7du3VrtW1hYqG6lPm3f5dG2XXu0weRKXJGKOhSpsNDax4bCwkLlFxRq3ZZm7fHsC2ZCv3UOaJ/nVmXnQpXGCNeavbvlametiisRvrz2crVLLiiMZ+NOqVtpQcwwKvS52+v1qp3brY4FeWpq3qOvQx9Tdzu53Ma35ZHUuNulTgXt1LHAH/z4fK2vN14wZvR8mnnuJKlx527Vb/O2Gq9HUv02r/IL3HGvQ5LlsXfoYHxdZo+pvPb5KowSzO1VO7m2J3esBF4F5Z0LVdDOrXZutzxer1Zv3qEuZdJ3D/DoxS93xJz2GBmkp2oaVqJVtaHsDJkipaq/m9mQ5pQBPZMOc+xeKTJUtEbcEx+P/TjbOf0yshLHrvAoldNPo8lELzYr1YbxXju79ib3nvXI4tV6ZPFqx01JtdqLLhozr9NknldWOc0cgi8AAFokszqX0clK56L2ksL7ZthxAhMI+JqbO6qubps6duxouK/L5VKnwnYJV321z/OHC4ko7ZCvksL2rYIKSZaDl3buGOmOjfLzkrudzkX52rIj9vS3PR6vNm7brfZ5rriPSaeQwLFx526t3mSu6i3Uhq3N2rDV/1wGmhcnEl4ZPZ9Gz10gpNrj8cb9hnzdlmaVFLaPeRw07txtGLyZHVdocLbHY+6MyOjY61iQp/Z57rDxJGrz9t0aUF4cHHMfl/S/b/aofZ5LXQrdqt/miXq5yCA9kUqaZMOoiY9/pJnjBset+LEzZIp2Umq1YspswGa2wufRxV/bEubE+luUaHhjVAkT73F+4+qTbenlZrYReKR4IUOqp59Gk+5ebGaPU7OvnT/88HBbxuWkfmaJ9KIzEu91mmxfWGQGwRcAACGSWZ3L6GRFUspXrDIzjS3R4Kiyc2FS0+Qiw5uAaNtisTNkMNI+z62unfK1cduuhG6nfZ5bxYXt4gZfklTfuDP4/2aq4Hw+n+Wpo5H2eLz6JkrV3x6PV6s37VCfroobfhk9n5GihVTxxrZ9l8fwuo1Cv8DYA6FAQLQwL9qY4lVpRga/kRVnlaWFWr3ZehgZ7X6E3v/SDvnK795Je7YU6IZRA1VW0inq6qmRQbrVaVipDqMC7A6ZjE5KzVZMmVn59frnl2vnHq9WbdhmOJ5QZo8DMxUj0f4WJRreJFIJE3icP1z9bdLTL600Ao8UK2RIx/TTaNKxsEMg4Gxo3KlbX/zM1GvP7GtHPlluz2B0fU6Zkmpm+qHX5+9Ft3rzDj2yeHXc6zR6nSa7kng82brKqdMRfAEAYCOj4MwJzUytBkdmq4DSxeVyqbJzYUIVTz2KC1TYPk+79nq1vil2nyp3ErdT2blQebHmHRrwRDkDjQyjApVTqRRZdWVlWmHovvEeZyNGVYlmQr/IxzDy8TMKzuKduJR1zFfjzj3BKYjrGltXnHUvLmhVRZeIyPvvcrlU0D5P3/1OTxUW+gOAkdWxq1KtTMNKVxhlJThLZBpZ5IniqVXlcat3zZwob96+R1c+tczUeCSpT1mRqf0SqRhJNLwxUwkTy4atzTpzUK+Ep18mevvRwqPQ57lbxwLdPDc9008jpboXm5XquNDXntnXzsbtu2KO3ycFK4Ot3L5dn3MSCX6s9KKrqiw1FXwZvU6T7QsbS7aucpoNCL4AAGgjzARHPUv29RkyM/0w3Uo75KtPV1mqJJKkTgXtg5U0he3dcftUGd2OmamCPp/P1sq0QBiV7AIFZoRWHcWaVhgZhlqt7jJiVJV44vCT1Oc7Vfr1zdMtX+e6Lc0qLmwfNziLPIEJnGitb2rWjVderq1NjZrxj8ckSRf/8AwdfMih+vXN04NVdH3KipTndgdDwmghWTxmqjLjVaWaDVW6dSzQr579OGVhVCgrwZnVaWSJnija2X8pENL8ZGhf/f3tOtsrgZKpuEu2kXngcU60FUAit280LdXKVEm7p59GSlUvtkSr4wLPiRk9igs1tH/XmOMPPNcvr6hPqjrKqkRfz1buux0Ve8n0hTWSqt6MElVkEsEXAABtSqxAp2dJgb5o2Bb+wcgBn4tmzZqlyZMna8uWLZLC+0wFekft9Xp1eO8uuvtvs3VKzaiwy0dOVTPbpyrWfuUlhYaXT6YyLZpAGJWuHmd7vd640wr7dPGodMO/pW3rtS2/q1Z3OExyJ9YHLiBWL7lkpuPs8Xi1advuuAGUT1JFaQe1z3PFrVj7418fVbv24R+j1zU2t/To2re9pEPr49RIMr30IithyksKtb7JeBpOWcf2everjSkLoyKZPSl+eUW9RlaVxxx/6ElpIieKgcfqy/VbTY0pntCQJr+dOyWVQMlU3CUTSJR1bK+GpmYtXrUpeKJstaonkduPDI+SmSpp5/TTSMn0BY0mmeq8L9dvi/vajwx04o0/8FybCb6+XL8t7DhJRDLBj5UwK9GKvUQqS81K5SqnVJH5EXwBANDGRAt03vryG/101geO/GB03nnn6Xvf+17w55tvvlkvvPCCli1bJklyuxUzZIrWo8xsnyqj/eJdPtHKNCN7vV6Vdmgft5Ksndulso75Ca/eGbiO/3670/D3JXUvq+iJadJ2/5L2nSQN6FihdcdOVVO/0xO+3Vi95JLNX3ebnt7rUmmH9vq8IXYoUtqlS6tt0XqUhR4niRynZkQ7qelc1D54shTtRGrz9j26//VVpq5/w9ZmnXFYZVIVEmaDs8BqcUbjDz0plWT5RDHRBuuxRIY0iVQCxavGSKbiLpkG66FTPa3+PbAaMN44aqC6FRe0uv/JTtVM5fRTybgCM5EKm2Sq8+57faXue31lzNeOT9L5R/fW/E/WhY0pVphptp9V4PYTrXhKNvixGmZZfZ3aER7FOiaSnU5uJJVVZNmG4AsAgDYo9IR8wYp6Xf6Ycz8YdejQQR06dDD8fSBkipTpHmWlHfLVIU/a7XUHVxAMbWhvRTu321QlWa8uHVRS2F7fJthvqn3LipZGly2pe1l9Xp2gyDil/fYG9Xl1glZ/d6bl8MvM85Tndsnn8ei2G67Wi889pXbt2uuHP/mpJv7qerlcLs3755N6/B9/0ddfrVSHoiINGXaCrr55urp26y5J2tHUqOuu+IUWv/m6dmzfrp4Vlbp40hSddd6PJUkN6/6nu269Ue+/9brcbrcOP+pY/Xra7erVe/+o4wmd6ihJpw89TGf/aLw21/9Xc57/p7p06aIbbrhBl156afAyTRvXa+rkK/XqK6/I5XJp8JCh+vW029W3b9+Ej1Ojk5rGlt48pSb79MTSo7gw6Z5GVptBG40/9KR08apNlpv4J1o1FGnSyf11UM9iw5N6K5VAZk6ozYYy0apuzDz2Zla7s/L3wErAGAhNLzyuX9THJ9EwKNnpp9FCisB47HhOo7FjuqDRa6e0ZYXpu1/50tKYrPazqm9s1uWPfxS2LdbtBB7nd1Z+k1DwE/k83f+jwXEXAAkw+zq1IzwyOiYCIeHLK+pjXj7AyjGSyiqybJSemnkAAOBIZlY2mzavNmrz9WTMnz9fnTt3lsfjkSQtW7ZMLpdL1157bXCfn/3sZxo3bpxmzZqlzp07S/JPe5w2bZo+/vhjuVwuuVwuzZo1KxgatNuzTddfPl7HfqdSY048Um8sXGBqPIsWLZLL5dKrr76qo446SkVFRRo2bJi++OKLsP1mzpyp/v37Kz8/XwcffLAeffTRsN+7XC7NnDlTY8aMUceOHXXbbbfpD7f/VicNG6K5T8/WyGMO1bEH76ffXX+VPB6PHpp5j04ZfLBOGnSQ/vanP0QdW+gUOH/IVxQMqEL36dO1SKUd8oMBWSIqOxdqr9Fz7fWo8r1pknytKrBcLUdL5XvTJK8n7u20c7vUu6xIB3TrpAHlxaZCn7nPPqF27drpsXmv6tfTpuvRv/1Zzz3xiCRp7569mnj19XrmX29pxt9na93/1uimKZdL8j82M+64VV99+YXuf+QZPf/6+/rNbXepc5n/RHbPnj2aMO4cderUSW+++aZefuV1FXXsqMt/co727I6/OmfAI3+9X0ceeaQ++ugjXX755ZowYULw+NmzZ49Gjhypss6levutN/XqojfVrUuprrjwXB1QVpBQ6GXmpKawnVuP/ewY3X3u4SrraO02XPKfmIVOi5o5brDKS8OPrfLSwrgnfoGT58D1xhM5/nvOH6QnLjlWb19zSvB2rFRBJVs1FOm4A7vrzEG9NLR/V8MTxkAlTaz9AifUkSf8gRPqBS0nw4HwKt5jd9/rKzX2b+/p+Dte00ufrNPiVZs0/5N1Ov9of4Db+nXr/+++sUfoiUuOjXmcmP17YHSfojETmiYSBoVWN728oj7m/Zf81WZL6jZrzrK1Wrxqk176pF7H3/Gaxv7tPV3x5DKN/dt7OvK3C3XkbxeGbTv+jteCz1FArOf0stlLdc8r/wneTuTjmEx1XkC0186VI76jxh17WoXgkceZEaPXvllGt7Ngxb7H+T4LFajRLh94Tm59sVY3jhqoJy45Nur7RqR4r1M7PiMZHROBkHDs394zNZ1Uan2MeLw+LV61KeoxZaWKrC2g4gsAgDYsVeX18ZxwwgnaunWrPvroIx111FF644031K1bNy1atCi4zxtvvKFrrrkm7HLnnXeeVqxYoQULFuiVV16RJJWWlgZ/f8dtv9Xvf/97zfjjXbr33nv14x//WKtXr1ZZmblm0r/5zW901113qXv37rrsssv005/+VO+8844k6fnnn9cVV1yhGTNmaMSIEZo/f74uuugi7bfffjr55JOD13HzzTfr9ttv14wZM9SuXTs9+OCDWrVqlRYsWKAX5s3X+x/X6leXXaj/rVmtPgf014PPzNeyfy/R1F9N0jEnnKTDjjgqbEyRU+DM9ChLpjn/tua9UR+bjg1LlL/d+ATJJZ/yt9erY8MSba8cGvNx7tWlg+Wwp3fv3rrvTzNU37hLffsfpC8/r9Xsv8/UueMu1PfPHxfcb78+fXXNtDv0ozNO0Y7t2zRw/x7673//qyMHD9Yhhx/hv/2QSq5/zXtOXq9X//jH39W5qEDbmvfqlrvu1/GH9NUHi9/WsOGnmBrfid89TZN/OUkul0vXXHON7r77br3++us6+OCD9dRTT8nr9ervf/978Hl67NGH1blzZ73xxhs67bTTLD0WkrnXbkPTLrldLpWXdtDm7eZDPKNAIpmeRkZTi8yM/8xBvVr93krfMbNVQ5ef1F9PfvBffbt9t63N6aOxWo2RbNVN55aKH6MKOklavGpTzOMkmdU7ozHTCD6RMChadZPR/R9zeIVuffGzuMdHtMrJyIofMyFJZMVV6LRAs/35xg7ZP+ZU5dDXzhmHVer4O15Luuon9LX/zspvTAdVobdz89xPVVzYXhu37dLXG3doxiv/sRxGhy5qYVSFNfHxjzRz3OCo7xtmhFaRbdy6K2Ur2loR7b0nE1Vk2YzgCwCANizZ1doSVVpaqkGDBmnRokU66qijtGjRIl155ZWaNm2atm3bpsbGRq1cuVLDhw8PBk+Sf9pjp06d1K5dO5WXl7e63gsvvFBjx46VJN12223605/+pCVLlqimpsbUuH73u99p+PDhkqRrr71Wo0aNUnNzswoLC/WHP/xBF154oS6/3F9JNGXKFL333nv6wx/+EBZ8/ehHP9JFF10Udr1er1cPPvigiouLNWDgQD3zyD/01aovdf8jT8vtdqv/Qd/RrJn36IN33woGX7GmAJrpUZZoc/6OBXlRe4m137nB1GMYa79kpp8ee+yx6lzkr47avsujEcOP16N/vV8H9+iod97/t26+eZo+r12upsZGeVuayLu2b1Jph/00YcIEnX322fp42UcacvxJGn7a9zToqGMkSSs/+1T//for7ddj30mL1yft2tWs/62uMz2+owcfHrbAQXl5uTZs8D8WH3/8sVauXKni4uKwyzQ3N2vVKvMnkKFS+dqNFUgk0uA8IPTkOdnV4qw0s57/yTpT4zu4vFi3fb/a9ub00Vj90sFqcBipccce+SRdOeIg9e3W0fZeYpL5aYmTTj5Qxx3YzVRoauZ57llSoLvOHRQzUIl2/7/dvlsTH098+mtkcGR1WqZRQBmrv91t3z9Uu/aam8a+YWuzrV9uBV77ibynBMK4H//9fcuXlcJfz+luBG9Gsq+JWKL1ZzM6dqMdU/HYUWmYDQi+AABow5JdrS0Zw4cP16JFi3TVVVfprbfe0vTp0/X000/r7bff1ubNm1VZWamDDjooLPiK57DDDgv+f8eOHVVSUhIMH6xevqKiZUrVhg3af//99dlnn4X1bJKk4447Tvfcc0/YtqOOCq/YkqS+ffsGQ4/SDvk6YP9eKirMV99unYLBU+9eFfJsb9T+ZUWGK01alUhzfqNeYns69DB1m5H79SwpVEE7t+33qWOBv4Jj165dOvvMMzRy5Ehde81sdelapvq1a3XW6O+pwO0/LTj99NO1evVqvfTSS1q4cKF+PvYs/ezSyzT9jjuV79utI488Uo899ljwNrY279bab5vVpWv8gKd9nlt5breKi8L70LlcrmAAt23btla3EdC9e/eEHodUvHatBBKJCg3OzARfRuO30nfMymM1tH9Xy83pE5FIyGRH1c2TH/xXb19zStTnN12rdx7Us5Pp8NTM83zzmEN03IHd5PH64lY3Be6/JMN9rQgNjlLZoyuyv50ZPYoLUxKQpzsoiXw9W+3vZ1YyfQCTfU3EEq2C0e1KbrVjyd4K1mxA8AUAQBtmpWrCbieddJIefPBBffzxx2rfvr0GDBigk046SYsWLdK3334brLyyon379mE/h4YPVi8fCGisXF7yB25mxtWhIF+di/LDtrVzK2xbpkSbKrm9fIj2dKxQu+0NwZ5eoXxyaU/Hcm0vHyLJ/sUF3n8/vFLgvffe00EHHaTPP/9cmzZt0u23367evXtLkmZ/+kmry3fv3l3jx4/X+PHjdcIJJ+jqq6/WvffcrSOPPFJPP/20evTooZKSkuD+B+zcHXWqaGVpoQrb56m4oJ0O6NZJHQvyFC8jGjx4sJ566qlWt5EMq69dM/teeep30tbk2I73HrMrs1m9rWSmdJqVaMiUbNVNrEAg2eckVV+kmH2erfY0snN1z8BxkqzIHl0bt+1qdfxZeZ7M9m+yMnari1UkK/J5TkWYl+iURLteE9FcMLSPunYsiFrBmGzbVbsrWLMBze0BAGjDYjWdTvUHo0Cfr7vvvjsYcgWCr0WLFumkk06Kern8/PxgU/x0GjhwYKvqs3feeUdVVVVpH0s6lHbI14DyYh3QrZP2LyvSAT1K1W7UHS3HRfSjxXPadO3frdhS03qz1qxZoylTpuiLL77QE088oXvvvVdXXHGF9t9/f+Xn5+vee+/VV199pblz5+rWW28Nu+xNN92kOXPmaOXKlfr00081f/58DRw4UJL04x//WN26ddOZZ56pt956S3V1dVq0aJFuvOZX6rS3sSXcaqei/Hb++1SUrzy3SwXt89SpsJ2pCjaj2/jlL3+p//3vfwk9HlZeu5l8nRuxa0w11RV6+5pTYjazTuS2zDSnT0a8hvWRiwtESuaE2igQSPY5SfY+xWLmebYSiNg9fT8QTplZhCCeyP52kceflecpFc+J1cUqEjXp5AOjPs+pCFgTmZJox2silpFV5XrygzUpCRfNLEqSaxIKvu6//3717dtXhYWFOuaYY7RkyZKY+z/zzDMaMGCACgsLdeihh+qll15KaLAAAMB+yazWlowuXbrosMMO02OPPRYMuU488UQtXbpU//nPfwwrvvr27au6ujotW7ZMGzdu1K5du1IyvkhXX321Zs2apZkzZ+rLL7/UH//4Rz333HP61a9+lZbbz4TAtMLORfn+kKfqTOncR6SSiGOipFKucx9Rh8O/v2/fJKc0Rrrgggu0c+dODRkyRBMnTtQVV1yhSy+9VN27d9esWbP0zDPPqKqqSrfffrv+8IfwFTLz8/N13XXX6bDDDtOJJ56ovLw8Pfnkk5KkoqIivfnmm9p///31gx/8QAMHDtTFF1+s5uZmlZaWqlNhO+W3c6tdnivh+xTrNpKpALPy2s3U6zwWu8ZkJqRy2v1PdcgUS6xAIFWrd9oRsMZ7nq0EInZN1wsNjuwOhGKFc2afp1Q9J8mu9BhL4DG98tTvRH2eUxHmJdoL0e4VbQP7VZQWSi57qxIlfxVZvJUuc5XlqY5PPfWUpkyZogceeEDHHHOMZsyYoZEjR+qLL75Qjx6tez+8++67Gjt2rKZPn64zzjhDjz/+uM466ywtXbpU1dXVttwJAACQnHRM7Ylm+PDhWrZsWTD4KisrU1VVldavX6+DDz446mXOPvtsPffcczr55JO1ZcsWPfTQQ7rwwgtTOk5JOuuss3TPPffoD3/4g6644gr169dPDz30kGFlWs6qGiMNGCWtflfatl7q1FPqM0xy56XsJkNX+5w5c2ar348dOza4qEGAz7fve/IbbrhBN9xwg+H1l5eX6+GHHzb8/axZswzHI0lff/11q8ssW7bM0m0kysprN1Ov81jSOSan3X+zU/iisbrSo2R+6noqVu+0u0daNHZO/zUjWnCU7CIEoeKFc2afp1Q9J9Fu/9vtu3Xri4nfdzNhnJX+fmaZDUJvHDVQ3YoLUraibej4N26z/0u906srbF2hO5u4fKGfCkw45phjdPTRR+u+++6T5O970bt3b/3iF7/Qtdde22r/8847T9u3b9f8+fOD24499lgNGjRIDzzwgKnbbGpqUmlpqRobG23riwAAQC5obm5WXV2d+vXrp8LCtrEyD9BW8PpuGzxeX8JhnNlV6ALXlq7qtmTuUzICDcql6IFI6P032jeazi0NxkMbzlfECI5C739gpUkztxMYa3lpoeEiBIlK13MSejvdOhboqmc+1vomcwFjrMc0UrRj38rlI8d8/B2vxQ1Nk3lOIh//aCFh6PgXr9qksX97L6HbSsX4nchKTmSp4mv37t368MMPdd111wW3ud1ujRgxQosXL456mcWLF2vKlClh20aOHKkXXnjB8HZ27doVNm2hqanJyjABAAAAICuErnRpldmqm3RUXIVK5j4lw0p1k9G+FaWFunHUQHXpWBAWEkkyHRxF3v+DyztZrvixO6BI13MSeTs3jzGuzvJJunLEQerbraPlMM7OCs5UVJFFu43Ix39ktfH4zSwi4HbFb3TfFhvZR2Mp+Nq4caM8Ho969uwZtr1nz576/PPPo16moaEh6v4NDQ2GtzN9+nRNmzbNytAAAAAMXXbZZZo9e3bU340bN850FToAOI3VE+pcl8rpv7kWUKZDKqe/2hnmZWKabqzxmwnj7ht7RFhA21aOqURY7vGVDtddd11YlVhTU1NweWoAAACrbrnlFsMm9LRRAJBrMlVx5RRW7n+mKqGkthNQOq2/nhGnjTORMK6tHFNWWQq+unXrpry8PK1fvz5s+/r161VeXh71MuXl5Zb2l6SCggIVFBRYGRoAAIChHj16RF2EBwCATGlLAWW23FenjdNqGOe08TuF28rO+fn5OvLII/Xqq68Gt3m9Xr366qsaOnRo1MsMHTo0bH9JWrhwoeH+AADAOq/Xm+khALAZr2sAQCDMOnNQLw3t35UKrgRYnuo4ZcoUjR8/XkcddZSGDBmiGTNmaPv27broooskSRdccIF69eql6dOnS5KuuOIKDR8+XHfddZdGjRqlJ598Uv/+97/117/+1d57AgBAG5Sfny+3261169ape/fuys/Pl8vFByIgm/l8Pu3evVvffPON3G638vPzMz0kAACyluXg67zzztM333yjm266SQ0NDRo0aJAWLFgQbGC/Zs0aud37CsmGDRumxx9/XDfccIOuv/56HXTQQXrhhRdUXV1t370AAKCNcrvd6tevn+rr67Vu3bpMDweAjYqKirT//vuHfbYGAADWuHw+X5wFMDOvqalJpaWlamxspAEtAABR+Hw+7d27Vx6PJ9NDAWCDvLw8tWvXjgpOAACisJITOXJVRwAAYI3L5VL79u3Vvn37TA8FAAAAcAzqpgEAAAAAAJCTCL4AAAAAAACQkwi+AAAAAAAAkJOyosdXoP9+U1NThkcCAAAAAACATArkQ2bWa8yK4Gvr1q2SpN69e2d4JAAAAAAAAHCCrVu3qrS0NOY+Lp+ZeCzDvF6v1q1bp+Li4pxZ0rmpqUm9e/fWf//737hLbyL3cTwgEscEInFMIBLHBEJxPCASxwQicUwgUjYfEz6fT1u3blVlZaXc7thdvLKi4svtdmu//fbL9DBSoqSkJOsOMKQOxwMicUwgEscEInFMIBTHAyJxTCASxwQiZesxEa/SK4Dm9gAAAAAAAMhJBF8AAAAAAADISQRfGVJQUKCpU6eqoKAg00OBA3A8IBLHBCJxTCASxwRCcTwgEscEInFMIFJbOSayork9AAAAAAAAYBUVXwAAAAAAAMhJBF8AAAAAAADISQRfAAAAAAAAyEkEXwAAAAAAAMhJBF8AAAAAAADISQRfGXD//ferb9++Kiws1DHHHKMlS5ZkekhIk+nTp+voo49WcXGxevToobPOOktffPFF2D4nnXSSXC5X2H+XXXZZhkaMVLr55ptbPdcDBgwI/r65uVkTJ05U165d1alTJ5199tlav359BkeMVOvbt2+rY8LlcmnixImSeH9oC958802NHj1alZWVcrlceuGFF8J+7/P5dNNNN6miokIdOnTQiBEj9OWXX4bts3nzZv34xz9WSUmJOnfurIsvvljbtm1L472AnWIdE3v27NE111yjQw89VB07dlRlZaUuuOACrVu3Luw6or233H777Wm+J7BLvPeJCy+8sNXzXVNTE7YP7xO5I97xEO1zhcvl0p133hnch/eI3GLmnNPMecaaNWs0atQoFRUVqUePHrr66qu1d+/edN4V2xB8pdlTTz2lKVOmaOrUqVq6dKkOP/xwjRw5Uhs2bMj00JAGb7zxhiZOnKj33ntPCxcu1J49e3Taaadp+/btYftdcsklqq+vD/73+9//PkMjRqodcsghYc/122+/HfzdlVdeqXnz5umZZ57RG2+8oXXr1ukHP/hBBkeLVPvggw/CjoeFCxdKkn74wx8G9+H9Ibdt375dhx9+uO6///6ov//973+vP/3pT3rggQf0/vvvq2PHjho5cqSam5uD+/z4xz/Wp59+qoULF2r+/Pl68803demll6brLsBmsY6JHTt2aOnSpbrxxhu1dOlSPffcc/riiy80ZsyYVvvecsstYe8dv/jFL9IxfKRAvPcJSaqpqQl7vp944omw3/M+kTviHQ+hx0F9fb0efPBBuVwunX322WH78R6RO8ycc8Y7z/B4PBo1apR2796td999Vw8//LBmzZqlm266KRN3KXk+pNWQIUN8EydODP7s8Xh8lZWVvunTp2dwVMiUDRs2+CT53njjjeC24cOH+6644orMDQppM3XqVN/hhx8e9XdbtmzxtW/f3vfMM88Et3322Wc+Sb7FixenaYTItCuuuMLXv39/n9fr9fl8vD+0NZJ8zz//fPBnr9frKy8v9915553BbVu2bPEVFBT4nnjiCZ/P5/PV1tb6JPk++OCD4D4vv/yyz+Vy+dauXZu2sSM1Io+JaJYsWeKT5Fu9enVwW58+fXx33313ageHjIh2TIwfP9535plnGl6G94ncZeY94swzz/SdcsopYdt4j8htkeecZs4zXnrpJZ/b7fY1NDQE95k5c6avpKTEt2vXrvTeARtQ8ZVGu3fv1ocffqgRI0YEt7ndbo0YMUKLFy/O4MiQKY2NjZKksrKysO2PPfaYunXrpurqal133XXasWNHJoaHNPjyyy9VWVmpAw44QD/+8Y+1Zs0aSdKHH36oPXv2hL1fDBgwQPvvvz/vF23E7t27NXv2bP30pz+Vy+UKbuf9oe2qq6tTQ0ND2PtCaWmpjjnmmOD7wuLFi9W5c2cdddRRwX1GjBght9ut999/P+1jRvo1NjbK5XKpc+fOYdtvv/12de3aVUcccYTuvPPOrJ2uAnMWLVqkHj166OCDD9aECRO0adOm4O94n2i71q9frxdffFEXX3xxq9/xHpG7Is85zZxnLF68WIceeqh69uwZ3GfkyJFqamrSp59+msbR26NdpgfQlmzcuFEejyfs4JGknj176vPPP8/QqJApXq9XkydP1nHHHafq6urg9h/96Efq06ePKisr9cknn+iaa67RF198oeeeey6Do0UqHHPMMZo1a5YOPvhg1dfXa9q0aTrhhBO0YsUKNTQ0KD8/v9WJS8+ePdXQ0JCZASOtXnjhBW3ZskUXXnhhcBvvD21b4LUf7XNE4HcNDQ3q0aNH2O/btWunsrIy3jvagObmZl1zzTUaO3asSkpKgtt/+ctfavDgwSorK9O7776r6667TvX19frjH/+YwdEiVWpqavSDH/xA/fr106pVq3T99dfr9NNP1+LFi5WXl8f7RBv28MMPq7i4uFXrDN4jcle0c04z5xkNDQ1RP28EfpdtCL6ADJk4caJWrFgR1tNJUlh/hUMPPVQVFRX67ne/q1WrVql///7pHiZS6PTTTw/+/2GHHaZjjjlGffr00dNPP60OHTpkcGRwgn/84x86/fTTVVlZGdzG+wMAI3v27NG5554rn8+nmTNnhv1uypQpwf8/7LDDlJ+fr5///OeaPn26CgoK0j1UpNj5558f/P9DDz1Uhx12mPr3769Fixbpu9/9bgZHhkx78MEH9eMf/1iFhYVh23mPyF1G55xtDVMd06hbt27Ky8trtVrC+vXrVV5enqFRIRMmTZqk+fPn6/XXX9d+++0Xc99jjjlGkrRy5cp0DA0Z1LlzZ33nO9/RypUrVV5ert27d2vLli1h+/B+0TasXr1ar7zyin72s5/F3I/3h7Yl8NqP9TmivLy81YI5e/fu1ebNm3nvyGGB0Gv16tVauHBhWLVXNMccc4z27t2rr7/+Oj0DREYdcMAB6tatW/BvBe8TbdNbb72lL774Iu5nC4n3iFxhdM5p5jyjvLw86ueNwO+yDcFXGuXn5+vII4/Uq6++Gtzm9Xr16quvaujQoRkcGdLF5/Np0qRJev755/Xaa6+pX79+cS+zbNkySVJFRUWKR4dM27Ztm1atWqWKigodeeSRat++fdj7xRdffKE1a9bwftEGPPTQQ+rRo4dGjRoVcz/eH9qWfv36qby8POx9oampSe+//37wfWHo0KHasmWLPvzww+A+r732mrxebzAoRW4JhF5ffvmlXnnlFXXt2jXuZZYtWya3291quhty0//+9z9t2rQp+LeC94m26R//+IeOPPJIHX744XH35T0iu8U75zRznjF06FAtX748LCQPfLFSVVWVnjtiI6Y6ptmUKVM0fvx4HXXUURoyZIhmzJih7du366KLLsr00JAGEydO1OOPP645c+aouLg4OD+6tLRUHTp00KpVq/T444/re9/7nrp27apPPvlEV155pU488UQddthhGR497ParX/1Ko0ePVp8+fbRu3TpNnTpVeXl5Gjt2rEpLS3XxxRdrypQpKisrU0lJiX7xi19o6NChOvbYYzM9dKSQ1+vVQw89pPHjx6tdu31/pnl/aBu2bdsWVsFXV1enZcuWqaysTPvvv78mT56s3/72tzrooIPUr18/3XjjjaqsrNRZZ50lSRo4cKBqamp0ySWX6IEHHtCePXs0adIknX/++WHTZpE9Yh0TFRUVOuecc7R06VLNnz9fHo8n+NmirKxM+fn5Wrx4sd5//32dfPLJKi4u1uLFi3XllVdq3Lhx6tKlS6buFpIQ65goKyvTtGnTdPbZZ6u8vFyrVq3Sr3/9ax144IEaOXKkJN4nck28vxuS/0uSZ555RnfddVery/MekXvinXOaOc847bTTVFVVpZ/85Cf6/e9/r4aGBt1www2aOHFidk5/zfCqkm3Svffe69t///19+fn5viFDhvjee++9TA8JaSIp6n8PPfSQz+fz+dasWeM78cQTfWVlZb6CggLfgQce6Lv66qt9jY2NmR04UuK8887zVVRU+PLz8329evXynXfeeb6VK1cGf79z507f5Zdf7uvSpYuvqKjI9/3vf99XX1+fwREjHf71r3/5JPm++OKLsO28P7QNr7/+etS/E+PHj/f5fD6f1+v13Xjjjb6ePXv6CgoKfN/97ndbHSubNm3yjR071tepUydfSUmJ76KLLvJt3bo1A/cGdoh1TNTV1Rl+tnj99dd9Pp/P9+GHH/qOOeYYX2lpqa+wsNA3cOBA32233eZrbm7O7B1DwmIdEzt27PCddtppvu7du/vat2/v69Onj++SSy7xNTQ0hF0H7xO5I97fDZ/P5/vLX/7i69Chg2/Lli2tLs97RO6Jd87p85k7z/j66699p59+uq9Dhw6+bt26+a666irfnj170nxv7OHy+Xy+FOZqAAAAAAAAQEbQ4wsAAAAAAAA5ieALAAAAAAAAOYngCwAAAAAAADmJ4AsAAAAAAAA5ieALAAAAAAAAOYngCwAAAAAAADmJ4AsAAAAAAAA5ieALAAAgx/Tt21czZszI9DAAAAAyjuALAAAgCRdeeKHOOussSdJJJ52kyZMnp+22Z82apc6dO7fa/sEHH+jSSy9N2zgAAACcql2mBwAAAIBwu3fvVn5+fsKX7969u42jAQAAyF5UfAEAANjgwgsv1BtvvKF77rlHLpdLLpdLX3/9tSRpxYoVOv3009WpUyf17NlTP/nJT7Rx48bgZU866SRNmjRJkydPVrdu3TRy5EhJ0h//+Ecdeuih6tixo3r37q3LL79c27ZtkyQtWrRIF110kRobG4O3d/PNN0tqPdVxzZo1OvPMM9WpUyeVlJTo3HPP1fr164O/v/nmmzVo0CA9+uij6tu3r0pLS3X++edr69atqX3QAAAAUozgCwAAwAb33HOPhg4dqksuuUT19fWqr69X7969tWXLFp1yyik64ogj9O9//1sLFizQ+vXrde6554Zd/uGHH1Z+fr7eeecdPfDAA5Ikt9utP/3pT/r000/18MMP67XXXtOvf/1rSdKwYcM0Y8YMlZSUBG/vV7/6Vatxeb1enXnmmdq8ebPeeOMNLVy4UF999ZXOO++8sP1WrVqlF154QfPnz9f8+fP1xhtv6Pbbb0/RowUAAJAeTHUEAACwQWlpqfLz81VUVKTy8vLg9vvuu09HHHGEbrvttuC2Bx98UL1799Z//vMffec735EkHXTQQfr9738fdp2h/cL69u2r3/72t7rsssv05z//Wfn5+SotLZXL5Qq7vUivvvqqli9frrq6OvXu3VuS9Mgjj+iQQw7RBx98oKOPPlqSPyCbNWuWiouLJUk/+clP9Oqrr+p3v/tdcg8MAABABlHxBQAAkEIff/yxXn/9dXXq1Cn434ABAyT5q6wCjjzyyFaXfeWVV/Td735XvXr1UnFxsX7yk59o06ZN2rFjh+nb/+yzz9S7d+9g6CVJVVVV6ty5sz777LPgtr59+wZDL0mqqKjQhg0bLN1XAAAAp6HiCwAAIIW2bdum0aNH64477mj1u4qKiuD/d+zYMex3X3/9tc444wxNmDBBv/vd71RWVqa3335bF198sXbv3q2ioiJbx9m+ffuwn10ul7xer623AQAAkG4EXwAAADbJz8+Xx+MJ2zZ48GD985//VN++fdWunfmPXh9++KG8Xq/uuusuud3+Iv2nn3467u1FGjhwoP773//qv//9b7Dqq7a2Vlu2bFFVVZXp8QAAAGQjpjoCAADYpG/fvnr//ff19ddfa+PGjfJ6vZo4caI2b96ssWPH6oMPPtCqVav0r3/9SxdddFHM0OrAAw/Unj17dO+99+qrr77So48+Gmx6H3p727Zt06uvvqqNGzdGnQI5YsQIHXroofrxj3+spUuXasmSJbrgggs0fPhwHXXUUbY/BgAAAE5C8AUAAGCTX/3qV8rLy1NVVZW6d++uNWvWqLKyUu+88448Ho9OO+00HXrooZo8ebI6d+4crOSK5vDDD9cf//hH3XHHHaqurtZjjz2m6dOnh+0zbNgwXXbZZTrvvPPUvXv3Vs3xJf+UxTlz5qhLly468cQTNWLECB1wwAF66qmnbL//AAAATuPy+Xy+TA8CAAAAAAAAsBsVXwAAAAAAAMhJBF8AAAAAAADISQRfAAAAAAAAyEkEXwAAAAAAAMhJBF8AAAAAAADISQRfAACgzbnwwgvVt2/fhC578803y+Vy2TsgAAAApATBFwAAcAyXy2Xqv0WLFmV6qAAAAMgCLp/P58v0IAAAACRp9uzZYT8/8sgjWrhwoR599NGw7aeeeqp69uyZ8O3s2bNHXq9XBQUFli+7d+9e7d27V4WFhQnfPgAAANKD4AsAADjWpEmTdP/99yvex5UdO3aoqKgoTaOCGT6fT83NzerQoUOmhwIAANowpjoCAICsctJJJ6m6uloffvihTjzxRBUVFen666+XJM2ZM0ejRo1SZWWlCgoK1L9/f916663yeDxh1xHZ4+vrr7+Wy+XSH/7wB/31r39V//79VVBQoKOPPloffPBB2GWj9fhyuVyaNGmSXnjhBVVXV6ugoECHHHKIFixY0Gr8ixYt0lFHHaXCwkL1799ff/nLX0z3DXvrrbf0wx/+UPvvv78KCgrUu3dvXXnlldq5c2erfT///HOde+656t69uzp06KCDDz5Yv/nNb8L2Wbt2rS6++OLg49WvXz9NmDBBu3fvNryvkjRr1iy5XC59/fXXwW19+/bVGWecoX/961866qij1KFDB/3lL3+RJD300EM65ZRT1KNHDxUUFKiqqkozZ86Meh9ffvllDR8+XMXFxSopKdHRRx+txx9/XJI0depUtW/fXt98802ry1166aXq3Lmzmpub4z6OAACg7WiX6QEAAABYtWnTJp1++uk6//zzNW7cuOC0x1mzZqlTp06aMmWKOnXqpNdee0033XSTmpqadOedd8a93scff1xbt27Vz3/+c7lcLv3+97/XD37wA3311Vdq3759zMu+/fbbeu6553T55ZeruLhYf/rTn3T22WdrzZo16tq1qyTpo48+Uk1NjSoqKjRt2jR5PB7dcsst6t69u6n7/cwzz2jHjh2aMGGCunbtqiVLlujee+/V//73Pz3zzDPB/T755BOdcMIJat++vS699FL17dtXq1at0rx58/S73/1OkrRu3ToNGTJEW7Zs0aWXXqoBAwZo7dq1evbZZ7Vjxw7l5+ebGlOoL774QmPHjtXPf/5zXXLJJTr44IMlSTNnztQhhxyiMWPGqF27dpo3b54uv/xyeb1eTZw4MXj5WbNm6ac//akOOeQQXXfddercubM++ugjLViwQD/60Y/0k5/8RLfccoueeuopTZo0KXi53bt369lnn9XZZ5/NFFQAABDOBwAA4FATJ070RX5cGT58uE+S74EHHmi1/44dO1pt+/nPf+4rKiryNTc3B7eNHz/e16dPn+DPdXV1Pkm+rl27+jZv3hzcPmfOHJ8k37x584Lbpk6d2mpMknz5+fm+lStXBrd9/PHHPkm+e++9N7ht9OjRvqKiIt/atWuD27788ktfu3btWl1nNNHu3/Tp030ul+v/2bvv8KjKvI3j98ykkwakh9ARCL0ICxZEWYFVELCLUlbXtWBji7qviuyuoq6rrKvi6qroCnZAbFgQrCgivfdOQk3vM+f942QmmRRIIMnJTL6f68qVmXOec85vMoTM3PMUY8+ePZ5t559/vhEREeG1zTAMw+VyeW5PmDDBsNvtxs8//1zpnO52VT1WwzCMV1991ZBk7Nq1y7OtTZs2hiRj0aJFNap7+PDhRvv27T33MzIyjIiICGPgwIFGfn5+tXUPGjTIGDhwoNf+efPmGZKMJUuWVLoOAABo2hjqCAAAfE5wcLAmT55caXv5+aSys7N19OhRnXfeecrLy9PmzZtPed6rr75azZs399w/77zzJEk7d+485bHDhg1Thw4dPPd79uypyMhIz7FOp1NffvmlxowZo6SkJE+7jh07auTIkac8v+T9+HJzc3X06FENHjxYhmFo1apVkqQjR47om2++0W9/+1u1bt3a63j3sEWXy6UFCxZo1KhR6t+/f6Xr1GTYZVXatWun4cOHn7TuzMxMHT16VEOGDNHOnTuVmZkpSfriiy+UnZ2t++67r1KvrfL1TJgwQT/99JN27Njh2TZnzhylpKRoyJAhp1U3AADwXwRfAADA5yQnJ1c5FG/Dhg0aO3asoqKiFBkZqdjYWF1//fWS5AlYTqZiUOQOwU6cOFHrY93Hu489fPiw8vPz1bFjx0rtqtpWlb1792rSpElq0aKFwsPDFRsb6wl73I/PHbR179692vMcOXJEWVlZJ21zOtq1a1fl9u+//17Dhg1Ts2bNFB0drdjYWM+8bO663UHWqWq6+uqrFRwcrDlz5niO/+ijjzR+/PjTDuwAAID/Yo4vAADgc6paKTAjI0NDhgxRZGSk/vrXv6pDhw4KCQnRypUrde+998rlcp3yvA6Ho8rtRg0WwT6TY2vC6XTq17/+tY4fP657771XXbp0UbNmzXTgwAFNmjSpRo+vtqoLkiouFuBW1fOyY8cOXXTRRerSpYueeuoppaSkKCgoSJ988omefvrpWtfdvHlzXXrppZozZ44eeughvffeeyosLPQEnAAAAOURfAEAAL+wdOlSHTt2TPPmzdP555/v2b5r1y4LqyoTFxenkJAQbd++vdK+qrZVtG7dOm3dulWvvfaaJkyY4Nn+xRdfeLVr3769JGn9+vXVnis2NlaRkZEnbSOV9XjLyMhQdHS0Z/uePXtOWa/bhx9+qMLCQi1cuNCrV9ySJUu82rmHia5fv/6UPeAmTJigyy67TD///LPmzJmjPn36qFu3bjWuCQAANB0MdQQAAH7B3eOqfA+roqIiPf/881aV5MXhcGjYsGFasGCBDh486Nm+fft2ffrppzU6XvJ+fIZh6F//+pdXu9jYWJ1//vl65ZVXtHfvXq997mPtdrvGjBmjDz/8UCtWrKh0LXc7dxj1zTffePbl5ubqtddeO2W9J6s7MzNTr776qle7iy++WBEREZoxY4YKCgqqrMdt5MiRiomJ0eOPP66vv/6a3l4AAKBa9PgCAAB+YfDgwWrevLkmTpyoO++8UzabTf/73//qbKhhXXj44Yf1+eef65xzztGtt94qp9OpZ599Vt27d9fq1atPemyXLl3UoUMH/fGPf9SBAwcUGRmp999/v8r5x5555hmde+656tu3r26++Wa1a9dOu3fv1scff+y5zqOPPqrPP/9cQ4YM0c0336yuXbvq0KFDevfdd/Xdd98pOjpaF198sVq3bq0bb7xRf/rTn+RwOPTKK68oNja2UqhWnYsvvlhBQUEaNWqUfv/73ysnJ0cvvfSS4uLidOjQIU+7yMhIPf3007rpppt09tln67rrrlPz5s21Zs0a5eXleYVtgYGBuuaaa/Tss8/K4XDo2muvrVEtAACg6aHHFwAA8AstW7bURx99pMTERD3wwAN68skn9etf/1pPPPGE1aV59OvXT59++qmaN2+uBx98UC+//LL++te/6qKLLqq0kmFFgYGB+vDDD9W7d2/NmDFD06dPV6dOnfT6669XaturVy/9+OOPOv/88zVr1izdeeedev/99zV69GhPm+TkZP3000+64oorNGfOHN155516/fXXdcEFFygsLMxzzfnz56tDhw568MEH9cwzz+imm27SlClTavyYO3furPfee082m01//OMf9cILL+jmm2/WXXfdVantjTfeqIULFyoyMlJ/+9vfdO+992rlypVVrnrpHu550UUXKTExscb1AACApsVmNKaPQQEAAJqgMWPGaMOGDdq2bZvVpfiMNWvWqHfv3nr99dd1ww03WF0OAABopOjxBQAA0IDy8/O97m/btk2ffPKJLrjgAmsK8lEvvfSSwsPDNW7cOKtLAQAAjRhzfAEAADSg9u3ba9KkSWrfvr327NmjWbNmKSgoSH/+85+tLs0nfPjhh9q4caNefPFFTZkyRc2aNbO6JAAA0Igx1BEAAKABTZ48WUuWLFFaWpqCg4M1aNAgPfroo+rbt6/VpfmEtm3bKj09XcOHD9f//vc/RUREWF0SAABoxAi+AAAAAAAA4JeY4wsAAAAAAAB+ySfm+HK5XDp48KAiIiJks9msLgcAAAAAAAAWMQxD2dnZSkpKkt1+8j5dPhF8HTx4UCkpKVaXAQAAAAAAgEZi3759atWq1Unb+ETw5Z60dN++fYqMjLS4GgAAAAAAAFglKytLKSkpNVrkxieCL/fwxsjISIIvAAAAAAAA1Gg6LCa3BwAAAAAAgF8i+AIAAAAAAIBfIvgCAAAAAACAXyL4AgAAAAAAgF8i+AIAAAAAAIBfIvgCAAAAAACAXyL4AgAAAAAAgF8i+AIAAAAAAIBfqnXw9c0332jUqFFKSkqSzWbTggULTnnM0qVL1bdvXwUHB6tjx46aPXv2aZQKAAAAAAAaC6fL0LIdx/TB6gNatuOYnC7D6pKASgJqe0Bubq569eql3/72txo3btwp2+/atUuXXHKJbrnlFs2ZM0eLFy/WTTfdpMTERA0fPvy0igYAAGhqnC5Dy3cd1+HsAsVFhGhAuxZy2G1Wl4UKeJ4aP54j38Dz1PgtWn9I0z/cqEOZBZ5tiVEhmjYqVSO6J1pYGeDNZhjGaUeyNptN8+fP15gxY6ptc++99+rjjz/W+vXrPduuueYaZWRkaNGiRVUeU1hYqMLCQs/9rKwspaSkKDMzU5GRkadbLgAAgE/izYVv4Hlq/HiOfAPPU+O3aP0h3frGSlUME9zR5Kzr+/JcoV5lZWUpKiqqRjlRvc/xtWzZMg0bNsxr2/Dhw7Vs2bJqj5kxY4aioqI8XykpKfVdJgCgntAFHjgz7jcX5d8ASlJaZoFufWOlFq0/ZFFlKI/nqfHjOfINPE8NxzAMOV2GikpcKih2KqewRJn5xTqRW6Qj2YVKzyrQwYx87Tuep91Hc7XjSI62pmdr/YFMPbBgfaXQS5KM0q9pCzfoeE6RCoqdcvHaz1K8Fj+NoY61lZaWpvj4eK9t8fHxysrKUn5+vkJDQysdc//992vq1Kme++4eXwAA38Intr6B4SSNl9NlaPqHG6t9c2GTNP3Djfp1akKjes7cAwoMo/RNkGGUfi/dL6PsdoVt5dt79hve+1XhnIbKdpTfVql9uWu6r1BdjfKqp0J7w/sxOF2G/u8kbwIl6YEF6xUXEdKonqemxOky9H/zT/0cJUaFymG3yWaTbCr9XvqUee7Lvc3cUbbNVm5fWXtPm9I7Ns8xZedz7/C+RuX28qrH+5q2cvWowjWqbG/zXLnROJP/89whjtP93WXI5ZKchqESl8tz2+n0buN0GXIZhkrK33aa3z3nKz3G5TLbufeVuMxt1Z3P5TLkdElOl6u0Teltl0rblN6ucN7Tuab7OFeF48uuachlyPtnUY8BSHpWofr+/QvP/UCHTUEOu4IDHaXf7V7fgwLsCg5wlH4vux/sdd/8XvV5vO+HBNoV5HBUOL9dAY6mtb4fr8VN9R58nY7g4GAFBwdbXQYA4AxU1wXe/YktXeAbB14QVc39BqrYaajI6VJx6VdJ+fsl5u0Sp0vFTkPFTpdnn3e70v0u8xj3uYpK25UdZ6i4xKUSl0tFpbeP5RZW6vXgVaekQ5kFOvuRLxQc4KgU+FQd4niHSp77JwmVysIpd7vqQyhU7WhOkcbN+sHqMnASR3OKdNlz31tdhiUqhndl20rDNlUXANoqBHel28oFet6BYMWw0DucKypx6lhucbV1uv/P6z5tkew2m1cg1AQ7sdQ7h91mftlsZbftNhWXuJRdWFKrc5l/J53KLXLWU7U1Y7epioDNrqDSkM19v3z4VjmoqxyoVR3UVXWdsvCuvsNnXouXqffgKyEhQenp6V7b0tPTFRkZWWVvLwCA7zvVJ7aS9NAHG5SaFKVAh0022WS3lb1YtttK78smm928b1Pp99JPu802jfuT68auIV8QGaWffnsFRq6y28WeL6PcbZeKSsxPxMsfVylYqnhciVFNsGS2K3GWBkvlgylPbS5P2OVLjucWS6r+zWJTcCZv3M1tqvTG3b39VG/cC4pdysw/9c+/eVigwoIa5efOfi+vqEQn8k79HEWGBigkwFGnwbGnraedd5jcGILjsvoqFtE406T84tr9H22zyTu8sdnkcJjf7XabAuzma4ry4Y7XPrtNDpsUYLfLbncHQnY5bPI6xl7xGhX2Bbhv1/ia5W7bVXrNcrdLXyN56qrqmtU8zvLXcP8sytdpP0nv1GU7junal3485c/9jRsHqHfr5ioqcamwxKmiElfpbfeX03Pf+7t32yKnS4XFztLvLhWWfq+4vcjpfbz7fCXlUlGXIeUXO5VfbG0AJ8krZKs2JHNU3ROuYtuK+wLtNv3lJL1cG2uP8fpS7395Bw0apE8++cRr2xdffKFBgwbV96UBAHWooNipE3lFOpFbrIy8Ip3IK9aJvKIKt83vhzIKlJZVfS8VSTqcXajzn1hSZ/W534S6AzHZzE/1yodmKh+qlftedpx3+Fb+e/lz27yOd7dxn0eV28gmu73szXb5c9tL30jbq7qmzeb1GKq8ZoXHaStXY6XHU+5xuiTN/n73ScPJqe+s0ddbj5hDK7yCJqNS4FRUUhpIucyeSkUVAqxiZ+N881QbgQ6bAh32cl82r+9BAXYF2Mtuu/cFOMwXrt7Hl90225pvXAID7Aoqt2/nkRw9/eW2U9b26Jju6tEquvohTpWGULmPPPmwqvJ5svvfZFXDqspCo4qhUOWhX+6hWjW+ZoXhYI1xqFZN3wQ+P76fBnVo2QAVoaKaPkf/ub5/o3iO3ENz3cGYVHkYb1nbykOFy7d3h21lbasefixDlbZ5tT/JNVXpnBWvV3mIc1VDntfuz9D/LShbFK06T13VS/3btCgXQpULbiqGSraThziovQHtWigxKkRpmQVVvo6wSUqICtGgDjFmqGLxYC73PGblA7HCqgI2T9DmLBeslQVshZUCNpeKSpzVBnfu++UDvPLc+7It+Jm4e08u33W8UfyfV99qHXzl5ORo+/btnvu7du3S6tWr1aJFC7Vu3Vr333+/Dhw4oNdff12SdMstt+jZZ5/Vn//8Z/32t7/VV199pXfeeUcff/xx3T0KAECNuVyGsgtKzBCrXFh1Is+czLTitozSdgW1/HS1JtwvTF2lL5ZdFV5Y14b7xbXL/SofZyyvyKk3l++rt/N7wqAAuwLspYFPwEmCJYddAeVuBzrsCgwwAyN3eFRlsFSunRk0Vd2uymuUtg2w2ywJWZwuQ2/9vO+Uby6uHtC6SXxi21jV9E3ggHYtGro0lPK156h8r8Oy2Nf/pSZF6tkl20/5PF3WO5n/8yzksNs0bVSqbn1jpWzyftXlflamjUptNM+Rw25TaJBDoUEOS+twucwPESsGat73zeDNO6CrOlCrNmArcSktK197j+efsqbD2Sf/oNpf1Dr4WrFihYYOHeq5756EfuLEiZo9e7YOHTqkvXv3eva3a9dOH3/8se655x7961//UqtWrfTf//5Xw4cPr4PyAaBpKyxxloVU1fTEqtgjKyOv6LTnwQiw2xQdFqTmYYFqHhakaPf3Zub35mGBig4L0sGMfE3/cOMpz/e/GwdW+SmTYZhzdbiDMPd3Q97b3e083+Xdvqydedvl+TTafVy57eXul4Vw7vNUcc1ytajCNV2ln3ZX+xhcqrYW9yfmLpf7Ou423rXIq03Zz6HyY6j88zQMQzuO5Oq77UdP+RyN7J6g7slRXiFVlcFSNYFRoL3c7XLBklVBkq/xtTcXTRXPU+PHc+QbeJ58x4juiZp1fd9K84QmME9otex2m0LsDoUEOqSQ+r1WTXu5xkXUcyGNhM1w94VtxLKyshQVFaXMzExFRkZaXQ4A1DnDMJRdWKKM3OIqe2JlVOh95Q65zmSC0GZBDjPEauYOscqCq4rBVvPSduHBATUKK5wuQ+c+/tUpP7H97t4LefFqkZq+IHrzd79qEl3gGzsWIfANPE+NH8+Rb+B58h2sDN04NYXX4rXJiQi+APisxvqHttjp8vSsqjQPVulQwophVkZesdfEm7Vht0nRXiFVYIUQK0gtmpXdbh4WqKiwQAUH1G93b/fE6VLVn9g2pZVkGqOm8ILI3zTW//Pgjeep8eM58g08T8CZ8ffX4gRfAPxeQ3wSaBiGcoucOpFbvvdV2e2q5sHKyC2u9fLO5YUE2r16XzVvVr73VdU9sSJCAhrtpK18Ytu4+fsLIgAAgKbMn1+LE3wB8GvuN+sV//M62Zt1p8uoZs6rcttyiysFWxVXX6kpm02KCq0wD1alHlnlbpcONwwJtHbSzfrAJ7aNmz+/IAIAAGjq/PW1OMEXAL9V4nTpnMe/UnpWYbVtQgMdGtCuuTLyS0oDrSJlFZx+L6ygAHvlydyr6n1VOpywRViQIkMD/eIPCpoGf31BBAAAAP9Um5yo1qs6AkBdKD+MMDO/2JwTK9/sfZXpXn3QvT2vyHP7RF6hTtUJK7/Yqa+3Vr1aXURIQI3mwYouN8wwNNDB6nPwaw67jQnsAQAA4JcIvgCcEcMwlFNYUhpQmeGVJ7TKLQusMvPLhhS6g67Tncy9Jq4dkKKhneM84VV0WJCiQwMV4LDX2zUBAAAAAI0LwRcASWaAlV1Yosxy81tl5Jf1vjpRGmpllm4/kVd223kGAZZ7GGF0aJCiwgI9t6NLVx6MDi1bhTA6NEg7j+RoypurTnne0b2S6cECAAAAAE0cwRdQBV+e78blKguwMqroZVU+sCo/hDDzDAOs4AC7Z4hgVGigZ96rqHJBVnRoac+rsMDS+0EKDardZO6dEyKU+MkmpWUWVJrcXjInuE+IMp8zAAAAAEDTRvAFVNBYVjhzuQxlF5R4hg6eKBdeeQ0pLA2vMsu1OZMRhCGBZoDlDq88oVVY+fCq/H3ze0OtRuiw2zRtVKpufWOlbJJX+OWOJqeNSvWZoBIAAAAAUH9Y1REoZ9H6Q7r1jZWVehK5I5RZ1/etdfjlchnKKiguN1l7UaUJ2zPzy4YXZuaX9dA6kwArNNBRFlB5wqpARYW657wyb0eXW5kwKrThAqwz1VgCSgAAAABAw2JVR+A0OF2Gpn+4scrhc4bM8Gvawg3qGBeh7IJir15W5QOrE+Xnxso3t59JvBwW5FB0aKCiwioHVtGh5YcTlq1SGOlDAdbpGtE9Ub9OTfDZIakAAAAAgPpH8AWUWr7ruFfvoYoMSelZhRr21Nendf5mQQ5FlxtC6B1Yle+ZVRZqRYUFKjjAvwOsM+Gw25jAHgAAAABQLYIvoNTh7OpDr/KCA+yKCQ/2mqC9/O2ockMH3eFVdGiQggLs9fwIAAAAAABAeQRfQKm4iJAatZs9eQC9jAAAAAAA8AF0QQFKDWjXQolRIapuhiibzMnTB7Rr0ZBlAQAAAACA00TwBZRy2G2aNiq1yn3uMGzaqFQmTwcAAAAAwEcQfAHljOieqGsHpFTanhAVolnX99WI7okWVAUAAAAAAE4Hc3wBFWxJz5EkjR+YogHtWiouwhzeSE8vAAAAAAB8C8EXUM6eY7n6Zc8J2W3SnRedpfjImk14DwAAAAAAGh+GOgLlzF91QJJ0TscYQi8AAAAAAHwcwRdQyjAMT/A1rm+yxdUAAAAAAIAzRfAFlFq5N0N7juUpLMih4d0SrC4HAAAAAACcIYIvoNS8lfslSSO6JSgsiOnvAAAAAADwdQRfgKTCEqc+WntIkjSWYY4AAAAAAPgFgi9A0pLNR5SZX6z4yGAN7hBjdTkAAAAAAKAOEHwBkuavMoc5jumdLIfdZnE1AAAAAACgLhB8ocnLyCvSV5sPS2KYIwAAAAAA/oTgC03eR2sPqdhpqGtipLokRFpdDgAAAAAAqCMEX2jy3Ks5jutDby8AAAAAAPwJwReatN1Hc7Vyb4bsNumy3klWlwMAAAAAAOoQwReatPmrDkiSzu0Uq7jIEIurAQAAAAAAdYngC02WYRhasNoMvhjmCAAAAACA/yH4QpO1cu8J7TmWp7Aghy7uFm91OQAAAAAAoI4RfKHJen+l2dtrRPcEhQUFWFwNAAAAAACoawRfaJIKS5z6eO0hSdK4Pq0srgYAAAAAANQHgi80SUs2H1ZmfrHiI4M1qENLq8sBAAAAAAD1gOALTdK80mGOY/oky2G3WVwNAAAAAACoDwRfaHJO5BZpyZbDkhjmCAAAAACAPyP4QpPz0bpDKnYaSk2MVOeECKvLAQAAAAAA9YTgC03OvJX7JUnj+iZbXAkAAAAAAKhPBF9oUnYdzdWqvRmy26TRvZKsLgcAAAAAANQjgi80KfNXmZPan9spVnGRIRZXAwAAAAAA6hPBF5oMwzC0oDT4upxhjgAAAAAA+D2CLzQZv+w5ob3H89QsyKGLUxOsLgcAAAAAANQzgi80GfNKe3uN6J6o0CCHxdUAAAAAAID6RvCFJqGg2KmP1hyUxGqOAAAAAAA0FQRfaBKWbD6srIISJUSG6FftW1pdDgAAAAAAaAAEX2gS3MMcx/RJlsNus7gaAAAAAADQEAi+4PdO5BZp6ZbDkhjmCAAAAABAU0LwBb/30dqDKnYa6pYUqbPiI6wuBwAAAAAANBCCL/i991eawxzH9qG3FwAAAAAATQnBF/zaziM5Wr0vQ3abNLp3ktXlAAAAAACABkTwBb+2oHRS+/M6xSouIsTiagAAAAAAQEMi+ILfMgxD81ebwReT2gMAAAAA0PQEWF0AUF9W7DmhfcfzFR4coItTE6wuBwAAAAD8i8sp7flBykmXwuOlNoMlu8PqqgAvBF/wW/NKJ7Uf0T1BoUH85wsAAAAAdWbjQmnRvVLWwbJtkUnSiMel1NHW1QVUwFBH+KWCYqc+Wmv+BzyO1RwBAAAAoO5sXCi9M8E79JKkrEPm9o0LrakLqMJpBV/PPfec2rZtq5CQEA0cOFDLly8/afuZM2eqc+fOCg0NVUpKiu655x4VFBScVsFATXy1+bCyC0qUGBWiX7VvaXU5AAAAAOAfXE6zp5eMKnaWblt0n9kOaARqPdTx7bff1tSpU/XCCy9o4MCBmjlzpoYPH64tW7YoLi6uUvu5c+fqvvvu0yuvvKLBgwdr69atmjRpkmw2m5566qk6eRBARe5hjpf1TpbdbrO4GgAAAADwIcUFUtYBs0dX1kHv20c2V+7p5cUw2//ymtT7WikwtMHKBqpiMwyjqpi2WgMHDtTZZ5+tZ599VpLkcrmUkpKiO+64Q/fdd1+l9lOmTNGmTZu0ePFiz7Y//OEP+umnn/Tdd9/V6JpZWVmKiopSZmamIiMja1MumqDjuUUa8MiXKnEZ+vye83VWfITVJQEAAKCxYDJuNHVFeRXCrP2VA668Y3VzLZtDiukkxXeXErpL8T2khB5SRHzdnB9NVm1yolr1+CoqKtIvv/yi+++/37PNbrdr2LBhWrZsWZXHDB48WG+88YaWL1+uAQMGaOfOnfrkk090ww03VHudwsJCFRYWej0goKY+WntQJS5D3ZMjCb0AAABQhsm44e8Kc0rDqyp6a2WWbi/IqNm5AkLN34+oZCky2bwdmSTlZ0hf/e3UxwdFSEXZZg+xI5ul9e+V7WsWWxaGJfQ0b8d0khyBp/OogZOqVfB19OhROZ1Oxcd7p7Px8fHavHlzlcdcd911Onr0qM4991wZhqGSkhLdcsst+stf/lLtdWbMmKHp06fXpjTAwz3McWyfVhZXAgAAgEbDPRl3xXmJ3JNxX/U64RcaL8OQCrOqDrI8AddBqTCzZucLbFYaaCWVhlrlb5cGXKHNJVsV08a4nNKKl83fnSrn+bKZx9+1Vso9LKWtl9LXSWnrzNvHtku5R6SdS8wvN0eQFNvF7BGW0KMsGAttfjo/McCj1nN81dbSpUv16KOP6vnnn9fAgQO1fft23XXXXfrb3/6mBx98sMpj7r//fk2dOtVzPysrSykpKfVdKvzAjiM5Wr0vQw67TaN7JVldDgAAABqDU07GbTMn4+5yCcMe0fAMQ8o/UfV8WuWHIRbl1Ox8wZHeAVb5UMsddgVHVh1q1YTdYfaSfGeCJJu8f69KzzniMckRUFbDWReXNSnKkw5vktLWSunrS4OxDWbvsLS15ld5ka1Ke4a5w7AeUvN2kv201upDE1Sr4CsmJkYOh0Pp6ele29PT05WQkFDlMQ8++KBuuOEG3XTTTZKkHj16KDc3VzfffLP+7//+T/Yq/rEGBwcrODi4NqUBkqQFq8zeXud1ilFsBP+GAAAAmgxnsVSQaQ7DKsiUCk6U3T60umaTcS+8U0rsaYYCIZEVvkdJwREMxULtGIaUd/zk82llHZSK82p2vpBo71ArqpV3wBWRaP6brW+po81eklUOHX7s5L0ng8KkVv3MLzeXS8rYY/YK84Rh66SMvaU/s/3S1kVl7QObSfHdSucNKw3D4lKl4PC6f6zwebUKvoKCgtSvXz8tXrxYY8aMkWRObr948WJNmTKlymPy8vIqhVsOh/kpSi3n1QdOyuUyNL80+BrXl2GOAAAAPsUwpKJcc/4hT4CVUSHMOsm+4twzr2H1G9LqU7QJCK0iFCsfjkWaAVl1+0IipQA+oPULLpc5CXxVYVb5YYjOwlOfS5JCW1SeT8tzu5UUmSgFNavfx1QbqaPNXpJ1sViE3S61aGd+lQ/N8jPM3mDp68tCscObzN/3/cvNLw+b1KJ9uUn0SwOxyOTT790Gv1DroY5Tp07VxIkT1b9/fw0YMEAzZ85Ubm6uJk+eLEmaMGGCkpOTNWPGDEnSqFGj9NRTT6lPnz6eoY4PPvigRo0a5QnAgLqwYs8J7T+Rr/DgAF2cyiohAAAADc5ZUi6gyqhdgFWQKblKzryG4EizV0xIlBRa+r24QNrx5amP7XSxFBhmzqVUkFXue3ZZsFaSL+Xkm2/0T5cjuCwQ8wrJoqoJ1Nxty90ODPXvN/NWr77pcprzUJXvlZVZIeDKPiQ5i2p2vmaxlYcdegVcSeZz6mvsDqndefV3/tBoqe055pebs8ScJ6x8GJa2XspJk47vML82flDWPiTae5hkQndzLjEC6Caj1sHX1VdfrSNHjuihhx5SWlqaevfurUWLFnkmvN+7d69XD68HHnhANptNDzzwgA4cOKDY2FiNGjVKjzzySN09CkDS/FX7JUkjuycoJJBQFQAAWMDqN+tnyjDMIVcVQ6oqA6sqAqyazkF0MvbAssCqYoAVEn3yfSFRVf+8XU5pZvdTT8Z97VvVP1/OEjMIqxSKub9nloVk1bUpyi49V6EZquQeOYOfU0D1PcqCI6oIzqIqB2lBzRpneFbfq2+6nFJ2WoXhhhW+Zx+qYRBrM3/Xq5pPy70iYkQiIUtdcgRIcV3Mrx5XlG3POVI6if76slDs6Fbz/6bd35pfbvYAKeYs7zAsvocUHtvgDwf1z2b4wHjDrKwsRUVFKTMzU5GRDTBeGT6noNipsx/5UtkFJZr7u4Ea3CHG6pIAAEBTU99v1mvK5aw+pDpZgOW+7So+8xqCwk8dUlW3LzCsfsIYz6qOUpWTcTfEqo4uZ4VgLLuK4Ky6YM39lS0Zrrqpx2YvDcmqCMVO1tus/PegiLqdZLy61Tdr+jw5iyuEWhWDrYPmfsN56lpsdik8ofr5tCKTzP0BQaf7aFHfSgqlI5u9w7C0deb/eVUJj/fuHRbfXWrZ0Qzb0KjUJici+IJf+HjtId0+d6WSokL03b0Xym5vhJ9cAQAA/3Wmb9YrKs6vWUhV1b7CrDN9NJLNUfNeVl77Sm831jeJVYaTyaeejLsxMQyzZ12lcCyzQkiWXU2PtNL7NQl+asRWuYfZqeY4q9gzLTjS/Dfj6ZlX3UIENik8Trr8ZbNXZflQK7P0dk66qu7VV/FUDu9hhl7DDksDrvD4xvtvGafPKF3Mwj2BflppL7HjO1Xlv52AECmuq3cYltDd/DcNyxB8ocm56bWf9eWmw7r1gg66d0QXq8sBAABNySnfrEsKi5F+86Q51O2Uc15l1nwy7JMJbFa7YYLlbzfWIXB1wdeHo9YFz5DWKnqUVdvrLLNyT7Wazm9VE4HNzOGA+cfP/Fz2QHMi+Crn03L31Ipres87Tq4wx5w4P21tuZUlN1S/cEZ067JJ9N2hWHSbuu0BiWoRfKFJOZZTqIGPLlaJy9AX95yvTvERVpcEAACakk0fSW+Pr/vz2uy1HCYYXbYvOJLhV6h/xQU1GKqZffJhnCUFtb9us1gppnPl3lruFRHDYggfUDdcLunELu9J9NPXS5n7qm4fFCHFdytbUTK+h9lbLCisYetuAmqTE9FvEz7vo7WHVOIy1CM5itALAADUr+J86dBa6cAK6cAv0v4VUsaemh3boqPUskPNe2AFR/hvryv4h8AQ8ys87vTPUVJU1qNs19fSR/ec+pgrXq3flQQBN7vd/H+7ZQep25iy7XnHzd5gnjBsndlbrChb2vej+eVms0stOniHYQndzUUP+D++QRB8wefNW3VAkjS2T7LFlQAAAL/icknHtpUFXAdWmG90arTSWxVGzeTNOlBRQJAUECM1i5Gat5W++cepV99sM7iBiwQqCGth/n9e/v90Z7F0dFvZJPru77lHzL8lx7ZJG+aXtQ9tUbqiZLl5w2I601u3HhB8waftOJKjNfsy5LDbNLp3ktXlAAAAX5ZzuCzgOvCLdGCVOUSromaxUnJ/qVU/83tCT+k/5/JmHThTdoe5Cuo7E2QuDFHF6psjHmNuLjROjkApPtX86nlV2fbs9NJJ9MsFYke3mfPZ7fra/HKzB0qxXcrNG9bd7CHWrOXp18W8hgRf8G3zV5q9vc7vFKOY8GCLqwEAAD6jKE86tMYMufaXBl1VzdkSECol9ZaS+5lfrfpLUSmVh6fwZh2oG6mjzVVQK62+meRbq28CbhHx5lfHYWXbigukI5u8w7C09eaHLenrzC+vcyR5h2EJPaUW7U/9d6XKlWyTzL9ZTeh3icnt4bNcLkPnPbFEBzLy9cy1fTS6Fz2+AABAFVxO6ejWsoDrwAopfaNkOCs0tJmftCf3K+3N1U+KSzU/xa+JKt9gJPNmHTgd9FJBU2MY5gcwnjCstJfYiV1Vtw8MMyfOd68omdDDnFg/uHTe640LSz+QqRj5lH4gc9XrPv23iVUd0ST8tPOYrn7xR0UEB+jnB4YpJJA/hAAAQOaQQ3fAtX+FdHC1OeFwReEJZg+u5L7mkMWkPlLIGb7W5M06AKAuFWab80uW7xl2eKNUnFd1++Ztpbhu0u5vzGOrVDoE/+51Pvs3ilUd0STML53UfmSPBEIvAACaqsIc6dDqcr25fpGyDlRuFxhmBlvu4YrJ/czeWHW9opbdwQT2AIC6Exwhtf6V+eXmckrHd3qHYenrzb9/J3abXydlmG33/NAk/mYRfMEnFRQ79fHaQ5KksX1aWVwNAABoEC6nuVy8pzfXL+YcKYbLu53NLsV2LRuumNzfHMLo4KUvAMAP2B1STCfzq/u4su15x80wbPUcae3bpz5PTnr91diI8NcfPunLTenKLixRcnSoBrZrYXU5AACgrhmGOVeWZ/L5ldLBVVJxbuW2kcllwxVb9ZcSe0vB4Q1eMgAAlgprIbUfYn4AVJPgKzy+/mtqBAi+4JPcqzle1jtJdnsdD1EAAAANrzDbDLfcwxX3r5By0iq3CwqvMGSxvxSZ2PD1AgDQWLUZbM7hlXVIlSe3lzxzfLUZ3NCVWYLgCz7nWE6hvt56RJI0rm+yxdUAAIBac5aYE/O6hyse+EU6slmVXpzbHOaqiq1Khysm95NiO/vsRLwAADQIu0Ma8Xjpqo42ef99Le04MuKxJvP3lOALPufDNQdV4jLUs1WUOsZFWF0OAAA4Gffy7O5eXAd+MVdZLMmv3DYqpXROrtLeXIm9pKBmDV4yAAA+L3W0dNXr0qJ7zakD3CKTzNArdbR1tTUwgi/4HPdqjmP70NsLAIBGpyCzdMhi6bxc+1dIuYcrtwuONIcsuocrJveTIprGXCMAADSI1NFSl0vM1Rtz0s05vdoMbjI9vdwIvuBTth/O0Zr9mXLYbRrVK8nqcgAAaNqcxeby6Qd+KR2yuEI6urVyO3uAFN+tbIXFVv2llp0ku73hawYAoCmxO6R251ldhaUIvuBT5q/aL0kaclasYsKDLa4GAIAmxDCkjD1lwxUP/CIdWiOVFFRuG926LOBK7mcOWQwMbfiaAQBAk0fwBZ/hchlasMocm8wwRwAA6ln+idKAa2VZ2JV3tHK7kKiyebncQxbDYxu+XgAAgCoQfMFnLN99XAcy8hURHKBfpzIHCAAAdaakSEpfVzZc8cAv0rHtldvZA6WE7t69uVp0YMgiAABotAi+4DPmrzQntf9Nj0SFBDatyfgAAKgzhiEd31k2XHH/CiltreQsqty2ebuygCu5v5TQQwoMafiaAQAAThPBF3xCQbFTn6w7JEka25dhjgCAJsjlPL1VmfKOlwVc7t5c+Scqtwtt7j35fFJfqVnLun8cAAAADYjgCz7hi43pyi4sUXJ0qAa0bWF1OQAANKyNC6VF90pZB8u2RSZJIx43lyp3Ky6Q0taVBVz7V0gndlU+nyNISuhpBl2eIYvtJZut/h8LAABAAyL4gk+Yv8oc5jimT5Lsdl6UAwCakI0LpXcmSDK8t2cdkt65QRrwe8lwmWFX2nrJVVz5HC06lAZcpSFXQncpgNWRAQCA/yP4QqN3NKdQX289Ikka26eVxdUAANCAXE6zp1fF0Esq27b8P96bw1qWBVyt+plDFsPoLQ0AAJomgi80eh+uOSiny1CvVlHqGBdudTkAADQMw5DWvOk9vLE6XS8zhzwm95Oat2XIIgAAQCmCLzR67mGOY/swqT0AwM/ln5B2LpW2fyntWCJlHajZcamjpR5X1GtpAAAAvojgC43a9sPZWrs/UwF2m0b1SrK6HAAA6pbLKR1YWRp0LTYnpDdcZfvtgVXP2VVReHz91QgAAODDCL7QqM1baX7SPeSsWLUMZxJeAIAfyDxghlzbF5u9uwoyvPfHdJY6XmR+pfxKeu5scyL7Kuf5spmrO7YZXP91AwAA+CCCLzRaLpehD1ab85qM7cswRwCAjyoukPZ8L+34ygy7jmzy3h8cJbUfInUcJnW4UIpO8d4/4vHSVR1t8g6/SufxGvGYZHfU4wMAAADwXQRfaLR+2nVcBzLyFREcoGFdGcIBAPARhiEd3WqGXDsWS7u/l0ryyzWwScl9S4Oui8wJ6R0neUmWOlq66nVzdcfyE91HJpmhV+roensoAAAAvo7gC43W/FX7JUmX9ExUSCCfZAMAGrH8DGnX16Vh11dS5j7v/RGJZsjV8UKp/VAprEXtzp86WupyibTnBykn3ZzTq81genoBAACcAsEXGqWCYqc+WZcmidUcAQCNkMspHVxdNlfX/p8lw1m23xFkBlMdSufqikuVbLYzu6bdIbU778zOAQAA0MQQfKFR+nxjunIKS5QcHaqz29byU3EAAOpD1iGzN9eOxdKOJVL+ce/9LTuZIVeHi6S250hBzaypEwAAAB4EX2iU5q80hzmO7ZMsu/0MPyEHAOB0lBRKe5eZPbq2L5YOb/DeHxwptTu/LOxq3saaOgEAAFAtgi80OkeyC/XNtqOSWM0RANCADEM6tqN0+OKX0u7vpOK8cg1sUlLvsuGLrc6WHIFWVQsAAIAaIPhCo/PhmoNyugz1SolWh9hwq8sBAPizgixp1zdm0LVjsZSx13t/eLzU4UJzBcb2F0jNYiwpEwAAAKeH4AuNzvxVByRJ45jUHgBQ11wuKW1N2fDF/cslV0nZfnug1PpXZtDV8SIpvvuZT0oPAAAAyxB8oVHZlp6tdQcyFWC3aVSvJKvLAQD4g+x070np845672/R3gy6OlwktT1XCqa3MQAAgL8g+EKjMq+0t9cFnWPVolmQxdUAAHxSSZG070ezR9eOxVLaOu/9QeFSuyFSxwvNsKtFO2vqBAAAQL0j+EKj4XIZ+qA0+Brbp5XF1QAAfMrxnWXDF3d/KxXleO9P7FVuUvoBUgAfrgAAADQFBF9oNH7cdUwHMwsUERKgi7rGWV0OAKAxK8yWdn1bugLjYunELu/9zWLNSek7XCR1GCqF83cFAACgKSL4QqMxf6XZ2+vSnokKCXRYXA0AoFFxuaT0daXDF7+S9v4ouYrL9tsDpJRfmcMXOw6T4ntIdrt19QIAAKBRIPhCo5Bf5NSn69MkMcwRAFAq54i0c0lZ2JV72Ht/87alwxeHSe3Ok4IjLCkTAAAAjRfBFxqFzzemKaewRK2ah6p/m+ZWlwMAsIKzWNq3vHT44pfSoTXe+wObmQGXe66ulh2sqRMAAAA+g+ALjcJ8z6T2ybLbbRZXAwBoMCd2l01Kv+sbqSjbe398j7LhiykDpYBgS8oEAACAbyL4guWOZBfq221HJZnBFwDAjxXlSru/Kw27vpSO7/DeH9ay3KT0F0oR8dbUCQAAAL9A8AXLLVxzUE6XoV4p0WofG251OQCAumQYUvqGsuGLe3+UnEVl+20OsydXx9KwK7E3k9IDAACgzhB8wXLzV+2XJF3el95eAOAXco95T0qfk+a9P7p12Txd7c6XQqKsqRMAAAB+j+ALltqanq31B7IUYLfp0p5JVpcDADgdzhLpwAqzR9f2xdLBVZKMsv2BYVLbc8tNSt9RsjGfIwAAAOofwRcsNW+lOan9BZ3j1KJZkMXVAEAT5XJKe36QctKl8HipzWDJ7jj5MRl7S3t0LZZ2fiMVZnrvj+tWNnyx9SApMKT+6gcAAACqQfAFy7hchj5YbQZf4xjmCADW2LhQWnSvlHWwbFtkkjTicSl1dNm2ojxpz/dlYdfRrd7nCW0utR9qrr7Y4UIpMrFh6gcAAABOguALlvlx5zEdyixQREiALuwSZ3U5AND0bFwovTNBXsMSJSnrkLl9+KOS4TTDrj0/SM7CsjY2u9Tq7NKg6yIpqfepe4kBAAAADYzgC5aZt8rs7XVpz0SFBPJmCQAalMtp9vSqGHpJZds+u997c2Qrc46ujhdJ7YZIodH1XCQAAABwZgi+YIn8Iqc+XXdIkjSubyuLqwGAJmjPD97DG6uT1FfqcaUZdsWcxaT0AAAA8CkEX7DE5xvTlFvkVEqLUPVv09zqcgCg6Sgpkvb+IH3/75q1H3S71OOK+q0JAAAAqCcEX7CEezXHsb2TZaP3AADUr7zj0rYvpK2fmvN1FWbV/Njw+PqrCwAAAKhn9tM56LnnnlPbtm0VEhKigQMHavny5Sdtn5GRodtvv12JiYkKDg7WWWedpU8++eS0CobvO5xdoG+3HZEkjWWYIwDUPcOQjmyVvv+X9MpI6R8dpPk3Sxvmm6FXs1ip93VSaAtJ1X34YJMik6U2gxuycgAAAKBO1brH19tvv62pU6fqhRde0MCBAzVz5kwNHz5cW7ZsUVxc5ZX5ioqK9Otf/1pxcXF67733lJycrD179ig6Orou6ocPWrj6oFyG1DslWu1imlldDgD4B2extHeZtGWR2bPr+E7v/XHdpM4jza+kvpLdXm5VR5u8J7kvDcNGPMZKjQAAAPBptQ6+nnrqKf3ud7/T5MmTJUkvvPCCPv74Y73yyiu67777KrV/5ZVXdPz4cf3www8KDAyUJLVt2/bMqoZPm1+6muPlfZMtrgQAfFz+CWnbl6VDGL+UCjLL9jmCpLbnSmeNlDqPkKJbVz4+dbR01evm6o7lJ7qPTDJDr9TR9f8YAAAAgHpUq+CrqKhIv/zyi+6/v2x5c7vdrmHDhmnZsmVVHrNw4UINGjRIt99+uz744APFxsbquuuu07333iuHo+pPkQsLC1VYWOi5n5VVi7lI0KhtScvWhoNZCnTYdGnPJKvLAQDfc3S7GXRtWWT28DKcZfvCWkqdhptBV4cLpeCIU58vdbTU5RJzlcecdHNOrzaD6ekFAAAAv1Cr4Ovo0aNyOp2Kj/ee6DY+Pl6bN2+u8pidO3fqq6++0vjx4/XJJ59o+/btuu2221RcXKxp06ZVecyMGTM0ffr02pQGHzFv1X5J0gWd49S8WZDF1QCAD3CWSPt+Kg27PpWObffeH9vVDLrOGim16n96gZXdIbU7r27qBQAAABqRel/V0eVyKS4uTi+++KIcDof69eunAwcO6B//+Ee1wdf999+vqVOneu5nZWUpJSWlvktFPXO6DH2wyhxKM64PwxwBoFoFmebQxS2LpG2fSwUZZfvsgVLbc8yg66zhUot2lpUJAAAANHa1Cr5iYmLkcDiUnp7utT09PV0JCQlVHpOYmKjAwECvYY1du3ZVWlqaioqKFBRUuddPcHCwgoODa1MafMCPO48pLatAkSEBurBr5YUQAKBJO76zbGL6PT9IrpKyfaEtpE4Xlw5hvEgKibSuTgAAAMCH1Cr4CgoKUr9+/bR48WKNGTNGktmja/HixZoyZUqVx5xzzjmaO3euXC6X7Ha7JGnr1q1KTEysMvSC/5q30pzU/pKeSQoOYO4YAE2cyyntW142X9fRLd77YzqXDWFMGcCcWwAAAMBpqPVQx6lTp2rixInq37+/BgwYoJkzZyo3N9ezyuOECROUnJysGTNmSJJuvfVWPfvss7rrrrt0xx13aNu2bXr00Ud155131u0jQaOWX+TUovWHJLGaI4AmrCBL2rG4bAhj/vGyffYAqfUgqfNI6awRUssO1tUJAAAA+IlaB19XX321jhw5ooceekhpaWnq3bu3Fi1a5Jnwfu/evZ6eXZKUkpKizz77TPfcc4969uyp5ORk3XXXXbr33nvr7lGg0ft8Y5pyi5xq3SJM/do0t7ocAGg4J3aXDWHc/b3kKi7bFxItdfq1GXR1HCaFRltUJAAAAOCfbIZhGFYXcSpZWVmKiopSZmamIiOZ18QXTXhlub7ZekR3XtRJU399ltXlAED9cTml/SvKhjAe2eS9v2VHM+jqPFJK+ZXkqPd1ZgAAAAC/UpuciFfbqHeHswr03bYjkqSxrOYIwB8VZks7viobwph3tGyfzVE6hLF0vq6YjtbVCQAAADQxBF+odwvXHJTLkPq0jla7mGZWlwMAdSNjn7R1kbTlU2n3t5KzqGxfcJTUaZgZdHW8SAprYV2dAAAAQBNG8IV6517NcRy9vQD4MpdLOrjSDLq2LpLS13vvb9HeDLo6jzB7eDkCrakTAAAAgAfBF+rV5rQsbTyUpUCHTZf2TLK6HAConaJcaccSc76urZ9LuYfL9tnsUsrA0lUYR0oxnSSbzbpaAQAAAFRC8IV6Nb+0t9fQznFq3izI4moAoAYyD5RNTL/rG8lZWLYvKMIcuth5pNTpYoYwAgAAAI0cwRfqjdNlaMHq0mGOfRnmCKCRcrmkQ6vMoGvrp1LaOu/90W1Ke3WNkNqcIwUQ4gMAAAC+guAL9WbZjmNKzypUVGighnaJs7ocAChTlCft+lra8ok5hDEnrdxOm5QywAy6Oo+UYrswhBEAAADwUQRfqDfzVu2XJF3SM1HBAQ6LqwHQ5GUdMiel37pI2rlUKiko2xcULnW4sGwIY7MYy8oEAAAAUHcIvlAv8opKtGi92YPicoY5ArCCYUiH1phB15ZPpUOrvfdHtTZXYDxrhNT2XCkg2JIyAQAAANQfgi/Ui883pCuvyKk2LcPUt3Vzq8sB0FQU55sT0m/5VNr6mZR9sNxOm9Sqf9kQxrhUhjACAAAAfo7gC/Xi/ZXmMMcxvZNl440lgPqUne49hLE4r2xfYDOpw9CyIYzhzDcIAAAANCUEX6hzh7MK9P32o5KksX0Y5gigjhmGufKiewjjwZXe+yOTy3p1tT1PCgyxpk4AAAAAliP4Qp37YPVBuQypb+totY1pZnU5APxBcYG0+9uyIYxZ+733J/U1g66zRkgJPRjCCAAAAEASwRfqwbxVByRJY/u2srgSAD4t57AZcm1dJO1YIhXnlu0LCDWHMJ41QjpruBSRYF2dAAAAABotgi/Uqc1pWdp0KEuBDptG9Uy0uhwAjYHLKe35QcpJl8LjpTaDJbujcjvDkA5vLO3VtUjav0KSUbY/IrFsCGO786XA0AZ7CAAAAAB8E8EX6tT8lWZvrwu7xCk6LMjiagBYbuNCadG9Ula51RUjk6QRj0upo6WSQmn3d6XzdS2SMvd6H5/Yu2wIY2IvhjACAAAAqBWCL9QZp8vQgtWlwxz7MMwRaPI2LpTemSCvXluSlHVIeucGKbm/dGSzVJRTti8gRGp/QdkQxsikhqwYAAAAgJ8h+EKd+WHHUaVnFSoqNFBDu8RaXQ4AK7mcZk+viqGXVLbtwArze3i8GXKdNdIMvYLCGqhIAAAAAP6O4At1xj3M8dKeiQoOqGL+HgBNx54fvIc3VueSp6R+kyW7vf5rAgAAANDk8E4DdSKvqESLNqRJksb1Tba4GgCWy0mvWbuQKEIvAAAAAPWGdxuoE59tSFNekVNtWoapb+vmVpcDwGqBNRyuGB5fv3UAAAAAaNIY6og6MW+le1L7ZNlYdQ1o2k7slj5/8BSNbObE9W0GN0RFAAAAAJooenzhjKVnFej77UclmcEXgCbswErpv8Ok49ul0BalGyuG4aX3Rzwm2ZkPEAAAAED9IfjCGftg9QG5DKlfm+Zq07KZ1eUAsMqWT6XZl0i5R6T4HtKt30tX/U+KTPRuF5kkXfW6lDramjoBAAAANBkMdcQZKz/MEUATtfwl6dM/S4ZL6nCRdOVsKSTSDLe6XGKu8piTbs7p1WYwPb0AAAAANAiCL5yRTYeytDktW0EOuy7tmXjqAwD4F5dL+nKa9MMz5v0+N0iXPi05Asva2B1Su/OsqQ8AAABAk0bwhTMyf5XZ2+vCLnGKDguyuBoADaq4QFpwi7Rhvnl/6APS+X+UWOACAAAAQCNB8IXT5nQZWlAafI3tyzBHoEnJOy69dZ20d5lkD5Que1bqdY3VVQEAAACAF4IvnLbvtx/V4exCRYcFamjnOKvLAdBQju+S5lwhHdsuBUdJV/9Paj/E6qoAAAAAoBKCL5w29zDHS3smKiiABUKBJmH/L9Lcq6S8o1JkK+n696S4rlZXBQAAAABVIvjCacktLNGi9WmSpLF9WllcDYAGsflj6b0bpZJ8KaGndN07UiSLWgAAAABovAi+cFo+25Cm/GKn2rYMU9/W0VaXA6C+/fSi9OmfJRlSx2HSlbOl4AirqwIAAACAkyL4wmlxD3Mc26eVbKzgBvgvl0v64kFp2bPm/b4TpUuekhz8+QAAAADQ+PHOBbWWllmg77YflSSN7cNqjoDfKs6X5v9e2viBef+ih6Rzp0qE3QAAAAB8BMEXau2D1QdkGFL/Ns3VumWY1eUAqA+5x6S3rpX2/STZA6Uxs6SeV1pdFQAAAADUCsEXas0zzLEvvb0Av3R8p/TGFdLxHVJIlHT1HKndeVZXBQAAAAC1RvCFWtl4MEub07IV5LDr0h5JVpcDoK7t+1l682op75gU1Voa/64U18XqqgAAAADgtBB8oVbmr9ovSbqoa5yiwgItrgZAndr0ofT+TVJJgZTYS7ruXSki3uqqAAAAAOC0EXyhxkqcLi1YfVASk9oDfufHWdKi+yUZUqfh0hWvSMHhVlcFAAAAAGeE4As19v2OYzqSXajmYYG6oHOc1eUAqAsup/T5A9KPz5v3+/9WGvkPycGfBwAAAAC+j3c2qLH5K81hjpf2TFJQgN3iagCcseJ8ad7vzCGOkjRsunTOXZLNZm1dAAAAAFBHCL5QI7mFJfpsQ7okVnME/ELuUenNa6T9P0uOIGnMLKnHFVZXBQAAAAB1iuALNbJofZryi51qF9NMfVKirS4HwJk4tkN643LpxC4pJFq6Zq7U9hyrqwIAAACAOkfwhRqZv+qAJHNSexvDoADftfcns6dX/nEpurU0/n0p9iyrqwIAAACAekHwhVM6lJmv73cclcRqjoBP2/iB9P7vJGehlNRHuu4dKZyFKgAAAAD4L4IvnNIHqw/KMKSz2zZXSoswq8sBUFuGYa7a+Nn/STKks0ZKV7wsBTWzujIAAAAAqFcEXzgpwzA0f6V7mGMri6sBUGsup7Tofmn5f8z7Z98kjXxCsjusrQsAAAAAGgDBF05q46EsbUnPVpDDrkt6JFpdDoDaKMqT3r9J2vKxef/Xf5MG3yExTx8AAACAJoLgCyfl7u01LDVOUWGBFlcDoMZyjkhvXi0d+EVyBEtjX5C6j7O6KgAAAABoUARfqFaJ06UFqw9KYpgj4FOObpPmXCGd2C2FNpeueVNqM8jqqgAAAACgwRF8oVrfbT+qozmFah4WqCFnxVpdDoCa2LNMeutaKf+EFN1Guv59KaaT1VUBAAAAgCUIvlCt+avMYY6jeiUpKMBucTUATmnDfGne7yVnoZTcT7r2bSmc0BoAAABA00XwhSrlFJbosw1pkqSxfZItrgbASRmG9MO/pS8eNO93vkS6/L9SUJi1dQEAAACAxQi+UKVF69NUUOxSu5hm6p0SbXU5AKrjckqf3iv9/JJ5f8DvpREzJLvD2roAAAAAoBEg+EKV5q/aL0ka1ydZNpvN4moAVKkoV3rvRmnrp5Js0vBHpF/dJvE7CwAAAACSCL5QhUOZ+fphxzFJ0hiGOQKNU85hae5V0sFVkiNYGvei1G2M1VUBAAAAQKNC8IVKFqw6KMOQBrRtoZQWzBEENDpHtkpzLpcy9kqhLaRr35JaD7S6KgAAAABodAi+4MUwDM8wx7F96e0FNDq7v5feuk4qyJCat5Ouf19q2cHqqgAAAACgUbKfzkHPPfec2rZtq5CQEA0cOFDLly+v0XFvvfWWbDabxowZczqXRQPYcDBLW9NzFBRg1296JFpdDoDy1r0n/W+MGXq1Olu66UtCLwAAAAA4iVoHX2+//bamTp2qadOmaeXKlerVq5eGDx+uw4cPn/S43bt3649//KPOO++80y4W9W/+qgOSpGFd4xQVGmhxNQAkSYYhffe09P6NkrNI6nKpNPFDqVmM1ZUBAAAAQKNW6+Drqaee0u9+9ztNnjxZqampeuGFFxQWFqZXXnml2mOcTqfGjx+v6dOnq3379mdUMOpPidOlD1YflCSN69PK4moASJKcJdLHU6UvHzbv/+o26arXpcBQS8sCAAAAAF9Qq+CrqKhIv/zyi4YNG1Z2Artdw4YN07Jly6o97q9//avi4uJ044031ug6hYWFysrK8vpC/ft2+1EdzSlUi2ZBGtI51upyABTmmPN5rXhFkk0a8Zg0YoZkd1hdGQAAAAD4hFpNbn/06FE5nU7Fx8d7bY+Pj9fmzZurPOa7777Tyy+/rNWrV9f4OjNmzND06dNrUxrqwPyV5jDHUT0TFeg4renfANSV7HRp7pXSoTVSQIg07iUpdbTVVQEAAACAT6nXdCM7O1s33HCDXnrpJcXE1Hwumvvvv1+ZmZmer3379tVjlZCknMISfb4xTZI0ti/DHAFLHdki/XeYGXqFtTTn8yL0AgAAAIBaq1WPr5iYGDkcDqWnp3ttT09PV0JCQqX2O3bs0O7duzVq1CjPNpfLZV44IEBbtmxRhw6VVyQLDg5WcHBwbUrDGfp03SEVFLvUPqaZerWKsrocoOna/Z05vLEgU2rRXhr/His3AgAAAMBpqlWPr6CgIPXr10+LFy/2bHO5XFq8eLEGDRpUqX2XLl20bt06rV692vM1evRoDR06VKtXr1ZKSsqZPwLUCfdqjuP6Jstms1lcDdBErX1X+t9YM/RqNUC68UtCLwAAAAA4A7Xq8SVJU6dO1cSJE9W/f38NGDBAM2fOVG5uriZPnixJmjBhgpKTkzVjxgyFhISoe/fuXsdHR0dLUqXtsM7BjHwt23lMknRZ72SLqwGaIMOQvntaWlw6t2HX0dK4F1m5EQAAAADOUK2Dr6uvvlpHjhzRQw89pLS0NPXu3VuLFi3yTHi/d+9e2e1MjO5LFqw+IMOQBrRroZQWYVaXAzQtzhLpkz9Iv8w27w+aIv36bxL/jwIAAADAGbMZhmFYXcSpZGVlKSoqSpmZmYqMjLS6HL9iGIYufvobbTuco8fG9dA1A1pbXRLQdBTmSO9NlrZ9LskmjXhM+tUtVlcFAAAAAI1abXKiWvf4gn/ZcDBL2w7nKCjArpE9Eq0uB2g6stOkuVeZKzcGhEqX/1fqeqnVVQEAAACAXyH4auLmrTQntf9113hFhQZaXA3QRBzeJM25UsrcJ4XFSNe9LbXqb3VVAAAAAOB3CL6asBKnSwvXlK3mCKAB7PpGeut6qTBTatFBuv49qUV7q6sCAAAAAL9E8NWEfbvtqI7mFKllsyCdf1as1eUA/m/N29IHt0uuYinlV9K1b0phLayuCgAAAAD8FsFXEzZvldnba1SvJAU6WEEOqDeGIX37pPTV3837qWOksf+RAkMsLQsAAAAA/B3BVxOVXVCszzekSZLG9mGYI1BvnMXSx1Olla+b9wffIQ37q2QnbAYAAACA+kbw1UR9uj5NhSUutY9tpp6toqwuB/BPhdnSOxOlHYslm10a+YQ04HdWVwUAAAAATQbBVxM1v3Q1x3F9kmWz2SyuBvBDWYekuVdKaeukgFDpilekLr+xuioAAAAAaFIIvpqgAxn5+nHXMUnSGIY5AnUvfaM050opa7/ULFa67m0puZ/VVQEAAABAk0Pw1QQtWHVAhiENbNdCrZqHWV0O4F92LpXevkEqzJJadpKuf09q3tbqqgAAAACgSSL4amIMw9D80tUcx/WltxdQp1a/KS2cIrlKpNaDpWvmSGEtrK4KAAAAAJosgq8mZv2BLG0/nKPgALtG9ki0uhzAPxiG9PUT0tJHzfvdL5cue14KDLG2LgAAAABo4gi+mph5q/ZLkoalxisyJNDiagA/4CyWPrxbWv2Gef+cu6WLpkl2u5VVAQAAAABE8NWkFDtd+nDNQUnS5QxzBM5cQZb0zgRp5xLJZpd+86R09o1WVwUAAAAAKEXw1YR8u+2IjuYUqWWzIJ3XKdbqcgDflnlAmnuVlL5eCgyTrnhV6jzC6qoAAAAAAOUQfDUh81aak9qP6pWkQAfDsIDTlrZemnOllH1QahYnXfe2lNzX6qoAAAAAABUQfDURWQXF+mJjuiRWcwTOyI6vpLcnSEXZUkxnafy7UvM2VlcFAAAAAKgCwVcTsWhdmgpLXOoQ20w9kqOsLgfwTavekD68S3KVSG3Ola55QwptbnVVAAAAAIBqEHw1Ee7VHMf1bSWbzWZxNYCPMQxp6WPS14+Z93tcKV32nBQQbG1dAAAAAICTIvhqAvafyNOPO49Lksb0YZgjUCslRWYvrzVzzfvn/UEa+oBkZ548AAAAAGjsCL6agA9WH5Qk/ap9CyVHh1pcDeBDCjKlt2+Qdn0t2RzSJf+U+k+2uioAAAAAQA0RfPk5wzA0b2XpMMc+rSyuBvAhmfvNlRsPb5QCm0lXvSZ1+rXVVQEAAAAAaoHgy8+tO5CpHUdyFRxg18geCVaXA/iGQ2uluVdJ2Yek8HjpunekpN5WVwUAAAAAqCWCLz83b+UBSdKvU+MVERJocTWAD9j+pfTORKkoR4rtIo1/V4pubXVVAAAAAIDTQPDlx4qdLn24xpzfa1xfJrUHTmnl69KHd0uGU2p7nnT1G1JotNVVAQAAAABOE8GXH/tm6xEdyy1STHiQzusUa3U5QONlGNKSR6Rv/mHe73m1NPpZKSDI2roAAAAAAGeE4MuPzVtlDnMc1StJgQ67xdUAjVRJkbTwDmntW+b98/8kDf0/yWazti4AAAAAwBkj+PJTWQXF+mJjuiRWcwSqlZ8hvX29tPtbyeaQRs2U+k6wuioAAAAAQB0h+PJTn647pKISlzrGhat7cqTV5QCNT8Y+ac6V0pFNUlC4dNVrUsdhVlcFAAAAAKhDBF9+yr2a49g+ybIxZAvwdmiNNOcqKSdNikiUrntHSuxpdVUAAAAAgDpG8OWH9p/I00+7jkuSxvRhNUfAy7YvpHcmSsW5UlyqNP5dKYrhwAAAAADgjwi+/NCC0kntB7VvqeToUIurARqRFa9KH/9BMpxSuyHS1f+TQqKsrgoAAAAAUE8IvvyMYRie1RzH9qW3FyBJMgzpq79J3/7TvN/rOmnUv6SAIGvrAgAAAADUK4IvP7N2f6Z2HslVcIBdI7snWF0OYL2SQumD26V175r3h9wnXXCfxNx3AAAAAOD3CL78zPzS3l4Xd0tQREigxdUAFss/Ib11vbTnO8keYPby6nO91VUBAAAAABoIwZcfKXa69OGag5KkcUxqj6buxB5pzpXS0S1SUIR09etShwutrgoAAAAA0IAIvvzIN1uP6FhukWLCg3RepxirywGsc3CVNPdqKSddikgyV25M6G51VQAAAACABkbw5UfmrTSHOY7ulawAh93iaoAG4HJKe34wA67weKnNYGn7YundSVJxrhTfXbruHSmKHpAAAAAA0BQRfPmJzPxifbEpXZI0jtUc0RRsXCgtulfKOli2LSRKKsiSZEjth0pXvS6FRFpWIgAAAADAWgRffuLTdYdUVOJSp7hwdUvijT783MaF0jsTJBne2wsyze9tzzeHNzpY4AEAAAAAmjLGw/mJeaWrOY7tmyybzWZxNUA9cjnNnl4VQ6/yju+QbPz3BgAAAABNHe8M/cC+43lavuu4bDZpTG+GOcLP7fnBe3hjVbIOmO0AAAAAAE0aQx39wILS3l6D2rdUUnSoxdUAdczllA5vkvYvl/YtNyevr4mc9PqtCwAAAADQ6BF8+TjDMDTfPcyxD7294Afyjkv7V5QFXQdWSkXZtT9PeHzd1wYAAAAA8CkEXz5uzf5M7Tyaq5BAu0b2SLS6HKB2XC7pyObSkOtn8/vRrZXbBUVIrfpJrQZIyf2kj+6SstNV9TxfNikySWozuL6rBwAAAAA0cgRfPm7+yv2SpItTExQezNOJRi4/Qzqwwgy59v0kHfhFKsyq3K5lRzPkSjlbShkoxXaR7I6y/c5/lK7qaJN3+FW6sMOIx7zbAwAAAACaJJISH1bsdOnDtYckmas5Ao2Ky2X23nIPWdz/s3Rkiyr10gpsJiX3NQOulAFSq7OlsBYnP3fqaOmq183VHctPdB+ZZIZeqaPr/OEAAAAAAHwPwZcP+3rLER3PLVJMeLDO6xhjdTlo6gqyynpz7S8NugoyK7dr0b6sN1erAVJcquQ4jf+KUkdLXS4xV2/MSTfn9GozmJ5eAAAAAAAPgi8fNm+VOczxst5JCnDYLa4GTYphSMe2l/bkKu3RdXiTKvfmCpOS+paFXK3OlsJj664Ou0Nqd17dnQ8AAAAA4FcIvnxUZn6xvtx0WBKrOaIBFGab83GV782Vf6Jyu+g2pcMVB5jf47tJjsCGrxcAAAAAABF8+axP1h1SUYlLZ8WHq1tSpNXlwJ8YhnR8Z7neXD9LhzdIhsu7XUCIlNSnLOhqdbYUEW9NzQAAAAAAVIHgy0fNX3lAkjS2TyvZbDaLq4FPK8qVDqwsC7n2L5fyjlVuF9W6bMhiytlSfA8pIKjh6wUAAAAAoIYIvnzQvuN5Wr77uGw2aUyfJKvLgS8xDOnEbnOo4r6fzF5d6Rskw+ndzhEsJfU2e3G5e3RFJlpRMQAAAAAAp43gywctWGX29hrcoaUSo0ItrgaNWlGedHCVd2+u3COV20Ume4dciT2lgOCGrxcAAAAAgDpE8OVjDMPQvFVlwxwBD8OQMvaW9uYqnZ8rbZ3kKvFuZw+UEntJKQPLhi5GsUACAAAAAMD/EHz5mNX7MrTraK5CAu0a0T3B6nJgpeIC6dBqM+Ta95MZeOWkV24XkVihN1cvKTCkwcsFAAAAAKChEXz5mPmlvb2Gd0tQeDBPX5OSub90Xq7SIYuH1kquYu829gApoWdpyFUadkWlSCyAAAAAAABogk4rOXnuuef0j3/8Q2lpaerVq5f+/e9/a8CAAVW2femll/T6669r/fr1kqR+/frp0UcfrbY9qldU4tKHaw5Kksb2YWiaXysplA6tKRuyuO9nKftg5XbN4sqFXAPNCekDmfcNAAAAAADpNIKvt99+W1OnTtULL7yggQMHaubMmRo+fLi2bNmiuLi4Su2XLl2qa6+9VoMHD1ZISIgef/xxXXzxxdqwYYOSkwlvauPrrUd0Iq9YMeHBOrdjjNXloC5lHSwNuUpXWzy0RnIWebexOaSE7mbA1WqAOT9XdBt6cwEAAAAAUA2bYRhGbQ4YOHCgzj77bD377LOSJJfLpZSUFN1xxx267777Tnm80+lU8+bN9eyzz2rChAk1umZWVpaioqKUmZmpyMjI2pTrV2594xd9uj5NN53bTg9cmmp1OThdJUXmpPP7firrzZW1v3K7sBjvIYtJfaSgZg1fLwAAAAAAjUhtcqJa9fgqKirSL7/8ovvvv9+zzW63a9iwYVq2bFmNzpGXl6fi4mK1aNGi2jaFhYUqLCz03M/KyqpNmX4pM69YizcdliSN7UtPOZ+SneY9ZPHQaqmkwLuNzS7FdyvtyVUadrVoT28uAAAAAADOQK2Cr6NHj8rpdCo+Pt5re3x8vDZv3lyjc9x7771KSkrSsGHDqm0zY8YMTZ8+vTal+b2P1x1SkdOlzvERSk1sur3eGozLKe35wVwlMTxeajNYsjtOfZyz2OzNtf/nsrArY2/ldqHNy4YrpgyUkvpKweF1/zgAAAAAAGjCGnRZwMcee0xvvfWWli5dqpCQkGrb3X///Zo6darnflZWllJSUhqixEZr/ipzKNzYvsmy0Quofm1cKC2615x3yy0ySRrxuJQ62rttzpHSnlylqy0eXCWV5Fc4oU2KSzV7cqUMMAOvlh3ozQUAAAAAQD2rVfAVExMjh8Oh9PR0r+3p6elKSEg46bFPPvmkHnvsMX355Zfq2bPnSdsGBwcrODi4NqX5tX3H8/Tz7hOy2aTLeidZXY5/27hQemeCpApT32UdMrf/+q/mqonu3lwndlc+R0h02bxcrc6WkvtJIfTSAwAAAACgodUq+AoKClK/fv20ePFijRkzRpI5uf3ixYs1ZcqUao974okn9Mgjj+izzz5T//79z6jgpmj+qgOSpMEdWioxKtTiavyYy2n29KoYekll2754sMJ2mxTbxRyy6J6fq2UnyW6v52IBAAAAAMCp1Hqo49SpUzVx4kT1799fAwYM0MyZM5Wbm6vJkydLkiZMmKDk5GTNmDFDkvT444/roYce0ty5c9W2bVulpaVJksLDwxUezpxGp2IYhuatNIc5juvTyuJq/FjecWnVG97DG6uT1EfqNNwMuZL7SaHR9V4eAAAAAACovVoHX1dffbWOHDmihx56SGlpaerdu7cWLVrkmfB+7969spfr7TJr1iwVFRXpiiuu8DrPtGnT9PDDD59Z9U3Aqn0Z2n0sT6GBDo3ofvLhpKihgizp0Brp4EpzTq6Dq6oeslidQVOkHlecuh0AAAAAALDUaU1uP2XKlGqHNi5dutTr/u7du0/nEig1f6U5zHF4t3g1C27QtQj8Q1GuucrigXIh17FtVbeNSJSyD536nOHxp24DAAAAAAAsR5LSiBWVuPThWnPo3di+DHM8peICKX2Dd0+uI5slw1W5bVRrKam3OWwxqY95OzhSmtndnMi+ynm+bObqjm0G1+/jAAAAAAAAdYLgqxFbuuWwMvKKFRsRrHM6tLS6nMalpEg6vLEs4Dq4yrzvKqncNiKxXMDVR0rsLYXHVn3eEY+Xrupok3f4ZSvd/5hkd9TtYwEAAAAAAPWC4KsRm1c6zHFM7yQFOJrwKoHOEunoVu+eXGnrJWdh5bZhLaWkvt5BV2Riza+VOlq66nVzdcfyE91HJpmhV+roM388AAAAAACgQRB8NVKZecX6avNhSdLYprSao8slHdvu3ZMrba1UnFe5bUiUd8CV1FeKaiXZbGdWQ+poqcsl0p4fpJx0c06vNoPp6QUAAAAAgI8h+GqkPlp3UEVOl7okRCg1KdLqcuqHYZirKXp6cq02v4qyK7cNCjeHKLrn5UruKzVvd+YhV3XsDqndefVzbgAAAAAA0CAIvhop92qOY/skW1xJHTEMKXO/d0+ug6ukgozKbQNCpcSe3j25WnaU7E14uCcAAAAAAKg1gq9GaO+xPK3Yc0I2m3RZbx8NvrLTvefkOrhKyj1SuZ0jSIrvXtaLK6mPFNNZcvBPEwAAAAAAnBnShUZo/iqzt9c5HWKUEBVicTU1kHusck+u7IOV29kDpLiu3pPPx6VKAUENXzMAAAAAAPB7BF+NjGEYmrdqvyRpXN9G2NsrP0M6tNo75MrYW7mdzW723EouF3LFd5MCQxu6YgAAAAAA0EQRfDUyK/dmaM+xPIUGOjS8W4K1xRRmS4fWeodcx3dU3bZlR++eXAk9pODwhq0XAAAAAACgHIKvRmZ+aW+vEd0T1Cy4AZ+e4nwpbZ13yHVkiySjctvoNt49uRJ7SSFRDVcrAAAAAABADRB8NSJFJS59tPaQpHpezbGkUErfUC7kWi0d3igZzsptI1tJSb3LrbDYRwprUX+1AQAAAAAA1BGCr0ZkyZbDysgrVlxEsM7pGFM3J3UWS0c2e/fkSt8gOYsqt20WV6EnV28pIr5u6gAA1Dun06ni4mKrywBQBwIDA+VwOKwuAwAAn0fw1YjMX2mu5nhZ7yQ57Lban8DllI5u8w650tZKJQWV24Y2956TK6mPFJkk2U7jugAASxmGobS0NGVkZFhdCoA6FB0drYSEBNl4fQYAwGkj+LKCyynt+UHKSZfC46U2g5VR4NTizemSpHF9W9XgHC7pxC7vkOvQGqkop3Lb4MjKwxWj2xByAYCfcIdecXFxCgsL400y4OMMw1BeXp4OHz4sSUpMTLS4IgAAfBfBV0PbuFBadK+UdbBsW2SS1rT/o4qdCeqSEKGuiZHexxiGlLHXO+Q6uFoqzKx8/sBm5mTz5UOuFu0lu71eHxYAwBpOp9MTerVs2dLqcgDUkdDQUEnS4cOHFRcXx7BHAABOE8FXQ9q4UHpngiqtlJh1SOevnqrh9rvVr88NZijmFXKtkvKOVT5fQIiU0KNcyNVXiukk2XlhBABNhXtOr7CwMIsrAVDX3L/XxcXFBF8AAJwmgq+G4nKaPb0qhl6Suc2Q/hX4nAJ/miMtOVK5iT1Qiu9mBlzuCehju0iOwPquHADgAxjeCPgffq8BADhzBF8NZc8P3sMbK7DZpBAVS3lHJJtDiutaOi9XacgV300KCG64egEAAAAAAHwcEz81lJz0mrW74H7p/v3Srd9Llz0nnX2j2cOL0AsA0ETNnj1b0dHRp2xns9m0YMGCeq8HDe+CCy7Q3Xffbdn1J02apDFjxjSaegAAQM3R46uhhMfXrF2bc6Qg5mkBADQ8p8vQ8l3HdTi7QHERIRrQroUcduuHWl199dX6zW9+47n/8MMPa8GCBVq9erV1RTV1VaxQ3ZTmGJ03b54CA5luAgAAX0Dw1VDaDJYik6SsQ6pqni9Dki0y2WwHAEADW7T+kKZ/uFGHMgs82xKjQjRtVKpGdE+0sDJzdTv3Cne+pri42P8CkmpWqNaIx6XU0dbV1YBatGhhdQkAAKCGGOrYUOwO8wWhJMn703OXUbptxGNN6tNSAEDjsGj9Id36xkqv0EuS0jILdOsbK7Vo/aE6v+ZHH32k6OhoOZ1OSdLq1atls9l03333edrcdNNNuv76672GOs6ePVvTp0/XmjVrZLPZZLPZNHv2bM8xR48e1dixYxUWFqZOnTpp4cKFNapn6dKlstlsWrx4sfr376+w14wsGwAAjgpJREFUsDANHjxYW7Zs8Wo3a9YsdejQQUFBQercubP+97//ee232WyaNWuWRo8erWbNmumRRx7Rww8/rN69e+uVV15R69atFR4erttuu01Op1NPPPGEEhISFBcXp0ceeeQ0fpINzL1CdcV5S7MOmds31uznfTpKSko0ZcoURUVFKSYmRg8++KAMw/ww8X//+5/69++viIgIJSQk6LrrrtPhw4c9x544cULjx49XbGysQkND1alTJ7366que/fv27dNVV12l6OhotWjRQpdddpl2795dbS0Vhzq2bdtWjz76qH77298qIiJCrVu31osvvuh1TG2vAQAA6gbBV0NKHS1d9boU6f3J+WFbS7mufK3JfEoKAKhfhmEor6ikRl/ZBcWatnBDdWsOS5IeXrhR2QXFNTqfO4g4lfPOO0/Z2dlatWqVJOnrr79WTEyMli5d6mnz9ddf64ILLvA67uqrr9Yf/vAHdevWTYcOHdKhQ4d09dVXe/ZPnz5dV111ldauXavf/OY3Gj9+vI4fP17jn93//d//6Z///KdWrFihgIAA/fa3v/Xsmz9/vu666y794Q9/0Pr16/X73/9ekydP1pIlS7zO8fDDD2vs2LFat26d5/gdO3bo008/1aJFi/Tmm2/q5Zdf1iWXXKL9+/fr66+/1uOPP64HHnhAP/30U41rrROGIRXl1uyrIEv69M+qdoVqyewJVpBVs/PV8N+K22uvvaaAgAAtX75c//rXv/TUU0/pv//9rySzZ93f/vY3rVmzRgsWLNDu3bs1adIkz7EPPvigNm7cqE8//VSbNm3SrFmzFBMT4zl2+PDhioiI0Lfffqvvv/9e4eHhGjFihIqKimpc3z//+U/1799fq1at0m233aZbb73VE5zW1TUAAEDtMdSxoaWOlrpcIu35Qf/9dJm+3G9Tr3NG6v5u3a2uDADgJ/KLnUp96LM6OZchKS2rQD0e/rxG7Tf+dbjCgk798iIqKkq9e/fW0qVL1b9/fy1dulT33HOPpk+frpycHGVmZmr79u0aMmSIvv/+e89xoaGhCg8PV0BAgBISEiqdd9KkSbr22mslSY8++qieeeYZLV++XCNGjKhR/Y888oiGDBkiSbrvvvt0ySWXqKCgQCEhIXryySc1adIk3XbbbZKkqVOn6scff9STTz6poUOHes5x3XXXafLkyV7ndblceuWVVxQREaHU1FQNHTpUW7Zs0SeffCK73a7OnTvr8ccf15IlSzRw4MAa1VonivOkR5Pq6GSG2RPssZSaNf/LQSmoWY3PnpKSoqefflo2m02dO3fWunXr9PTTT+t3v/udV0DZvn17PfPMMzr77LOVk5Oj8PBw7d27V3369FH//v0lmT203N5++225XC7997//lc1m9sp/9dVXFR0draVLl+riiy+uUX2/+c1vPP827r33Xj399NNasmSJOnfuXGfXAAAAtUePLws4ZdcXeZ00Y183/ehK1eg+NXyBCACAHxkyZIiWLl0qwzD07bffaty4ceratau+++47ff3110pKSlKnTp1qdc6ePXt6bjdr1kyRkZFeQ95qc3xiotlD2338pk2bdM4553i1P+ecc7Rp0yavbe5wpby2bdsqIiLCcz8+Pl6pqamy2+1e22pTa1Pzq1/9yhMaSdKgQYO0bds2OZ1O/fLLLxo1apRat26tiIgIT3i5d+9eSdKtt96qt956S71799af//xn/fDDD57zrFmzRtu3b1dERITCw8MVHh6uFi1aqKCgQDt27KhxfeX/7dhsNiUkJHiez7q6BgAAqD16fDWwqiYPvum1FY1i8mAAgH8IDXRo41+H16jt8l3HNenVn0/ZbvbkszWg3akn9A4NrPlclRdccIFeeeUVrVmzRoGBgerSpYsuuOACLV26VCdOnPCEF7VRcSJ5m80ml8t1Wse7Q5baHC+ZgVtN6jrTWutEYJjZ86om9vwgzbni1O3Gv1ezxXoC62YV64KCAg0fPlzDhw/XnDlzFBsbq71792r48OGeYYQjR47Unj179Mknn+iLL77QRRddpNtvv11PPvmkcnJy1K9fP82ZM6fSuWNjY2tcx8mez7q6BgAAqD2Crwbknjy44owW7smDZ13fl/ALAHDGbDZbjYYbStJ5nWKVGBWitMyCKmdusklKiArReZ1i5bDbqmhx+tzzfD399NOekOuCCy7QY489phMnTugPf/hDlccFBQV5JsVvSF27dtX333+viRMnerZ9//33Sk1NbfBa6ozNVvPhhh0uPOkK1ZLN3N/hwnpZrKfi/Gc//vijOnXqpM2bN+vYsWN67LHHlJJi9qJfsWJFpeNjY2M1ceJETZw4Ueedd57+9Kc/6cknn1Tfvn319ttvKy4uTpGRkXVet6QGuQYAAKgaQx0biNNlaPqHG086efD0DzfK6ardRK8AAJwJh92maaPM4KZirOW+P21Uap2HXpLUvHlz9ezZU3PmzPFMYn/++edr5cqV2rp1a7U9vtq2batdu3Zp9erVOnr0qAoLC+u8tqr86U9/0uzZszVr1ixt27ZNTz31lObNm6c//vGPDXJ9y51khWrP/XpcoXrv3r2aOnWqtmzZojfffFP//ve/ddddd6l169YKCgrSv//9b+3cuVMLFy7U3/72N69jH3roIX3wwQfavn27NmzYoI8++khdu3aVJI0fP14xMTG67LLL9O2332rXrl1aunSp7rzzTu3fv79Oam+IawAAgKoRfDWQ5buOV1omvjxD0qHMAi3fVfOVpwAAqAsjuidq1vV9lRAV4rU9ISqk3nsjDxkyRE6n0xN8tWjRQqmpqUpISFDnzp2rPObyyy/XiBEjNHToUMXGxurNN9+st/rKGzNmjP71r3/pySefVLdu3fSf//xHr776aqWVJ/1aNStUKzLJ3F6PK1RPmDBB+fn5GjBggG6//XbddddduvnmmxUbG6vZs2fr3XffVWpqqh577DE9+eSTXscGBQXp/vvvV8+ePXX++efL4XDorbfekiSFhYXpm2++UevWrT3zzN14440qKCios95ZDXENAABQNZtR03XHLZSVlaWoqChlZmb67IuDD1Yf0F1vrT5lu39d01uX9U6u/4IAAH6hoKBAu3btUrt27RQSEnLqA07C6TK0fNdxHc4uUFxEiAa0a1EvPb3gB1xOc86vnHQpPN6c06ueeno1ZXX5+w0AgD+pTU7EHF8NJC6iZi9WatoOAIC65rDbNKhDS6vLgC+wO6R251ldBQAAwCkx1LGBDGjXQolRIZVmxHCzSUqMCqnRilkAAKB2brnlFoWHh1f5dcstt1hdHgAAAOoJPb4aiHvy4FvfWCmbvNdCqu/JgwEAaOr++te/VjsJva9OowAAAIBTI/hqQO7Jg6d/uNFrovuEqBBNG5Var5MHAwDQlMXFxSkuLs7qMgAAANDACL4a2Ijuifp1agKTBwMA6pQPrFUDoJb4vQYA4MwRfFmAyYMBAHUlMDBQkpSXl6fQ0FCLqwFQl/Ly8iSV/Z4DAIDaI/gCAMCHORwORUdH6/Dhw5KksLAw2Wz0IgZ8mWEYysvL0+HDhxUdHS2Hw2F1SQAA+CyCLwAAfFxCQoIkecIvAP4hOjra8/sNAABOD8EXAAA+zmazKTExUXFxcSouLra6HAB1IDAwkJ5eAADUAYIvAAD8hMPh4I0yAAAAUI7d6gIAAAAAAACA+kDwBQAAAAAAAL9E8AUAAAAAAAC/5BNzfBmGIUnKysqyuBIAAAAAAABYyZ0PufOik/GJ4Cs7O1uSlJKSYnElAAAAAAAAaAyys7MVFRV10jY2oybxmMVcLpcOHjyoiIgI2Ww2q8upE1lZWUpJSdG+ffsUGRlpdTmAz+J3Cagb/C4BdYffJ6Bu8LsE1A1//F0yDEPZ2dlKSkqS3X7yWbx8oseX3W5Xq1atrC6jXkRGRvrNPzzASvwuAXWD3yWg7vD7BNQNfpeAuuFvv0un6unlxuT2AAAAAAAA8EsEXwAAAAAAAPBLBF8WCQ4O1rRp0xQcHGx1KYBP43cJqBv8LgF1h98noG7wuwTUjab+u+QTk9sDAAAAAAAAtUWPLwAAAAAAAPglgi8AAAAAAAD4JYIvAAAAAAAA+CWCLwAAAAAAAPglgi8LPPfcc2rbtq1CQkI0cOBALV++3OqSAJ8zY8YMnX322YqIiFBcXJzGjBmjLVu2WF0W4PMee+wx2Ww23X333VaXAvicAwcO6Prrr1fLli0VGhqqHj16aMWKFVaXBfgcp9OpBx98UO3atVNoaKg6dOigv/3tb2JdNuDkvvnmG40aNUpJSUmy2WxasGCB137DMPTQQw8pMTFRoaGhGjZsmLZt22ZNsQ2I4KuBvf3225o6daqmTZumlStXqlevXho+fLgOHz5sdWmAT/n66691++2368cff9QXX3yh4uJiXXzxxcrNzbW6NMBn/fzzz/rPf/6jnj17Wl0K4HNOnDihc845R4GBgfr000+1ceNG/fOf/1Tz5s2tLg3wOY8//rhmzZqlZ599Vps2bdLjjz+uJ554Qv/+97+tLg1o1HJzc9WrVy8999xzVe5/4okn9Mwzz+iFF17QTz/9pGbNmmn48OEqKCho4Eobls0gNm9QAwcO1Nlnn61nn31WkuRyuZSSkqI77rhD9913n8XVAb7ryJEjiouL09dff63zzz/f6nIAn5OTk6O+ffvq+eef19///nf17t1bM2fOtLoswGfcd999+v777/Xtt99aXQrg8y699FLFx8fr5Zdf9my7/PLLFRoaqjfeeMPCygDfYbPZNH/+fI0ZM0aS2dsrKSlJf/jDH/THP/5RkpSZman4+HjNnj1b11xzjYXV1i96fDWgoqIi/fLLLxo2bJhnm91u17Bhw7Rs2TILKwN8X2ZmpiSpRYsWFlcC+Kbbb79dl1xyidffKAA1t3DhQvXv319XXnml4uLi1KdPH7300ktWlwX4pMGDB2vx4sXaunWrJGnNmjX67rvvNHLkSIsrA3zXrl27lJaW5vVaLyoqSgMHDvT7PCLA6gKakqNHj8rpdCo+Pt5re3x8vDZv3mxRVYDvc7lcuvvuu3XOOeeoe/fuVpcD+Jy33npLK1eu1M8//2x1KYDP2rlzp2bNmqWpU6fqL3/5i37++WfdeeedCgoK0sSJE60uD/Ap9913n7KystSlSxc5HA45nU498sgjGj9+vNWlAT4rLS1NkqrMI9z7/BXBFwCfd/vtt2v9+vX67rvvrC4F8Dn79u3TXXfdpS+++EIhISFWlwP4LJfLpf79++vRRx+VJPXp00fr16/XCy+8QPAF1NI777yjOXPmaO7cuerWrZtWr16tu+++W0lJSfw+Aag1hjo2oJiYGDkcDqWnp3ttT09PV0JCgkVVAb5typQp+uijj7RkyRK1atXK6nIAn/PLL7/o8OHD6tu3rwICAhQQEKCvv/5azzzzjAICAuR0Oq0uEfAJiYmJSk1N9drWtWtX7d2716KKAN/1pz/9Sffdd5+uueYa9ejRQzfccIPuuecezZgxw+rSAJ/lzhyaYh5B8NWAgoKC1K9fPy1evNizzeVyafHixRo0aJCFlQG+xzAMTZkyRfPnz9dXX32ldu3aWV0S4JMuuugirVu3TqtXr/Z89e/fX+PHj9fq1avlcDisLhHwCeecc462bNnitW3r1q1q06aNRRUBvisvL092u/dbVYfDIZfLZVFFgO9r166dEhISvPKIrKws/fTTT36fRzDUsYFNnTpVEydOVP/+/TVgwADNnDlTubm5mjx5stWlAT7l9ttv19y5c/XBBx8oIiLCMy49KipKoaGhFlcH+I6IiIhKc+M1a9ZMLVu2ZM48oBbuueceDR48WI8++qiuuuoqLV++XC+++KJefPFFq0sDfM6oUaP0yCOPqHXr1urWrZtWrVqlp556Sr/97W+tLg1o1HJycrR9+3bP/V27dmn16tVq0aKFWrdurbvvvlt///vf1alTJ7Vr104PPvigkpKSPCs/+iubYRiG1UU0Nc8++6z+8Y9/KC0tTb1799YzzzyjgQMHWl0W4FNsNluV21999VVNmjSpYYsB/MwFF1yg3r17a+bMmVaXAviUjz76SPfff7+2bdumdu3aaerUqfrd735ndVmAz8nOztaDDz6o+fPn6/Dhw0pKStK1116rhx56SEFBQVaXBzRaS5cu1dChQyttnzhxombPni3DMDRt2jS9+OKLysjI0Lnnnqvnn39eZ511lgXVNhyCLwAAAAAAAPgl5vgCAAAAAACAXyL4AgAAAAAAgF8i+AIAAAAAAIBfIvgCAAAAAACAXyL4AgAAAAAAgF8i+AIAAAAAAIBfIvgCAAAAAACAXyL4AgAAAAAAgF8i+AIAAPBzNptNCxYssLoMAACABkfwBQAAUI8mTZokm81W6WvEiBFWlwYAAOD3AqwuAAAAwN+NGDFCr776qte24OBgi6oBAABoOujxBQAAUM+Cg4OVkJDg9dW8eXNJ5jDEWbNmaeTIkQoNDVX79u313nvveR2/bt06XXjhhQoNDVXLli118803Kycnx6vNK6+8om7duik4OFiJiYmaMmWK1/6jR49q7NixCgsLU6dOnbRw4cL6fdAAAACNAMEXAACAxR588EFdfvnlWrNmjcaPH69rrrlGmzZtkiTl5uZq+PDhat68uX7++We9++67+vLLL72CrVmzZun222/XzTffrHXr1mnhwoXq2LGj1zWmT5+uq666SmvXrtVvfvMbjR8/XsePH2/QxwkAANDQbIZhGFYXAQAA4K8mTZqkN954QyEhIV7b//KXv+gvf/mLbDabbrnlFs2aNcuz71e/+pX69u2r559/Xi+99JLuvfde7du3T82aNZMkffLJJxo1apQOHjyo+Ph4JScna/Lkyfr73/9eZQ02m00PPPCA/va3v0kyw7Tw8HB9+umnzDUGAAD8GnN8AQAA1LOhQ4d6BVuS1KJFC8/tQYMGee0bNGiQVq9eLUnatGmTevXq5Qm9JOmcc86Ry+XSli1bZLPZdPDgQV100UUnraFnz56e282aNVNkZKQOHz58ug8JAADAJxB8AQAA1LNmzZpVGnpYV0JDQ2vULjAw0Ou+zWaTy+Wqj5IAAAAaDeb4AgAAsNiPP/5Y6X7Xrl0lSV27dtWaNWuUm5vr2f/999/Lbrerc+fOioiIUNu2bbV48eIGrRkAAMAX0OMLAACgnhUWFiotLc1rW0BAgGJiYiRJ7777rvr3769zzz1Xc+bM0fLly/Xyyy9LksaPH69p06Zp4sSJevjhh3XkyBHdcccduuGGGxQfHy9Jevjhh3XLLbcoLi5OI0eOVHZ2tr7//nvdcccdDftAAQAAGhmCLwAAgHq2aNEiJSYmem3r3LmzNm/eLMlccfGtt97SbbfdpsTERL355ptKTU2VJIWFhemzzz7TXXfdpbPPPlthYWG6/PLL9dRTT3nONXHiRBUUFOjpp5/WH//4R8XExOiKK65ouAcIAADQSLGqIwAAgIVsNpvmz5+vMWPGWF0KAACA32GOLwAAAAAAAPglgi8AAAAAAAD4Jeb4AgAAsBCzTgAAANQfenwBAAAAAADALxF8AQAAn7Z7927ZbDbNnj3bs+3hhx+WzWar0fE2m00PP/xwndZ0wQUX6IILLqjTcwIAAKD2CL4AAECDGT16tMLCwpSdnV1tm/HjxysoKEjHjh1rwMpqb+PGjXr44Ye1e/duq0sBAABANQi+AABAgxk/frzy8/M1f/78Kvfn5eXpgw8+0IgRI9SyZcvTvs4DDzyg/Pz80z6+JjZu3Kjp06dXGXx9/vnn+vzzz+v1+gAAADg1gi8AANBgRo8erYiICM2dO7fK/R988IFyc3M1fvz4M7pOQECAQkJCzugcZyIoKEhBQUGWXd9X5ObmWl0CAADwcwRfAACgwYSGhmrcuHFavHixDh8+XGn/3LlzFRERodGjR+v48eP64x//qB49eig8PFyRkZEaOXKk1qxZc8rrVDXHV2Fhoe655x7FxsZ6rrF///5Kx+7Zs0e33XabOnfurNDQULVs2VJXXnmlV8+u2bNn68orr5QkDR06VDabTTabTUuXLpVU9Rxfhw8f1o033qj4+HiFhISoV69eeu2117zauOcre/LJJ/Xiiy+qQ4cOCg4O1tlnn62ff/75lI+7Nj+zgoICPfzwwzrrrLMUEhKixMREjRs3Tjt27PC0cblc+te//qUePXooJCREsbGxGjFihFasWOFVb/n51dwqzp3mfk42btyo6667Ts2bN9e5554rSVq7dq0mTZqk9u3bKyQkRAkJCfrtb39b5XDXAwcO6MYbb1RSUpKCg4PVrl073XrrrSoqKtLOnTtls9n09NNPVzruhx9+kM1m05tvvnnKnyMAAPAfAVYXAAAAmpbx48frtdde0zvvvKMpU6Z4th8/flyfffaZrr32WoWGhmrDhg1asGCBrrzySrVr107p6en6z3/+oyFDhmjjxo1KSkqq1XVvuukmvfHGG7ruuus0ePBgffXVV7rkkksqtfv555/1ww8/6JprrlGrVq20e/duzZo1SxdccIE2btyosLAwnX/++brzzjv1zDPP6C9/+Yu6du0qSZ7vFeXn5+uCCy7Q9u3bNWXKFLVr107vvvuuJk2apIyMDN11111e7efOnavs7Gz9/ve/l81m0xNPPKFx48Zp586dCgwMrPYx7ty5s0Y/M6fTqUsvvVSLFy/WNddco7vuukvZ2dn64osvtH79enXo0EGSdOONN2r27NkaOXKkbrrpJpWUlOjbb7/Vjz/+qP79+9fq5+925ZVXqlOnTnr00UdlGIYk6YsvvtDOnTs1efJkJSQkaMOGDXrxxRe1YcMG/fjjj54Q8+DBgxowYIAyMjJ08803q0uXLjpw4IDee+895eXlqX379jrnnHM0Z84c3XPPPV7XnTNnjiIiInTZZZedVt0AAMBHGQAAAA2opKTESExMNAYNGuS1/YUXXjAkGZ999plhGIZRUFBgOJ1Orza7du0ygoODjb/+9a9e2yQZr776qmfbtGnTjPIvc1avXm1IMm677Tav81133XWGJGPatGmebXl5eZVqXrZsmSHJeP311z3b3n33XUOSsWTJkkrthwwZYgwZMsRzf+bMmYYk44033vBsKyoqMgYNGmSEh4cbWVlZXo+lZcuWxvHjxz1tP/jgA0OS8eGHH1a6Vnk1/Zm98sorhiTjqaeeqnQOl8tlGIZhfPXVV4Yk484776y2TVU/e7eKP1f3c3LttddWalvVz/zNN980JBnffPONZ9uECRMMu91u/Pzzz9XW9J///MeQZGzatMmzr6ioyIiJiTEmTpxY6TgAAODfGOoIAAAalMPh0DXXXKNly5Z5DR+cO3eu4uPjddFFF0mSgoODZbebL1WcTqeOHTum8PBwde7cWStXrqzVNT/55BNJ0p133um1/e67767UNjQ01HO7uLhYx44dU8eOHRUdHV3r65a/fkJCgq699lrPtsDAQN15553KycnR119/7dX+6quvVvPmzT33zzvvPElmj66TqenP7P3331dMTIzuuOOOSudw9656//33ZbPZNG3atGrbnI5bbrml0rbyP/OCggIdPXpUv/rVryTJU7fL5dKCBQs0atSoKnubuWu66qqrFBISojlz5nj2ffbZZzp69Kiuv/76064bAAD4JoIvAADQ4NyT17snud+/f7++/fZbXXPNNXI4HJLMoOPpp59Wp06dFBwcrJiYGMXGxmrt2rXKzMys1fX27Nkju93uGcLn1rlz50pt8/Pz9dBDDyklJcXruhkZGbW+bvnrd+rUyRNKubmHRu7Zs8dre+vWrb3uu0OwEydOnPQ6Nf2Z7dixQ507d1ZAQPWzXuzYsUNJSUlq0aLFqR9gLbRr167StuPHj+uuu+5SfHy8QkNDFRsb62nnrvvIkSPKyspS9+7dT3r+6OhojRo1ymsBhTlz5ig5OVkXXnhhHT4SAADgCwi+AABAg+vXr5+6dOnimWj8zTfflGEYXqs5Pvroo5o6darOP/98vfHGG/rss8/0xRdfqFu3bnK5XPVW2x133KFHHnlEV111ld555x19/vnn+uKLL9SyZct6vW557vCvIqN0TqzqNPTPrLqeX06ns9pjyvfucrvqqqv00ksv6ZZbbtG8efP0+eefa9GiRZJ0WnVPmDBBO3fu1A8//KDs7GwtXLhQ1157baXgEQAA+D8mtwcAAJYYP368HnzwQa1du1Zz585Vp06ddPbZZ3v2v/feexo6dKhefvllr+MyMjIUExNTq2u1adNGLpfL09PJbcuWLZXavvfee5o4caL++c9/erYVFBQoIyPDq11thvu1adNGa9eulcvl8gpfNm/e7NlfF2r6M+vQoYN++uknFRcXVztZfocOHfTZZ5/p+PHj1fb6cvdEq/izqdiD7WROnDihxYsXa/r06XrooYc827dt2+bVLjY2VpGRkVq/fv0pzzlixAjFxsZqzpw5GjhwoPLy8nTDDTfUuCYAAOA/+NgLAABYwt2766GHHtLq1au9entJZq+nij2c3n33XR04cKDW1xo5cqQk6ZlnnvHaPnPmzEptq7ruv//970q9mJo1ayapcuhTld/85jdKS0vT22+/7dlWUlKif//73woPD9eQIUNq8jBOqaY/s8svv1xHjx7Vs88+W+kc7uMvv/xyGYah6dOnV9smMjJSMTEx+uabb7z2P//887Wqufw53So+N3a7XWPGjNGHH36oFStWVFuTJAUEBOjaa6/VO++8o9mzZ6tHjx7q2bNnjWsCAAD+gx5fAADAEu3atdPgwYP1wQcfSFKl4OvSSy/VX//6V02ePFmDBw/WunXrNGfOHLVv377W1+rdu7euvfZaPf/888rMzNTgwYO1ePFibd++vVLbSy+9VP/73/8UFRWl1NRULVu2TF9++aVatmxZ6ZwOh0OPP/64MjMzFRwcrAsvvFBxcXGVznnzzTfrP//5jyZNmqRffvlFbdu21Xvvvafvv/9eM2fOVERERK0fU1Vq+jObMGGCXn/9dU2dOlXLly/Xeeedp9zcXH355Ze67bbbdNlll2no0KG64YYb9Mwzz2jbtm0aMWKEXC6Xvv32Ww0dOlRTpkyRJN1000167LHHdNNNN6l///765ptvtHXr1hrXHBkZqfPPP19PPPGEiouLlZycrM8//1y7du2q1PbRRx/V559/riFDhujmm29W165ddejQIb377rv67rvvFB0d7fUYn3nmGS1ZskSPP/746f1AAQCAzyP4AgAAlhk/frx++OEHDRgwQB07dvTa95e//EW5ubmaO3eu3n77bfXt21cff/yx7rvvvtO61iuvvOIZ/rZgwQJdeOGF+vjjj5WSkuLV7l//+pccDofmzJmjgoICnXPOOfryyy81fPhwr3YJCQl64YUXNGPGDN14441yOp1asmRJlcFXaGioli5dqvvuu0+vvfaasrKy1LlzZ7366quaNGnSaT2eqtT0Z+ZwOPTJJ5/okUce0dy5c/X++++rZcuWOvfcc9WjRw9Pu1dffVU9e/bUyy+/rD/96U+KiopS//79NXjwYE+bhx56SEeOHNF7772nd955RyNHjtSnn35a5c+hOnPnztUdd9yh5557ToZh6OKLL9ann36qpKQkr3bJycn66aef9OCDD2rOnDnKyspScnKyRo4cqbCwMK+2/fr1U7du3bRp06ZKoSoAAGg6bMapZkkFAAAAfFCfPn3UokULLV682OpSAACARZjjCwAAAH5nxYoVWr16tSZMmGB1KQAAwEL0+AIAAIDfWL9+vX755Rf985//1NGjR7Vz506FhIRYXRYAALAIPb4AAADgN9577z1NnjxZxcXFevPNNwm9AABo4ujxBQAAAAAAAL9Ejy8AAAAAAAD4JYIvAAAAAAAA+KUAqwuoCZfLpYMHDyoiIkI2m83qcgAAAAAAAGARwzCUnZ2tpKQk2e0n79PlE8HXwYMHlZKSYnUZAAAAAAAAaCT27dunVq1anbSNTwRfERERkswHFBkZaXE1AAAAAAAAsEpWVpZSUlI8edHJ+ETw5R7eGBkZSfAFAAAAAACAGk2HxeT2AAAAAAAA8EsEXwAAAAAAAPBLBF8AAAAAAADwS7UOvr755huNGjVKSUlJstlsWrBgwSmPWbp0qfr27avg4GB17NhRs2fPPo1SAQAAAAAAgJqrdfCVm5urXr166bnnnqtR+127dumSSy7R0KFDtXr1at1999266aab9Nlnn9W6WAAAAAAAAKCmar2q48iRIzVy5Mgat3/hhRfUrl07/fOf/5Qkde3aVd99952efvppDR8+vLaXBwAAAAAAAGqk3uf4WrZsmYYNG+a1bfjw4Vq2bFm1xxQWFiorK8vrCwAAAAAAAKiNeg++0tLSFB8f77UtPj5eWVlZys/Pr/KYGTNmKCoqyvOVkpJS32UCAID/b+/Ow6Osz/2Pf2Ymk30jZGdHEQj7ogho1UoFtSCgxw0FsdWfVq2V9hyXU0VrFW3VUpfiqafqcddaQNxoLYp1LSqERRbZQchCWLKvM8/vj2eSzCSTDZI8s7xf1zVXZp55ZuYOkJD55P7eXwAAAIQUl9vQFzsP663cA/pi52G53IbVJXW7Di917A533nmnFixY0HC7pKSE8AsAAAABz+U2tGb3ERWWVik9IVqnDUiRw26zuiwAQBhauSlP9729WXnFVQ3HspKitXB6jqYNz7Kwsu7V5cFXZmamCgoKfI4VFBQoMTFRMTExfh8TFRWlqKiori4NAAAA6DS8wQgOhJMAwsHKTXm68aW1atrflV9cpRtfWqslV40Nm/+bujz4mjhxot577z2fYx988IEmTpzY1S8NAAAAdAveYAQHwkkA4cDlNnTf25ub/Z8kSYYkm6T73t6sH+VkhkXw3+Hgq6ysTDt27Gi4vXv3buXm5iolJUV9+/bVnXfeqQMHDuiFF16QJN1www168skn9V//9V+69tpr9eGHH+qNN97Qu+++23mfBQAAAGAR3mAEB8JJoPPRQdl96lxuVdW5VVXr8lx8r1c2XHdpS16JT8DflCEpr7hKa3Yf0cSTenbfJ2GRDgdfX3/9tc4555yG2/WzuObNm6fnn39eeXl52rdvX8P9AwYM0LvvvqvbbrtNf/zjH9W7d2/97//+r6ZOndoJ5QMAAh0/EAEIBdV1LpVV1amsuk6lVXUqrzavl1XXaf3+Y+16g3HjS98oOzlGdptNdptkt9sar/s71ux+87rDbpPNc9thl891u63+Psnhfd3zXLam1202z+t4v4ZNdnvjbe/XaDjP7v81/L6erfG6zWbN93/CSaDzhXsHpWEYqq5zq7LGpaq6xiCqPoCq9rntCanqXKqqcTUEWJVe15sFWnUuVda4Ve25Xuvq/KH0haUt/98VSmyGYQT8SP+SkhIlJSWpuLhYiYmJVpcDAGincP+BKFgQTiJUud2GKmobA6uy6jrP9VqVVnnfrlOp53q513Xvx9S43FZ/OiHBO2BrFsK1EPS1GQbafQM4e5Mwz2G3qbiyVhu+L26zvl+dd4pOH9hTybFOJcdGKinGKafD3g1/MkBwaamDsv6nBys6KA3DUK3L8IRQLlXVuBuvN+mIqva53Xi92hNgNYZZzTurzEDKpeo66/5fiIqwK9rpULTTrhinQ9FOh6KcDkVH2BUT6VB5dZ2+2nO0zed59brTg7bjqyM5EcEXAKBLBOIPRGiOcDJ4hFNAWetymwFUk/Cp1Cu4qr9d33nVNMgqq6pTWU2dOvsn3dhIh+KjIhQfHaEEz8fqWre+3tv2G4zZY7KVnRwrl2HIbRgyDPPvtel1t2GGdvXXDcPwPEbmsRbOc3ue1+32uu7zGPO6y22+ntvzvIaf4+Zt87X9PaahLs/zhrKEqAglxTrVIzayIRDrEetUcox5PdlzX8M5MU4lxjhD9usTcLkNnfHwhy12utokZSZF69PbfyhJjaGRV5dTdbMOKf9dT5VeYVO133Mbr1fWuiz7fhRhtymmPnxymqFUjNf1qAiHYiLNYMo7sIryhFbe5zZ+dCg6wnNuZP11h6Ii7LK38f2l/u8ov7jKb6er999RsH6vIvgCEBbC6U1gsOnID0T8nVmHcDJ4BENAaRiGqmrdKvWEUs3DqqZdV/VhVW2zY539W3SH3WaGVVERSoiOaAiu4qI84ZXnduP9zobb3vfFRToU4af7JxzeYLTF8IRhPkFefcDnbrzeLJjzF9L5OW4Gb82vN4Z35mMarjepZVt+qZZ8vLPNz+OktDjVuQ0dq6hVcWXtcf952GxSYrRTPWKdSmoSlDUGaN4hWqSS45xKiIqwbDkoYBiGKmpczZZ0N/5iwfx+va2gVG+vz2vz+SLsNtVZlETZbGoMjbw7orxuRzvt5jmRjmbnRjvtimoIpJqHWVGeEKo+zPL3f4PV6n/Ok+Tzf1Oo/JxH8AUg5AXDm8BAZBiG6tyGaurc5sXV5GOT69VN76tz+d7vc1/juQUlVVq771ib9QzJTFByrNNrqYtNDlvjjBqH3XspS+OyFe+ZMd5zaFpbEtO4DKaNmTh23/k47V6OY295Pk57l+e0OoOnvha7/7rsHZydQzgZPLo6oHS5jTaXAfpbEugbVtWqvMYlVye/wYl22hUf5WwMq7xCqOZhVYRXJ5ZTcVGOhuvRTnuXhwmh/gYj2B1POOlyGyqprNXRihodrahVcWWNjpbX6lhlrY5V1OhYhXlfseecYxW1OlZhBgPHy2G3KTnG6dM91thV1hii9fAsw+wRZ54TG+kgMAtjdS53s18uNP1e7d0xW17t8tyu9bm/vLquSzumIiPsXh1PXiFSw227T5dTtFeXU4x3F5TP9SaBVYRDUU67oiK6/vt+MAjl90wEXwBCWjB1qdS53Kp1GZ6QyOU3XGotQKq/Xu11X22TsKoxnHL5fW6fx7vcnb7sB4HD5j0gu4XQrT7Yq3O5dbSi7W6G/j1jlRjj7IQwsPncHb8Dtf0MyPZ5bXvzYdn1M3zqwz+f6/5CyXYFpM3DSoe9hddrIYhsz5Dy+j+HlrQVUEpSekKUnpk7vuG39L4zqmr9d115Xa+ocR3Xv7eW2GwywyjvkCraqfgozxJBTzeV7/3m7Tivrqy4qIigm60Uym8wQkF3hZO1LrenY8wMzI6W1zQJy/yHaJW1x/+1GOmwe8IyT/eYp6Os6RLM5NhI9YhrPCfa6Tjhz7erhHpnf32XrG9YVevz/dn7lxD+5g/Wd81W1XZul6y9/vt4tNP3Fw6e79UllbV6b1N+m8/zxBVjNPnk1IYOqVD6+wsmofq1RPAFIGS1p0slPTFKb910hlyG0SQAcvkESH67nFoJoNoKpxpCKK+uqECfe2KzmT8sR0aYvxmrv95wabjtUKTDc04r50VF2OV02PX90Qo988nuNl//1nMH6aT0+IYZMv6WqbS6fMYz38bfvBnv643LZVqYidP0PD9Lb5rOvvE+x2V4z8HxPxOn2efTwnKclj5PhDa/A7ltNrncblV08hualkRG2H3DKK8AymemVUOQ1dhpFed1Pdw7T0L1DUaoCORwsqrW1aR7rDEo8+4w8w7LjlXUntDGC9FOe2P3mNcMs+QmIVp9Z1n9wP/IiK4NpQP578nlNlRe03TJdvOO2ZbmD3rPLuzsLtmoCLvPcu76Xzb4+17ddFah92NinK1/H2d5NwIBwReAoOdyGzpSXqOismoVlVXrUKn5ccP3xXpnQ9szBQJVZIRdUS2GS74BUvP7HF7XbV73O3weH9XKcze9r6vmEfADUecy/ARhLQ3Bbm/At27fUd21bFObr337tMEakpnYEOQ1HbTdLGBsFiL6qavpce+h200CTe+h2/7Czfrg0e0JElsNPtsY+O37Z9byYwNx4HdSjFNpCVGNYVWkn2WATWdYeb3xiYsyB+8C4SCUwknDMFRZ6/IJx3yWYJZ7LdP0Pqey9oRCl/ioCM9SSz8dZjF+NgLwHG/Pn3NXdfZX17laCKt8d3X1N3/Qu9OqS7pkI5uHTz5hVcNxPx2zFnXJsrwbViP4AhCQ6lxuHamo8YRYNSryhFnmpcYn4DpSXnPCbxr9BUpNrzfrcvJ3vZXupuaBlP/gyumwy+mwhVUXBD8QBTbCye5xogO/1+47qgVvrG/zdYJ5O3IA3c8wDJVW16m4orHDrDEsq9WxSq8Os/rArNIc+H8i7x4ToyN8usd8wzKnEqOduv/dza0uxU+Ji9T9M4apvNZlBlUtzB8srW5c7l1e7Tqhzjh/nA5b41JAf91TbWycUR9WxTodbe7QF6gCuTMPoY/gC0C3qXO5dbi8piGwqg+wikqrdag+1Co1jx2pqOnQD0s2m5QSG6nU+CilJpgfa+vc7Zop8Op1EzTxpNQT+MzQGfiBKLARTgY+AkoAgcTlNlRaVduke8x3XlnT+46V16r0BAb+d7a4SIfP/MGmO7u2tXEGXbK+QqmDEsGF4AvACal1uXW4vgPLO8QqrfHq0DJDrqPHEWb1jPOEWfFRSo2PVFpCVOPtBM+x+CilxEU2W4rHm8Dgww9EgY1wMvARUAIIdrUut4ora5vMLmu+I+Z3BaXaXljW5vMNTI1Tv56xDfMGzc0zWt44I97TXRUXGcHPIECIIPgC0ExNnVuHyxvDq0PeHVleyw4PlVXrWDt2evNmt0kpcY0hVppXgNUYcEUpLcEMs070Bw7eBAKdi3Ay8BFQAggHX+w8rCue+bLN81jeDYDgCwgT1XUuHS7zXmZohlj1t72XHxZXdizMcthtSomL9Amx0hq6siKVFh/dsPywR+yJh1kdxZtAAOGGgBJAqKOzH0B7EXwBJ8jKNxdVta5mXVjegVbj3KxqlVR1bF5ChN2mnk26sMwQK8p3uWF8pHrERgb8oE3eBAIAAIQWOvsBtAfBF3ACuqKTqKrW1Wz4u0+XVmnjPK3SDoZZTodNPeMah783dmh5dWl5bifHOAM+zAIAAEB4o7MfQFsIvoDjVP8bpqZfFP5+w1RZ42oIqxpCrCbD383jNSrr4E42ToetYSZWfYDVdPh7uue+pBinbDbCLAAAAIQOOvsBtKYjOVFEN9UEBDyX29B9b2/2O0+g/tgtr65TVtIWHS6rUXmNq0PPH+mwe4KsSJ9lhqlNlhmmxUcpMSaCMAsAAABhy2G3McAeQKcg+AI81uw+4tNO7U+ty9C+I5UNt6Mi7A1dWGnxzUOs1PjIhmWGidGEWQAAAAAAdCeCL8CjsLT10Kvez88dpJmjs5WWEKX4KMIsAAAAAAACFcEX4JGeEN2u8yYO7KmBafFdXA0AAAAAADhRdqsLAALFaQNSlJUUrZb6t2wyd5M5bUBKd5YFAAAAAACOE8EX4OGw27Rweo7f4fb1YdjC6TnsJgMAAAAAQJAg+AK8TBuepYkDm+8ek5kUrSVXjdW04VkWVAUAAAAAAI4HM74AL5U1Lm08UCzJ7O5KiYtUeoK5vJFOLwAAAAAAggvBF+Dlgy0FKquuU6/kGM2b2F92wi4AAAAAAIIWSx0BL8vXHZAkzRrTi9ALAAAAAIAgR/AFeBSVVevj7w5JkmaN7WVxNQAAAAAA4EQRfAEeb68/KJfb0KjeSTopLd7qcgAAAAAAwAki+AI8lnmWOc4cQ7cXAAAAAAChgOALkLSjsEwbvi+Ww27T9FHZVpcDAAAAAAA6AcEXoMah9medkqbU+CiLqwEAAAAAAJ2B4Athz+02GpY5zmKZIwAAAAAAIYPgC2Hvqz1HdOBYpeKjIvSjnAyrywEAAAAAAJ2E4Athr77b6/zhmYp2OiyuBgAAAAAAdBaCL4S1qlqX3t2YJ0maNZZljgAAAAAAhBKCL4S1D7cWqrSqTtlJ0Tp9QE+rywEAAAAAAJ2I4Athbelac5njRWN6yW63WVwNAAAAAADoTARfCFtHymu0eluhJHZzBAAAAAAgFBF8IWy9s+Gg6tyGhmUn6pSMBKvLAQAAAAAAnYzgC2GrfjdHur0AAAAAAAhNBF8IS7uLyrVu3zHZbdKM0dlWlwMAAAAAALoAwRfCUn2315mD0pSeEG1xNQAAAAAAoCsQfCHsGIah5SxzBAAAAAAg5BF8Ieys3XdU+45UKDbSofOGZVhdDgAAAAAA6CIEXwg7S9ea3V7ThmcqNjLC4moAAAAAAEBXIfhCWKmuc+mdDXmSpNljeltcDQAAAAAA6EoEXwgrH209pOLKWmUkRmniST2tLgcAAAAAAHQhgi+Elfqh9heN7iWH3WZxNQAAAAAAoCsRfCFsFFfU6sOthZLYzREAAAAAgHBA8IWw8c7Gg6pxuTUkM0FDsxKtLgcAAAAAAHQxgi+EjWWe3Rxnj6XbCwAAAACAcEDwhbCw73CFvt57VDabNGMUwRcAAAAAAOGA4AthYXmu2e01+aRUZSZFW1wNAAAAAADoDgRfCHmGYWiZZzdHhtoDAAAAABA+CL4Q8nL3H9PuonLFOB2aNjzT6nIAAAAAAEA3IfhCyKvv9jpvWIbioiIsrgYAAAAAAHQXgi+EtFqXW2+vPyiJZY4AAAAAAIQbgi+EtI+3HdLRilqlxkfpjJNTrS4HAAAAAAB0I4IvhLT6ZY4Xjc5WhIN/7gAAAAAAhBOSAISs4spafbClQBLLHAEAAAAACEfHFXw99dRT6t+/v6KjozVhwgStWbOm1fMXL16swYMHKyYmRn369NFtt92mqqqq4yoYaK/3N+apps6tQenxGpadaHU5AAAAAACgm3U4+Hr99de1YMECLVy4UGvXrtWoUaM0depUFRYW+j3/lVde0R133KGFCxdqy5Yt+stf/qLXX39dd9111wkXD7SmfpnjrLG9ZLPZLK4GAAAAAAB0tw4HX4899piuu+46zZ8/Xzk5OXr66acVGxurZ5991u/5n3/+uSZPnqwrr7xS/fv313nnnacrrriizS4x4ER8f7RC/959RDabNHM0yxwBAAAAAAhHHQq+ampq9M0332jKlCmNT2C3a8qUKfriiy/8PmbSpEn65ptvGoKuXbt26b333tMFF1zQ4utUV1erpKTE5wJ0xFu5ByVJpw/oqezkGIurAQAAQMBwu6Tdn0gb3zQ/ul1WVwQA6EIRHTm5qKhILpdLGRkZPsczMjK0detWv4+58sorVVRUpDPOOEOGYaiurk433HBDq0sdFy1apPvuu68jpQENDMPQ0rXfS2KoPQAAALxsXiGtvF0qOdh4LDFbmvawlDPDuroAAF2my3d1XL16tR588EH96U9/0tq1a7V06VK9++67uv/++1t8zJ133qni4uKGy/79+7u6TISQTQdKtPNQuaIi7Dp/RKbV5QAAACAQbF4hvTHXN/SSpJI88/jmFdbUBf/ozAPQSTrU8ZWamiqHw6GCggKf4wUFBcrM9B8w3H333br66qv105/+VJI0YsQIlZeX6/rrr9d///d/y25vnr1FRUUpKiqqI6UBDZauM7u9fpSToYRop8XVAAAAwHJul9npJcPPnYYkm7TyDmnIhZLd0c3FoRk68wB0og4FX5GRkRo3bpxWrVqlmTNnSpLcbrdWrVqlm2++2e9jKioqmoVbDof5n4lh+PuPBzh+dS633l5v/gc5eyzLHAEAAMKW2yVVHJbKCqWdHzbv9PJhSCUHpFcuk5L7SPYIye40QzCH03M9QnJENLnuue3wnNtwPaLx0vB4h5/r9Y9v8lw2uxSuu5LXd+Y1DSnrO/MufYHwC0CHdCj4kqQFCxZo3rx5Gj9+vE477TQtXrxY5eXlmj9/viRp7ty56tWrlxYtWiRJmj59uh577DGNGTNGEyZM0I4dO3T33Xdr+vTpDQEY0Fk+2V6korIa9YyL1JmD0qwuBwAAAJ3JVWeGWeWFZqBVVth4vfyQ18cC8zzD3bHn3/FB19R9PBoCNmeTEK2lQK3pOU0CNb/Bnfdr+Anu2hXidTQEjGg51KMzD0AX6HDwddlll+nQoUO65557lJ+fr9GjR2vlypUNA+/37dvn0+H161//WjabTb/+9a914MABpaWlafr06XrggQc677MAPJauOyBJmj4qW05Hl4+wAwAAwIly1UkVRWZYVXaoeZBVVtB4veKw/IciLbFJsT2lyDjp2N62Tx9ztZTUR3LXSu46yeX56Pd6rRnU+D2n1vy86q+76zy3/Vx31/mvxe15jbrKDny+QcLWQveb22X+/bfI05n376elQedJ8RlSVEL4dscBaBebEQTrDUtKSpSUlKTi4mIlJiZaXQ4CVGlVrcb/9p+qrnPrrZsma1SfZKtLAgAACE+u2iYdWIV+urQ8IVfFEXUozLLZzTArLl2K91zi0jwf06X4NDMQiUs3z3N4ApXFw83lcn5fy2bOkPrFxu7vJDKMVsI1P4FaQ9jmL1Br6/EdeS6v4M7lOdfv9bpWXqe26//8nHFSQoaUkGX+vSdk+b8dlUhABoSQjuREHe74AgLVyk35qq5za2BanEb2TrK6HAAAgNBSV2OGWPWhVVmBb4DlHXJVHunYc9vsUmyqGVTEpzUGWD7hludjbM+Oh1N2hzkY/Y25kmzyDb88Yci0h6xZPmezmR1PDqfkjOn+1+9KhmEuN22pE65piPb9N9J7C9p+3sReUlWJVFMq1ZZLR3aZl9Y4Y1sPxupvRycRkAEhhuALIWOZZ5nj7DG9ZOM/KwAAgLbVVTfOxPIXYDXcVyhVHevYc9scnk4srwCrvjMrPsO3Sys2petDp5wZ5mB0v7sFPsTA9K5gs5n/Dtr7d5s5Qvr0kfZ35tWUS6X55qUsv/G6z+0CqbpYqq2Qju42L62JiPEfjMVnSglel+hkAjIgSBB8ISTkFVfqi12HJUkXjWY3RwAAYBG3S9r7uRkWxWdI/SZ1fxdRbZWfTizvLi2vpYdVxR17bnuEGVi1tLTQO+SKSZHsATZzNWeGORjd6r8j+NfRzrzIOKnnSealNTUVrQRjXreris2Zakf3mJfWRER7gjFPENY0GKu/HdODgAywGMEXQsJbuQdlGNJp/VPUJyXW6nIAAEA42ryihW6ih0+8m6i2sn3zssoOmd0tHWF3NunMyvDfpRWXbr6JD7Qwq6PsDmnAmVZXgZZ0RWdeZKyUMtC8tKa2so0OsgKpNM/sfqyrMjdMaGvTBEeU/44xn9tZBGRAFyL4QtAzDEPL1prLHGeNpdsLAABYYPMKT5dKk+VZJXnm8UtfaP6GvaaiHfOyPEsQa0o7Vo/d6RVatRBk1S835A03Ao1VnXnOGCllgHlpTW2lWVd9EFbm+dj0duVRyVUtHdtnXlrjiPSEYRnNgzHv64HYSQkEOIIvBL3NeSXaVlCqSIddF4zIsrocAAAQbtwuszvF70wiz7FlN0jrX5MqihpDrZqyjr2OI7J9OxnGpzF/CMEvkDvznDFSj/7mpTW1VZ7wuoVgrP525RHJVSMV7zMvrbE7fZdYtrTMMrZn9wRkgbC8G2gDwReCXn2317lD05UU47S4GgAAEFYMQ9rwuu+SLH9qy6Vt7zY/7ohq306GcWnsNgcEG2e01KOfeWlNXXVjB1lrs8gqiswdMEu+Ny+tsUc0BmStLbOMTT3+gKwrl3cDnYjgC0HN5Tb01nrzG+2sMSxzBAAAXcztlgo3S3s/k/Z8anY6VBS177Gj50iDzvPt0opKJMwCwl1ElJTc17y0pq7GXArdEIw17R7zBGXlhyR3nVRywLy0xh5hfj9qKRir39kyLtW3k+t4lncDFiH4QlD7bEeRDpVWq0esU2cPTre6HAAAEGpcdVL+BjPo2vu5eak65nuO3Wl2YbRl1BWBu3QLQOCLiJSSepuX1rhqzSXVTTvGmnaT1QdkpQfNS2tsjsbZgPGZ0p5/qeXl3TZp5R3mnDaWPSIAEHwhqC1bZ/4G48cjsxUZwZBHAABwgupqpIPrpL2ebq59/24+WN4ZJ/WdIPWbbF6yRklPjjM7Hfy+EbSZy3/6TeqOzwBAuHM4paRe5qU1rjrfDrKWllmWH5IMl6ezLK8dBRhmp9kH95jhV89BZscY3a2wCMEXglZ5dZ1WbsqXxG6OAADgONVWSt9/7eno+kza/5VUV+l7TlSS1G+ib9DlaPJj9LSHPct+bPINvzxv9KY9ROcDgMDiiDBD+cTs1s9z1Xl2mfUEYVvflda92Pbzf/GkeZHMGYWpp5ghWKrn0nOQlDLQ7GQDuhDBF4LW37/NV2WtS/17xmpMn2SrywEAAMGgukza/2/PssXPpAPfmLupeYvtaXZn9TvD/JgxrO3QKmeGOdPG76Dnh5h1AyB4OSKkxCzzIkmR8e0LvrLHmjMQj+2Xqoql778yL95sDnP4v3cgVh+Q0SWGTkLwhaBVv8xx5phesvENEQAA+FN5TNr3ZWNH18Fcc8mOt/hMqf/kxrArbfDxvdnKmWEu69n7uTlwOj7DfE46vQCEkn6TzFC/reXdP/2n+f2vtlI6sksq+k4q2mF+PLzdvF5Tat53ZJe0/e++T+PTJXZy43W6xNBBBF8ISoUlVfpsh7mDErs5AgCABuWHvQbRfyrlb1KzN2ZJfb2Crsnmm6jO+iWa3cEAewChze7o2PJuZ4zZOZsxzPd5DMNcOnl4u1TkudRfP7avlS4xu9Sjf/Nlk6mn0CUGvwi+EJTeyj0otyGN69dD/XrGWV0OAACwSmm+GXTt8XR0Hdra/JyUkzxBlyfsSu7b/XUCQCjpjOXdNlvjEsoBP/C9r2mX2OHtjdfb6hKrD8FST268njJAiog68c8bQYngC0FpqdcyRwAAEEaO7TO7ufZ4dl08srP5OWlDzYCrPuxKyOz+OgEg1HXl8u7WusTKCjwhWAtdYge+Ni/e6BILawRfCDpb80u0Ja9ETodNPx6RZXU5AACgqxiG+Rv9ho6uz6XifU1OskmZI8yAq/9kqe9E800MAKDrdffybpvN/GVGQmYrXWJNArGi7XSJhTmCLwSd+qH25wxOV484hhoCABAyDMNcqugddJXl+55jc0jZYzwdXWdIfSZIMcmWlAsACCDt7RI7vKPxeltdYsn9PIHYIKnnyY3X49LoEgsiBF8IKi63obfWmWvIZ49lmSMAAEHN7ZIKvm3ccXHv51LFYd9zHJFSr/GNSxd7nyZFxVtTLwAg+JxIl9jR3ealxS4x72WT9TtO0iUWaAi+EFS+3HVY+SVVSoyO0DlD0q0uBwAAdISrVsrbYO62uPdzae8XUnWx7zkRMVKfU6V+Z5hhV+/x5m/xAQDobG12iXmG6h/e0XidLrGgQ/CFoLJ0rbnM8cKR2YqK6IShiQAAoOvUVUsH1jYGXfv+LdWW+54TmSD1neDZcXGyuYwxglEGAAAL+XSJNZlhVltlbqzStEOsrS6xqCSvDjGvQIwusS5H8IWgUVnj0spNeZJY5ggAQECqqZC+/6px2eL3X0l1Vb7nRCebnVz9JpsfM0dKDn4kBQAECWf08XWJVbfVJeYZqt/z5MbrndEl5nZ1zc6bQYSfMhA0/rE5X+U1LvVJidH4fj2sLgcAAFSXml1c9TO6DqyV3LW+58SleYIuz9LF9BzJbremXgAAukqbXWK7PIHYdqloR2M4Vl3i1SX2D9/HnWiX2OYV0srbpZKDjccSs6VpD0s5M078cw4SBF8IGvW7Oc4a3Us21kYDAND9Ko+ac7nqg6689ZLh9j0nIdscQl8fdqUOYqYJACC8OaOljBzz4s27S8xn2WQHusS8h+x7d4ltXiG9MVeS4fvYkjzz+KUvhE34RfCFoHCotFqfbC+SJM0cwzJHAAC6RdmhxmWLez8zd2Bs+gN0cj+p/xmNyxd79CfoAgCgPbqqS6znSdKhLWr2f7bkOWaTVt4hDbkwLJY9EnwhKLy9/qBcbkOj+iRrYBpbmAMA0CVKDpoh155PzaCr6Lvm5/Qc5Ono8nR1JfXu/joBAAh1rXaJFXoFYtsbO8aO7jW7xA6ubePJDankgPl/ftPALQQRfCEo1C9znE23FwAAncMwpGN7PUGXZ+ni0d3Nz0sfZgZc9WFXfHr31woAAEw2m5SQYV5a6hJb94L05ZK2n6usoGtqDDAEXwh4OwpLtfFAsSLsNk0flW11OQAABCfDMJdH7P3ME3R9LpV873uOzW7usthvshl09Z0oxaZYUy8AAOiY+i6xwRe2L/iKz+j6mgIAwRcC3tK1ZrfX2YPTlBIXaXE1AABYpKPbkbvd0qGtnqDrU/Ox5YW+59gjpOyxno6uM6Q+p0nRSV37eQAAgK7Vb5K5e2NJnvzP+bKZ9/eb1N2VWYLgCwHN7Tb0Vq659SpD7QEAYas925G7XVL+Rq9h9J9LlUd8n8cRJfUe39jR1ftUKTKu+z4PAADQ9ewO82eEN+ZKssk3/PJsQDPtobAYbC8RfCHArdlzRAeOVSohKkJThoZHGyYAAD5a3Y78amnkpVLlMWnfl+YuT96csWYXVz/Prou9xpnLIAAAQGjLmSFd+kILvzh7qPEXZ2GA4AsBbZlnmeMFI7IU7QyPNBoAgAZul/kDa4vbkUva8EbjoahEqe/pnh0XJ0vZoyWHsxsKBQAAASdnhjTkwo6NSghBBF8IWFW1Lr23MU+SNGssyxwBAGHCVWtuS17wrfTd+76/pW3JqddJY66SMkeE3Q+zAACgFXZH890fwwzBFwLWP7cUqLS6Tr2SY3Raf3aUAgCEoLJDUsFGM+Qq+FYq2CQd2ia5ajr2PH1PN7u7AAAA4IPgCwFr+TpzmeNFo7Nlt9ssrgYAgBNQVyMVfWcGWwWbpPxNZtDVdJfFepEJUsYwKSZF+u69tp8/TLYjBwAA6CiCLwSkw2XVWr3tkCRpNsscAQDBwjDMGRoFmxq7uPI3SUXbJHednwfYpJSBZsiVOcL8mDFMSu4n2WzmjK/Fw9mOHAAA4DgRfCEgvbMhT3VuQyN6Jenk9ASrywEAoLnaKjPQqg+36sOuiiL/50cleQKu4Z6Aa7iUPlSKjGv5NdiOHAAA4IQQfCEgLfUsc5w1hm4vAIDFDEMqzfMEXF7zuIq+kwxX8/NtdqnnyY3dWxnDzUtSb7OLq6PYjhwAAOC4EXwh4Ow6VKb1+4/JYbdp+qhsq8sBAIST2krp0NbGGVz1M7kqj/o/PzrZa4mip5MrbYgUGdu5dbEdOQAAwHEh+ELAqR9qf+agVKUlRFlcDQAgJBmGVHLAd4liwSbp8A7JcDc/3+aQUgf5dnBlDDO7ro6ni+t4sB05AABAhxF8IaAYhqFluSxzBAB0oppyqXCrb8BVsEmqKvZ/fkyKZw6X17D5tCGSM7p76wYAAMAJI/hCQPl671HtP1KpuEiHzsvJtLocAEAwMQzp2D7fcKvgW+nwTvndEdEeIaUObgy3Mj2dXPEZ3dfFBQAAgC5F8IWAsnSt2e01bXiWYiKZWwIAaEF1mVS4RSrwGjZf8K1UXeL//Li0xuWJ9TO5Uk+RIlhSDwAAEMoIvhAwqutceneDuVvV7LEscwQASHK7pWN7fZcp5m+Sju72f77daS5LbOjg8szkik/v3roBAAAQEAi+EDA+2lqokqo6ZSZG6/SBPa0uBwDQ3apKpMLNjSFX/ibzdk2Z//PjM32XKNZ3cTmc3Vs3AAAAAhbBFwJG/TLHi8Zky2FntgoAhCy32+zY8g64CjaZnV3+OCLNLq5Mr2HzGcOluNTurRsAAABBh+ALAeFoeY0+2lYoSZo9prfF1QAAOk3lMU8X17dSvmceV+FmqbbC//mJvXzDrYzhUs+TJQc/sgAAAKDj+CkSAeHdjXmqdRkampWowZkJVpcDAOHF7ZL2fi6VFZg7GvabJNk7uMGI2yUd2dUYbtVfivf5Pz8iWkof6gm4vDq5YlNO/PMBAAAAPAi+EBCWrTOXOc4ew1B7AOhWm1dIK2+XSg42HkvMlqY9LOXM8P+YiiNe4ZZnmWLhVqmu0v/5SX28Org8H3ue1PFwDQAAAOgggi9Ybu/hcn2z96jsNumi0dlWlwMA4WPzCumNuZIM3+MleebxS54zu7K8d1Qs+FYqOeD/+Zyxni4ur2HzGcOkmOSu/kwAAAAAvwi+YLn6bq/JJ6cqPTHa4moAIEy4XWanV9PQS2o89uY1LT8+ua/vEsXMEVKP/nRxAQAAIKAQfMFShmE0BF+zWOYIAN1n7+e+yxtb4oiWskZ6wi1PJ1f6UCk6qetrBAAAAE4QwRcstW7/Me09XKEYp0NTh2VaXQ4AhIfKo9L6V9t37kVPSCMv7dp6AAAAgC5C8AVLLVtrdntNG56puCj+OQJAl3G7pb2fSmtflLaskOqq2ve4hKyurQsAAADoQiQNsExNnVtvbzCX2bDMEQC6SEmetP4VM/A6urvxeFqOOaS+ukT+53zZzN0d+03qrkoBAACATkfwBcus3laoYxW1SkuI0qSTelpdDgCEDlettP0fZti1/R+S4TKPRyZIIy6Wxs6VssdKW9727Opok2/4ZTM/THuIYfUAAAAIagRfsMzyXHOZ40WjshXhsFtcDQCEgMM7pXUvSrmvSGUFjcf7nG6GXcNmSpFxjcdzZkiXvmDu7ug96D4x2wy9cmZ0W+kAAABAVyD4giWKK2v1zy2FkqRZY1nmCADHrabCnNm19kVzhle92FRp9BXSmLlS2iktPz5nhjTkQnOXx7ICKT7DXN5IpxcAAABCwHG12Tz11FPq37+/oqOjNWHCBK1Zs6bV848dO6abbrpJWVlZioqK0imnnKL33nvvuApGaHhvY55q6twanJGgnKxEq8sBgOBzMFd695fSo0OkZf/PDL1sdunkH0mXvigt2CKd99vWQ696doc04ExpxCXmR0IvAAAAhIgOd3y9/vrrWrBggZ5++mlNmDBBixcv1tSpU7Vt2zalp6c3O7+mpkY/+tGPlJ6erjfffFO9evXS3r17lZyc3Bn1I0jV7+Y4a2wv2Ww2i6sBgCBReVTa+Ka09gUpf0Pj8aS+0tirpdFXSkm9rasPAAAACDAdDr4ee+wxXXfddZo/f74k6emnn9a7776rZ599VnfccUez85999lkdOXJEn3/+uZxOpySpf//+rb5GdXW1qqurG26XlJR0tEwEsP1HKrRmzxHZbNJFo7OtLgcAApthSHs+NWd3bX5LqqsyjzsipSE/Nmd3DThLsjMrEQAAAGiqQz8l19TU6JtvvtGUKVMan8Bu15QpU/TFF1/4fcyKFSs0ceJE3XTTTcrIyNDw4cP14IMPyuVytfg6ixYtUlJSUsOlT58+HSkTAe4tz1D7iQN7KispxuJqACBAleRJnzwqPTFW+r8fSxteN0Ov9Bxz8Pwvt0n/8Zx00jmEXgAAAEALOtTxVVRUJJfLpYyMDJ/jGRkZ2rp1q9/H7Nq1Sx9++KHmzJmj9957Tzt27NDPfvYz1dbWauHChX4fc+edd2rBggUNt0tKSgi/QoRhGFq6zrPMcQxD7QHAh6tO2v4Pcynj9n9IhueXRJHx5vytMXOlXmMllogDAAAA7dLluzq63W6lp6frz3/+sxwOh8aNG6cDBw7o97//fYvBV1RUlKKiorq6NFhgw/fF2nWoXNFOu6YNz7S6HAAIDId3mksZc18xd1as1+d0c3ZXzkwpKt6y8gAAAIBg1aHgKzU1VQ6HQwUFBT7HCwoKlJnpP8TIysqS0+mUw9G4Q9TQoUOVn5+vmpoaRUZGHkfZCFbLPN1eP8rJVEK00+JqAMBCtZXS5hVmd9feTxuPx6ZKo6+QxlwtpQ22rj4AAAAgBHQo+IqMjNS4ceO0atUqzZw5U5LZ0bVq1SrdfPPNfh8zefJkvfLKK3K73bJ7ZpB89913ysrKIvQKM7Uut95ef1CSNJtljgDCVd56M+za8Feputhz0CadPMXs7jrlfCmC/x8BAACAztDhpY4LFizQvHnzNH78eJ122mlavHixysvLG3Z5nDt3rnr16qVFixZJkm688UY9+eSTuvXWW3XLLbdo+/btevDBB/Xzn/+8cz8TBLxPth/S4fIapcZH6sxBqVaXAwDdp/KYtPGvZuCVv6HxeFJfM+wafaWU1Nuy8gAAAIBQ1eHg67LLLtOhQ4d0zz33KD8/X6NHj9bKlSsbBt7v27evobNLkvr06aO///3vuu222zRy5Ej16tVLt956q26//fbO+ywQFJauNZc5Th+VrQgHO5ABCHGGIe39zAy7Nr9l7sgoSY5IaciPzcBrwNnsyAgAAAB0IZthGIbVRbSlpKRESUlJKi4uVmJiotXl4DiUVNXq1N/+U9V1br198xka0TvJ6pIAoGuU5ptD6te9KB3Z1Xg8PUcaO1caeZkUm2JdfQAAAECQ60hO1OW7OgKStHJTvqrr3DopLU7DexFeAggxrjppxwdmd9d3f5cMl3k8Ml4afrEZePUaJ9ls1tYJAAAAhBmCL3SLZZ5ljrPH9paNN34AQsXhndK6l8wOr7L8xuN9JphhV85MKSresvIAAACAcEfwhS538Filvtx9WJJ00ehsi6sBgBNUWyltedvs7trzSePx2J7SqCvMwCttsHX1AQAAAGhA8IUutzz3gAxDmjAgRb17xFpdDgAcn7z10toXpY1vSFXFnoM26eRzzbDrlPOliEhLSwQAAADgi+ALXcowjIZljrPG9LK4GgDooMpj0qY3ze6uvPWNx5P6SmOukkZfKSX3saw8AAAAAK0j+EKX+vZgibYXlikywq7zR2RZXQ4AtM0wpL2fmd1dm5dLdVXmcUekNORCs7trwNmS3W5hkQAAAADag+ALXWrZOrPb60dDM5QU47S4GgBoRWmBtP4VM/A6srPxeNpQM+waeZkU19O6+gAAAAB0GMEXukydy623cg9KYpkjgADlqpN2fGCGXd+tlAyXeTwyXho+Wxo7T+o1TmI3WgAAACAoEXyhy3y6o0hFZdXqEevUWYPTrC4HABod3imte0nKfUUqy2883meCNOZqadgsKSreuvoAAAAAdAqCL3SZ5Z5ljtNHZcvpYBYOAIvVVkpb3jYH1e/5pPF4bE9p1BVm4JU+xLr6AAAAAHQ6gi90ifLqOv392wJJLHMEYLG8DWbYtfENqarYc9AmnXyuGXYNvkCKiLS0RAAAAABdg+ALXWLlpnxV1ro0IDVOo/skW10OgHBTeUza9KYZeOWtbzye1Ecac5U0eo6U3Mey8gAAAAB0D4IvdIn63RxnjeklG0OhAXQHw5D2fm6GXZvfkuoqzeN2pzT0x2Z318CzJbvD0jIBAAAAdB+CL3S6/OIqfbazSJI0czTLHAF0sdICaf0r5s6MR3Y2Hk8bKo29Whp5uRTX07r6AAAAAFiG4AudbsX6AzIMaXy/HurbM9bqcgCEIledtOOfZnfXdyslw2Ued8ZJIy6WxsyVeo+X6DgFAAAAwhrBFzrd0rWeZY5j6fYC0MmO7JLWvSTlviKV5jUe732a2d01bJYUlWBdfQAAAAACCsEXOtWWvBJtzS9VpMOuH4/ItrocAKGgtkra8ra09v+kPZ80Ho/tKY26wpzdlT7EuvoAAAAABCyCL3Sq+qH2PxySrqRYp8XVAAhq+RvNpYwbXpeqij0HbdJJP5TGzpUGXyBFRFpaIgAAAIDARvCFTuNyG3or1wy+Zo5hmSOA41BVLG180wy88nIbjyf1kcZcJY2eIyX3saw8AAAAAMGF4Aud5oudh1VQUq2kGKfOGZJmdTkAAoXbJe39XCorkOIzpH6TJLuj8X7DkPZ9YYZd3y6X6irN43anNORCs7tr4Nm+jwEAAACAdiD4QqdZuu57SdKPR2YpKoI3qAAkbV4hrbxdKjnYeCwxW5r2sNRngrT+VWndi9LhHY33pw0xw66Rl0lxqd1fMwAAAICQQfCFTlFRU6eVm/IlSbPZzRGAZIZeb8yVZPgeLzkovXG1JLskt3nMGScNny2NnSf1Hi/ZbN1cLAAAAIBQRPCFTvHB5gJV1LjUNyVWY/v2sLocAFZzu8xOr6ahl+9JUq/x0rh50rBZUlRCd1UHAAAAIEwQfKFTLF3bONTeRqcGgL2f+y5vbMmUe6UBZ3Z5OQAAAADCk93qAhD8Ckur9Mn2Q5KkWezmCEAyB9l35nkAAAAAcBwIvnDCVuQelNuQxvRN1oDUOKvLARAIYts5lD4+o2vrAAAAABDWWOqIE7ZsnbnMcTbdXgAkqbZS+nJJGyfZzN0d+03qlpIAAAAAhCeCL5yQ7wpK9e3BEkXYbbpwZLbV5QCwWuUx6dXLpX1fSHan5K6VZJPvkHvPHMBpD0l2R/fXCAAAACBssNQRJ6S+2+vswelKiYu0uBoAlirNl567wAy9opKkeSukS1+UErN8z0vMli59QcqZYU2dAAAAAMIGHV84bm63obfqlzmOZZkjENYO75RenCkd22fO7bpqqZQ53LxvyIXmLo9lBeZ9/SbR6QUAAACgWxB84bh9ufuwDhZXKSE6Qj8ckm51OQCscjBXevkSqfyQlDJQunqZ1KN/4/12hzTgTKuqAwAAABDGCL5w3JZ7ur0uHJGlaCfdG0BY2vWx9NocqaZUyhwpXfU3KZ4gHAAAAEBgIPjCcamqden9jfmSpFns5giEp81vSX/7qeSqkfqfKV3+ihSdaHVVAAAAANCA4AvH5YPNBSqtrlOv5Bid2j/F6nIAdLevn5XeWSDJkIZOl2b/r+SMtroqAAAAAPBB8IXjUr+b46wxvWS32yyuBkC3MQzpX49IH/3WvD1uvnThowyrBwAAABCQCL7QYUVl1fr4u0OSpJkscwTCh9strbxDWvM/5u0f/Jd0zl2SjfAbAAAAQGAi+EKHvbP+oFxuQyN7J+nk9HirywHQHepqpOU3SpveNG+f/ztpwv+ztiYAAAAAaAPBFzrMe5kjgDBQXSa9cbW080PJHiHN+h9pxCVWVwUAAAAAbSL4QofsPFSm9d8Xy2G3afqobKvLAdDVyg9Lr/yHdOAbyRkrXfaidPIUq6sCAAAAgHYh+EKHLFtrdnuddUqaUuOjLK4GQJc6tl96cZZ0eLsU00Oa86bUe7zVVQEAAABAuxF8od3cbkPLc83gi6H2QIgr3Cq9NFsqOSAl9pauXiqlDba6KgAAAADoEIIvtNvXe4/q+6OVio+K0Hk5GVaXA6Cr7P/KXN5YeVRKHWyGXkm9ra4KAAAAADqM4Avttmzd95Kk84dnKtrpsLgaAF1i+z/NQfa1FVKv8dKcv0qxKVZXBQAAAADHheAL7VJV69I7G/IkSbPGsswRCEkb/iotv0Fy10knnWsOso+Ms7oqAAAAADhudqsLQHD4aGuhSqvqlJUUrdMH9LS6HACd7csl0tKfmqHXiP+QrniN0AsAAABA0KPjC+2ydJ051P6i0b1kt9ssrgZApzEM6cP7pU8eNW9PuEGaukiy83sRAAAAAMGP4AttOlpeo9XbCiVJs1nmCIQOV5307m3S2hfM2z+8Wzrzl5KNcBsAAABAaCD4Qpve2XBQtS5Dw7ITdUpGgtXlAOgMtVXS334ibX1HstmlHy+Wxs2zuioAAAAA6FQEX2hT/TLHWWPo9gJCQlWx9OqV0t5PJUeUdMlfpKHTra4KAAAAADodwRdataeoXOv2HZPdJs0YlW11OQBOVFmh9NJsKX+jFJkgXfGqNOBMq6sCAAAAgC5B8IVWLfN0e50xKE3pidEWVwPghBzZLb04Szq6W4pLk676m5Q1yuqqAAAAAKDLEHyhRYZhaHmuGXzNZpkjENzyNkgvXSyVF0rJ/aSrl0k9T7K6KgAAAADoUgRfaNHafUe193CFYiMdOm9YhtXlADheez6VXr1Cqi6RMkZIV70pJWRaXRUAAAAAdDmCL7SofpnjtGGZio3knwoQlLa+K/11vuSqlvpNli5/RYpJtroqAAAAAOgWpBnwq6bOrXc25EmSZo1lmSMQlNa+KL39c8lwS4MvNHdvdMZYXRUAAAAAdBu71QUgMH20rVDHKmqVkRilSSelWl0OgI4wDOnTP0grbjZDrzFXSZe+QOgFAAAAIOzQ8QW/lq01lzleNLqXHHabxdUAaDe3W/rHr6UvnzJvn3GbdO5CycbXMQAAAIDwc1wdX0899ZT69++v6OhoTZgwQWvWrGnX41577TXZbDbNnDnzeF4W3aS4olYfbi2UJM1iN0cgeLhqpeU3NIZe5z0gTbmX0AsAAABA2Opw8PX6669rwYIFWrhwodauXatRo0Zp6tSpKiwsbPVxe/bs0a9+9SudeeaZx10suse7G/NU43JrSGaChmYlWl0OgPaoKTd3btzwumSPkGb9jzTpZqurAgAAAABLdTj4euyxx3Tddddp/vz5ysnJ0dNPP63Y2Fg9++yzLT7G5XJpzpw5uu+++zRw4MATKhhdb9m67yXR7QUEjYoj0gszpR0fSBEx0uWvSqMut7oqAAAAALBch4KvmpoaffPNN5oyZUrjE9jtmjJlir744osWH/eb3/xG6enp+slPftKu16murlZJSYnPBd1j/5EKfbXnqGw2c74XgABXfEB67nzp+zVSdLI0b4V0ynlWVwUAAAAAAaFDwVdRUZFcLpcyMjJ8jmdkZCg/P9/vYz799FP95S9/0TPPPNPu11m0aJGSkpIaLn369OlImTgBy9aZQ+0nn5SqzKRoi6sB0Kqi7dKzU6VDW6WEbOnalVKf06yuCgAAAAACxnENt2+v0tJSXX311XrmmWeUmpra7sfdeeedKi4ubrjs37+/C6tEPcMwtNwTfM1kmSMQ2A58Y4ZexfulnidLP/m7lD7U6qoAAAAAIKBEdOTk1NRUORwOFRQU+BwvKChQZmZms/N37typPXv2aPr06Q3H3G63+cIREdq2bZtOOumkZo+LiopSVFRUR0pDJ1j/fbF2FZUr2mnXtOHN/z4BBIidH0qvXSXVlkvZY6Q5b0px7f/lAgAAAACEiw51fEVGRmrcuHFatWpVwzG3261Vq1Zp4sSJzc4fMmSINm7cqNzc3IbLjBkzdM455yg3N5cljAFm2VpzqP3UYZmKj+pQJgqgu2z6m/TypWboNfBsad7bhF4AAAAA0IIOpxsLFizQvHnzNH78eJ122mlavHixysvLNX/+fEnS3Llz1atXLy1atEjR0dEaPny4z+OTk5MlqdlxWKvW5dbbG/IksZsjELDWPCO995+SDGnYbGnW01IE3bEAAAAA0JIOB1+XXXaZDh06pHvuuUf5+fkaPXq0Vq5c2TDwft++fbLbu3R0GLrAx9sO6Uh5jVLjo3TGyXSPAAHFMKTVi6SPHzZvn3qddP7Dkt1hbV0AAAAAEOBshmEYVhfRlpKSEiUlJam4uFiJiYlWlxOSbnplrd7dkKdrJw/QPdNzrC4HQD23y+zy+vov5u2z75LO+i/JZrO2LgAAAACwSEdyIgY5QSVVtfpgs7lhweyxLHMEAkZdtbT0emnzckk26cJHpFN/anVVAAAAABA0CL6g9zfmqabOrUHp8RqWTUcdEBCqS6XX5ki7P5bsTuniZ6Rhs6yuCgAAAACCCsEXtHTtAUnSrLG9ZGP5FGC9skPSy5dIeblSZLx02UvSSedYXRUAAAAABB2CrzB34Fil/r37iCTpotEscwQsd3Sv9OIs6chOKTZVmvNXqddYq6sCAAAAgKBE8BXmlq8zu71OH5iiXskxFlcDhLmCb6UXZ0tl+VJSX+nqZVLqyVZXBQAAAABBi+ArjBmGoWWe4Gv2mN4WVwOEuX1fSq9cKlUVS+k50lVLpcQsq6sCAAAAgKBG8BXGNh0o0Y7CMkVF2HX+iEyrywHC17aV0l/nSXVVUp/TpStfk2J6WF0VAAAAAAQ9gq8wVt/t9aOcDCVEOy2uBghTua9Ib90sGS5p0FTpP56XImOtrgoAAAAAQoLd6gJgjTqXWyvWH5QkzRrDUHvAEp89Li2/0Qy9Rl0hXf4yoRcAAAAAdCI6vsLUJzuKVFRWrZ5xkfrBKWlWlwOEF8OQPrhH+vxx8/akW6Qpv5Hs/C4CAAAAADoTwVeYWrbWXOY4fVS2nA7ebAPdxlUnvf1zKfdl8/aPfiNNvtXamgAAAAAgRBF8haGy6jr9Y3O+JJY5At2qtlL663zpu/clm0Oa8bg05iqrqwIAAACAkEXwFYZWbspXVa1bA1PjNLJ3ktXlAOGh8pj06uXSvi+kiGjpkuekIRdYXRUAAAAAhDSCrzC0bN33ksxuL5vNZnE1QBgoyZNeulgq/FaKSpKufE3qN8nqqgAAAAAg5BF8hZm84kp9vvOwJGkmyxyBrnd4p/TiTOnYPik+Q7pqqZQ53OqqAAAAACAsEHyFmbdyD8owpNP6p6hPSqzV5QCh7WCu2elVUSSlDJSuXib16G91VQAAAAAQNgi+wszydeZujrPG0u0FdKldH0uvzZFqSqWsUdKcv0nxaVZXBQAAAABhheArjGw+WKKt+aWKdNh1wfAsq8sBQtfmt6S//VRy1Uj9z5Quf0WKTrS6KgAAAAAIO3arC0D3qR9qf+7QdCXFOi2uBghRXz8rvTHPDL2GzpDmvEnoBQAAAAAWoeMrTLjcht7KPSjJ3M0RQCczDOlfj0gf/da8PW6+dOGjkt1hbV0AAAAAEMYIvsLEZzuKVFhareRYp84enG51OUBocbullbdLa/5s3v7Bf0nn3CXZbNbWBQAAAABhjuArTNQPtf/xyCxFRrDCFeg0dTXS8hukTX+TZJPOf1ia8P+srgoAAAAAIIKvsFBRU6eV3+ZLkmaN6W1xNUAIqS6T3rha2vmhZHdKs56WRlxidVUAAAAAAA+CrzDw92/zVVHjUv+esRrbN9nqcoDQUH5YeuU/pAPfSM446bIXpZPPtboqAAAAAIAXgq8wsHStucxx5phesjFzCDhxx/ZLL86SDm+XYlKkOX+Veo+3uioAAAAAQBMEXyGusKRKn+0oksRujkCnKNwqvTRbKjkgJfaWrl4qpQ22uioAAAAAgB8EXyFuxfqDchvS2L7J6tczzupygOC2f430yqVS5VEpdbAZeiUxNw8AAAAAAhXBV4irX+Y4ayxvzoETsv0D6Y25Um2F1Gu8ubwxNsXqqgAAAAAArSD4CmHb8ku1Oa9ETodNPx6RZXU5QPDa8Ia0/EbJXSedPEW69AUpkg5KAAAAAAh0dqsLQNdZts7s9jpncLp6xEVaXA0QpL5cIi29zgy9RlwqXfEaoRcAAAAABAk6vkKU223orVzPMkeG2gMdZxjSh/dLnzxq3p5wozT1QcnO7wsAAAAAIFgQfIWoL3cdVl5xlRKjI/TDoelWlwMEF1ed9O5t0toXzNvn3iOdsUCy2aytCwAAAADQIQRfIWqpZ5njhSOzFRXhsLgaIIjUVkl/+4m09R3JZpd+vFgaN8/qqgAAAAAAx4HgKwRV1rj0/sY8SdLssSxzBNqtqlh69Upp76eSI0q65C/S0OlWVwUAAAAAOE4EXyHogy0FKq9xqXePGI3r28PqcoDgUFogvXSxVLBRikqULn9FGnCm1VUBAAAAAE4AwVcIWrb2e0nmUHu7nZlEQJuO7JJenCUd3SPFpUtX/U3KGml1VQAAAACAE0TwFWIOlVbrX9uLJLGbI9AueRvMTq/yQqlHf+nqZVLKQKurAgAAAAB0AoKvEPP2+oNyuQ2N6pOsgWnxVpcDBLY9n0qvXiFVl0gZI6Sr3pQSMq2uCgAAAADQSQi+QszyXHM3x9l0ewGt2/KO9Oa1kqta6jfZnOkVk2x1VQAAAACATkTwFUJ2FJZpw/fFirDb9OORWVaXAwSutS9Ib98qGW5p8IXm7o3OGKurAgAAAAB0MrvVBaDzLFtnDrU/65Q09YyPsrgaIAAZhvTJY9KKW8zQa8xV0qUvEHoBAAAAQIii4ytEuN2Glq87KEmaNZZljkAzbrf0j19LXz5l3j5jgXTuPZKNnU8BAAAAIFQRfIWINXuO6MCxSiVERWjK0AyrywECi6tWeusmacPr5u2pD0oTb7K2JgAAAABAlyP4ChHL15lD7c8fkalop8PiaoAAUlMuvTFP2vGBZI+QLvqTNOoyq6sCAAAAAHQDgq8QUFXr0rsb8yRJs8b0trgaIIBUHJFeuVT6/ispIsac53XKeVZXBQAAAADoJgRfIWDVlkKVVtWpV3KMJgxIsbocIDAUH5Bemi0d2ipFJ0tz/ir1Oc3qqgAAAAAA3YjgKwTU7+Z40ehs2e0M6gZ06DvpxVlSyfdSQrZ09VIpfajVVQEAAAAAuhnBV5A7Ul6j1dsOSZJmjWE3R0DffyO9fIlUeUTqOUi6epmU3MfqqgAAAAAAFiD4CnLvbDioOreh4b0SNSgjwepygO7ldkl7P5fKCqT4DKmuyhxkX1suZY+V5rwpxfW0ukoAAAAAgEUIvoLc0rXmbo4MtUfY2bxCWnm7VHKw+X0Dz5Eue0mKiu/+ugAAAAAAAYPgK4jtOlSm3P3H5LDbNGNUttXlAN1n8wrpjbmSDP/3j72a0AsAAAAAILvVBeD4LV9ndnudOShVaQlRFlcDdBO3S3r/drUYeskm/eNu8zwAAAAAQFij4ytIGYahZbn1yxwZao8QZBhSeZFU9J10eLtU5Lnkb5BK81p7oFRywJz9NeDMbisXAAAAABB4CL6C1Dd7j2r/kUrFRTp0Xk6m1eUAx6+uWjqy2xNufScV7Wi8XlV8/M9bVtB5NQIAAAAAghLBV5Ba6lnmOG14lmIiHRZXA7Shpe6tw9ulo3skw93CA21Sch8p9RSp5yAp9WQzKPv7XW2/ZnxGZ34GAAAAAIAgRPAVhKrrXHp3g7nUa/ZYljkigNR3bzUEXDsar7fWvRWZYIZaPQeZIVfqyebHlIGSM8b3XLdL+uJJqSRP/ud82aTEbKnfpM78zAAAAAAAQYjgKwh9tPWQiitrlZkYrdMH9rS6HISbE+re6iulDvIEXIMarydkSjZb+17f7pCmPezZ1dEm3/DL8xzTHjLPAwAAAACENYKvILRs3feSpItGZ8thb2dYAHTUiXZveS9PbKl763jlzJAufUFaebtUcrDxeGK2GXrlzOic1wEAAAAABLXjCr6eeuop/f73v1d+fr5GjRqlJ554Qqeddprfc5955hm98MIL2rRpkyRp3LhxevDBB1s8H607VlGjD7cWSpJmscwRJ8owpPJDjR1b9d1bRd9Jx/Z2vHsr9RRztlZ7u7dORM4MaciF5u6NZQXm6/abRKcXAAAAAKBBh4Ov119/XQsWLNDTTz+tCRMmaPHixZo6daq2bdum9PT0ZuevXr1aV1xxhSZNmqTo6Gg9/PDDOu+88/Ttt9+qVy+Cm456Z0Oeal2GhmYlakhmotXlIFg0697yWp7YZvfWoOYBV2d2b50Iu0MacKbVVQAAAAAAApTNMAx/06FbNGHCBJ166ql68sknJUlut1t9+vTRLbfcojvuuKPNx7tcLvXo0UNPPvmk5s6d267XLCkpUVJSkoqLi5WYGN5hzyVLPtfXe4/qvy8Yqut+MNDqchBIvLu3ir6TDu/oePdW6ilSz5O7v3sLAAAAAIB26khO1KGOr5qaGn3zzTe68847G47Z7XZNmTJFX3zxRbueo6KiQrW1tUpJSWnxnOrqalVXVzfcLikp6UiZIWvf4Qp9vfeo7DZpxuhsq8uBVeqqpSO7mi9PDPbuLQAAAAAAOlmHgq+ioiK5XC5lZGT4HM/IyNDWrVvb9Ry33367srOzNWXKlBbPWbRoke67776OlBYWlq07IEmafHKqMhKjLa4GXcpv99Z35u12dW+d4gm4Tm68TvcWAAAAACDMdOuujg899JBee+01rV69WtHRLQc3d955pxYsWNBwu6SkRH369OmOEgOWYRgNuznOGsNstJDh3b3lszxxu1TdSvdWVKLXkkSvDq6UkyQnoSgAAAAAAFIHg6/U1FQ5HA4VFBT4HC8oKFBmZmarj33kkUf00EMP6Z///KdGjhzZ6rlRUVGKiorqSGkhb93+Y9pzuEIxToemDmv9zxqdwO3qvN0CG7q3PB1bdG8BAAAAANAtOhR8RUZGaty4cVq1apVmzpwpyRxuv2rVKt18880tPu53v/udHnjgAf3973/X+PHjT6jgcLXcs8xx2vBMxUV1a6Ne+Nm8Qlp5u1RysPFYYrY07WEpZ0bLjzvh7q1TpNSTPd1bp3hmb9G9BQAAAADA8epwgrJgwQLNmzdP48eP12mnnabFixervLxc8+fPlyTNnTtXvXr10qJFiyRJDz/8sO655x698sor6t+/v/Lz8yVJ8fHxio+P78RPJXTV1Ln19nozhJnJMseutXmF9MZcSU02Oy3JM49f+n9S34mNHVveA+bb6t7q0c93qHz9dbq3AAAAAADoEh0Ovi677DIdOnRI99xzj/Lz8zV69GitXLmyYeD9vn37ZLfbG85fsmSJampqdMkll/g8z8KFC3XvvfeeWPVh4uPvDuloRa3SEqI0+aSeVpcTutwus9OraeglNR57Y14L93s07d5KPcUMuOjeAgAAAACg2x3Xmrmbb765xaWNq1ev9rm9Z8+e43kJeKkfan/RqGxFOOxtnI0Ocbul4v1S4RZp23u+yxv98oRePfo3Lkn0Xp4Yn073FgAAAAAAAYJhUQGuuLJW/9xSKEmaNZZljsfNMKSyQqlwsxly1X88tFWqKevYc81cIo2+smvqBAAAAAAAnYbgK8C9vzFPNXVunZIRr5ysRKvLCQ6VR6XCrV4hlyfoqjzi/3xHpNmtFdtT2v1x28+f1Kdz6wUAAAAAAF2C4CvALfXs5jhrTG/ZWELnq6ZcOrTNt4OrcItU2sJyRZvdnLWVPlRKz2n8mDJQcjjNGV+Lh5uD7P3O8bKZuzv2m9SVnxUAAAAAAOgkBF8BbP+RCq3ZfUQ2mzRzTLbV5VinrkY6vKN5B9fRPWpx0HxSH0+w5RVypZ4iOWNafh27Q5r2sGdXR1uT5/aEjtMeMs8DAAAAAAABj+ArgL2Va3Z7TRzYU1lJrQQ2ocLtMsMs73CrcIt0eLvkrvP/mLi05h1caYOl6KTjqyFnhnTpC+bujt6D7hOzzdArZ8bxPS8AAAAAAOh2BF8ByjAMLWtY5hhiQ+0NwwyVfJYobjaXLdZV+n9MVGLzDq60oVJ8WufXlzNDGnKhtPdzqaxAis8wlzfS6QUAAAAAQFAh+ApQGw8Ua+ehckVF2DVteKbV5Ry/8sPNd1Is3CJVF/s/PyLa7Njy7uBKHyol9pK6c8aZ3SENOLP7Xg8AAAAAAHQ6gq8AtXSt2e113rBMJUQ7La6mHapLm+yk6PlYXuj/fJtDSh3UfJlij/50VgEAAAAAgE5B8BWAal1uvb3enC81O9CWOdZWSUXfNe/gKt7X8mN69G/ewdXzZCkiqtvKBgAAAAAA4YfgKwB9ur1Ih8tr1DMuUmcOSrWmCFeddGRX8w6uIzslw+3/MQlZTTq4hkqpg6Wo+O6tHQAAAAAAQARfAWmpZ6j99FHZinDYu/bF3G6peH/zDq6ibZKrxv9jopOljGG+w+bThkixKV1bKwAAAAAAQAcQfAWY0qpa/ePbfEnS7LGduMzRMKSywuYdXIe2SjVl/h/jjJPShzSfwxWf0b2D5gEAAAAAAI4DwVeAeX9Tvqrr3DopLU4jeiUd35NUHm0yaN4TdFUe8X++I1JKPcW3gyt9qJTUV7J3cccZAAAAAABAFyH4soLbJe39XCorMLun+k1q2MlwuWeZ4+yxvWVrq6uqplw6tK35MsXSg/7Pt9mllIHNO7hSBkqOINg5EgAAAAAAoAMIvrrb5hXSytulEq9wKjFbmvaw8nr9SF/sOixJmjEqu/H+uhrp8I7mHVxH90gy/L9OUp/mHVypp0jOmC771AAAAAAAAAIJwVd32rxCemOumoVVJXnSG3O1Iech9ZVTP846pj4bNzcGXYe3S+46/88Zl9a8gyttsBR9nMskAQBByTAM1dXVyeVyWV0KgE7gcDgUERHR9goAAADQKpthGC20DAWOkpISJSUlqbi4WImJiVaXc3zcLmnxcN9OLy/1fwkt/mgTldi8gyttqBSf1hXVAgCCSE1NjfLy8lRRUWF1KQA6UWxsrLKyshQZGWl1KQAABJSO5ER0fHWXvZ+3GHpJjYFXjeGQI3OYHJnDfEOuxF7spAgAaMbtdmv37t1yOBzKzs5WZGQkHSJAkDMMQzU1NTp06JB2796tQYMGyc6GQwAAHBeCr+5SVtCu017OvF3zb7y9i4sBAISKmpoaud1u9enTR7GxsVaXA6CTxMTEyOl0au/evaqpqVF0dLTVJQEAEJT41VF3ic9o12kjhg7t4kIAAKGIbhAg9PB1DQDAieN/0+7Sb5K5e2MLU7zchpSvnho56fzurQsAAAAAACBEEXx1F7tDmvaw54Zv+FU/2P6jAQsUGens1rIAAAAAAABCFcFXd8qZIV36gpSY5XM43+ipG2t/oSHnzLGoMAAAJJfb0Bc7D+ut3AP6YudhudyBsfHz888/r+Tk5DbPs9lsWr58eZfXA5m7Ve/+RNr4pvnR7erSlzv77LP1i1/8oktfozXXXHONZs6cGTD1AACA9mO4fXfLmSENudDc5bGsQP/Ks+uaD53ql5qg0X2Sra4OABCmVm7K031vb1ZecVXDsaykaC2cnqNpw7NaeWTXu+yyy3TBBRc03L733nu1fPly5ebmWldUONu8Qlp5u+9u1YnZZmd7zgzr6upGS5culdNJlz4AAMGAji8r2B3SgDOlEZfomf295JZdM0f3Yvt5AIAlVm7K040vrfUJvSQpv7hKN760Vis35VlUmSkmJkbp6emW1nC8amtrrS6hc21eIb0x1zf0kqSSPPP45hXW1NXNUlJSlJCQYHUZAACgHQi+LFRQUqXPdhRJkmaN6WVxNQCAUGEYhipq6tp1Ka2q1cIV38rfosb6Y/eu2KzSqtp2PZ9htG955DvvvKPk5GS5XOYSudzcXNlsNt1xxx0N5/z0pz/VVVdd5bPU8fnnn9d9992n9evXy2azyWaz6fnnn294TFFRkWbNmqXY2FgNGjRIK1a0L4hZvXq1bDabVq1apfHjxys2NlaTJk3Stm3bfM5bsmSJTjrpJEVGRmrw4MF68cUXfe632WxasmSJZsyYobi4OD3wwAO69957NXr0aD377LPq27ev4uPj9bOf/Uwul0u/+93vlJmZqfT0dD3wwAPtqrVTGYZUU96+S1WJ9P5/Sa39a1l5u3lee56vnf9W6tXV1enmm29WUlKSUlNTdffddzf8e3vxxRc1fvx4JSQkKDMzU1deeaUKCwsbHnv06FHNmTNHaWlpiomJ0aBBg/Tcc8813L9//35deumlSk5OVkpKii666CLt2bOnxVqaLnXs37+/HnzwQV177bVKSEhQ37599ec//9nnMR19DQAA0DlY6miht3IPyG1I4/v1UN+esVaXAwAIEZW1LuXc8/dOeS5DUn5JlUbc+492nb/5N1MVG9n2jxdnnnmmSktLtW7dOo0fP14ff/yxUlNTtXr16oZzPv74Y91+++0+j7vsssu0adMmrVy5Uv/85z8lSUlJSQ3333ffffrd736n3//+93riiSc0Z84c7d27VykpKe2q/7//+7/16KOPKi0tTTfccIOuvfZaffbZZ5KkZcuW6dZbb9XixYs1ZcoUvfPOO5o/f7569+6tc845p+E57r33Xj300ENavHixIiIi9Oyzz2rnzp16//33tXLlSu3cuVOXXHKJdu3apVNOOUUff/yxPv/8c1177bWaMmWKJkyY0K5aO0VthfRgdic9mWF2gj3Up32n33VQioxr97P/3//9n37yk59ozZo1+vrrr3X99derb9++uu6661RbW6v7779fgwcPVmFhoRYsWKBrrrlG7733niTp7rvv1ubNm/X+++8rNTVVO3bsUGVlpSSzK2/q1KmaOHGiPvnkE0VEROi3v/2tpk2bpg0bNigyMrJd9T366KO6//77ddddd+nNN9/UjTfeqLPOOkuDBw/utNcAAAAdR/BlAZfb0JrdR/T853slSReN6awfOAEACA5JSUkaPXq0Vq9erfHjx2v16tW67bbbdN9996msrEzFxcXasWOHzjrrrIbgSTKXPcbHxysiIkKZmZnNnveaa67RFVdcIUl68MEH9fjjj2vNmjWaNm1au+p64IEHdNZZZ0mS7rjjDl144YWqqqpSdHS0HnnkEV1zzTX62c9+JklasGCBvvzySz3yyCM+wdeVV16p+fPn+zyv2+3Ws88+q4SEBOXk5Oicc87Rtm3b9N5778lut2vw4MF6+OGH9dFHH3Vv8BVE+vTpoz/84Q+y2WwaPHiwNm7cqD/84Q+67rrrdO211zacN3DgQD3++OM69dRTVVZWpvj4eO3bt09jxozR+PHjJZkdWvVef/11ud1u/e///m/D2InnnntOycnJWr16tc4777x21XfBBRc0/Nu4/fbb9Yc//EEfffSRBg8e3GmvAQAAOo7gq5v5Gx785Ic7lBYfZfnwYABAaIhxOrT5N1Pbde6a3Ud0zXNftXne8/NP1WkD2u6ainE62vW6knTWWWdp9erV+uUvf6lPPvlEixYt0htvvKFPP/1UR44cUXZ2tgYNGuQTfLVl5MiRDdfj4uKUmJjos+StI4/PyjL/Xy4sLFTfvn21ZcsWXX/99T7nT548WX/84x99jtWHK9769+/vMxMqIyNDDodDdrvd51hHau0Uzliz86o99n4uvXxJ2+fNeVPqN6l9r90Bp59+us881IkTJ+rRRx+Vy+VSbm6u7r33Xq1fv15Hjx6V2+2WJO3bt085OTm68cYbdfHFF2vt2rU677zzNHPmTE2aZNa4fv167dixo9nMrqqqKu3cubPd9Xn/27HZbMrMzGz4++ys1wAAAB1H8NWN6ocHN51oUVhSrRtfWqslV40l/AIAnDCbzdau5YaSdOagNGUlRSu/uMrv5CabpMykaJ05KE0Oe+duwnL22Wfr2Wef1fr16+V0OjVkyBCdffbZWr16tY4ePdrQedURTXfas9lsDSFIRx9fH7J05PGSGbi1p64TrbVT2GztX2540g/N3RtL8uR/zpfNvP+kH5ob+XSTqqoqTZ06VVOnTtXLL7+stLQ07du3T1OnTlVNTY0k6fzzz9fevXv13nvv6YMPPtC5556rm266SY888ojKyso0btw4vfzyy82eOy0trd11tPb32VmvAQAAOo7h9t3E5TZ039ubWx0efN/bm+Vyd2zQKwAAJ8Jht2nh9BxJZsjlrf72wuk5nR56SY1zvv7whz80hFz1wdfq1at19tln+31cZGRkw1D87jR06NBm3WefffaZcnJyur0WS9gd0rSHPTda+Ncy7aEuC73+/e9/+9z+8ssvNWjQIG3dulWHDx/WQw89pDPPPFNDhgzx2zmXlpamefPm6aWXXtLixYsbhs+PHTtW27dvV3p6uk4++WSfi/f8uBPRHa8BAAD8I/jqJmt2H2m2Tbw3Q1JecZXW7D7SfUUBACBp2vAsLblqrDKTon2OZyZFd2k3co8ePTRy5Ei9/PLLDSHXD37wA61du1bfffddix1f/fv31+7du5Wbm6uioiJVV1d3SX1N/ed//qeef/55LVmyRNu3b9djjz2mpUuX6le/+lW3vH5AyJkhXfqClNjk30Ritnk8Z0aXvfS+ffu0YMECbdu2Ta+++qqeeOIJ3Xrrrerbt68iIyP1xBNPaNeuXVqxYoXuv/9+n8fec889euutt7Rjxw59++23eueddzR06FBJ0pw5c5SamqqLLrpIn3zyiXbv3q3Vq1fr5z//ub7//vtOqb07XgMAAPjHUsduUljacuh1POcBANCZpg3P0o9yMrVm9xEVllYpPSFapw1I6ZJOL29nnXWWcnNzG4KvlJQU5eTkqKCgQIMHD/b7mIsvvlhLly7VOeeco2PHjum5557TNddc06V1StLMmTP1xz/+UY888ohuvfVWDRgwQM8991yLnWkhK2eGNORCc+ZXWYEUn2HO9Ori5Y1z585VZWWlTjvtNDkcDt166626/vrrZbPZ9Pzzz+uuu+7S448/rrFjx+qRRx7RjBmNIVxkZKTuvPNO7dmzRzExMTrzzDP12muvSZJiY2P1r3/9S7fffrtmz56t0tJS9erVS+eee64SExM7pfbueA0AAOCfzTCMgF9bV1JSoqSkJBUXFwftDwdf7DysK575ss3zXr3udE08qWc3VAQACAVVVVXavXu3BgwYoOjo6LYfACBo8PUNAIB/HcmJWOrYTU4bkKKspOhmEzHq2SRlJUW3a8csAAAAAAAAtI3gq5tYOTwYAIBwd8MNNyg+Pt7v5YYbbrC6PAAAAHQRZnx1o/rhwfe9vdln0H1mUrQWTs/psuHBAACEu9/85jctDqEP1jEKAAAAaBvBVzezangwAADhLD09Xenp6VaXAQAAgG5G8GUBh93GAHsAQKcKgr1qAHQQX9cAAJw4ZnwBABDEnE6nJKmiosLiSgB0tvqv6/qvcwAA0HF0fAEAEMQcDoeSk5NVWFgoSYqNjZXNxvJ5IJgZhqGKigoVFhYqOTlZDofD6pIAAAhaBF8AAAS5zMxMSWoIvwCEhuTk5IavbwAAcHwIvgAACHI2m01ZWVlKT09XbW2t1eUA6AROp5NOLwAAOgHBFwAAIcLhcPBGGQAAAPDCcHsAAAAAAACEJIIvAAAAAAAAhCSCLwAAAAAAAISkoJjxZRiGJKmkpMTiSgAAAAAAAGCl+nyoPi9qTVAEX6WlpZKkPn36WFwJAAAAAAAAAkFpaamSkpJaPcdmtCces5jb7dbBgweVkJAgm81mdTmdoqSkRH369NH+/fuVmJhodTlA0OJrCegcfC0BnYevJ6Bz8LUEdI5Q/FoyDEOlpaXKzs6W3d76FK+g6Piy2+3q3bu31WV0icTExJD5hwdYia8loHPwtQR0Hr6egM7B1xLQOULta6mtTq96DLcHAAAAAABASCL4AgAAAAAAQEgi+LJIVFSUFi5cqKioKKtLAYIaX0tA5+BrCeg8fD0BnYOvJaBzhPvXUlAMtwcAAAAAAAA6io4vAAAAAAAAhCSCLwAAAAAAAIQkgi8AAAAAAACEJIIvAAAAAAAAhCSCLws89dRT6t+/v6KjozVhwgStWbPG6pKAoLNo0SKdeuqpSkhIUHp6umbOnKlt27ZZXRYQ9B566CHZbDb94he/sLoUIOgcOHBAV111lXr27KmYmBiNGDFCX3/9tdVlAUHH5XLp7rvv1oABAxQTE6OTTjpJ999/v9iXDWjdv/71L02fPl3Z2dmy2Wxavny5z/2GYeiee+5RVlaWYmJiNGXKFG3fvt2aYrsRwVc3e/3117VgwQItXLhQa9eu1ahRozR16lQVFhZaXRoQVD7++GPddNNN+vLLL/XBBx+otrZW5513nsrLy60uDQhaX331lf7nf/5HI0eOtLoUIOgcPXpUkydPltPp1Pvvv6/Nmzfr0UcfVY8ePawuDQg6Dz/8sJYsWaInn3xSW7Zs0cMPP6zf/e53euKJJ6wuDQho5eXlGjVqlJ566im/9//ud7/T448/rqefflr//ve/FRcXp6lTp6qqqqqbK+1eNoPYvFtNmDBBp556qp588klJktvtVp8+fXTLLbfojjvusLg6IHgdOnRI6enp+vjjj/WDH/zA6nKAoFNWVqaxY8fqT3/6k377299q9OjRWrx4sdVlAUHjjjvu0GeffaZPPvnE6lKAoPfjH/9YGRkZ+stf/tJw7OKLL1ZMTIxeeuklCysDgofNZtOyZcs0c+ZMSWa3V3Z2tn75y1/qV7/6lSSpuLhYGRkZev7553X55ZdbWG3XouOrG9XU1Oibb77RlClTGo7Z7XZNmTJFX3zxhYWVAcGvuLhYkpSSkmJxJUBwuummm3ThhRf6/B8FoP1WrFih8ePH6z/+4z+Unp6uMWPG6JlnnrG6LCAoTZo0SatWrdJ3330nSVq/fr0+/fRTnX/++RZXBgSv3bt3Kz8/3+dnvaSkJE2YMCHk84gIqwsIJ0VFRXK5XMrIyPA5npGRoa1bt1pUFRD83G63fvGLX2jy5MkaPny41eUAQee1117T2rVr9dVXX1ldChC0du3apSVLlmjBggW666679NVXX+nnP/+5IiMjNW/ePKvLA4LKHXfcoZKSEg0ZMkQOh0Mul0sPPPCA5syZY3VpQNDKz8+XJL95RP19oYrgC0DQu+mmm7Rp0yZ9+umnVpcCBJ39+/fr1ltv1QcffKDo6GirywGCltvt1vjx4/Xggw9KksaMGaNNmzbp6aefJvgCOuiNN97Qyy+/rFdeeUXDhg1Tbm6ufvGLXyg7O5uvJwAdxlLHbpSamiqHw6GCggKf4wUFBcrMzLSoKiC43XzzzXrnnXf00UcfqXfv3laXAwSdb775RoWFhRo7dqwiIiIUERGhjz/+WI8//rgiIiLkcrmsLhEICllZWcrJyfE5NnToUO3bt8+iioDg9Z//+Z+64447dPnll2vEiBG6+uqrddttt2nRokVWlwYErfrMIRzzCIKvbhQZGalx48Zp1apVDcfcbrdWrVqliRMnWlgZEHwMw9DNN9+sZcuW6cMPP9SAAQOsLgkISueee642btyo3Nzchsv48eM1Z84c5ebmyuFwWF0iEBQmT56sbdu2+Rz77rvv1K9fP4sqAoJXRUWF7Hbft6oOh0Nut9uiioDgN2DAAGVmZvrkESUlJfr3v/8d8nkESx272YIFCzRv3jyNHz9ep512mhYvXqzy8nLNnz/f6tKAoHLTTTfplVde0VtvvaWEhISGdelJSUmKiYmxuDogeCQkJDSbjRcXF6eePXsyMw/ogNtuu02TJk3Sgw8+qEsvvVRr1qzRn//8Z/35z3+2ujQg6EyfPl0PPPCA+vbtq2HDhmndunV67LHHdO2111pdGhDQysrKtGPHjobbu3fvVm5urlJSUtS3b1/94he/0G9/+1sNGjRIAwYM0N13363s7OyGnR9Dlc0wDMPqIsLNk08+qd///vfKz8/X6NGj9fjjj2vChAlWlwUEFZvN5vf4c889p2uuuaZ7iwFCzNlnn63Ro0dr8eLFVpcCBJV33nlHd955p7Zv364BAwZowYIFuu6666wuCwg6paWluvvuu7Vs2TIVFhYqOztbV1xxhe655x5FRkZaXR4QsFavXq1zzjmn2fF58+bp+eefl2EYWrhwof785z/r2LFjOuOMM/SnP/1Jp5xyigXVdh+CLwAAAAAAAIQkZnwBAAAAAAAgJBF8AQAAAAAAICQRfAEAAAAAACAkEXwBAAAAAAAgJBF8AQAAAAAAICQRfAEAAAAAACAkEXwBAAAAAAAgJBF8AQAAAAAAICQRfAEAAIQ4m82m5cuXW10GAABAtyP4AgAA6ELXXHONbDZbs8u0adOsLg0AACDkRVhdAAAAQKibNm2annvuOZ9jUVFRFlUDAAAQPuj4AgAA6GJRUVHKzMz0ufTo0UOSuQxxyZIlOv/88xUTE6OBAwfqzTff9Hn8xo0b9cMf/lAxMTHq2bOnrr/+epWVlfmc8+yzz2rYsGGKiopSVlaWbr75Zp/7i4qKNGvWLMXGxmrQoEFasWJF137SAAAAAYDgCwAAwGJ33323Lr74Yq1fv15z5szR5Zdfri1btkiSysvLNXXqVPXo0UNfffWV/vrXv+qf//ynT7C1ZMkS3XTTTbr++uu1ceNGrVixQieffLLPa9x333269NJLtWHDBl1wwQWaM2eOjhw50q2fJwAAQHezGYZhWF0EAABAqLrmmmv00ksvKTo62uf4XXfdpbvuuks2m0033HCDlixZ0nDf6aefrrFjx+pPf/qTnnnmGd1+++3av3+/4uLiJEnvvfeepk+froMHDyojI0O9evXS/Pnz9dvf/tZvDTabTb/+9a91//33SzLDtPj4eL3//vvMGgMAACGNGV8AAABd7JxzzvEJtiQpJSWl4frEiRN97ps4caJyc3MlSVu2bNGoUaMaQi9Jmjx5stxut7Zt2yabzaaDBw/q3HPPbbWGkSNHNlyPi4tTYmKiCgsLj/dTAgAACAoEXwAAAF0sLi6u2dLDzhITE9Ou85xOp89tm80mt9vdFSUBAAAEDGZ8AQAAWOzLL79sdnvo0KGSpKFDh2r9+vUqLy9vuP+zzz6T3W7X4MGDlZCQoP79+2vVqlXdWjMAAEAwoOMLAACgi1VXVys/P9/nWEREhFJTUyVJf/3rXzV+/HidccYZevnll7VmzRr95S9/kSTNmTNHCxcu1Lx583Tvvffq0KFDuuWWW3T11VcrIyNDknTvvffqhhtuUHp6us4//3yVlpbqs88+0y233NK9nygAAECAIfgCAADoYitXrlRWVpbPscGDB2vr1q2SzB0XX3vtNf3sZz9TVlaWXn31VeXk5EiSYmNj9fe//1233nqrTj31VMXGxuriiy/WY4891vBc8+bNU1VVlf7whz/oV7/6lVJTU3XJJZd03ycIAAAQoNjVEQAAwEI2m03Lli3TzJkzrS4FAAAg5DDjCwAAAAAAACGJ4AsAAAAAAAAhiRlfAAAAFmLqBAAAQNeh4wsAAAAAAAAhieALAAAAAAAAIYngCwAAAAAAACGJ4AsAAAAAAAAhieALAAAAAAAAIYngCwAAAAAAACGJ4AsAAAAAAAAhieALAAAAAAAAIen/A2yuC7jGrJ2XAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_history(title, label, baseline, bn_solvers, plot_fn, bl_marker='.', bn_marker='.', labels=None):\n",
    "    \"\"\"utility function for plotting training history\"\"\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(label)\n",
    "    bn_plots = [plot_fn(bn_solver) for bn_solver in bn_solvers]\n",
    "    bl_plot = plot_fn(baseline)\n",
    "    num_bn = len(bn_plots)\n",
    "    for i in range(num_bn):\n",
    "        label='with_norm'\n",
    "        if labels is not None:\n",
    "            label += str(labels[i])\n",
    "        plt.plot(bn_plots[i], bn_marker, label=label)\n",
    "    label='baseline'\n",
    "    if labels is not None:\n",
    "        label += str(labels[0])\n",
    "    plt.plot(bl_plot, bl_marker, label=label)\n",
    "    plt.legend(loc='lower center', ncol=num_bn+1) \n",
    "\n",
    "    \n",
    "plt.subplot(3, 1, 1)\n",
    "plot_training_history('Training loss','Iteration', solver, [bn_solver], \\\n",
    "                      lambda x: x.loss_history, bl_marker='o', bn_marker='o')\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_training_history('Training accuracy','Epoch', solver, [bn_solver], \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_training_history('Validation accuracy','Epoch', solver, [bn_solver], \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование нормализации позволяет увеличить скорость обучения (скорость сходимости увеличилась на графиках)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите 6-тислойную сеть с батч-нормализацией и без нее, используя разные размеры батча. Визуализируйте графики обучения. Сделайте выводы по результатам эксперимента. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No normalization: batch size =  5\n",
      "Normalization: batch size =  5\n",
      "Normalization: batch size =  10\n",
      "Normalization: batch size =  50\n"
     ]
    }
   ],
   "source": [
    "def run_batchsize_experiments(normalization_mode):\n",
    "    np.random.seed(231)\n",
    "    # Try training a very deep net with batchnorm\n",
    "    hidden_dims = [100, 100, 100, 100, 100]\n",
    "    num_train = 1000\n",
    "    small_data = {\n",
    "      'X_train': data['X_train'][:num_train],\n",
    "      'y_train': data['y_train'][:num_train],\n",
    "      'X_val': data['X_val'],\n",
    "      'y_val': data['y_val'],\n",
    "    }\n",
    "    n_epochs=10\n",
    "    weight_scale = 2e-2\n",
    "    batch_sizes = [5,10,50]\n",
    "    lr = 10**(-3.5)\n",
    "    solver_bsize = batch_sizes[0]\n",
    "\n",
    "    print('No normalization: batch size = ',solver_bsize)\n",
    "    model = FullyConnectedNet(hidden_dims, input_dim=8*8, weight_scale=weight_scale, normalization=None)\n",
    "    solver = Solver(model, small_data,\n",
    "                    num_epochs=n_epochs, batch_size=solver_bsize,\n",
    "                    update_rule='adam',\n",
    "                    optim_config={\n",
    "                      'learning_rate': lr,\n",
    "                    },\n",
    "                    verbose=False)\n",
    "    solver.train()\n",
    "    \n",
    "    bn_solvers = []\n",
    "    for i in range(len(batch_sizes)):\n",
    "        b_size=batch_sizes[i]\n",
    "        print('Normalization: batch size = ',b_size)\n",
    "        bn_model = FullyConnectedNet(hidden_dims, input_dim=8*8, weight_scale=weight_scale, normalization=normalization_mode)\n",
    "        bn_solver = Solver(bn_model, small_data,\n",
    "                        num_epochs=n_epochs, batch_size=b_size,\n",
    "                        update_rule='adam',\n",
    "                        optim_config={\n",
    "                          'learning_rate': lr,\n",
    "                        },\n",
    "                        verbose=False)\n",
    "        bn_solver.train()\n",
    "        bn_solvers.append(bn_solver)\n",
    "        \n",
    "    return bn_solvers, solver, batch_sizes\n",
    "\n",
    "batch_sizes = [5,10,50]\n",
    "bn_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Training accuracy (Batch Normalization)','Epoch', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Validation accuracy (Batch Normalization)','Epoch', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "большие батчи дают более точные оценки градиентов и улучшают процесс обучения. \n",
    "Глядя на графики, можно сделать вывод, что чем больше батч, тем лучше точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте прямой проход для dropout-слоя в scripts/layers.py\n",
    "\n",
    "http://cs231n.github.io/neural-networks-2/#reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests with p =  0.25\n",
      "Mean of input:  10.000207878477502\n",
      "Mean of train-time output:  10.014059116977283\n",
      "Mean of test-time output:  10.000207878477502\n",
      "Fraction of train-time output set to zero:  0.749784\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.4\n",
      "Mean of input:  10.000207878477502\n",
      "Mean of train-time output:  9.977917658761159\n",
      "Mean of test-time output:  10.000207878477502\n",
      "Fraction of train-time output set to zero:  0.600796\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.7\n",
      "Mean of input:  10.000207878477502\n",
      "Mean of train-time output:  9.987811912159426\n",
      "Mean of test-time output:  10.000207878477502\n",
      "Fraction of train-time output set to zero:  0.30074\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(500, 500) + 10\n",
    "\n",
    "for p in [0.25, 0.4, 0.7]:\n",
    "  out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n",
    "  out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n",
    "\n",
    "  print('Running tests with p = ', p)\n",
    "  print('Mean of input: ', x.mean())\n",
    "  print('Mean of train-time output: ', out.mean())\n",
    "  print('Mean of test-time output: ', out_test.mean())\n",
    "  print('Fraction of train-time output set to zero: ', (out == 0).mean())\n",
    "  print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте обратный проход для dropout-слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx relative error:  5.44560814873387e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dropout_param = {'mode': 'train', 'p': 0.2, 'seed': 123}\n",
    "out, cache = dropout_forward(x, dropout_param)\n",
    "dx = dropout_backward(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n",
    "\n",
    "# Error should be around e-10 or less\n",
    "print('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте в реализацию класса FullyConnectedNet поддержку dropout. Если параметр dropout != 1, то добавьте в модель dropout-слой после каждого слоя активации. Проверьте свою реализацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with dropout =  1\n",
      "Initial loss:  2.3004790897684924\n",
      "W1 relative error: 1.48e-07\n",
      "W2 relative error: 2.21e-05\n",
      "W3 relative error: 3.53e-07\n",
      "b1 relative error: 5.38e-09\n",
      "b2 relative error: 2.09e-09\n",
      "b3 relative error: 5.80e-11\n",
      "\n",
      "Running check with dropout =  0.75\n",
      "Initial loss:  2.302371489704412\n",
      "W1 relative error: 1.90e-07\n",
      "W2 relative error: 4.76e-06\n",
      "W3 relative error: 2.60e-08\n",
      "b1 relative error: 4.73e-09\n",
      "b2 relative error: 1.82e-09\n",
      "b3 relative error: 1.70e-10\n",
      "\n",
      "Running check with dropout =  0.5\n",
      "Initial loss:  2.3042759220785896\n",
      "W1 relative error: 3.11e-07\n",
      "W2 relative error: 1.84e-08\n",
      "W3 relative error: 5.35e-08\n",
      "b1 relative error: 5.37e-09\n",
      "b2 relative error: 2.99e-09\n",
      "b3 relative error: 1.13e-10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for dropout in [1, 0.75, 0.5]:\n",
    "  print('Running check with dropout = ', dropout)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            weight_scale=5e-2, dtype=np.float64,\n",
    "                            dropout=dropout, seed=123)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Relative errors should be around e-6 or less; Note that it's fine\n",
    "  # if for dropout=1 you have W2 error be on the order of e-5.\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите две двухслойные сети с dropout-слоем (вероятность отсева 0,25) и без на наборе из 500 изображений. Визуализируйте графики обучения. Сделайте выводы по результатам эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(Iteration 1 / 125) loss: 2.308719\n",
      "(Epoch 0 / 25) train acc: 0.296000; val_acc: 0.319444\n",
      "(Epoch 1 / 25) train acc: 0.618000; val_acc: 0.580556\n",
      "(Epoch 2 / 25) train acc: 0.834000; val_acc: 0.786111\n",
      "(Epoch 3 / 25) train acc: 0.906000; val_acc: 0.875000\n",
      "(Epoch 4 / 25) train acc: 0.898000; val_acc: 0.869444\n",
      "(Epoch 5 / 25) train acc: 0.918000; val_acc: 0.872222\n",
      "(Epoch 6 / 25) train acc: 0.914000; val_acc: 0.891667\n",
      "(Epoch 7 / 25) train acc: 0.926000; val_acc: 0.891667\n",
      "(Epoch 8 / 25) train acc: 0.940000; val_acc: 0.925000\n",
      "(Epoch 9 / 25) train acc: 0.944000; val_acc: 0.911111\n",
      "(Epoch 10 / 25) train acc: 0.952000; val_acc: 0.919444\n",
      "(Epoch 11 / 25) train acc: 0.958000; val_acc: 0.925000\n",
      "(Epoch 12 / 25) train acc: 0.962000; val_acc: 0.927778\n",
      "(Epoch 13 / 25) train acc: 0.960000; val_acc: 0.936111\n",
      "(Epoch 14 / 25) train acc: 0.962000; val_acc: 0.938889\n",
      "(Epoch 15 / 25) train acc: 0.968000; val_acc: 0.936111\n",
      "(Epoch 16 / 25) train acc: 0.974000; val_acc: 0.944444\n",
      "(Epoch 17 / 25) train acc: 0.974000; val_acc: 0.950000\n",
      "(Epoch 18 / 25) train acc: 0.974000; val_acc: 0.950000\n",
      "(Epoch 19 / 25) train acc: 0.974000; val_acc: 0.947222\n",
      "(Epoch 20 / 25) train acc: 0.978000; val_acc: 0.950000\n",
      "(Iteration 101 / 125) loss: 0.087685\n",
      "(Epoch 21 / 25) train acc: 0.974000; val_acc: 0.938889\n",
      "(Epoch 22 / 25) train acc: 0.976000; val_acc: 0.955556\n",
      "(Epoch 23 / 25) train acc: 0.982000; val_acc: 0.950000\n",
      "(Epoch 24 / 25) train acc: 0.982000; val_acc: 0.941667\n",
      "(Epoch 25 / 25) train acc: 0.982000; val_acc: 0.952778\n",
      "\n",
      "0.25\n",
      "(Iteration 1 / 125) loss: 2.311590\n",
      "(Epoch 0 / 25) train acc: 0.244000; val_acc: 0.213889\n",
      "(Epoch 1 / 25) train acc: 0.558000; val_acc: 0.533333\n",
      "(Epoch 2 / 25) train acc: 0.814000; val_acc: 0.783333\n",
      "(Epoch 3 / 25) train acc: 0.870000; val_acc: 0.813889\n",
      "(Epoch 4 / 25) train acc: 0.890000; val_acc: 0.827778\n",
      "(Epoch 5 / 25) train acc: 0.912000; val_acc: 0.858333\n",
      "(Epoch 6 / 25) train acc: 0.900000; val_acc: 0.875000\n",
      "(Epoch 7 / 25) train acc: 0.902000; val_acc: 0.877778\n",
      "(Epoch 8 / 25) train acc: 0.918000; val_acc: 0.877778\n",
      "(Epoch 9 / 25) train acc: 0.918000; val_acc: 0.900000\n",
      "(Epoch 10 / 25) train acc: 0.926000; val_acc: 0.894444\n",
      "(Epoch 11 / 25) train acc: 0.934000; val_acc: 0.908333\n",
      "(Epoch 12 / 25) train acc: 0.938000; val_acc: 0.905556\n",
      "(Epoch 13 / 25) train acc: 0.946000; val_acc: 0.913889\n",
      "(Epoch 14 / 25) train acc: 0.944000; val_acc: 0.894444\n",
      "(Epoch 15 / 25) train acc: 0.950000; val_acc: 0.902778\n",
      "(Epoch 16 / 25) train acc: 0.952000; val_acc: 0.913889\n",
      "(Epoch 17 / 25) train acc: 0.958000; val_acc: 0.919444\n",
      "(Epoch 18 / 25) train acc: 0.956000; val_acc: 0.916667\n",
      "(Epoch 19 / 25) train acc: 0.958000; val_acc: 0.913889\n",
      "(Epoch 20 / 25) train acc: 0.956000; val_acc: 0.922222\n",
      "(Iteration 101 / 125) loss: 0.224968\n",
      "(Epoch 21 / 25) train acc: 0.964000; val_acc: 0.933333\n",
      "(Epoch 22 / 25) train acc: 0.966000; val_acc: 0.936111\n",
      "(Epoch 23 / 25) train acc: 0.966000; val_acc: 0.933333\n",
      "(Epoch 24 / 25) train acc: 0.970000; val_acc: 0.933333\n",
      "(Epoch 25 / 25) train acc: 0.972000; val_acc: 0.930556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train two identical nets, one with dropout and one without\n",
    "np.random.seed(231)\n",
    "num_train = 500\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "dropout_choices = [1, 0.25]\n",
    "for dropout in dropout_choices:\n",
    "  model = FullyConnectedNet([500], input_dim=8*8, dropout=dropout)\n",
    "  print(dropout)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=25, batch_size=100,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 5e-4,\n",
    "                  },\n",
    "                  verbose=True, print_every=100)\n",
    "  solver.train()\n",
    "  solvers[dropout] = solver\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train and validation accuracies of the two models\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for dropout in dropout_choices:\n",
    "  solver = solvers[dropout]\n",
    "  train_accs.append(solver.train_acc_history[-1])\n",
    "  val_accs.append(solver.val_acc_history[-1])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "for dropout in dropout_choices:\n",
    "  plt.plot(solvers[dropout].train_acc_history, 'o', label='%.2f dropout' % dropout)\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for dropout in dropout_choices:\n",
    "  plt.plot(solvers[dropout].val_acc_history, 'o', label='%.2f dropout' % dropout)\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты при dropout = 1 и при dropout = 0.25 почти не отличаются. Можно сказать, что величина dropout практически не влияет на точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточные нейронные сети (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте прямой проход для сверточного слоя - функция conv_forward_naive в scripts/layers.py юПроверьте свою реализацию, запустив код ниже "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  2.2121476417505994e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте обратный проход - функция conv_backward_naive в scripts/layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx error:  1.159803161159293e-08\n",
      "dw error:  2.2471264748452487e-10\n",
      "db error:  3.37264006649648e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around e-8 or less.\n",
    "print('Testing conv_backward_naive function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте прямой проход для max-pooling слоя -функция  max_pool_forward_naive в scripts/layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.1666665157267834e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be on the order of e-8.\n",
    "print('Testing max_pool_forward_naive function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте обратный проход для max-pooling слоя в max_pool_backward_naive . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27562514223145e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be on the order of e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В скрипте scripts/fast_layers.py представлены быстрые реализации слоев свертки и пуллинга, написанных с использованием  Cython. \n",
    "\n",
    "Для компиляции выполните следующую команду в директории scripts\n",
    "\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните ваши реализации слоев свертки и пуллинга с быстрыми реализациями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_fast:\n",
      "Naive: 6.447542s\n",
      "Fast: 0.016000s\n",
      "Speedup: 402.964551x\n",
      "Difference:  4.926407851494105e-11\n",
      "\n",
      "Testing conv_backward_fast:\n",
      "Naive: 9.399119s\n",
      "Fast: 0.011261s\n",
      "Speedup: 834.679829x\n",
      "dx difference:  1.949764775345631e-11\n",
      "dw difference:  4.957046344783224e-13\n",
      "db difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Rel errors should be around e-9 or less\n",
    "from scripts.fast_layers import conv_forward_fast, conv_backward_fast\n",
    "from time import time\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing conv_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting conv_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))\n",
    "print('dw difference: ', rel_error(dw_naive, dw_fast))\n",
    "print('db difference: ', rel_error(db_naive, db_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pool_forward_fast:\n",
      "Naive: 0.014003s\n",
      "fast: 0.046002s\n",
      "speedup: 0.304401x\n",
      "difference:  0.0\n",
      "\n",
      "Testing pool_backward_fast:\n",
      "Naive: 0.044999s\n",
      "fast: 0.016839s\n",
      "speedup: 2.672401x\n",
      "dx difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Relative errors should be close to 0.0\n",
    "from scripts.fast_layers import max_pool_forward_fast, max_pool_backward_fast\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing pool_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting pool_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В layer_utils.py вы можете найти  часто используемые комбинации слоев, используемых в сверточных сетях. Ознакомьтесь с ними и запустите код ниже для проверки их работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu_pool\n",
      "dx error:  9.591132621921372e-09\n",
      "dw error:  5.802401370096438e-09\n",
      "db error:  1.0146343411762047e-09\n"
     ]
    }
   ],
   "source": [
    "from scripts.layer_utils import conv_relu_pool_forward, conv_relu_pool_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "dx, dw, db = conv_relu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], b, dout)\n",
    "\n",
    "# Relative errors should be around e-8 or less\n",
    "print('Testing conv_relu_pool')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu:\n",
      "dx error:  1.5218619980349303e-09\n",
      "dw error:  2.702022646099404e-10\n",
      "db error:  1.451272393591721e-10\n"
     ]
    }
   ],
   "source": [
    "from scripts.layer_utils import conv_relu_forward, conv_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out, cache = conv_relu_forward(x, w, b, conv_param)\n",
    "dx, dw, db = conv_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "# Relative errors should be around e-8 or less\n",
    "print('Testing conv_relu:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите реализацию класса ThreeLayerConvNet в scripts/classifiers/cnn.py . Вы можете использовать готовые реализации слоев и их комбинаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте вашу реализацию. Ожидается, что значение функции потерь softmax будет порядка `log(C)` для `C` классов для случая без регуляризации. В случае регуляризации значение функции потерь должно немного возрасти. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.302586071243987\n",
      "Initial loss (with regularization):  2.508255638232932\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте реализацию обратного прохода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 1.380104e-04\n",
      "W2 max relative error: 1.822723e-02\n",
      "W3 max relative error: 3.064049e-04\n",
      "b1 max relative error: 3.477652e-05\n",
      "b2 max relative error: 2.516375e-03\n",
      "b3 max relative error: 7.945660e-10\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "np.random.seed(231)\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "# Errors should be small, but correct implementations may have\n",
    "# relative errors up to the order of e-2\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте добиться эффекта переобучения. Обучите модель на небольшом наборе данных.Сравните значения accuracy на обучающих данных и на валидационных. Визуализируйте графики обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 30) loss: 2.303193\n",
      "(Epoch 0 / 15) train acc: 0.100000; val_acc: 0.127778\n",
      "(Iteration 2 / 30) loss: 2.328368\n",
      "(Epoch 1 / 15) train acc: 0.200000; val_acc: 0.177778\n",
      "(Iteration 3 / 30) loss: 2.292526\n",
      "(Iteration 4 / 30) loss: 2.248104\n",
      "(Epoch 2 / 15) train acc: 0.250000; val_acc: 0.152778\n",
      "(Iteration 5 / 30) loss: 2.142559\n",
      "(Iteration 6 / 30) loss: 2.085752\n",
      "(Epoch 3 / 15) train acc: 0.270000; val_acc: 0.197222\n",
      "(Iteration 7 / 30) loss: 1.969358\n",
      "(Iteration 8 / 30) loss: 1.855714\n",
      "(Epoch 4 / 15) train acc: 0.550000; val_acc: 0.513889\n",
      "(Iteration 9 / 30) loss: 1.450224\n",
      "(Iteration 10 / 30) loss: 1.236353\n",
      "(Epoch 5 / 15) train acc: 0.620000; val_acc: 0.552778\n",
      "(Iteration 11 / 30) loss: 1.359073\n",
      "(Iteration 12 / 30) loss: 0.859892\n",
      "(Epoch 6 / 15) train acc: 0.850000; val_acc: 0.741667\n",
      "(Iteration 13 / 30) loss: 0.633245\n",
      "(Iteration 14 / 30) loss: 0.672284\n",
      "(Epoch 7 / 15) train acc: 0.840000; val_acc: 0.752778\n",
      "(Iteration 15 / 30) loss: 0.652955\n",
      "(Iteration 16 / 30) loss: 0.378358\n",
      "(Epoch 8 / 15) train acc: 0.870000; val_acc: 0.766667\n",
      "(Iteration 17 / 30) loss: 0.384548\n",
      "(Iteration 18 / 30) loss: 0.483852\n",
      "(Epoch 9 / 15) train acc: 0.890000; val_acc: 0.816667\n",
      "(Iteration 19 / 30) loss: 0.385071\n",
      "(Iteration 20 / 30) loss: 0.440068\n",
      "(Epoch 10 / 15) train acc: 0.900000; val_acc: 0.758333\n",
      "(Iteration 21 / 30) loss: 0.232413\n",
      "(Iteration 22 / 30) loss: 0.179088\n",
      "(Epoch 11 / 15) train acc: 0.940000; val_acc: 0.825000\n",
      "(Iteration 23 / 30) loss: 0.258084\n",
      "(Iteration 24 / 30) loss: 0.044056\n",
      "(Epoch 12 / 15) train acc: 0.950000; val_acc: 0.841667\n",
      "(Iteration 25 / 30) loss: 0.085943\n",
      "(Iteration 26 / 30) loss: 0.156428\n",
      "(Epoch 13 / 15) train acc: 0.970000; val_acc: 0.838889\n",
      "(Iteration 27 / 30) loss: 0.068564\n",
      "(Iteration 28 / 30) loss: 0.036110\n",
      "(Epoch 14 / 15) train acc: 0.990000; val_acc: 0.880556\n",
      "(Iteration 29 / 30) loss: 0.024825\n",
      "(Iteration 30 / 30) loss: 0.051928\n",
      "(Epoch 15 / 15) train acc: 1.000000; val_acc: 0.869444\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "    'X_train': data['X_train'][:num_train],\n",
    "    'y_train': data['y_train'][:num_train],\n",
    "    'X_val': data['X_val'],\n",
    "    'y_val': data['y_val']\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=15, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-2,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small data training accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Print final training accuracy\n",
    "print(\n",
    "    \"Small data training accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_train'], small_data['y_train'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small data validation accuracy: 0.8805555555555555\n"
     ]
    }
   ],
   "source": [
    "# Print final validation accuracy\n",
    "print(\n",
    "    \"Small data validation accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_val'], small_data['y_val'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите сеть на полном наборе данных. Выведите accuracy на обучающей и валидационной выборках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 21) loss: 2.302718\n",
      "(Epoch 0 / 1) train acc: 0.093000; val_acc: 0.125000\n",
      "(Iteration 6 / 21) loss: 2.309857\n",
      "(Iteration 11 / 21) loss: 2.302539\n",
      "(Iteration 16 / 21) loss: 2.257842\n",
      "(Iteration 21 / 21) loss: 2.142452\n",
      "(Epoch 1 / 1) train acc: 0.649000; val_acc: 0.619444\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=1, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=5)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data training accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Print final training accuracy\n",
    "print(\n",
    "    \"Full data training accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_train'], small_data['y_train'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data validation accuracy: 0.6194444444444445\n"
     ]
    }
   ],
   "source": [
    "# Print final validation accuracy\n",
    "print(\n",
    "    \"Full data validation accuracy:\",\n",
    "    solver.check_accuracy(data['X_val'], data['y_val'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируйте фильтры на первом слое обученной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
